{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from os import path\n",
    "from urllib.request import urlretrieve\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import unidecode\n",
    "EPSILON = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - Loading and visualizing pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large cosine\\_similarity(w_1, w_2) = \\frac{\\langle w_1, w_2 \\rangle}{||w_1|| \\cdot ||w_2||}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedEmbeddings():\n",
    "    def __init__(self, language, embeddings):\n",
    "        self.vec_file = None\n",
    "        if language == 'en':\n",
    "            if embeddings == 'glove':\n",
    "                self.vec_file = 'glove_100k.en.vec'\n",
    "            elif embeddings == 'ft':\n",
    "                self.vec_file = 'ft_300k.en.vec'\n",
    "        elif language == 'fr':\n",
    "            if embeddings == 'glove':\n",
    "                print('No GloVe french embeddings!')\n",
    "                return None\n",
    "            elif embeddings == 'ft':\n",
    "                self.vec_file = 'ft_50k.fr.vec'\n",
    "        self.language = language\n",
    "        self.url = \"https://github.com/ECE-Deep-Learning/courses_labs/releases/download/0.1/\" + self.vec_file\n",
    "        self.file_location = os.path.join('data', self.vec_file)\n",
    "        self.embeddings_index = None\n",
    "        self.embeddings_index_inversed = None\n",
    "        self.embeddings_vectors = None\n",
    "        self.voc_size = None\n",
    "        self.dim = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def _normalize(array):\n",
    "        return array / np.linalg.norm(array, axis=-1, keepdims=True)\n",
    "        \n",
    "    def download(self):\n",
    "        if not path.exists(self.file_location):\n",
    "            print('Downloading from %s to %s...' % (self.url, self.file_location))\n",
    "            urlretrieve(self.url, self.file_location)\n",
    "            print('Downloaded embeddings')        \n",
    "            \n",
    "    # Note that you can choose to normalize directly the embeddings \n",
    "    # to make the cosine similarity computation easier afterward\n",
    "    def load(self, normalize=False):\n",
    "        self.embeddings_index, self.embeddings_index_inversed = {}, {}\n",
    "        self.embeddings_vectors = []\n",
    "        file = open(self.file_location)\n",
    "        header = next(file)\n",
    "        self.voc_size, self.dim = [int(token) for token in header.split()]\n",
    "        print('Vocabulary size: {0}\\nEmbeddings dimension: {1}'.format(self.voc_size, self.dim))\n",
    "        print('Loading embeddings in memory...')\n",
    "        for idx, line in enumerate(file):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            self.embeddings_index[word] = idx\n",
    "            self.embeddings_index_inversed[idx] = word\n",
    "            self.embeddings_vectors.append(vector)\n",
    "        self.embeddings_vectors = np.asarray(self.embeddings_vectors)\n",
    "        print('Embeddings loaded')\n",
    "        if normalize:\n",
    "            self.embeddings_vectors = self._normalize(self.embeddings_vectors)\n",
    "            print('Embeddings normalized')\n",
    "        file.close()\n",
    "        \n",
    "    # Return an embedding vector associated to a given word\n",
    "    # Be sure to handle the case where the received word is not in the embeddings' vocabulary\n",
    "    def word_to_vec(self, word):\n",
    "        # TODO:\n",
    "        return None\n",
    "    \n",
    "    # Return the closer word associated to a given embedding vector\n",
    "    # In other terms, you have to compute every cosine similarities and return the most similar word\n",
    "    def vec_to_word(self, vec):\n",
    "        # TODO:\n",
    "        return None\n",
    "\n",
    "    # Return the n top similar words from a given string input\n",
    "    # The similarities are based on the cosine similarities between the embeddings vectors\n",
    "    # Note that the string could be a full sentence composed of several words\n",
    "    # Maybe you should split the sentence and average every word embedding in it\n",
    "    def most_similar(self, query, top=10):\n",
    "        # TODO:\n",
    "        return None\n",
    "    \n",
    "    def project_and_visualize(self, sample=1000):\n",
    "        embeddings_tsne = TSNE(perplexity=30).fit_transform(self.embeddings_vectors[:sample])\n",
    "        plt.figure(figsize=(40, 40))\n",
    "        axis = plt.gca()\n",
    "        np.set_printoptions(suppress=True)\n",
    "        plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], marker=\".\", s=1)\n",
    "        for idx in range(sample):\n",
    "            plt.annotate(\n",
    "                self.embeddings_index_inversed[idx],\n",
    "                xy=(embeddings_tsne[idx, 0], embeddings_tsne[idx, 1]),\n",
    "                xytext=(0, 0), textcoords='offset points'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = PretrainedEmbeddings(language='en', embeddings='glove')\n",
    "pretrained_embeddings.download()\n",
    "pretrained_embeddings.load(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings.project_and_visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings.most_similar('french city')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - Language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling random text from the model\n",
    "\n",
    "First part of language modelling will be about predicting the next character of a finite sequence of characters of size $k$.\n",
    "\n",
    "Recursively generate one character at a time:\n",
    "\n",
    "Your model outputs the probability distribution $p_{\\theta}(c_{n} | c_{n-1}, \\ldots, c_{n-k})$\n",
    "\n",
    "Using this probability distribution, a predicted character $c_{n}$ will be sampled. The temperature parameter makes it possible to remove additional entropy (bias) into the parameterized multinoulli distribution of the output of the model.\n",
    "\n",
    "Then use your prediction $c_{n}$ to compute $c_{n+1}$. Your model outputs:\n",
    "$p_{\\theta}(c_{n+1} | $<span style=\"color:red\">$c_{n}$</span>$, \\ldots, c_{n-k+1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel():\n",
    "    def __init__(self):\n",
    "        self.corpus_path = None\n",
    "        self.corpus = None\n",
    "    \n",
    "    def load_data(self, corpus_path):\n",
    "        self.corpus_path = os.path.join('data', corpus_path)\n",
    "        file = open(self.corpus_path)\n",
    "        self.corpus = unidecode.unidecode(file.read().lower().replace(\"\\n\", \" \"))\n",
    "        print('Corpus length: {0} characters'.format(len(self.corpus)))\n",
    "        file.close()\n",
    "    \n",
    "    def get_contiguous_sample(self, size):\n",
    "        index = np.random.randint(1, len(self.corpus) - size)\n",
    "        return self.corpus[index:index+size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 500\n",
    "\n",
    "language_model = LanguageModel()\n",
    "language_model.load_data('rousseau.txt')\n",
    "print('Sample of {0} characters:\\n{1}'.format(\n",
    "    sample_size, language_model.get_contiguous_sample(sample_size)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - a) Character-based language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring per-character perplexity\n",
    "\n",
    "To measure the quality of a language model we usually use the perplexity.\n",
    "(https://en.wikipedia.org/wiki/Perplexity)\n",
    "\n",
    "Here is how it is defined:\n",
    "\n",
    "$$perplexity_\\theta = 2^{-\\frac{1}{n} \\sum_{i=1}^{n} log_2 (p_\\theta(c_i)^T\\cdot y_i)}$$\n",
    "$p_\\theta(c_i)$ is your predicted column vector of probabilities over the possible next characters for the $i^{th}$ sequence.\n",
    "$y_i$ is the one-hot encoding vector of the answer: the next character of the $i^{th}$ sequence.\n",
    "\n",
    "You just compute the average negative loglikelihood like you have done previously, only using a log2 logarithm. Then just perform a base $2$ exponentiation of the quantity just computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class CharLanguageModel(LanguageModel):\n",
    "    def __init__(self):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.char_index = None\n",
    "        self.char_index_inversed = None\n",
    "        self.vocabulary_size = None\n",
    "        self.max_length_sequence = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.model = None\n",
    "    \n",
    "    def extract_vocabulary(self):\n",
    "        chars = sorted(set(self.corpus))\n",
    "        self.char_index = dict((c, i) for i, c in enumerate(chars))\n",
    "        self.char_index_inversed = dict((i, c) for i, c in enumerate(chars))\n",
    "        self.vocabulary_size = len(self.char_index)\n",
    "        print('Vocabulary size: {0}'.format(self.vocabulary_size))\n",
    "        \n",
    "    def plot_vocabulary_distribution(self):\n",
    "        counter = Counter(self.corpus)\n",
    "        chars, counts = zip(*counter.most_common())\n",
    "        indices = np.arange(len(counts))\n",
    "        plt.figure(figsize=(16, 5))\n",
    "        plt.bar(indices, counts, 0.8)\n",
    "        plt.xticks(indices, chars)\n",
    "        \n",
    "    \"\"\"\n",
    "    Convert X and y into one-hot encoded matrices\n",
    "    \n",
    "    Importante note: if the sequence length if smaller than max_length_sequence, \n",
    "    we pad the input with zeros vectors at the beginning of the one-hot encoded matrix\n",
    "    \"\"\"\n",
    "    def _one_hot_encoding(self, X, y):\n",
    "        X_one_hot = np.zeros(\n",
    "            (len(X), self.max_length_sequence, self.vocabulary_size), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        y_one_hot = np.zeros(\n",
    "            (len(X), self.vocabulary_size), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        # TODO:\n",
    "              \n",
    "        return X_one_hot, y_one_hot \n",
    "    \n",
    "    \"\"\"\n",
    "    The matrices X and y are created in this method\n",
    "    It consists of sampling sentences in the corpus as training vectors with the next character as target\n",
    "    \"\"\"\n",
    "    def build_dataset(self, \n",
    "                      max_length_sequence=40, min_length_sentence=5, max_length_sentence=200, \n",
    "                      step=3):\n",
    "        self.X, self.y = [], []\n",
    "        \n",
    "        sentences = sent_tokenize(self.corpus)\n",
    "        sentences = filter(\n",
    "            lambda x: len(x) >= min_length_sentence and len(x) <= max_length_sentence, \n",
    "            sentences\n",
    "        )\n",
    "        for sentence in sentences:\n",
    "            for i in range(0, max(len(sentence) - max_length_sequence, 1), step):\n",
    "                last_index = min(i+max_length_sequence, i+len(sentence)-1)\n",
    "                self.X.append(sentence[i:last_index])\n",
    "                self.y.append(sentence[last_index])\n",
    "\n",
    "        self.max_length_sequence = max_length_sequence\n",
    "        self.X, self.y = sklearn.utils.shuffle(self.X, self.y)\n",
    "        print('Number of training sequences: {0}'.format(len(self.X)))\n",
    "        self.X, self.y = self._one_hot_encoding(self.X, self.y)\n",
    "        print('X shape: {0}\\ny shape: {1}'.format(self.X.shape, self.y.shape))\n",
    "       \n",
    "    \"\"\"\n",
    "    Define, compile, and fit a Keras model on (self.X, self.y)\n",
    "    It should be composed of :\n",
    "        - one recurrent LSTM layer projecting into hidden_size dimensions\n",
    "        - one Dense layer with a softmax activation projecting into vocabulary_size dimensions\n",
    "    \"\"\"\n",
    "    def train(self, hidden_size=128, batch_size=128, epochs=10):\n",
    "        self.model = None\n",
    "        # TODO:\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the prediction of our model, meaning the next token given an input sequence\n",
    "    \n",
    "    If preprocessed is specified as True, we consider X as an array of strings and we will transform\n",
    "    it to a one-hot encoded matrix\n",
    "    Importante note: if the sequence length if smaller than max_length_sequence, \n",
    "    we pad the input with zeros vectors at the beginning of the one-hot encoded matrix\n",
    "    \n",
    "    If preprocessed is specified as False, we apply the model predict on X as it is\n",
    "    \"\"\"\n",
    "    def predict(self, X, verbose=1, preprocessed=True):\n",
    "        if not preprocessed:\n",
    "            X_one_hot = np.zeros(\n",
    "                (len(X), self.max_length_sequence, self.vocabulary_size), dtype=np.float32\n",
    "            )\n",
    "            # TODO:\n",
    "        else:\n",
    "            X_one_hot = X\n",
    "        return self.model.predict(X_one_hot, verbose=verbose)\n",
    "    \n",
    "    # Perplexity metric used to appreciate the performance of our model\n",
    "    def perplexity(self, y_true, y_pred):\n",
    "        likelihoods = np.sum(y_pred * y_true, axis=1)\n",
    "        return 2 ** -np.mean(np.log2(likelihoods + EPSILON))\n",
    "    \n",
    "    \"\"\"\n",
    "    Sample the next character according to the predictions.\n",
    "    \n",
    "    Use a lower temperature to force the model to output more\n",
    "    confident predictions: more peaky distribution.\n",
    "    \"\"\"\n",
    "    def _sample_next_char(self, preds, temperature=1.0):\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds + EPSILON) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds + EPSILON)\n",
    "        probs = np.random.multinomial(1, preds, size=1)\n",
    "        return np.argmax(probs)\n",
    "    \n",
    "    def generate_text(self, seed_string, length=300, temperature=1.0):\n",
    "        if self.model is None:\n",
    "            print('The language model has not been trained yet!')\n",
    "            return None\n",
    "        generated = seed_string\n",
    "        prefix = seed_string\n",
    "        for i in range(length):\n",
    "            predictions = np.ravel(self.predict([prefix], verbose=0, preprocessed=False))\n",
    "            next_index = self._sample_next_char(predictions, temperature)\n",
    "            next_char = self.char_index_inversed[next_index]\n",
    "            generated += next_char\n",
    "            prefix = prefix[1:] + next_char\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = CharLanguageModel()\n",
    "language_model.load_data('rousseau.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.extract_vocabulary()\n",
    "language_model.char_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.plot_vocabulary_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "language_model.train(epochs=epochs)\n",
    "if language_model.model is not None:\n",
    "    print('Perplexity after {0} epochs: {1}'.format(\n",
    "        epochs, language_model.perplexity(language_model.y, language_model.model.predict(language_model.X))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.generate_text(\"l'etat n'est pas au-dessus de la loi\", temperature=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.generate_text(\"la republique\", temperature=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - b) Word-based language modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr import French\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class WordLanguageModel(LanguageModel):\n",
    "    def __init__(self):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.pretrained_embeddings = PretrainedEmbeddings(language='fr', embeddings='ft')\n",
    "        self.pretrained_embeddings.download()\n",
    "        self.pretrained_embeddings.load()\n",
    "        self.parser = None\n",
    "        self.word_index = None\n",
    "        self.word_index_inversed = None\n",
    "        self.vocabulary_size = None\n",
    "        self.max_length_sequence = None\n",
    "        self.tokens = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.model = None\n",
    "        \n",
    "    def extract_vocabulary(self, max_vocabulary=1500000):\n",
    "        self.parser = French(max_length=max_vocabulary)\n",
    "        self.tokens = [token.orth_ for token in self.parser(self.corpus) if token.is_alpha]\n",
    "        unique_tokens = set(self.tokens)\n",
    "        self.word_index = dict((w, i) for i, w in enumerate(unique_tokens))\n",
    "        self.word_index_inversed = dict((i, w) for i, w in enumerate(unique_tokens))\n",
    "        self.vocabulary_size = len(self.word_index)\n",
    "        print('Vocabulary size: {0}'.format(self.vocabulary_size))\n",
    "        \n",
    "    \"\"\"\n",
    "    Convert X and y into embedded matrices\n",
    "    Hint: use the self.pretrained_embeddings.word_to_vec method for each token found\n",
    "    \n",
    "    Importante note: if the sequence length if smaller than max_length_sequence, \n",
    "    we pad the input with zeros vectors at the beginning of the embedded matrix\n",
    "    \"\"\"\n",
    "    def _token_embedding(self, X, y):\n",
    "        X_embedding = np.zeros(\n",
    "            (len(X), self.max_length_sequence, self.pretrained_embeddings.dim), \n",
    "            dtype=np.float32\n",
    "        )       \n",
    "        y_one_hot = np.zeros(\n",
    "            (len(X), self.vocabulary_size), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        # TODO:\n",
    "        \n",
    "        return X_embedding, y_one_hot\n",
    "        \n",
    "    def build_dataset(self, max_length_sequence=40, step=3):\n",
    "        self.X, self.y = [], []\n",
    "        for i in range(0, len(self.tokens) - max_length_sequence, step):\n",
    "            self.X.append(self.tokens[i:i+max_length_sequence])\n",
    "            self.y.append(self.tokens[i+max_length_sequence])\n",
    "        self.max_length_sequence = max_length_sequence\n",
    "        self.X, self.y = sklearn.utils.shuffle(self.X, self.y)\n",
    "        print('Number of training sequences: {0}'.format(len(self.X)))\n",
    "        self.X, self.y = self._token_embedding(self.X, self.y)\n",
    "        print('X shape: {0}\\ny shape: {1}'.format(self.X.shape, self.y.shape))\n",
    "        \n",
    "    \"\"\"\n",
    "    Define, compile, and fit a Keras model on (self.X, self.y)\n",
    "    It should be composed of :\n",
    "        - one or many recurrent LSTM layers projecting into hidden_size dimensions\n",
    "        - one Dense layer with a relu activation projecting into hidden_size dimensions\n",
    "        - one Dense layer with a softmax activation projecting into vocabulary_size dimensions\n",
    "    \"\"\"\n",
    "    def train(self, hidden_size=128, batch_size=128, epochs=10):\n",
    "        self.model = None\n",
    "        # TODO:\n",
    "        \n",
    "    \"\"\"\n",
    "    Return the prediction of our model, meaning the next token given an input sequence\n",
    "    \n",
    "    If preprocessed is specified as True, we consider X as an array of strings and we will transform\n",
    "    it to an embedded matrix\n",
    "    Importante note: if the sequence length if smaller than max_length_sequence, \n",
    "    we pad the input with zeros vectors at the beginning of the embedded matrix\n",
    "    \n",
    "    If preprocessed is specified as False, we apply the model predict on X as it is\n",
    "    \"\"\"\n",
    "    def predict(self, X, verbose=1, preprocessed=True):\n",
    "        if not preprocessed:\n",
    "            X_embedding = np.zeros(\n",
    "                (len(X), self.max_length_sequence, self.pretrained_embeddings.dim), \n",
    "                dtype=np.float32\n",
    "            )\n",
    "            # TODO:\n",
    "        else:\n",
    "            X_embedding = X\n",
    "        return self.model.predict(X_embedding, verbose=verbose)\n",
    "    \n",
    "    \"\"\"\n",
    "    Sample the next word according to the predictions.\n",
    "    \n",
    "    Use a lower temperature to force the model to output more\n",
    "    confident predictions: more peaky distribution.\n",
    "    \"\"\"\n",
    "    def _sample_next_word(self, preds, temperature=1.0):\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds + EPSILON) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds + EPSILON)\n",
    "        probs = np.random.multinomial(1, preds, size=1)\n",
    "        return np.argmax(probs)\n",
    "    \n",
    "    def generate_text(self, seed_string, length=50, temperature=1.0):\n",
    "        if self.model is None:\n",
    "            print('The language model has not been trained yet!')\n",
    "            return None\n",
    "        seed_tokens = [token.orth_ for token in self.parser(seed_string) if token.is_alpha]\n",
    "        prefix = seed_tokens\n",
    "        generated = seed_tokens\n",
    "        for i in range(length):\n",
    "            predictions = np.ravel(self.predict([prefix], verbose=0, preprocessed=False))\n",
    "            next_index = self._sample_next_word(predictions)\n",
    "            next_word = self.word_index_inversed[next_index]\n",
    "            generated += [next_word]\n",
    "            prefix = prefix[1:] + [next_word]\n",
    "        return \" \".join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = WordLanguageModel()\n",
    "language_model.load_data('rousseau.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.extract_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "language_model.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.generate_text(\"un état ne saurait réussir à\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.generate_text(\"la république ne doit pas\", temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - Supervised text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this third part, we will:\n",
    "- Train a simple network to learn embeddings on a classification task\n",
    "- Use pre-trained embeddings like GloVe or FastText and see the difference\n",
    "- Train a recurrent neural network to handle the text structure\n",
    "\n",
    "However keep in mind:\n",
    "- We are here to learn deep learning methods for NLP tasks, but simple sparse TF-IDF bigrams features without any embedding or Logistic Regression are often competitive in small to medium datasets for text classification.\n",
    "\n",
    "#### 20 Newsgroups Dataset\n",
    "\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - a) Load, handle, and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sklearn object type : {}\".format(type(newsgroups_train)))\n",
    "print(\"sklearn object keys :\")\n",
    "for k in newsgroups_train:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classes to predict : {}\".format(os.linesep.join(newsgroups_train['target_names'])))\n",
    "print()\n",
    "print(\"Integer mapped-classes to predict :\")\n",
    "print(newsgroups_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_int_str = dict(\n",
    "    zip(range(len(newsgroups_train['target_names'])), newsgroups_train['target_names'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_int_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of document in dataset:\", os.linesep)\n",
    "sample_idx = np.random.randint(len(newsgroups_train[\"data\"]))\n",
    "print(newsgroups_train[\"data\"][sample_idx])\n",
    "sample_idx_class = class_int_str[newsgroups_train[\"target\"][sample_idx]]\n",
    "print(\"Example class to predict : {}\".format(sample_idx_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing text for the (supervised) CBOW model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a simple classification model in Keras. \n",
    "Also we will have to perform preprocessing on raw text.\n",
    "\n",
    "The following cells use Keras to preprocess text.\n",
    "\n",
    "- Use a tokenizer: https://keras.io/preprocessing/text/#tokenizer\n",
    "   - This converts the texts into sequences of integers representing the MAX_NB_WORDS most frequent words\n",
    "   \n",
    "- The following methods from the Tokenizer object should be useful:\n",
    "   - tokenizer.fit_on_texts(corpus)\n",
    "   - tokenizer.texts_to_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000  # number of different integers mapping our vocabulary\n",
    "\n",
    "# get the raw text data\n",
    "texts_train = newsgroups_train[\"data\"]\n",
    "texts_test = newsgroups_test[\"data\"]\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "# TODO:\n",
    "tokenizer = None\n",
    "sequences_train = None\n",
    "sequences_test = None\n",
    "\n",
    "if tokenizer is not None:\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each text has been converted to a list of token_ids\n",
    "- Each token_id represents $1$ of MAX_NB_WORDS most frequent words in train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First raw text example: \", os.linesep, texts_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sequences_train is not None:\n",
    "    print(\"First text conversion to token_ids: \", os.linesep, sequences_train[0])\n",
    "    print(\"First text number of token_ids: {}\".format(len(sequences_train[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer object stores a mapping (vocabulary) from word strings to token ids that can be inverted to reconstruct the original message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer is not None:\n",
    "    word_to_index = tokenizer.word_index.items()\n",
    "    index_to_word = dict((i, w) for w, i in word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sequences_train is not None:\n",
    "    print(\"Original sentence retrieved :\", os.linesep)\n",
    "    print(\" \".join([index_to_word[i] for i in sequences_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's truncate and pad all the sequences to $1000$ symbols to build the training set.\n",
    "\n",
    "Use a padding function: https://keras.io/preprocessing/sequence/#pad_sequences\n",
    "   - Function actually also truncates sequences longer than max length\n",
    "   - Default mode is to remove first elems of sequences longer than max length or pad with $0$s the beginning of sequences shorter than max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "# pad sequences with 0s\n",
    "# use the pad_sequences method on your sequences\n",
    "# TODO:\n",
    "x_train = None\n",
    "x_test = None\n",
    "if x_train is not None and x_test is not None:\n",
    "    print('Shape of data tensor:', x_train.shape)\n",
    "    print('Shape of data test tensor:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x_train is not None:\n",
    "    print(\"Example of tensor after padding/truncating : \", os.linesep, x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = newsgroups_train[\"target\"]\n",
    "y_test = newsgroups_test[\"target\"]\n",
    "\n",
    "# One-hot encode integer-mapped classes\n",
    "y_train_onehot = to_categorical(np.asarray(y_train))\n",
    "print('Shape of train target tensor:', y_train_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - b) Simple classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following computes a very simple model:\n",
    "\n",
    "<img src=\"../images/supervised_text_classification.png\" style=\"width: 600px;\" />\n",
    "\n",
    "Use either Sequential or Functional Keras API:\n",
    "\n",
    "- Embedding() Layer: build an embedding layer mapping each word to a vector representation\n",
    "- GlobalAveragePooling1D() Layer: average the vector representation of all words in each sequence\n",
    "- Dense Layer(): end with a dense layer with softmax to output 20 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "N_CLASSES = y_train_onehot.shape[1]\n",
    "\n",
    "model = None\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() if model is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and x_train is not None:\n",
    "    model.fit(x_train, y_train_onehot, validation_split=0.1,\n",
    "              epochs=150, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and x_test is not None:\n",
    "    print(\"test accuracy:\", np.mean(model.predict(x_test).argmax(axis=-1) == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - c) Simple classification model with pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get an input tensor and replace the word->integer mapping with pretrained embeddings\n",
    "Be sure that the word is existing (not a 0 padding) and is in the embeddings' vocabulary\n",
    "\"\"\"\n",
    "\n",
    "def preprocess_with_pretrained_embeddings(X, language, embeddings):\n",
    "    pretrained_embeddings = PretrainedEmbeddings(language=language, embeddings=embeddings)\n",
    "    pretrained_embeddings.download()\n",
    "    pretrained_embeddings.load()\n",
    "    X_embedding = np.zeros((X.shape[0], X.shape[1], pretrained_embeddings.dim))\n",
    "    # TODO:\n",
    "    return X_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x_train is not None and x_test is not None:\n",
    "    x_train_embedding = preprocess_with_pretrained_embeddings(x_train, language='en', embeddings='glove')\n",
    "    x_test_embedding = preprocess_with_pretrained_embeddings(x_test, language='en', embeddings='glove')\n",
    "    print('Embedded training matrix shape:', x_train_embedding.shape)\n",
    "    print('Embedded test matrix shape:', x_test_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use either Sequential or Functional Keras API:\n",
    "\n",
    "- GlobalAveragePooling1D() Layer: average the vector representation of all words in each sequence\n",
    "- Dense Layer(): end with a dense layer with softmax to output 20 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "N_CLASSES = y_train_onehot.shape[1]\n",
    "\n",
    "model = None\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() if model is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and x_train_embedding is not None:\n",
    "    model.fit(x_train_embedding, y_train_onehot, validation_split=0.1, \n",
    "              epochs=200, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and x_test_embedding is not None:\n",
    "    print(\"test accuracy:\", np.mean(model.predict(x_test_embedding).argmax(axis=-1) == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - d) Recurrent classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to enrich the previous model with recurrent LSTM layers:\n",
    "\n",
    "<img src=\"../images/rnn.png\" style=\"width: 600px;\" />\n",
    "\n",
    "Use either Sequential or Functional Keras API:\n",
    "\n",
    "- Embedding() Layer: build an embedding layer mapping each word to a vector representation\n",
    "- MaxPooling1D() Layer: add a MaxPooling1D layer in your sequence to reduce the dimension\n",
    "- LSTM() Layer: add a LSTM layer to extract information from the reduced sequence\n",
    "- Dense Layer(): end with a dense layer with softmax to output 20 classes\n",
    "\n",
    "N.B. you can either use the pretrained embeddings or recompute them with a Embedding layer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, MaxPooling1D, LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "N_CLASSES = y_train_onehot.shape[1]\n",
    "pooling_size = 5\n",
    "hidden_size = 64\n",
    "\n",
    "model = None\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() if model is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and x_train is not None:\n",
    "    model.fit(x_train, y_train_onehot, validation_split=0.1, \n",
    "              epochs=25, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and x_test is not None:\n",
    "    print(\"test accuracy:\", np.mean(model.predict(x_test).argmax(axis=-1) == y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
