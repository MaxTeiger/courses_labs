{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an introduction to basic sequence-to-sequence learning using a Long short term memory (LSTM) module.\n",
    "\n",
    "Given a string of characters representing a math problem \"3141+42\" we would like to generate a string of characters representing the correct solution: \"3183\". Our network will learn how to do basic mathematical operations.\n",
    "\n",
    "The important part is that we will not first use our human intelligence to break the string up into integers and a mathematical operator. We want the computer to figure all that out by itself.\n",
    "\n",
    "Each math problem is an input sequence: a list of {0,...,9} integers and math operation symbols\n",
    "The result of the operation (\"$3141+42$\" $\\rightarrow$ \"$3183$\"</span>) is the sequence to decode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**math_operators** is the set of $5$ operations we are going to use to build are input sequences.<br/>\n",
    "The math_expressions_generation function uses them to generate a large set of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_expressions_generation(n_samples=1000, n_digits=3, invert=True):\n",
    "    X, Y = [], []\n",
    "    math_operators = {\n",
    "        '+': operator.add, \n",
    "        '-': operator.sub,\n",
    "        '*': operator.mul,\n",
    "        '/': operator.truediv,\n",
    "        '%': operator.mod\n",
    "    }\n",
    "    for i in range(n_samples):\n",
    "        a, b = np.random.randint(1, 10**n_digits, size=2)\n",
    "        op = np.random.choice(list(math_operators.keys()))\n",
    "        res = math_operators[op](a, b)\n",
    "        x = \"\".join([str(elem) for elem in (a, op, b)])\n",
    "        if invert is True:\n",
    "            x = x[::-1]\n",
    "        y = \"{:.5f}\".format(res) if isinstance(res, float) else str(res)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483*920 = 444360\n",
      "671/833 = 0.80552\n",
      "489-55 = 434\n",
      "653%684 = 653\n",
      "72%231 = 72\n",
      "78-866 = -788\n",
      "248%221 = 27\n",
      "788-273 = 515\n",
      "287%598 = 287\n",
      "477*786 = 374922\n",
      "380*163 = 61940\n",
      "245+975 = 1220\n",
      "884*924 = 816816\n",
      "310-765 = -455\n",
      "823-694 = 129\n",
      "57%587 = 57\n",
      "335*734 = 245890\n",
      "517%588 = 517\n",
      "732-581 = 151\n",
      "934-289 = 645\n"
     ]
    }
   ],
   "source": [
    "X, y = math_expressions_generation(n_samples=int(1e5), n_digits=3, invert=True)\n",
    "for X_i, y_i in list(zip(X, y))[:20]:\n",
    "    print(X_i[::-1], '=', y_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Sequence to sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Seq2Seq architecture\n",
    "Two LSTMs: an encoder and a decoder\n",
    "\n",
    "<img src=\"../images/teacher_forcing_train.png\" style=\"width: 600px;\" />\n",
    "\n",
    "## Training with teacher forcing\n",
    "   - Build the Seq2Seq model for training\n",
    "   - Example:\n",
    "        - Input sequence \"94+8\" must is given to the encoding LSTM\n",
    "        - True previous answers \"1\", \"0\", \"2\" are given to the decoder LSTM\n",
    "            - Helps the decoder predict well the next token during training\n",
    "   - We advice to use Keras functional API here\n",
    "\n",
    "### encoder\n",
    "   - Define a layer **encoder_inputs** of shape $(None, $**self.encoder_vocabulary_size**$)$ \n",
    "   - Define the encoder LSTM layer before connecting it\n",
    "        - Call it **encoder_lstm**\n",
    "        - Use **return_state** param so it returns its last **state_h** and **state_c**\n",
    "           - Have to be passed to the decoder LSTM afterwards to connect the $2$ LSTMs\n",
    "   - Connect **encoder_lstm** to **encoder_inputs**\n",
    "       - Get **encoder_lstm**'s last **state_h** and **state_c** node variables\n",
    "           - Stack them in a **encoder_states** $=$ $[$**state_h**$,  $**state_c**$]$ variable\n",
    "      \n",
    "### decoder\n",
    "   - Define a layer **decoder_inputs** of shape $(None, $**self.decoder_vocabulary_size**$)$ \n",
    "   - Define the decoder LSTM layer before connecting it\n",
    "        - Call it **decoder_lstm**\n",
    "        - Pass encoder's last $[$**state_h**$, $**state_c**$]$ to decoder **initial_state** argument to connect the two LSTM\n",
    "        - Use the **return_sequences** param so the decoder returns all the $h_{t}^{dec}$\n",
    "            - We need them to compute the predictions using the $h_{t}^{dec}$\n",
    "        - Use the **return_state** param so the decoder also returns its last **state_h** and **state_c**\n",
    "            - We ignore those now but we will need them for inference\n",
    "   - Connect **decoder_lstm** to **decoder_inputs**\n",
    "       - Get the $h_{t}^{dec}$ hidden layers in a **decoder_all_hdec** node variable\n",
    "       - Ignore **decoder_lstm**'s last **state_h** and **state_c** returned\n",
    "\n",
    "### output\n",
    "   - At this point, all the $h_{t}^{dec}$ are in a **decoder_all_hdec** node\n",
    "      - Ready to be used to perform a token prediction for all timesteps\n",
    "   - Define a Dense layer. Call it **decoder_dense**\n",
    "       - Give it a softmax activation and **self.decoder_vocabulary_size** output dimensionality\n",
    "   - Connect **decoder_dense** to **decoder_all_hdec**\n",
    "     - Get the $\\hat{y}^t$ predictions in a **decoder_outputs** node variable\n",
    "     - Each $h_{t}^{dec}$ has been mapped to a $($**self.decoder_vocabulary_size**$,1)$ probability distribution over the next token\n",
    "   - **decoder_outputs** should be of shape $(batch,$ **self.max_decoder_sequence_length**$,$ **self.decoder_vocabulary_size**$)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference (testing time - no teacher forcing)\n",
    "\n",
    "   - We are going to see how to perform inference\n",
    "       - Decoding a new sequence with trained weights without using teacher forcing\n",
    "   - We won't provide the <...EOS> part of the sequences like during training\n",
    "   - Predictions have to be performed one step at a time\n",
    "   - At first we will use encoder's last state and GO token\n",
    "       - Produces the $1{st}$ decoder hidden layer  $h_{0}^{dec}$\n",
    "   - Secondly we will use $h_{0}^{dec}$ to predict $\\hat{y}^0$ token\n",
    "   - Thirdly we will use $h_{0}^{dec}$ and $\\hat{y}^0$ token\n",
    "       - Produces $h_{1}^{dec}$ then used to predict $\\hat{y}^1$ token\n",
    "   - etc.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "   - To perform inference we are going to need an **inference encoder model**\n",
    "       - Takes in the input sequence and returns the last $h$ and $c$ state\n",
    "           - To be passed to the decoder\n",
    "   - To perform inference we also are going to need a **inference decoder model**\n",
    "       - Takes in the previous hidden state $h_{t-1}^{dec}$\n",
    "       - Takes in a token: GO or previous $\\hat{y}^{t-1}$\n",
    "       - Returns next $h_{t}^{dec}$ and $\\hat{y}^{t}$\n",
    "       - Iterates over these steps\n",
    "           - Until it produces the EOS token or decoded sequence is too long\n",
    "\n",
    "We are going to reuse layers and nodes from before:\n",
    "   - **encoder_inputs**, **encoder_states** and **decoder_all_hdec** nodes that are already connected\n",
    "   - **decoder_lstm**, **decoder_inputs** and **decoder_dense** layers\n",
    "\n",
    "### inference_encoder_model\n",
    "   - Use the class Model from keras.models\n",
    "   - Make the node **encoder_inputs** the model's input\n",
    "   - Make the node **encoder_states** the model's output\n",
    "\n",
    "### inference_decoder_model\n",
    "   - Define $2$ $Input$ keras.layers of dimensionality **latent_dim**\n",
    "       - **decoder_state_input_h** and **decoder_state_input_c**\n",
    "          - **decoder_model**'s last state\n",
    "          - To be given later to **inference_decoder_model**'s predict function\n",
    "          - Stack them in a **decoder_states_inputs** variable\n",
    "              - **decoder_states_inputs**$ = [$**decoder_state_input_h**$, $**decoder_state_input_c**$]$\n",
    "              \n",
    "   - Connect **decoder_lstm** to **decoder_inputs**\n",
    "       - While connecting use the argument **initial_state**$ = $**decoder_states_inputs**\n",
    "       - Get **decoder_all_hdec**, **decoder_state_h**, **decoder_state_c** from that connection\n",
    "          - **decoder_all_hdec** is all the $h_{t}^{dec}$ produced\n",
    "             - $1$ token at a time is given, thus **decoder_all_hdec** shape is $(1,$**latent_dim**$)$\n",
    "          - **decoder_state_h** is the last $h_{t}^{dec}$\n",
    "              - First part of **decoder_lstm**'s last state\n",
    "              - Will be **decoder_state_input_h** at next iteration\n",
    "          - **decoder_state_c**, is the last $c_{t}^{dec}$\n",
    "              - Second part of **decoder_lstm**'s last state\n",
    "              - Will be **decoder_state_input_c** at next iteration\n",
    "          - Stack **decoder_state_h** and **decoder_state_c** in a **decoder_states** variable\n",
    "              - **decoder_states**$ = [$**decoder_state_h**$, $**decoder_state_c**$]$\n",
    "          - **decoder_state_h** and **decoder_state_c** will be returned along with prediction $\\hat{y}^{t}$\n",
    "          \n",
    "   - Connect **decoder_dense** layer from before to **decoder_all_hdec**\n",
    "       - Produces the distribution probability $\\hat{y}^t$ over the next token\n",
    "       - Get the $\\hat{y}^t$ prediction in a **decoder_outputs** node variable\n",
    "       \n",
    "   - At this point we have\n",
    "       - Next prediction **decoder_outputs**\n",
    "       - Last state **decoder_states**\n",
    "       \n",
    "   - We are ready to define the decoder_model\n",
    "       - Make $[$**decoder_inputs**$] + $**decoder_states_inputs** the model's inputs\n",
    "       - Make $[$**decoder_outputs**$] + $**decoder_states** the model's outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GO** is the character (\"=\") that marks the beginning of decoding for the decoder LSTM<br/>\n",
    "**EOS** is the character (\"\\n\") that marks the end of sequence to decode for the decoder LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Seq2seq():\n",
    "    def __init__(self, X, y):\n",
    "        # Special tokens\n",
    "        self.GO = '='\n",
    "        self.EOS = '\\n'\n",
    "        # Dataset properties\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_tr = None\n",
    "        self.X_val = None\n",
    "        self.y_tr = None\n",
    "        self.y_val = None\n",
    "        self.n = None\n",
    "        self.encoder_char_index = None\n",
    "        self.encoder_char_index_inversed = None\n",
    "        self.decoder_char_index = None\n",
    "        self.decoder_char_index_inversed = None\n",
    "        self.encoder_vocabulary_size = None\n",
    "        self.decoder_vocabulary_size = None\n",
    "        self.max_encoder_sequence_length = None\n",
    "        self.max_decoder_sequence_length = None\n",
    "        # Preprocessed data\n",
    "        self.encoder_input_data_tr = None\n",
    "        self.encoder_input_data_val = None\n",
    "        self.decoder_input_data_tr = None\n",
    "        self.decoder_input_data_val = None\n",
    "        self.decoder_target_data_tr = None\n",
    "        self.decoder_target_data_val = None\n",
    "        # Model properties\n",
    "        self.training_model = None\n",
    "        self.inference_encoder_model = None\n",
    "        self.inference_decoder_model = None\n",
    "        self.batch_size = None\n",
    "        self.epochs = None\n",
    "        self.latent_dim = None\n",
    "        # Model layers and states that we want to keep in memory between training and inference\n",
    "        ## Encoder\n",
    "        self.encoder_inputs = None\n",
    "        self.encoder_states = None\n",
    "        ## Decoder\n",
    "        self.decoder_inputs = None\n",
    "        self.decoder_lstm = None\n",
    "        self.decoder_all_hdec = None\n",
    "        self.decoder_dense = None\n",
    "        # Dataset construction call\n",
    "        self.load_and_preprocess_data(X, y)\n",
    "        self.construct_dataset()\n",
    "        \n",
    "    def load_and_preprocess_data(self, X, y):\n",
    "        self.X = list(X)\n",
    "        self.y = list(map(lambda token: self.GO + token + self.EOS, y))\n",
    "        self.n = len(self.X)\n",
    "        encoder_characters = sorted(list(set(\"\".join(self.X))))\n",
    "        decoder_characters = sorted(list(set(\"\".join(self.y))))\n",
    "        self.encoder_char_index = dict((c, i) for i, c in enumerate(encoder_characters))\n",
    "        self.encoder_char_index_inversed = dict((i, c) for i, c in enumerate(encoder_characters))\n",
    "        self.decoder_char_index = dict((c, i) for i, c in enumerate(decoder_characters))\n",
    "        self.decoder_char_index_inversed = dict((i, c) for i, c in enumerate(decoder_characters))\n",
    "        self.encoder_vocabulary_size = len(self.encoder_char_index)\n",
    "        self.decoder_vocabulary_size = len(self.decoder_char_index)\n",
    "        self.max_encoder_sequence_length = max([len(sequence) for sequence in self.X])\n",
    "        self.max_decoder_sequence_length = max([len(sequence) for sequence in self.y])\n",
    "        print('Number of samples:', self.n)\n",
    "        print('Number of unique encoder tokens:', self.encoder_vocabulary_size)\n",
    "        print('Number of unique decoder tokens:', self.decoder_vocabulary_size)\n",
    "        print('Max sequence length for encoding:', self.max_encoder_sequence_length)\n",
    "        print('Max sequence length for decoding:', self.max_decoder_sequence_length)\n",
    "        (self.X_tr, self.X_val, \n",
    "         self.y_tr, self.y_val) = train_test_split(\n",
    "            self.X, \n",
    "            self.y,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "    def construct_dataset(self):\n",
    "        encoder_input_data = np.zeros(\n",
    "            (self.n, self.max_encoder_sequence_length, self.encoder_vocabulary_size),\n",
    "            dtype='float32')\n",
    "        decoder_input_data = np.zeros(\n",
    "            (self.n, self.decoder_vocabulary_size, self.decoder_vocabulary_size),\n",
    "            dtype='float32')\n",
    "        decoder_target_data = np.zeros(\n",
    "            (self.n, self.decoder_vocabulary_size, self.decoder_vocabulary_size),\n",
    "            dtype='float32')\n",
    "        for i, (X_i, y_i) in enumerate(zip(self.X, self.y)):\n",
    "            for t, char in enumerate(X_i):\n",
    "                encoder_input_data[i, t, self.encoder_char_index[char]] = 1.\n",
    "            for t, char in enumerate(y_i):\n",
    "                decoder_input_data[i, t, self.decoder_char_index[char]] = 1.\n",
    "                if t > 0:\n",
    "                    decoder_target_data[i, t - 1, self.decoder_char_index[char]] = 1.\n",
    "        (self.encoder_input_data_tr, self.encoder_input_data_val, \n",
    "         self.decoder_input_data_tr, self.decoder_input_data_val,\n",
    "         self.decoder_target_data_tr, self.decoder_target_data_val) = train_test_split(\n",
    "            encoder_input_data, \n",
    "            decoder_input_data, \n",
    "            decoder_target_data,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    \"\"\"\n",
    "    ENCODER LAYERS:\n",
    "        - define a Input Keras object in self.encoder_inputs\n",
    "        - apply a LSTM layer on self.encoder_inputs to get the last state_h and state_c\n",
    "        - stack those states into an array self.encoder_states\n",
    "    DECODER LAYERS:\n",
    "        - define an Input Keras object in self.decoder_inputs\n",
    "        - define a LSTM layer in self.decoder_lstm, make sure you set return_sequences=True\n",
    "        to be able to return all hidden states\n",
    "        - apply this LSTM layer on self.decoder_inputs with the states initialized with self.encoder_states\n",
    "        and output all the hidden states in self.decoder_all_hdec\n",
    "        - define a Dense layer in self.decoder_dense with a softmax activation, and output the results \n",
    "        in decoder_outputs using self.decoder_all_hdec as inputs\n",
    "    MODEL DEFINITION:\n",
    "        - now you can build your global Model:\n",
    "        Model([self.encoder_inputs, self.decoder_inputs], decoder_outputs)\n",
    "    \"\"\"\n",
    "    def design_and_compile_training_model(self, batch_size=64, latent_dim=256):\n",
    "        # Hyperparameters\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        # Encoder layers\n",
    "        self.encoder_inputs = Input(shape=(None, self.encoder_vocabulary_size))\n",
    "        encoder_lstm = LSTM(self.latent_dim, return_state=True)\n",
    "        _, state_h, state_c = encoder_lstm(self.encoder_inputs)\n",
    "        self.encoder_states = [state_h, state_c]\n",
    "        # Decoder layers\n",
    "        self.decoder_inputs = Input(shape=(None, self.decoder_vocabulary_size))\n",
    "        self.decoder_lstm = LSTM(self.latent_dim, return_state=True, return_sequences=True)\n",
    "        self.decoder_all_hdec, _, _ = self.decoder_lstm(self.decoder_inputs, initial_state=self.encoder_states)\n",
    "        self.decoder_dense = Dense(self.decoder_vocabulary_size, activation='softmax')\n",
    "        decoder_outputs = self.decoder_dense(self.decoder_all_hdec)\n",
    "        # Model definition and compilation\n",
    "        self.training_model = Model([self.encoder_inputs, self.decoder_inputs], decoder_outputs)\n",
    "        self.training_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "        self.training_model.summary()\n",
    "        \n",
    "    def train(self, epochs=15):\n",
    "        # Hyperparameters\n",
    "        self.epochs = epochs\n",
    "        # Model actual training\n",
    "        self.training_model.fit(\n",
    "            [self.encoder_input_data_tr, self.decoder_input_data_tr], self.decoder_target_data_tr,\n",
    "            batch_size=self.batch_size,\n",
    "            epochs=self.epochs,\n",
    "            validation_data=(\n",
    "                [self.encoder_input_data_val, self.decoder_input_data_val], self.decoder_target_data_val\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    \"\"\"\n",
    "    ENCODER MODEL:\n",
    "        - create a Keras Model self.inference_encoder_model \n",
    "        with self.encoder_inputs as inputs and self.encoder_states as output\n",
    "    DECODER MODEL:\n",
    "        - define two Input Keras objects: one for the h_state and the other for the c_state, then stack\n",
    "        them into a decoder_states_inputs array\n",
    "        - reuse the already trained self.decoder_lstm layer with self.decoder_inputs as input\n",
    "        and decoder_states_inputs as initial_state\n",
    "            - you should get three outputs: decoder_all_hdec, decoder_state_h and decoder_state_c\n",
    "        - again, stack the outputed decoder_state_h and decoder_state_c into a decoder_states list\n",
    "        - now reuse the already trained self.decoder_dense layer with decoder_all_hdec as input,\n",
    "        and store the output into decoder_outputs\n",
    "        - you can finally create a Keras Model self.inference_decoder_model\n",
    "        with [self.decoder_inputs] + decoder_states_inputs as inputs \n",
    "        and [decoder_outputs] + decoder_states as output\n",
    "    \"\"\"\n",
    "    def design_inference_model(self):\n",
    "        if self.training_model is None:\n",
    "            print(\"No training model has been defined yet!\")\n",
    "            return None\n",
    "        # Encoder model\n",
    "        self.inference_encoder_model = Model(self.encoder_inputs, self.encoder_states)\n",
    "        # Decoder model\n",
    "        ## Inputs: latent variables from the encoder\n",
    "        decoder_state_input_h = Input(shape=(self.latent_dim,))\n",
    "        decoder_state_input_c = Input(shape=(self.latent_dim,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        ## Decoding using the LSTM trained layer from the decoder\n",
    "        decoder_all_hdec, decoder_state_h, decoder_state_c = self.decoder_lstm(\n",
    "            self.decoder_inputs, initial_state=decoder_states_inputs\n",
    "        )\n",
    "        decoder_states = [decoder_state_h, decoder_state_c]\n",
    "        ## Get outputs using the Dense trained layer from the decoder\n",
    "        decoder_outputs = self.decoder_dense(decoder_all_hdec)\n",
    "        ## Define the whole decoding model\n",
    "        self.inference_decoder_model = Model(\n",
    "            [self.decoder_inputs] + decoder_states_inputs,\n",
    "            [decoder_outputs] + decoder_states)\n",
    "        \n",
    "    def decode_sequence(self, input_sequence):\n",
    "        states_value = self.inference_encoder_model.predict(input_sequence)\n",
    "        target_sequence = np.zeros((1, 1, self.decoder_vocabulary_size))\n",
    "        target_sequence[0, 0, self.decoder_char_index[self.GO]] = 1.\n",
    "        decoded_sentence = ''\n",
    "        while len(decoded_sentence) <= self.max_decoder_sequence_length:\n",
    "            output_tokens, h, c = self.inference_decoder_model.predict(\n",
    "                [target_sequence] + states_value\n",
    "            )\n",
    "            states_value = [h, c]\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_char = self.decoder_char_index_inversed[sampled_token_index]\n",
    "            decoded_sentence += sampled_char\n",
    "            if sampled_char == self.EOS:\n",
    "                break\n",
    "            target_sequence = np.zeros((1, 1, self.decoder_vocabulary_size))\n",
    "            target_sequence[0, 0, sampled_token_index] = 1.\n",
    "        return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100000\n",
      "Number of unique encoder tokens: 15\n",
      "Number of unique decoder tokens: 14\n",
      "Max sequence length for encoding: 7\n",
      "Max sequence length for decoding: 11\n"
     ]
    }
   ],
   "source": [
    "seq2seq = Seq2seq(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 15)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 14)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 64), (None,  20480       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 64), ( 20224       input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 14)     910         lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 41,614\n",
      "Trainable params: 41,614\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq.design_and_compile_training_model(latent_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "75000/75000 [==============================] - 68s 908us/step - loss: 0.7158 - val_loss: 0.6673\n",
      "Epoch 2/5\n",
      "75000/75000 [==============================] - 61s 820us/step - loss: 0.6562 - val_loss: 0.6438\n",
      "Epoch 3/5\n",
      "75000/75000 [==============================] - 60s 805us/step - loss: 0.6341 - val_loss: 0.6187\n",
      "Epoch 4/5\n",
      "75000/75000 [==============================] - 58s 771us/step - loss: 0.6121 - val_loss: 0.5986\n",
      "Epoch 5/5\n",
      "75000/75000 [==============================] - 61s 810us/step - loss: 0.5916 - val_loss: 0.5790\n"
     ]
    }
   ],
   "source": [
    "seq2seq.train(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq.design_inference_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 581+495\n",
      "Decoded sentence: 1041\n",
      "\n",
      "-\n",
      "Input sentence: 79+231\n",
      "Decoded sentence: 704\n",
      "\n",
      "-\n",
      "Input sentence: 648/414\n",
      "Decoded sentence: 1.91515\n",
      "\n",
      "-\n",
      "Input sentence: 248*10\n",
      "Decoded sentence: 1040\n",
      "\n",
      "-\n",
      "Input sentence: 625+909\n",
      "Decoded sentence: 1149\n",
      "\n",
      "-\n",
      "Input sentence: 423+441\n",
      "Decoded sentence: 1014\n",
      "\n",
      "-\n",
      "Input sentence: 937+215\n",
      "Decoded sentence: 1019\n",
      "\n",
      "-\n",
      "Input sentence: 662-651\n",
      "Decoded sentence: 12\n",
      "\n",
      "-\n",
      "Input sentence: 698%467\n",
      "Decoded sentence: 18\n",
      "\n",
      "-\n",
      "Input sentence: 920-281\n",
      "Decoded sentence: 581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sequence_index in range(10):\n",
    "    input_sequence = seq2seq.encoder_input_data_val[sequence_index: sequence_index + 1]\n",
    "    decoded_sentence = seq2seq.decode_sequence(input_sequence)\n",
    "    print('-')\n",
    "    raw_input_sequence = \"\".join(\n",
    "        [seq2seq.encoder_char_index_inversed[np.argmax(token)] for token in np.squeeze(input_sequence)][::-1]\n",
    "    )\n",
    "    print('Input sentence:', seq2seq.X_val[sequence_index][::-1])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "- 1) Explain the interest in using teacher forcing during training. What is specific about this process?\n",
    "\n",
    "<span style=\"color:green\">\n",
    "Teacher forcing means that, at each timestep, we provide the previous correct token to be decoded to the decoder. In case we did not use it the prediction would have been provided. What is interesting with this process is it helps stabilizing training by focusing updates on wrong weights' states leading to actual misanswers from the network, not weights in right state which would have provided correct answer but for being given the correct input.\n",
    "</span>\n",
    "\n",
    "\n",
    "- 2) Describe step by step how the encoder-decoder couple works in this case (~ 5-10 lines)\n",
    "\n",
    "<span style=\"color:green\">\n",
    "There are $2$ symmetric networks, $2$ LSTMs, that have similar purposes. The first one, the encoder, processes vectors, $1$ at a time, and outputs a vector, $h_{t}^{enc}$. The position in vector space of $h_{t}^{enc}$ tells the decoder which token to predict first and contains information about all the vectors that have been processed. Thus at time $t$ there are $2$ choices: either provide $h_{t}^{enc}$ to the decoder so it knows which $1^{st}$ token to decode or process the next intput vector and produce $h_{t+1}^{enc}$.\n",
    "The final $h_{T}^{enc}$ is provided to the decoder along with the GO token. Position of $h_{T}^{enc}$ and GO token combined lead to a $h_{0}^{dec}$ vector to be produced, used as input by a fully connected layer to predict the first decoded token $\\hat{y}^{0}$. We iterate over this process, only instead providing $h_{t-1}^{dec}$ and true $\\hat{y}^{t-1}$ as input, until it predicts the END decoded token.\n",
    "</span>\n",
    "\n",
    "- 3) Why is it mandatory to have different implementations between training and inference? Why do we need different models? (~ 3-6 lines)\n",
    "\n",
    "<span style=\"color:green\">\n",
    "During training, since we use teacher forcing, the sequence of input tokens for the decoder is already ready: the sequence of $\\hat{y}^{t-1}$ for each timestep $t$. That is how we organize the data and feed it to the LSTM decoder. During inference, we cannot use teacher forcing, we have no way to provide with a full sequence of inputs to the decoder from the start. Thus we have to iteratively produce the previous prediction and feed it as input token to the decoder LSTM at the next timestep. These $2$ different ways of providing the input data to the decoder LSTM are the reason why different implementations are needed.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Sequence to sequence model with attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to improve your previous model from part I with attention mechanism<br/>\n",
    "Note that it will be a bit different from the implementation seen in the course for practical reasons<br/>\n",
    "In this part, you will concatenate the attention weights with the hidden decoder states after the decoding pass, and feed the result to final Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, dot, concatenate, TimeDistributed\n",
    "\n",
    "class Seq2seqAttention(Seq2seq):\n",
    "    def __init__(self, X, y):\n",
    "        # Attention layers hyperparameters\n",
    "        self.latent_attention_dim = 64\n",
    "        # All hidden states from the encoder that must now be stored\n",
    "        self.encoder_outputs = None\n",
    "        # Attention layers and states\n",
    "        self.dense_tanh = None\n",
    "        self.dense_final = None\n",
    "        # Seq2Seq class initialization\n",
    "        super(Seq2seqAttention, self).__init__(X, y)\n",
    "    \n",
    "    \"\"\"\n",
    "    ENCODER LAYERS:\n",
    "        - define a Input Keras object in self.encoder_inputs       \n",
    "        - apply a LSTM layer with return_sequences=True on self.encoder_inputs \n",
    "        to get (self.encoder_outputs, state_h, state_c)\n",
    "        - stack state_h and state_c into an array self.encoder_states\n",
    "    DECODER LAYERS:\n",
    "        - define an Input Keras object in self.decoder_inputs\n",
    "        - define a LSTM layer in self.decoder_lstm, make sure you set return_sequences=True\n",
    "        to be able to return all hidden states\n",
    "        - apply this LSTM layer on self.decoder_inputs with the states initialized with self.encoder_states\n",
    "        and output all the hidden states in self.decoder_all_hdec\n",
    "    ATTENTION LAYERS:\n",
    "        - apply a dot product between self.decoder_all_hdec and self.encoder_outputs along their last\n",
    "        dimension (the latent one), then a softmax activation, and store the result into attention\n",
    "        - compute the context tensor with a dot product between the attention tensor and self.encoder_outputs\n",
    "        along the last dimension (softmax values) for attention and time dimension for self.encoder_outputs\n",
    "        - concatenate the result with self.decoder_all_hdec\n",
    "        - define the two final Dense layers: \n",
    "            - the first with tanh activation and self.latent_attention_dim size\n",
    "            - the second with softmax activation and self.decoder_vocabulary_size\n",
    "        - output the final result into attention_outputs\n",
    "    MODEL DEFINITION:\n",
    "        - now you can build your global Model:\n",
    "        Model([self.encoder_inputs, self.decoder_inputs], attention_outputs)\n",
    "    \"\"\"\n",
    "    def design_and_compile_training_model(self, batch_size=64, latent_dim=256, latent_attention_dim=64):\n",
    "        # Hyperparameters\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.latent_attention_dim = latent_attention_dim\n",
    "        # Encoder layers\n",
    "        self.encoder_inputs = Input(shape=(None, self.encoder_vocabulary_size))\n",
    "        encoder_lstm = LSTM(self.latent_dim, return_state=True, return_sequences=True)\n",
    "        encoder_outputs, state_h, state_c = encoder_lstm(self.encoder_inputs)\n",
    "        self.encoder_states = [state_h, state_c]\n",
    "        self.encoder_outputs = encoder_outputs\n",
    "        print(encoder_outputs.shape)\n",
    "        # Decoder layers\n",
    "        self.decoder_inputs = Input(shape=(None, self.decoder_vocabulary_size))\n",
    "        self.decoder_lstm = LSTM(self.latent_dim, return_state=True, return_sequences=True)\n",
    "        self.decoder_all_hdec, _, _ = self.decoder_lstm(self.decoder_inputs, initial_state=self.encoder_states)\n",
    "        # Attention layers\n",
    "        attention = dot([self.decoder_all_hdec, self.encoder_outputs], axes=[2, 2])\n",
    "        attention = Activation('softmax', name='attention')(attention)\n",
    "        context = dot([attention, self.encoder_outputs], axes=[2, 1])\n",
    "        decoder_combined_context = concatenate([context, self.decoder_all_hdec])\n",
    "        self.dense_tanh = Dense(self.latent_attention_dim, activation=\"tanh\")\n",
    "        self.dense_final = Dense(self.decoder_vocabulary_size, activation=\"softmax\")\n",
    "        attention_outputs = self.dense_final(self.dense_tanh(decoder_combined_context))\n",
    "        # Model definition and compilation\n",
    "        self.training_model = Model([self.encoder_inputs, self.decoder_inputs], attention_outputs)\n",
    "        self.training_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "        self.training_model.summary()\n",
    "        \n",
    "    \"\"\"\n",
    "    ENCODER MODEL:\n",
    "        - create a Keras Model self.inference_encoder_model \n",
    "        with self.encoder_inputs as inputs and self.encoder_states as output\n",
    "    DECODER MODEL:\n",
    "        - define two Input Keras objects: one for the h_state and the other for the c_state, then stack\n",
    "        them into a decoder_states_inputs array\n",
    "        - reuse the already trained self.decoder_lstm layer with self.decoder_inputs as input\n",
    "        and decoder_states_inputs as initial_state\n",
    "            - you should get three outputs: decoder_all_hdec, decoder_state_h and decoder_state_c\n",
    "        - again, stack the outputed decoder_state_h and decoder_state_c into a decoder_states array        \n",
    "        - now apply a dot product between decoder_all_hdec and self.encoder_outputs along their last\n",
    "        dimension (the latent one), then a softmax activation: it is your attention tensor\n",
    "        - compute the context tensor with a dot product between the attention tensor and self.encoder_outputs\n",
    "        along the last dimension (softmax values) for attention and time dimension for self.encoder_outputs\n",
    "        - concatenate the result with decoder_all_hdec\n",
    "        - reuse the two trained Denser layers and output the final result into attention_outputs\n",
    "        - you can finally create a Keras Model self.inference_decoder_model\n",
    "        with [self.encoder_inputs] + [self.decoder_inputs] + decoder_states_inputs as inputs \n",
    "        and [attention_outputs] + decoder_states as output\n",
    "    \"\"\"\n",
    "    def design_inference_model(self):\n",
    "        if self.training_model is None:\n",
    "            print(\"No training model has been defined yet!\")\n",
    "            return None\n",
    "        # Encoder model\n",
    "        self.inference_encoder_model = Model(self.encoder_inputs, self.encoder_states)\n",
    "        # Decoder model\n",
    "        decoder_state_input_h = Input(shape=(self.latent_dim,))\n",
    "        decoder_state_input_c = Input(shape=(self.latent_dim,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        decoder_all_hdec, decoder_state_h, decoder_state_c = self.decoder_lstm(\n",
    "            self.decoder_inputs, initial_state=decoder_states_inputs\n",
    "        )\n",
    "        decoder_states = [decoder_state_h, decoder_state_c]\n",
    "        \n",
    "        attention = dot([decoder_all_hdec, self.encoder_outputs], axes=[2, 2])\n",
    "        attention = Activation('softmax', name='attention')(attention)\n",
    "        context = dot([attention, self.encoder_outputs], axes=[2, 1])\n",
    "        decoder_combined_context = concatenate([context, decoder_all_hdec])\n",
    "        attention_outputs = self.dense_final(self.dense_tanh(decoder_combined_context))\n",
    "        \n",
    "        self.inference_decoder_model = Model(\n",
    "            [self.encoder_inputs] + [self.decoder_inputs] + decoder_states_inputs,\n",
    "            [attention_outputs] + decoder_states)\n",
    "        \n",
    "    def decode_sequence(self, input_sequence):\n",
    "        states_value = self.inference_encoder_model.predict(input_sequence)\n",
    "        target_sequence = np.zeros((1, 1, self.decoder_vocabulary_size))\n",
    "        target_sequence[0, 0, self.decoder_char_index[self.GO]] = 1.\n",
    "        decoded_sentence = ''\n",
    "        while len(decoded_sentence) <= self.max_decoder_sequence_length:\n",
    "            output_tokens, h, c = self.inference_decoder_model.predict(\n",
    "                [input_sequence] + [target_sequence] + states_value\n",
    "            )\n",
    "            states_value = [h, c]\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_char = self.decoder_char_index_inversed[sampled_token_index]\n",
    "            decoded_sentence += sampled_char\n",
    "            if sampled_char == self.EOS:\n",
    "                break\n",
    "            target_sequence = np.zeros((1, 1, self.decoder_vocabulary_size))\n",
    "            target_sequence[0, 0, sampled_token_index] = 1.\n",
    "        return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100000\n",
      "Number of unique encoder tokens: 15\n",
      "Number of unique decoder tokens: 14\n",
      "Max sequence length for encoding: 7\n",
      "Max sequence length for decoding: 11\n"
     ]
    }
   ],
   "source": [
    "seq2seq_attention = Seq2seqAttention(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 64)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, None, 15)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, None, 14)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 64), ( 20480       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 64), ( 20224       input_6[0][0]                    \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, None)   0           lstm_4[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, None, None)   0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, None, 64)     0           attention[0][0]                  \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 128)    0           dot_2[0][0]                      \n",
      "                                                                 lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 64)     8256        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 14)     910         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 49,870\n",
      "Trainable params: 49,870\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_attention.design_and_compile_training_model(latent_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "75000/75000 [==============================] - 72s 957us/step - loss: 0.7008 - val_loss: 0.6570\n",
      "Epoch 2/5\n",
      "75000/75000 [==============================] - 64s 859us/step - loss: 0.6481 - val_loss: 0.6322\n",
      "Epoch 3/5\n",
      "75000/75000 [==============================] - 72s 961us/step - loss: 0.6260 - val_loss: 0.6098\n",
      "Epoch 4/5\n",
      "75000/75000 [==============================] - 85s 1ms/step - loss: 0.5980 - val_loss: 0.5823\n",
      "Epoch 5/5\n",
      "75000/75000 [==============================] - 66s 880us/step - loss: 0.5677 - val_loss: 0.5526\n"
     ]
    }
   ],
   "source": [
    "seq2seq_attention.train(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_attention.design_inference_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 581+495\n",
      "Decoded sentence: 1041\n",
      "\n",
      "-\n",
      "Input sentence: 79+231\n",
      "Decoded sentence: 542\n",
      "\n",
      "-\n",
      "Input sentence: 648/414\n",
      "Decoded sentence: 1.64441\n",
      "\n",
      "-\n",
      "Input sentence: 248*10\n",
      "Decoded sentence: 12280\n",
      "\n",
      "-\n",
      "Input sentence: 625+909\n",
      "Decoded sentence: 1441\n",
      "\n",
      "-\n",
      "Input sentence: 423+441\n",
      "Decoded sentence: 942\n",
      "\n",
      "-\n",
      "Input sentence: 937+215\n",
      "Decoded sentence: 1041\n",
      "\n",
      "-\n",
      "Input sentence: 662-651\n",
      "Decoded sentence: -13\n",
      "\n",
      "-\n",
      "Input sentence: 698%467\n",
      "Decoded sentence: 188\n",
      "\n",
      "-\n",
      "Input sentence: 920-281\n",
      "Decoded sentence: 418\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sequence_index in range(10):\n",
    "    input_sequence = seq2seq_attention.encoder_input_data_val[sequence_index: sequence_index + 1]\n",
    "    decoded_sentence = seq2seq_attention.decode_sequence(input_sequence)\n",
    "    print('-')\n",
    "    raw_input_sequence = \"\".join(\n",
    "        [seq2seq_attention.encoder_char_index_inversed[np.argmax(token)] \n",
    "         for token in np.squeeze(input_sequence)][::-1]\n",
    "    )\n",
    "    print('Input sentence:', seq2seq_attention.X_val[sequence_index][::-1])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "- 1) Explain the main differences with the previous part, how does the attention mechanism work? (~ 5-10 lines)\n",
    "\n",
    "<span style=\"color:green\">\n",
    "Attention mechanism works by being able to focus on a specific subsequence in a long sequence to predict the right token at some timestep. That means not having to rely solely on final $h_{T}^{enc}$ to predict the whole decoded sequence, but rather recombining and weighing all the $h_{t}^{enc}$ at each decoding step to focus those related to the prediction.\n",
    "At each decoding step, a scalar product is performed between $h_{t}^{dec}$ and all the $h_{t}^{enc}$. This gives a similarity measure between $h_{t}^{dec}$ and each $h_{t}^{enc}$. A softmax is applied to this vector to rescale the similarity coefficients and make them sum to $1$. This way we can use them to compute a mean $h^{enc}$ vector to be used for prediction that allows the network to focus on some input tokens by making some coefficient relatively much greater than the others. Mean $h^{enc}$ vector is then computed and followed by $tanh$ operation to reduce vector input space of next operation. Final step is a softmax fully connected layer over the $tanh$ vector for prediction of the next decoded token. Applying attention mechanism involves iterating over this for each decoding timestep.\n",
    "</span>\n",
    "\n",
    "- 2) Compare the perfomances of your model at inference time with and without attention mechanism\n",
    "\n",
    "<span style=\"color:green\">\n",
    "In this example, no noticeable difference is to be found between the performances of the $2$ different implementations, with and without attention mechanism. Also some quick visualization tells us that the network does not really focus much on part of the input to predict $1$ decoded token at a time. The reason for that is the encoding-decoding problem here is specific in the way that almost all input tokens are involved in producting each output token.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
