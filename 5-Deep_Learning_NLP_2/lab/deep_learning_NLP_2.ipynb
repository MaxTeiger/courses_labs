{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an introduction to basic sequence-to-sequence learning using a Long short term memory (LSTM) module.\n",
    "\n",
    "Given a string of characters representing a math problem \"3141+42\" we would like to generate a string of characters representing the correct solution: \"3183\". Our network will learn how to do basic mathematical operations.\n",
    "\n",
    "The important part is that we will not first use our human intelligence to break the string up into integers and a mathematical operator. We want the computer to figure all that out by itself.\n",
    "\n",
    "Each math problem is an input sequence: a list of {0,...,9} integers and math operation symbols\n",
    "The result of the operation (\"$3141+42$\" $\\rightarrow$ \"$3183$\"</span>) is the sequence to decode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**math_operators** is the set of $5$ operations we are going to use to build are input sequences.<br/>\n",
    "The math_expressions_generation function uses them to generate a large set of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_expressions_generation(n_samples=1000, n_digits=3, invert=True):\n",
    "    X, Y = [], []\n",
    "    math_operators = {\n",
    "        '+': operator.add, \n",
    "        '-': operator.sub,\n",
    "        '*': operator.mul,\n",
    "        '/': operator.truediv,\n",
    "        '%': operator.mod\n",
    "    }\n",
    "    for i in range(n_samples):\n",
    "        a, b = np.random.randint(1, 10**n_digits, size=2)\n",
    "        op = np.random.choice(list(math_operators.keys()))\n",
    "        res = math_operators[op](a, b)\n",
    "        x = \"\".join([str(elem) for elem in (a, op, b)])\n",
    "        if invert is True:\n",
    "            x = x[::-1]\n",
    "        y = \"{:.5f}\".format(res) if isinstance(res, float) else str(res)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651+109 = 760\n",
      "974-96 = 878\n",
      "943-648 = 295\n",
      "546/546 = 1.00000\n",
      "57/430 = 0.13256\n",
      "221/830 = 0.26627\n",
      "604+272 = 876\n",
      "384+781 = 1165\n",
      "243+739 = 982\n",
      "715*803 = 574145\n",
      "488%50 = 38\n",
      "602+229 = 831\n",
      "475%309 = 166\n",
      "429+279 = 708\n",
      "832+403 = 1235\n",
      "471*457 = 215247\n",
      "194*483 = 93702\n",
      "329+32 = 361\n",
      "1%110 = 1\n",
      "351+436 = 787\n"
     ]
    }
   ],
   "source": [
    "X, y = math_expressions_generation(n_samples=int(1e5), n_digits=3, invert=True)\n",
    "for X_i, y_i in list(zip(X, y))[:20]:\n",
    "    print(X_i[::-1], '=', y_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Standard sequence to sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Seq2Seq architecture\n",
    "\n",
    "### Two LSTM: an encoder and a decoder\n",
    "\n",
    "<img src=\"../images/teacher_forcing_train.png\" style=\"width: 600px;\" />\n",
    "\n",
    "- ## Training with teacher forcing\n",
    "    - Build the Seq2Seq model for training\n",
    "    - Example:\n",
    "        - Input sequence \"94+8\" must be given the the encoding LSTM\n",
    "        - True answers \"1\", \"0\", \"2\" are given to the LSTM encoder after the prediction is made to help it predict well the next token during training\n",
    "    - We advice to use Keras functional API here\n",
    "\n",
    "- ### Encoder:\n",
    "    - Define a layer encoder_inputs of shape (None, self.encoder_vocabulary_size) \n",
    "    - Instantiate the encoder LSTM layer before connecting it. Call it $\\text{encoder_lstm}$\n",
    "        - Use the return_state param so it returns its last  state_h and state_c\n",
    "        - We need to pass those to the decoder LSTM afterwards to connect the $2$ LSTMs\n",
    "    - Connect $\\text{encoder_lstm}$ to  encoder_inputs and get $\\text{encoder_lstm}$'s last  state_h and state_c in a variable $\\text{encoder_states = [state_h,  state_c]}$\n",
    "      \n",
    "- ### Decoder:\n",
    "    - Define a layer decoder_inputs of shape (None, self.decoder_vocabulary_size) \n",
    "    - Instantiate the decoder LSTM layer before connecting it. Call it $\\text{decoder_lstm}$.\n",
    "        - Pass encoder's last[state_h, state_c] to decoder initial_state argument to connect the two LSTM\n",
    "        - Use the return_sequences param so the decoder returns all the $h_{t}^{dec}$\n",
    "            - We need them to compute the predictions using the $h_{t}^{dec}$\n",
    "    - Connect $\\text{decoder_lstm}$ to decoder_inputs and get the $h_{t}^{dec}$ hidden layers in a $\\text{decoder_outputs}$ node\n",
    "\n",
    "- ### Output:\n",
    "     - At this point we have all our $h_{t}^{dec}$ in a $\\text{decoder_outputs}$ node, ready to be used to perform a token prediction for each timestep\n",
    "     - Define a Dense layer. Call it $\\text{decoder_dense}$, with a softmax activation and self.decoder_vocabulary_size dimensionality\n",
    "     - Connect $\\text{decoder_dense}$ to $\\text{decoder_outputs}$ and make the result the $outputs$ node\n",
    "     - At this point each $h_{t}^{dec}$ has been mapped to a $( \\text{num_decoder_tokens},1)$ vector of probability distribution over the next token. \n",
    "     - Tensor at this point is shape $(batch, \\text{max_decoder_seq_length}, \\text{num_decoder_tokens})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Inference (testing time, no teacher forcing)\n",
    "   - We are going to see how to perform inference, that is decoding an input sequence without using teacher forcing\n",
    "   - We won't provide the answer from previous timesteps like during training.\n",
    "   - Thus we cannot provide with the <...EOS> part of the sequences like in training\n",
    "   - Predictions have to be performed one step at a time, first using the last state of encoder and GO token to produce the $1{st}$ decoder hidden layer  $h_{0}^{dec}$. Then $h_{0}^{dec}$ will be used to predict the $1{st}$ token. The $1{st}$ token predicted and $h_{0}^{dec}$ will be used to produce $h_{1}^{dec}$ and so on\n",
    "\n",
    "- ## Requirements\n",
    "    - To perform inference we are going to need an $\\text{encoder_model}$ that takes in the input sequence and returns the last $h$ and $c$ state to pass to the decoder\n",
    "\n",
    "    - To perform inference we also are going to need a $\\text{decoder model}$ that takes in the previous hidden state, a token like GO or previous prediction and returns next hidden state and prediction. We must iterate over those successive steps of taking in prev hidden state and token and output next hidden state and token to produce the whole decoded sentence without teacher forcing\n",
    "\n",
    "To do this we are going to reuse layers and nodes used before to take advantage of the **trained weights**:\n",
    "   - encoder_inputs and encoder_states nodes that are already connected through a trained lstm to produce an encoder's last state\n",
    "   - decoder_lstm and decoder_inputs \n",
    "\n",
    "- ## encoder_model\n",
    "   - Use the class Model from keras.models\n",
    "   - Define the node $\\text{encoder_inputs}$ as input to the model\n",
    "   - Define the node $\\text{encoder_states}$ as output to the model\n",
    "\n",
    "-  ## decoder_model:\n",
    "   - Define $2$ $Input$ keras.layers of dimensionality $\\text{latent_dim}$: $\\text{decoder_state_input_h}$ and $\\text{decoder_state_input_c}$\n",
    "      - They are the parameters refering to the decoder_model's last state and to be received upon each call to function predict at each iteration\n",
    "          - Stack them in a  decoder_states_inputs variable: $\\text{decoder_states_inputs} = [decoder\\_state\\_input\\_h, decoder\\_state\\_input\\_c]$\n",
    "          - connect decoder_lstm to decoder_inputs using the argument $\\text{initial_state}$ = $\\text{decoder_states_inputs}$ and get $\\text{decoder_outputs}$, $\\text{decoder_state_h}$, $\\text{decoder_state_c}$ from that connection:\n",
    "          - this way we specify the fact that at each call to predict decoder_lstm will use the previous state received in argument to produce the next state\n",
    "          - decoder_outputs is all the $h_{t}^{dec}$ produced. There we give 1 token at a time, 1 prediction at a time thus decoder_output shape is $\\text{(1,latent_dim)}$\n",
    "          - $\\text{decoder_state_h}$  is the last  $h_{t}^{dec}$ and $\\text{decoder_state_c}$, is the second part of decoder_lstm's last state\n",
    "          - Stack $\\text{decoder_state_h}$ and $\\text{decoder_state_c}$ in a $\\text{decoder_states}$ variable:  $\\text{decoder_states = [state_h, state_c]}$\n",
    "              - Those have to be returned after prediction\n",
    "          - Connect $\\text{decoder_dense}$ layer from previous section to $\\text{decoder_outputs}$ to have the distribution probability over the next token and call $output$ the node from that connection\n",
    "          - At this point we have our next prediction ($ouput$ node) and newest state ( [state_h, state_c]). We are ready to define the decoder_model:\n",
    "             - Make $\\text{[decoder_inputs] + decoder_states_inputs}$ the inputs to the model (input args upon call to predict)\n",
    "             - Make $\\text{[output] + decoder_states}$ the output to the model (output args upon call to predict)\n",
    "          \n",
    "          \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GO** is the character (\"=\") that marks the beginning of decoding for the decoder LSTM<br/>\n",
    "**EOS** is the character (\"\\n\") that marks the end of sequence to decode for the decoder LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Seq2seq():\n",
    "    def __init__(self, X, y):\n",
    "        # Special tokens\n",
    "        self.GO = '='\n",
    "        self.EOS = '\\n'\n",
    "        # Dataset properties\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_tr = None\n",
    "        self.X_val = None\n",
    "        self.y_tr = None\n",
    "        self.y_val = None\n",
    "        self.n = None\n",
    "        self.encoder_char_index = None\n",
    "        self.encoder_char_index_inversed = None\n",
    "        self.decoder_char_index = None\n",
    "        self.decoder_char_index_inversed = None\n",
    "        self.encoder_vocabulary_size = None\n",
    "        self.decoder_vocabulary_size = None\n",
    "        self.max_encoder_sequence_length = None\n",
    "        self.max_decoder_sequence_length = None\n",
    "        # Preprocessed data\n",
    "        self.encoder_input_data_tr = None\n",
    "        self.encoder_input_data_val = None\n",
    "        self.decoder_input_data_tr = None\n",
    "        self.decoder_input_data_val = None\n",
    "        self.decoder_target_data_tr = None\n",
    "        self.decoder_target_data_val = None\n",
    "        # Model properties\n",
    "        self.training_model = None\n",
    "        self.inference_encoder_model = None\n",
    "        self.inference_decoder_model = None\n",
    "        self.batch_size = None\n",
    "        self.epochs = None\n",
    "        self.latent_dim = None\n",
    "        # Model layers and states that we want to keep in memory between training and inference\n",
    "        ## Encoder\n",
    "        self.encoder_inputs = None\n",
    "        self.encoder_states = None\n",
    "        ## Decoder\n",
    "        self.decoder_inputs = None\n",
    "        self.decoder_lstm = None\n",
    "        self.decoder_all_hdec = None\n",
    "        self.decoder_dense = None\n",
    "        # Dataset construction call\n",
    "        self.load_and_preprocess_data(X, y)\n",
    "        self.construct_dataset()\n",
    "        \n",
    "    def load_and_preprocess_data(self, X, y):\n",
    "        self.X = list(X)\n",
    "        self.y = list(map(lambda token: self.GO + token + self.EOS, y))\n",
    "        self.n = len(self.X)\n",
    "        encoder_characters = sorted(list(set(\"\".join(self.X))))\n",
    "        decoder_characters = sorted(list(set(\"\".join(self.y))))\n",
    "        self.encoder_char_index = dict((c, i) for i, c in enumerate(encoder_characters))\n",
    "        self.encoder_char_index_inversed = dict((i, c) for i, c in enumerate(encoder_characters))\n",
    "        self.decoder_char_index = dict((c, i) for i, c in enumerate(decoder_characters))\n",
    "        self.decoder_char_index_inversed = dict((i, c) for i, c in enumerate(decoder_characters))\n",
    "        self.encoder_vocabulary_size = len(self.encoder_char_index)\n",
    "        self.decoder_vocabulary_size = len(self.decoder_char_index)\n",
    "        self.max_encoder_sequence_length = max([len(sequence) for sequence in self.X])\n",
    "        self.max_decoder_sequence_length = max([len(sequence) for sequence in self.y])\n",
    "        print('Number of samples:', self.n)\n",
    "        print('Number of unique encoder tokens:', self.encoder_vocabulary_size)\n",
    "        print('Number of unique decoder tokens:', self.decoder_vocabulary_size)\n",
    "        print('Max sequence length for encoding:', self.max_encoder_sequence_length)\n",
    "        print('Max sequence length for decoding:', self.max_decoder_sequence_length)\n",
    "        (self.X_tr, self.X_val, \n",
    "         self.y_tr, self.y_val) = train_test_split(\n",
    "            self.X, \n",
    "            self.y,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "    def construct_dataset(self):\n",
    "        encoder_input_data = np.zeros(\n",
    "            (self.n, self.max_encoder_sequence_length, self.encoder_vocabulary_size),\n",
    "            dtype='float32')\n",
    "        decoder_input_data = np.zeros(\n",
    "            (self.n, self.decoder_vocabulary_size, self.decoder_vocabulary_size),\n",
    "            dtype='float32')\n",
    "        decoder_target_data = np.zeros(\n",
    "            (self.n, self.decoder_vocabulary_size, self.decoder_vocabulary_size),\n",
    "            dtype='float32')\n",
    "        for i, (X_i, y_i) in enumerate(zip(self.X, self.y)):\n",
    "            for t, char in enumerate(X_i):\n",
    "                encoder_input_data[i, t, self.encoder_char_index[char]] = 1.\n",
    "            for t, char in enumerate(y_i):\n",
    "                decoder_input_data[i, t, self.decoder_char_index[char]] = 1.\n",
    "                if t > 0:\n",
    "                    decoder_target_data[i, t - 1, self.decoder_char_index[char]] = 1.\n",
    "        (self.encoder_input_data_tr, self.encoder_input_data_val, \n",
    "         self.decoder_input_data_tr, self.decoder_input_data_val,\n",
    "         self.decoder_target_data_tr, self.decoder_target_data_val) = train_test_split(\n",
    "            encoder_input_data, \n",
    "            decoder_input_data, \n",
    "            decoder_target_data,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    \"\"\"\n",
    "    ENCODER LAYERS:\n",
    "        - define a Input Keras object in self.encoder_inputs\n",
    "        - apply a LSTM layer on self.encoder_inputs to get the last state_h and state_c\n",
    "        - stack those states into an array self.encoder_states\n",
    "    DECODER LAYERS:\n",
    "        - define an Input Keras object in self.decoder_inputs\n",
    "        - define a LSTM layer in self.decoder_lstm, make sure you set return_sequences=True\n",
    "        to be able to return all hidden states\n",
    "        - apply this LSTM layer on self.decoder_inputs with the states initialized with self.encoder_states\n",
    "        and output all the hidden states in self.decoder_all_hdec\n",
    "        - define a Dense layer in self.decoder_dense with a softmax activation, and output the results \n",
    "        in decoder_outputs using self.decoder_all_hdec as inputs\n",
    "    MODEL DEFINITION:\n",
    "        - now you can build your global Model:\n",
    "        Model([self.encoder_inputs, self.decoder_inputs], decoder_outputs)\n",
    "    \"\"\"\n",
    "    def design_and_compile_training_model(self, batch_size=64, latent_dim=256):\n",
    "        # Hyperparameters\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        # Encoder layers\n",
    "        # TODO:\n",
    "        self.encoder_inputs = None\n",
    "        self.encoder_states = None\n",
    "        # Decoder layers\n",
    "        # TODO:\n",
    "        self.decoder_inputs = None\n",
    "        self.decoder_lstm = None\n",
    "        self.decoder_all_hdec = None\n",
    "        self.decoder_dense = None\n",
    "        decoder_outputs = None\n",
    "        # Model definition and compilation\n",
    "        if all(tensor is not None for tensor in [self.encoder_inputs, self.decoder_inputs, decoder_outputs]):\n",
    "            self.training_model = Model([self.encoder_inputs, self.decoder_inputs], decoder_outputs)\n",
    "            self.training_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "            self.training_model.summary()\n",
    "        else:\n",
    "            print(\"Inputs and outputs of the model are not correctly defined!\")\n",
    "        \n",
    "    def train(self, epochs=15):\n",
    "        # Hyperparameters\n",
    "        self.epochs = epochs\n",
    "        # Model actual training\n",
    "        if self.training_model is not None:\n",
    "            self.training_model.fit(\n",
    "                [self.encoder_input_data_tr, self.decoder_input_data_tr], self.decoder_target_data_tr,\n",
    "                batch_size=self.batch_size,\n",
    "                epochs=self.epochs,\n",
    "                validation_data=(\n",
    "                    [self.encoder_input_data_val, self.decoder_input_data_val], self.decoder_target_data_val\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    \"\"\"\n",
    "    ENCODER MODEL:\n",
    "        - create a Keras Model self.inference_encoder_model \n",
    "        with self.encoder_inputs as inputs and self.encoder_states as output\n",
    "    DECODER MODEL:\n",
    "        - define two Input Keras objects: one for the h_state and the other for the c_state, then stack\n",
    "        them into a decoder_states_inputs array\n",
    "        - reuse the already trained self.decoder_lstm layer with self.decoder_inputs as input\n",
    "        and decoder_states_inputs as initial_state\n",
    "            - you should get three outputs: decoder_all_hdec, decoder_state_h and decoder_state_c\n",
    "        - again, stack the outputed decoder_state_h and decoder_state_c into a decoder_states array\n",
    "        - now reuse the already trained self.decoder_dense layer with decoder_all_hdec as input,\n",
    "        and store the output into decoder_outputs\n",
    "        - you can finally create a Keras Model self.inference_decoder_model\n",
    "        with [self.decoder_inputs] + decoder_states_inputs as inputs \n",
    "        and [decoder_outputs] + decoder_states as output\n",
    "    \"\"\"\n",
    "    def design_inference_model(self):\n",
    "        if self.training_model is None:\n",
    "            print(\"No training model has been defined yet!\")\n",
    "            return None\n",
    "        # Encoder model\n",
    "        # TODO:\n",
    "        self.inference_encoder_model = None\n",
    "        # Decoder model\n",
    "        ## Inputs: latent variables from the encoder\n",
    "        # TODO:\n",
    "        decoder_states_inputs = None\n",
    "        ## Decoding using the LSTM trained layer from the decoder\n",
    "        # TODO:\n",
    "        decoder_states = None\n",
    "        ## Get outputs using the Dense trained layer from the decoder\n",
    "        # TODO:\n",
    "        decoder_outputs = None\n",
    "        ## Define the whole decoding model\n",
    "        # TODO:\n",
    "        self.inference_decoder_model = None\n",
    "        \n",
    "    def decode_sequence(self, input_sequence):\n",
    "        if self.inference_encoder_model is None or self.inference_decoder_model is None:\n",
    "            print(\"Inference models have not been designed yet!\")\n",
    "            return None\n",
    "        states_value = self.inference_encoder_model.predict(input_sequence)\n",
    "        target_sequence = np.zeros((1, 1, self.decoder_vocabulary_size))\n",
    "        target_sequence[0, 0, self.decoder_char_index[self.GO]] = 1.\n",
    "        decoded_sentence = ''\n",
    "        while len(decoded_sentence) <= self.max_decoder_sequence_length:\n",
    "            output_tokens, h, c = self.inference_decoder_model.predict(\n",
    "                [target_sequence] + states_value\n",
    "            )\n",
    "            states_value = [h, c]\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_char = self.decoder_char_index_inversed[sampled_token_index]\n",
    "            decoded_sentence += sampled_char\n",
    "            if sampled_char == self.EOS:\n",
    "                break\n",
    "            target_sequence = np.zeros((1, 1, self.decoder_vocabulary_size))\n",
    "            target_sequence[0, 0, sampled_token_index] = 1.\n",
    "        return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100000\n",
      "Number of unique encoder tokens: 15\n",
      "Number of unique decoder tokens: 14\n",
      "Max sequence length for encoding: 7\n",
      "Max sequence length for decoding: 11\n"
     ]
    }
   ],
   "source": [
    "seq2seq = Seq2seq(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs and outputs of the model are not correctly defined!\n"
     ]
    }
   ],
   "source": [
    "seq2seq.design_and_compile_training_model(latent_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq.train(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No training model has been defined yet!\n"
     ]
    }
   ],
   "source": [
    "seq2seq.design_inference_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if seq2seq.inference_encoder_model is not None and seq2seq.inference_decoder_model is not None:\n",
    "    for sequence_index in range(10):\n",
    "        input_sequence = seq2seq.encoder_input_data_val[sequence_index: sequence_index + 1]\n",
    "        decoded_sentence = seq2seq.decode_sequence(input_sequence)\n",
    "        print('-')\n",
    "        raw_input_sequence = \"\".join(\n",
    "            [seq2seq.encoder_char_index_inversed[np.argmax(token)] for token in np.squeeze(input_sequence)][::-1]\n",
    "        )\n",
    "        print('Input sentence:', seq2seq.X_val[sequence_index][::-1])\n",
    "        print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Sequence to sequence model with attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, dot, concatenate, TimeDistributed\n",
    "\n",
    "class Seq2seqAttention(Seq2seq):\n",
    "    def __init__(self, X, y):\n",
    "        # Attention layers hyperparameters\n",
    "        self.latent_attention_dim = 64\n",
    "        # All hidden states from the encoder that must now be stored\n",
    "        self.encoder_outputs = None\n",
    "        # Attention layers and states\n",
    "        self.dense_tanh = None\n",
    "        self.dense_final = None\n",
    "        # Seq2Seq class initialization\n",
    "        super(Seq2seqAttention, self).__init__(X, y)\n",
    "    \n",
    "    \"\"\"\n",
    "    ENCODER LAYERS:\n",
    "        - define a Input Keras object in self.encoder_inputs       \n",
    "        - apply a LSTM layer with return_sequences=True on self.encoder_inputs \n",
    "        to get (self.encoder_outputs, state_h, state_c)\n",
    "        - stack state_h and state_c into an array self.encoder_states\n",
    "    DECODER LAYERS:\n",
    "        - define an Input Keras object in self.decoder_inputs\n",
    "        - define a LSTM layer in self.decoder_lstm, make sure you set return_sequences=True\n",
    "        to be able to return all hidden states\n",
    "        - apply this LSTM layer on self.decoder_inputs with the states initialized with self.encoder_states\n",
    "        and output all the hidden states in self.decoder_all_hdec\n",
    "    ATTENTION LAYERS:\n",
    "        - apply a dot product between self.decoder_all_hdec and self.encoder_outputs along their last\n",
    "        dimension (the latent one), then a softmax activation, and store the result into attention\n",
    "        - compute the context tensor with a dot product between attention and self.encoder_outputs\n",
    "        - concatenate the result with self.decoder_all_hdec\n",
    "        - define the two final Dense layers: \n",
    "            - the first with tanh activation and self.latent_attention_dim size\n",
    "            - the second with softmax activation and self.decoder_vocabulary_size\n",
    "        - output the final result into attention_outputs\n",
    "    MODEL DEFINITION:\n",
    "        - now you can build your global Model:\n",
    "        Model([self.encoder_inputs, self.decoder_inputs], attention_outputs)\n",
    "    \"\"\"\n",
    "    def design_and_compile_training_model(self, batch_size=64, latent_dim=256, latent_attention_dim=64):\n",
    "        # Hyperparameters\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.latent_attention_dim = latent_attention_dim\n",
    "        # Encoder layers\n",
    "        # TODO:\n",
    "        self.encoder_inputs = None\n",
    "        self.encoder_states = None\n",
    "        self.encoder_outputs = None\n",
    "        # Decoder layers\n",
    "        # TODO:\n",
    "        self.decoder_inputs = None\n",
    "        self.decoder_lstm = None\n",
    "        self.decoder_all_hdec = None\n",
    "        # Attention layers\n",
    "        # TODO:\n",
    "        self.dense_tanh = None\n",
    "        self.dense_final = None\n",
    "        attention_outputs = None\n",
    "        # Model definition and compilation\n",
    "        if all(tensor is not None for tensor in [self.encoder_inputs, self.decoder_inputs, attention_outputs]):\n",
    "            self.training_model = Model([self.encoder_inputs, self.decoder_inputs], attention_outputs)\n",
    "            self.training_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "            self.training_model.summary()\n",
    "        else:\n",
    "            print(\"Inputs and outputs of the model are not correctly defined!\")\n",
    "        \n",
    "    \"\"\"\n",
    "    ENCODER MODEL:\n",
    "        - create a Keras Model self.inference_encoder_model \n",
    "        with self.encoder_inputs as inputs and self.encoder_states as output\n",
    "    DECODER MODEL:\n",
    "        - define two Input Keras objects: one for the h_state and the other for the c_state, then stack\n",
    "        them into a decoder_states_inputs array\n",
    "        - reuse the already trained self.decoder_lstm layer with self.decoder_inputs as input\n",
    "        and decoder_states_inputs as initial_state\n",
    "            - you should get three outputs: decoder_all_hdec, decoder_state_h and decoder_state_c\n",
    "        - again, stack the outputed decoder_state_h and decoder_state_c into a decoder_states array        \n",
    "        - now apply a dot product between decoder_all_hdec and self.encoder_outputs along their last\n",
    "        dimension (the latent one), then a softmax activation: it is your attention tensor\n",
    "        - compute the context tensor with a dot product between the attention tensor and self.encoder_outputs\n",
    "        - concatenate the result with decoder_all_hdec\n",
    "        - reuse the two trained Denser layers and output the final result into attention_outputs\n",
    "        - you can finally create a Keras Model self.inference_decoder_model\n",
    "        with [self.decoder_inputs] + [self.decoder_inputs] + decoder_states_inputs as inputs \n",
    "        and [attention_outputs] + decoder_states as output\n",
    "    \"\"\"\n",
    "    def design_inference_model(self):\n",
    "        if self.training_model is None:\n",
    "            print(\"No training model has been defined yet!\")\n",
    "            return None\n",
    "        # Encoder model\n",
    "        # TODO:\n",
    "        self.inference_encoder_model = None\n",
    "        # Decoder model\n",
    "        ## Inputs: latent variables from the encoder\n",
    "        # TODO:\n",
    "        decoder_states_inputs = None\n",
    "        ## Decoding using the LSTM trained layer from the decoder\n",
    "        # TODO:\n",
    "        decoder_states = None\n",
    "        ## Get outputs using multiple dot products and softmax activation followed by the two trained Dense layers\n",
    "        # TODO:\n",
    "        attention_outputs = None\n",
    "        ## Define the whole decoding model\n",
    "        # TODO:\n",
    "        self.inference_decoder_model = None\n",
    "        \n",
    "    def decode_sequence(self, input_sequence):\n",
    "        if self.inference_encoder_model is None or self.inference_decoder_model is None:\n",
    "            print(\"Inference models have not been designed yet!\")\n",
    "            return None\n",
    "        states_value = self.inference_encoder_model.predict(input_sequence)\n",
    "        target_sequence = np.zeros((1, 1, self.decoder_vocabulary_size))\n",
    "        target_sequence[0, 0, self.decoder_char_index[self.GO]] = 1.\n",
    "        decoded_sentence = ''\n",
    "        while len(decoded_sentence) <= self.max_decoder_sequence_length:\n",
    "            output_tokens, h, c = self.inference_decoder_model.predict(\n",
    "                [input_sequence] + [target_sequence] + states_value\n",
    "            )\n",
    "            states_value = [h, c]\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_char = self.decoder_char_index_inversed[sampled_token_index]\n",
    "            decoded_sentence += sampled_char\n",
    "            if sampled_char == self.EOS:\n",
    "                break\n",
    "            target_sequence = np.zeros((1, 1, self.decoder_vocabulary_size))\n",
    "            target_sequence[0, 0, sampled_token_index] = 1.\n",
    "        return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100000\n",
      "Number of unique encoder tokens: 15\n",
      "Number of unique decoder tokens: 14\n",
      "Max sequence length for encoding: 7\n",
      "Max sequence length for decoding: 11\n"
     ]
    }
   ],
   "source": [
    "seq2seq_attention = Seq2seqAttention(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs and outputs of the model are not correctly defined!\n"
     ]
    }
   ],
   "source": [
    "seq2seq_attention.design_and_compile_training_model(latent_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_attention.train(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No training model has been defined yet!\n"
     ]
    }
   ],
   "source": [
    "seq2seq_attention.design_inference_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if seq2seq_attention.inference_encoder_model is not None and seq2seq_attention.inference_decoder_model is not None:\n",
    "    for sequence_index in range(10):\n",
    "        input_sequence = seq2seq_attention.encoder_input_data_val[sequence_index: sequence_index + 1]\n",
    "        decoded_sentence = seq2seq_attention.decode_sequence(input_sequence)\n",
    "        print('-')\n",
    "        raw_input_sequence = \"\".join(\n",
    "            [seq2seq_attention.encoder_char_index_inversed[np.argmax(token)] \n",
    "             for token in np.squeeze(input_sequence)][::-1]\n",
    "        )\n",
    "        print('Input sentence:', seq2seq_attention.X_val[sequence_index][::-1])\n",
    "        print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
