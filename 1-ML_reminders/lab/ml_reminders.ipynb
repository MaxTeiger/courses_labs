{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "EPSILON = 1e-8 # small constant to avoid underflow or divide per 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Linear multidimensional regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I a) - Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/linear_multi.csv', header=None)\n",
    "X, Y = data.values[:, :-1], data.values[:, -1]\n",
    "Y = Y.reshape((Y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I b) - Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define our MSE cost function $J(\\theta)$ as follows:\n",
    "$$\n",
    "J(\\mathbf{\\theta}, \\mathbf{X}, \\mathbf{Y}) = \\frac{1}{2n} \\sum_{i=1}^n (Y_i - \\mathbf{X_i} \\mathbf{\\theta})^2\n",
    "= \\frac{1}{2 n} \\| \\mathbf{Y} - \\mathbf{X} \\mathbf{\\theta} \\|_2^2,\n",
    "$$\n",
    "with $\\mathbf{Y} = [y_1 \\cdots y_n]$, $\\mathbf{X} = [\\mathbf{x_1} \\cdots \\mathbf{x_n}]$,\n",
    "\n",
    "$\\mathbf{Y} \\in \\mathbb{R}^{n\\times{1}}$ being the vector of labels, $\\mathbf{X} \\in \\mathbb{R}^{n\\times{p}}$ being the vector of features, $\\mathbf{\\theta} \\in \\mathbb{R}^{p\\times{1}}$ being our parameters, and $n$ the number of samples.\n",
    "\n",
    "Now we can calculate the gradient $\\nabla J(\\mathbf{\\theta})$:\n",
    "$$\n",
    "\\nabla J(\\mathbf{\\theta}, \\mathbf{X}, \\mathbf{Y}) = - \\frac{1}{n} \\sum_{i=1}^n \\mathbf{X_i}(Y_i - \\mathbf{X_i} \\mathbf{\\theta})\n",
    "= - \\frac{1}{n} \\mathbf{X^\\top}(\\mathbf{Y} - \\mathbf{X} \\mathbf{\\theta}),\n",
    "$$\n",
    "\n",
    "It leads us to the gradient descend (GD) iterative algorithm:\n",
    "$$\n",
    "\\mathbf{\\theta}^{t+1} := \\mathbf{\\theta}^{t} - \\eta_t \\nabla J(\\mathbf{\\theta}, \\mathbf{X}, \\mathbf{Y})\n",
    "$$\n",
    "\n",
    "It is common to decay the learning rate linearly until iteration $\\tau$:\n",
    "$$\n",
    "\\eta_t = (1-\\alpha)\\eta_0 + \\alpha \\eta_\\tau\n",
    "$$\n",
    "with $\\alpha = \\frac{t}{\\tau}$, and usually here we are going to take $\\tau \\approx 200$ and $\\eta_\\tau = \\frac{\\eta_0}{100}$\n",
    "\n",
    "And finally, our linear prediction $\\mathbf{\\hat{Y}}$ can be expressed as:\n",
    "$$\n",
    "\\mathbf{\\hat{Y}} = \\mathbf{X}\\mathbf{\\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor():\n",
    "    def __init__(self, X, Y, penalization=None):\n",
    "        p = X.shape[1]\n",
    "        self.theta = np.random.normal(0, 1, p).reshape((p, 1))\n",
    "        self.grad_theta = None\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        self.X_tr, self.X_val, self.Y_tr, self.Y_val = self.split_train_validation(X, Y)\n",
    "        self.penalization = penalization\n",
    "        self.gamma = None\n",
    "        \n",
    "    def split_train_validation(self, X, Y, test_size=0.25, seed=False):\n",
    "        random_state = 42 if seed else np.random.randint(1e3)\n",
    "        X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "        return X_tr, X_val, Y_tr, Y_val\n",
    "    \n",
    "    # Must return the linear prediction given X\n",
    "    def predict(self, X):\n",
    "        # TODO:\n",
    "        return np.zeros((X.shape[0], 1))\n",
    "    \n",
    "    # Must return the gradient, penalized or not, of the given loss\n",
    "    def compute_grad(self, X, Y, loss='MSE'):\n",
    "        \"\"\"Least-squares gradient\"\"\"\n",
    "        assert(X.shape[0] == Y.shape[0])\n",
    "        n = X.shape[0]\n",
    "        if loss == 'MSE':\n",
    "            # TODO:\n",
    "            grad = np.zeros(self.theta.shape)\n",
    "            if self.penalization == 'l2':\n",
    "                # TODO:\n",
    "                pass\n",
    "            if self.penalization == 'l1':\n",
    "                # TODO:\n",
    "                pass\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return grad\n",
    "    \n",
    "    #Â Call the gradient computation function with the used loss and proceed to a gradient descent step\n",
    "    def step_gradient_descent(self, step, X, Y):\n",
    "        self.grad_theta = self.compute_grad(X, Y, loss='MSE')\n",
    "        # TODO:\n",
    "        pass\n",
    "        \n",
    "    # Must return the given loss, penalized or not, for Y_true and Y_pred as inputs\n",
    "    def loss(self, Y_true, Y_pred, loss='MSE'):\n",
    "        \"\"\"Least-squares loss\"\"\"\n",
    "        assert(Y_true.shape[0] == Y_pred.shape[0])\n",
    "        n = Y_true.shape[0]\n",
    "        if loss == 'MSE':\n",
    "            # TODO:\n",
    "            loss = 0.\n",
    "            if self.penalization == 'l2':\n",
    "                # TODO:\n",
    "                pass\n",
    "            elif self.penalization == 'l1':\n",
    "                # TODO:\n",
    "                pass\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return loss\n",
    "    \n",
    "    def loss_history_flush(self):\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "    \n",
    "    # Step decay strategy, the idea is to have a step value that fluctuates with k until the tau value\n",
    "    # Return the step value for iteration k\n",
    "    def apply_step_decay(self, initial_step, k, tau=250):\n",
    "        # TODO:\n",
    "        return initial_step\n",
    "        \n",
    "    def fit(self, initial_step=1e-1, min_iterations=50, max_iterations=5000, gamma=1e-3, \n",
    "            early_stopping=True, early_stopping_delta=1e-3, early_stopping_lookbehind=50, \n",
    "            step_decay=True, flush_history=True, verbose=True):\n",
    "        if flush_history:\n",
    "            self.loss_history_flush()\n",
    "        cpt_patience, best_validation_loss = 0, np.inf\n",
    "        iteration_number = 0\n",
    "        step = initial_step\n",
    "        self.gamma = gamma\n",
    "        while len(self.training_losses_history) < max_iterations:\n",
    "            iteration_number += 1\n",
    "            self.step_gradient_descent(step, self.X_tr, self.Y_tr)\n",
    "            if step_decay:\n",
    "                step = self.apply_step_decay(initial_step, iteration_number)\n",
    "            training_loss = self.loss(self.Y_tr, self.predict(self.X_tr))\n",
    "            self.training_losses_history.append(training_loss)\n",
    "            validation_loss = self.loss(self.Y_val, self.predict(self.X_val))\n",
    "            self.validation_losses_history.append(validation_loss)\n",
    "            if iteration_number > min_iterations and early_stopping:\n",
    "                if validation_loss + early_stopping_delta < best_validation_loss:\n",
    "                    best_validation_loss = validation_loss\n",
    "                    cpt_patience = 0\n",
    "                else:\n",
    "                    cpt_patience += 1\n",
    "            if verbose:\n",
    "                msg = \"iteration number: {0}\\t training loss: {1:.4f}\\t validation loss: {2:.4f}\"\n",
    "                print(msg.format(iteration_number, training_loss, validation_loss))\n",
    "            if cpt_patience >= early_stopping_lookbehind:\n",
    "                break\n",
    "\n",
    "    def plot_loss_history(self):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        if not np.any(linear_regressor.training_losses_history):\n",
    "            return None\n",
    "        plt.plot(range(len(self.training_losses_history)), \n",
    "                 self.training_losses_history, label='Training loss evolution')\n",
    "        plt.plot(range(len(self.validation_losses_history)), \n",
    "                 self.validation_losses_history, label='Validation loss evolution')\n",
    "        plt.legend(fontsize=15)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel(\"iteration number\", fontsize=15)\n",
    "        plt.ylabel(\"MSE loss\", fontsize=15)\n",
    "        if not self.penalization:\n",
    "            title = \"MSE loss evolution during training, no penalization\"\n",
    "        else:\n",
    "            title = \"MSE loss evolution during training, penalization {}\"\n",
    "            title = title.format(self.penalization)\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_parameters(self, title=None):\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.scatter(range(len(self.theta)), self.theta)\n",
    "        plt.xlabel(\"parameter dimension\", fontsize=15)\n",
    "        plt.ylabel(\"value\", fontsize=15)\n",
    "        if title is None:\n",
    "            if not self.penalization:\n",
    "                plt.title(\"Scatter plot of the learned parameters, no penalization\", fontsize=15)\n",
    "            else:\n",
    "                plt.title(\n",
    "                    \"Scatter plot of the learned parameters, penalization {}\".format(self.penalization), fontsize=15\n",
    "                )\n",
    "        else:\n",
    "                plt.title(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor = LinearRegressor(X, Y)\n",
    "ridge_linear_regressor = LinearRegressor(X, Y, penalization='l2')\n",
    "lasso_linear_regressor = LinearRegressor(X, Y, penalization='l1')\n",
    "\n",
    "linear_regressor.fit(verbose=True)\n",
    "ridge_linear_regressor.fit(gamma=1e-2, verbose=True, step_decay=True)\n",
    "lasso_linear_regressor.fit(gamma=1e-2, verbose=True, step_decay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor.plot_loss_history()\n",
    "ridge_linear_regressor.plot_loss_history()\n",
    "lasso_linear_regressor.plot_loss_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "####Â - Did you succeed to have a low validation loss?\n",
    "#### - What are l1 and l2 penalization strategies useful for? Did you achieve to improve your model's performance with them? Try different penalization values.\n",
    "####Â - Consider the parameters plot. What could you conclude about the features information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Non linear unidimensional regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II a) - Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/non_linear.csv', header=None)\n",
    "X, Y = data.values[:, :-1], data.values[:, -1]\n",
    "X, Y = X.reshape((Y.shape[0], 1)), Y.reshape((Y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II b) - Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(X, Y, s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegressor(LinearRegressor):\n",
    "    def __init__(self, X, Y, degree=2, penalization=None):\n",
    "        self.degree = degree\n",
    "        self.initial_X_tr, self.initial_X_val, _, _ = self.split_train_validation(X, Y)\n",
    "        X_transformed = self.non_linear_mapping(X)\n",
    "        super(PolynomialRegressor, self).__init__(X_transformed, Y, penalization)\n",
    "    \n",
    "    # Must return a new features matrix based on a polynomial transformation\n",
    "    def non_linear_mapping(self, X):\n",
    "        # TODO:\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X, apply_features_transformation=False):\n",
    "        if apply_features_transformation:\n",
    "            X_transformed = self.non_linear_mapping(X)\n",
    "            return super(PolynomialRegressor, self).predict(X_transformed)\n",
    "        else:\n",
    "            return super(PolynomialRegressor, self).predict(X)\n",
    "        \n",
    "    def plot_prediction(self):\n",
    "        assert self.initial_X_val.shape[1] == 1, \"Too high dimensional design matrix to be plotted!\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.scatter(self.initial_X_val, self.Y_val, s=5)\n",
    "        plt.scatter(\n",
    "            self.initial_X_val, self.predict(self.initial_X_val, apply_features_transformation=True), \n",
    "            color='red', s=5\n",
    "        )\n",
    "        plt.title(\"Prediction with polynomial regressor, degree={}\".format(self.degree), fontsize=15)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_regressor = PolynomialRegressor(X, Y, degree=2)\n",
    "polynomial_regressor.fit(initial_step=1e-2, max_iterations=500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_regressor.plot_loss_history()\n",
    "polynomial_regressor.plot_parameters()\n",
    "polynomial_regressor.plot_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "#### - Did you succeed to fit the data using a non-linear mapping transformation?\n",
    "#### - Try different degrees, which one leads you to the best validation error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III a) - Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/breast_cancer_classification.csv', header=None)\n",
    "X, Y = data.values[:, :-1], data.values[:, -1]\n",
    "Y = Y.reshape((Y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III b) - Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sigmoid function $\\sigma(x)$ defined as:\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$\n",
    "\n",
    "We can define our binary cross-entropy cost function $J(\\theta)$ as follows:\n",
    "$$\n",
    "J(\\mathbf{\\theta}, \\mathbf{X}, \\mathbf{Y}) = -\\frac{1}{n} \\sum_{i=1}^n Y_i \\log \\sigma(\\mathbf{X_i}\\mathbf{\\theta}) + (1 - Y_i) \\log (1 - \\sigma(\\mathbf{X_i}^\\top \\mathbf{\\theta})),\n",
    "$$\n",
    "with $\\mathbf{Y} = [y_1 \\cdots y_n]$, $\\mathbf{X} = [\\mathbf{x_1} \\cdots \\mathbf{x_n}]$,\n",
    "\n",
    "$\\mathbf{Y} \\in \\mathbb{R}^{n\\times{1}}$ being the vector of labels, $\\mathbf{X} \\in \\mathbb{R}^{n\\times{p}}$ being the vector of features, $\\mathbf{\\theta} \\in \\mathbb{R}^{p\\times{1}}$ being our parameters, and $n$ the number of samples.\n",
    "\n",
    "Now we can calculate the gradient $\\nabla J(\\mathbf{\\theta})$:\n",
    "$$\n",
    "\\nabla J(\\mathbf{\\theta}, \\mathbf{X}, \\mathbf{Y}) = - \\frac{1}{n} \\sum_{i=1}^n \\mathbf{X_i}(Y_i - \\sigma(\\mathbf{X_i}\\mathbf{\\theta})),\n",
    "$$\n",
    "\n",
    "It leads us to the gradient descend (GD) iterative algorithm:\n",
    "$$\n",
    "\\mathbf{\\theta}^{t+1} := \\mathbf{\\theta}^{t} - \\eta_t \\nabla J(\\mathbf{\\theta}, \\mathbf{X}, \\mathbf{Y})\n",
    "$$\n",
    "\n",
    "And finally, our linear prediction $\\mathbf{\\hat{Y}}$ can be expressed as:\n",
    "$$\n",
    "\\mathbf{\\hat{Y}} = \\sigma(\\mathbf{X}\\mathbf{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(LinearRegressor):\n",
    "    def __init__(self, X, Y, penalization=None):\n",
    "        super(LogisticRegressor, self).__init__(X, Y, penalization)\n",
    "        self.X_tr, self.X_val = self.normalize(self.X_tr), self.normalize(self.X_val)\n",
    "    \n",
    "    # Return a normalized matrix with standardized features 0 mean 1 std\n",
    "    def normalize(self, X):\n",
    "        # TODO:\n",
    "        return X\n",
    "    \n",
    "    # Return the model's accuracy on the validation dataset\n",
    "    def accuracy_on_validation(self):\n",
    "        # TODO:\n",
    "        return 0.\n",
    "        \n",
    "    # Sigmoid function\n",
    "    def sigmoid(self, X):\n",
    "        # TODO:\n",
    "        return X\n",
    "\n",
    "    def predict(self, X, apply_normalization=False):\n",
    "        if apply_normalization:\n",
    "            X = self.normalize(X)\n",
    "        return self.sigmoid(X.dot(self.theta))\n",
    "\n",
    "    def compute_grad(self, X, Y, loss='CE'):\n",
    "        \"\"\"Binary cross-entropy gradient\"\"\"\n",
    "        assert(X.shape[0] == Y.shape[0])\n",
    "        n = X.shape[0]\n",
    "        if loss == 'CE':\n",
    "            # TODO:\n",
    "            grad = np.zeros(self.theta.shape)\n",
    "            if self.penalization == 'l2':\n",
    "                # TODO:\n",
    "                pass\n",
    "            if self.penalization == 'l1':\n",
    "                # TODO:\n",
    "                pass\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return grad\n",
    "\n",
    "    # Call the gradient computation function with the used loss and proceed to a gradient descent step on self.theta\n",
    "    def step_gradient_descent(self, step, X, Y):\n",
    "        self.grad_theta = self.compute_grad(X, Y, loss='CE')\n",
    "        # TODO:\n",
    "        pass\n",
    "    \n",
    "    # Must return the given loss, penalized or not, for Y_true and Y_pred as inputs\n",
    "    def loss(self, Y_true, Y_pred, loss='CE'):\n",
    "        \"\"\"Binary cross-entropy loss\"\"\"\n",
    "        assert(Y_true.shape[0] == Y_pred.shape[0])\n",
    "        n = Y_true.shape[0]\n",
    "        if loss == 'CE':\n",
    "            # TODO:\n",
    "            loss = 0.\n",
    "            if self.penalization == 'l2':\n",
    "                # TODO:\n",
    "                pass\n",
    "            elif self.penalization == 'l1':\n",
    "                # TODO:\n",
    "                pass\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regressor = LogisticRegressor(X, Y)\n",
    "logistic_regressor.fit(max_iterations=1000, initial_step=1e-1, step_decay=False)\n",
    "logistic_regressor.plot_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logistic_regressor.accuracy_on_validation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "#### - Did you succeed to reach an accuracy of at least 95% on the validation dataset? If not, what could be the reason?\n",
    "#### - Why do we need a different loss function for a classification problem (why not MSE again?)\n",
    "#### - Why is the normalization particulary relevant in this case? Compare the performances with and without it. Comment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
