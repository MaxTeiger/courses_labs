{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "EPSILON = 1e-8 # small constant to avoid underflow or divide per 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time, the data will correspond to greyscale images. <br> Two different datasets can be used here:\n",
    "- The MNIST dataset, small 8*8 images, corresponding to handwritten digits &rightarrow; 10 classes\n",
    "- The Fashion MNIST dataset, medium 28*28 images, corresponding to clothes pictures &rightarrow; 10 classes\n",
    "\n",
    "#### Starting with the simple MNIST is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MNIST\"\n",
    "# dataset = \"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset='MNIST'):\n",
    "    if dataset == 'MNIST':\n",
    "        digits = load_digits()\n",
    "        X, Y = np.asarray(digits['data'], dtype='float32'), np.asarray(digits['target'], dtype='int32')\n",
    "        return X, Y\n",
    "    elif dataset == 'FASHION_MNIST':\n",
    "        import tensorflow as tf\n",
    "        fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "        (X, Y), (_, _) = fashion_mnist.load_data()\n",
    "        X = X.reshape((X.shape[0], X.shape[1] * X.shape[2]))\n",
    "        X, Y = np.asarray(X, dtype='float32'), np.asarray(Y, dtype='int32')\n",
    "        return X, Y\n",
    "X, Y = load_data(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1797\n",
      "Input dimension: 64\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: {:d}'.format(X.shape[0]))\n",
    "print('Input dimension: {:d}'.format(X.shape[1])) # images 8x8 or 28*28 actually\n",
    "print('Number of classes: {:d}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range max-min of greyscale pixel values: (16.0, 0.0)\n",
      "First image sample:\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "First image label: 0\n",
      "Input design matrix shape: (1797, 64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Range max-min of greyscale pixel values: ({0:.1f}, {1:.1f})\".format(np.max(X), np.min(X)))\n",
    "print(\"First image sample:\\n{0}\".format(X[0]))\n",
    "print(\"First image label: {0}\".format(Y[0]))\n",
    "print(\"Input design matrix shape: {0}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?\n",
    "Each image in the dataset consists of a 8 x 8 (or 28 x 28) matrix, of greyscale pixels. For the MNIST dataset, the values are between 0 and 16 where 0 represents white, 16 represents black and there are many shades of grey in-between. For the Fashion MNIST dataset, the values are between 0 and 255.<br>Each image is assigned a corresponding numerical label, so the image in ```X[i]``` has its corresponding label stored in ```Y[i]```.\n",
    "\n",
    "The next cells below demonstrate how to visualise the input data. Make sure you understand what's happening, particularly how the indices correspond to individual items in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_sample(X, Y, nrows=2, ncols=2):\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    for row in ax:\n",
    "        for col in row:\n",
    "            index = random.randint(0, X.shape[0])\n",
    "            dim = np.sqrt(X.shape[1]).astype(int)\n",
    "            col.imshow(X[index].reshape((dim, dim)), cmap=plt.cm.gray_r)\n",
    "            col.set_title(\"image label: %d\" % Y[index])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_data_sample(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â II - Multiclass classification MLP with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II a) - Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/mlp_mnist.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here will be to implement \"from scratch\" a Multilayer Perceptron for classification.\n",
    "\n",
    "We will define the formal categorical cross entropy loss as follows:\n",
    "$$\n",
    "l(\\mathbf{\\Theta}, \\mathbf{X}, \\mathbf{Y}) = - \\frac{1}{n} \\sum_{i=1}^n \\log \\mathbf{f}(\\mathbf{x}^i ; \\mathbf{\\Theta})^\\top y^i\n",
    "$$\n",
    "<center>with $y^i$ being the one-hot encoded true label for the sample $i$, and $\\Theta = (\\mathbf{W}^h; \\mathbf{b}^h; \\mathbf{W}^o; \\mathbf{b}^o)$</center>\n",
    "<center>In addition, $\\mathbf{f}(\\mathbf{x}) = softmax(\\mathbf{z^o}(\\mathbf{x})) = softmax(\\mathbf{W}^o\\mathbf{h}(\\mathbf{x}) + \\mathbf{b}^o)$</center>\n",
    "<center>and $\\mathbf{h}(\\mathbf{x}) = g(\\mathbf{z^h}(\\mathbf{x})) = g(\\mathbf{W}^h\\mathbf{x} + \\mathbf{b}^h)$, $g$ being the activation function and could be implemented with $sigmoid$ or $relu$</center>\n",
    "\n",
    "## Objectives:\n",
    "- Write the categorical cross entropy loss function\n",
    "- Write the activation functions with their associated gradient\n",
    "- Write the softmax function that is going to be used to output the predicted probabilities\n",
    "- Implement the forward pass through the neural network\n",
    "- Implement the backpropagation according to the used loss: progagate the gradients using the chain rule and return $(\\mathbf{\\nabla_{W^h}}l ; \\mathbf{\\nabla_{b^h}}l ; \\mathbf{\\nabla_{W^o}}l ; \\mathbf{\\nabla_{b^o}}l)$\n",
    "- Implement dropout regularization in the forward pass: be careful to consider both training and prediction cases\n",
    "- Implement the SGD optimization algorithm, and improve it with simple momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple graph function to let you have a global overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/function_graph.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â II b) - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron():\n",
    "    \"\"\"MLP with one hidden layer having a hidden activation,\n",
    "    and one output layer having a softmax activation\"\"\"\n",
    "    def __init__(self, X, Y, hidden_size, activation='relu',\n",
    "                 initialization='uniform', dropout=False, dropout_rate=0):\n",
    "        # input, hidden, and output dimensions on the MLP based on X, Y\n",
    "        self.input_size, self.output_size = X.shape[1], len(np.unique(Y))\n",
    "        self.hidden_size = hidden_size\n",
    "        # initialization strategies: avoid a full-0 initialization of the weight matrices\n",
    "        if initialization == 'uniform':\n",
    "            self.W_h = np.random.uniform(size=(self.input_size, self.hidden_size), high=0.01, low=-0.01)\n",
    "            self.W_o = np.random.uniform(size=(self.hidden_size, self.output_size), high=0.01, low=-0.01)\n",
    "        elif initialization == 'normal':\n",
    "            self.W_h = np.random.normal(size=(self.input_size, self.hidden_size), loc=0, scale=0.01)\n",
    "            self.W_o = np.random.normal(size=(self.hidden_size, self.output_size), loc=0, scale=0.01)\n",
    "        # the bias could be initializated to 0 or a random low constant\n",
    "        self.b_h = np.zeros(self.hidden_size)\n",
    "        self.b_o = np.zeros(self.output_size)\n",
    "        # our namedtuple structure of gradients\n",
    "        self.Grads = namedtuple('Grads', ['W_h', 'b_h', 'W_o', 'b_o'])\n",
    "        # and the velocities associated which are going to be useful for the momentum\n",
    "        self.velocities = {'W_h': 0., 'b_h': 0., 'W_o': 0., 'b_o': 0.}\n",
    "        #Â the hidden activation function used\n",
    "        self.activation = activation\n",
    "        #Â arrays to track back the losses and accuracies evolution\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        self.validation_acc_history = []\n",
    "        #Â train val split and normalization of the features\n",
    "        self.X_tr, self.X_val, self.Y_tr, self.Y_val = self.split_train_validation(X, Y)\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "        self.X_tr = self.scaler.fit_transform(self.X_tr)\n",
    "        self.X_val = self.scaler.transform(self.X_val)\n",
    "        #Â dropout parameters\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # step used for the optimization algorithm and setted later\n",
    "        self.step = None\n",
    "    \n",
    "    # One-hot encoding of the target\n",
    "    # Transform the integer represensation to a sparse one\n",
    "    @staticmethod\n",
    "    def one_hot(n_classes, Y):\n",
    "        return np.eye(n_classes)[Y]\n",
    "    \n",
    "    # Reverse one-hot encoding of the target\n",
    "    # Recover the former integer representation\n",
    "    # ex: from (0,0,1,0) to 2\n",
    "    @staticmethod\n",
    "    def reverse_one_hot(Y_one_hot):\n",
    "        return np.asarray(np.where(Y_one_hot==1)[1], dtype='int32')\n",
    "    \n",
    "    \"\"\"\n",
    "    Activation functions and their gradient\n",
    "    \"\"\"\n",
    "    # In implementations below X is a matrix of shape (n_samples, p)\n",
    "    \n",
    "    #Â A max_value value is indicated for the relu and grad_relu functions\n",
    "    # Make sure to clip the output to it to prevent numerical overflow (exploding gradient)\n",
    "    # Make it so the max value reachable is max_value\n",
    "    @staticmethod\n",
    "    def relu(X, max_value=20):\n",
    "        assert max_value > 0\n",
    "        return np.minimum(np.maximum(X, 0), max_value)\n",
    "    \n",
    "    # Make it so the gradient becomes 0 when X becomes greater than max_value\n",
    "    @staticmethod\n",
    "    def grad_relu(X, max_value=20):\n",
    "        return ((X > 0) & (X < max_value)).astype('int32')\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "    def grad_sigmoid(self, X):\n",
    "        return self.sigmoid(X) * (1 - self.sigmoid(X))\n",
    "    \n",
    "    # Softmax function to output probabilities\n",
    "    @staticmethod\n",
    "    def softmax(X):\n",
    "        exp = np.exp(X)\n",
    "        return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "    \n",
    "    #Â Loss function\n",
    "    #Â Consider using EPSILON to prevent numerical issues (log(0) is undefined)\n",
    "    # Y_true and Y_pred are of shape (n_samples,n_classes)\n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy(Y_true, Y_pred):\n",
    "        loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n",
    "        result = -np.mean(loglikelihoods)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_train_validation(X, Y, test_size=0.25, seed=False):\n",
    "        random_state = 42 if seed else np.random.randint(1e3)\n",
    "        X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "        return X_tr, X_val, Y_tr, Y_val\n",
    "    \n",
    "    # Sample random batch in (X, Y) with a given batch_size for SGD\n",
    "    @staticmethod\n",
    "    def get_random_batch(X, Y, batch_size):\n",
    "        indexes = np.random.choice(X.shape[0], size=batch_size, replace=False)\n",
    "        return X[indexes], Y[indexes]\n",
    "        \n",
    "    #Â Forward pass: compute f(x) as y, and return optionally the hidden states h(x) and z_h(x) for compute_grads\n",
    "    def forward(self, X, return_activation=False, training=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_activation = self.relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_activation = self.sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "        z_h = np.dot(X, self.W_h) + self.b_h\n",
    "        h = g_activation(z_h)\n",
    "        if self.dropout:\n",
    "            if training:\n",
    "                dropout_mask = np.random.binomial(1, 1-self.dropout_rate, size=h.shape)\n",
    "                h *= dropout_mask\n",
    "            else:\n",
    "                h *= (1-self.dropout_rate)\n",
    "        z_o = np.dot(h, self.W_o) + self.b_o\n",
    "        y = self.softmax(z_o)\n",
    "            \n",
    "        if return_activation:\n",
    "            return y, h, z_h\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    #Â Backpropagation: return an instantiation of self.Grads that contains the average gradients for the given batch\n",
    "    def compute_grads(self, X, Y_true, vectorized=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_grad = self.grad_relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_grad = self.grad_sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape((1,) + X.shape)\n",
    "        \n",
    "        if not vectorized:\n",
    "            n = X.shape[0]\n",
    "            grad_W_h = np.zeros((self.input_size, self.hidden_size))\n",
    "            grad_b_h = np.zeros((self.hidden_size, )) \n",
    "            grad_W_o = np.zeros((self.hidden_size, self.output_size))\n",
    "            grad_b_o = np.zeros((self.output_size, ))\n",
    "            for x, y_true in zip(X, Y_true):\n",
    "                y_pred, h, z_h = self.forward(x, return_activation=True, training=True)\n",
    "                \n",
    "                grad_z_o = y_pred - self.one_hot(self.output_size, y_true)\n",
    "                grad_W_o += np.outer(h, grad_z_o)\n",
    "                grad_b_o += grad_z_o\n",
    "                grad_h = np.dot(grad_z_o, np.transpose(self.W_o))\n",
    "                \n",
    "                grad_z_h = grad_h * g_grad(z_h)\n",
    "                grad_W_h += np.outer(x, grad_z_h)\n",
    "                grad_b_h += grad_z_h\n",
    "                \n",
    "            grads = self.Grads(grad_W_h/n, grad_b_h/n, grad_W_o/n, grad_b_o/n)\n",
    "            \n",
    "        else: \n",
    "            Y_pred, h, z_h = self.forward(X, return_activation=True, training=True)\n",
    "\n",
    "            grad_z_o = Y_pred - self.one_hot(self.output_size, Y_true)\n",
    "            grad_W_o = np.matmul(h[:, :, np.newaxis], np.swapaxes(grad_z_o[:, :, np.newaxis], 2, 1))\n",
    "            grad_b_o = grad_z_o\n",
    "            grad_h = np.dot(grad_z_o, np.transpose(self.W_o))\n",
    "\n",
    "            grad_z_h = grad_h * g_grad(z_h)\n",
    "            grad_W_h = np.matmul(X[:, :, np.newaxis], np.swapaxes(grad_z_h[:, :, np.newaxis], 2, 1))\n",
    "            grad_b_h = grad_z_h\n",
    "                    \n",
    "            grads = self.Grads(\n",
    "                np.mean(grad_W_h, axis=0),\n",
    "                np.mean(grad_b_h, axis=0),\n",
    "                np.mean(grad_W_o, axis=0),\n",
    "                np.mean(grad_b_o, axis=0)\n",
    "            )\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    # Perform the update of the parameters (W_h, b_h, W_o, b_o) based of their gradient\n",
    "    def optimizer_step(self, optimizer='gd', momentum=False, momentum_alpha=0.9, \n",
    "                       batch_size=None, vectorized=True):\n",
    "        if optimizer == 'gd':\n",
    "            grads = self.compute_grads(self.X_tr, self.Y_tr, vectorized=vectorized)\n",
    "        elif optimizer == 'sgd':\n",
    "            batch_X_tr, batch_Y_tr = self.get_random_batch(self.X_tr, self.Y_tr, batch_size)\n",
    "            grads = self.compute_grads(batch_X_tr, batch_Y_tr, vectorized=vectorized)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if not momentum:\n",
    "            self.W_h -= self.step * grads.W_h\n",
    "            self.b_h -= self.step * grads.b_h\n",
    "            self.W_o -= self.step * grads.W_o\n",
    "            self.b_o -= self.step * grads.b_o\n",
    "        else:\n",
    "            self.velocities['W_h'] = momentum_alpha * self.velocities['W_h'] - self.step * grads.W_h\n",
    "            self.W_h += self.velocities['W_h']\n",
    "            self.velocities['b_h'] = momentum_alpha * self.velocities['b_h'] - self.step * grads.b_h\n",
    "            self.b_h += self.velocities['b_h']\n",
    "            self.velocities['W_o'] = momentum_alpha * self.velocities['W_o'] - self.step * grads.W_o\n",
    "            self.W_o += self.velocities['W_o']\n",
    "            self.velocities['b_o'] = momentum_alpha * self.velocities['b_o'] - self.step * grads.b_o\n",
    "            self.b_o += self.velocities['b_o']\n",
    "    \n",
    "    # Loss wrapper\n",
    "    def loss(self, Y_true, Y_pred):\n",
    "        return self.categorical_cross_entropy(self.one_hot(self.output_size, Y_true), Y_pred)\n",
    "    \n",
    "    def loss_history_flush(self):\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        \n",
    "    # Main function that trains the MLP with a design matrix X and a target vector Y\n",
    "    def train(self, optimizer='sgd', momentum=False, min_iterations=500, max_iterations=5000, initial_step=1e-1,\n",
    "              batch_size=64, early_stopping=True, early_stopping_lookbehind=100, early_stopping_delta=1e-4, \n",
    "              vectorized=False, flush_history=True, verbose=True):\n",
    "        if flush_history:\n",
    "            self.loss_history_flush()\n",
    "        cpt_patience, best_validation_loss = 0, np.inf\n",
    "        iteration_number = 0\n",
    "        self.step = initial_step\n",
    "        while len(self.training_losses_history) < max_iterations:\n",
    "            iteration_number += 1\n",
    "            self.optimizer_step(\n",
    "                optimizer=optimizer, momentum=momentum, batch_size=batch_size, vectorized=vectorized\n",
    "            )\n",
    "            training_loss = self.loss(self.Y_tr, self.forward(self.X_tr))\n",
    "            self.training_losses_history.append(training_loss)\n",
    "            validation_loss = self.loss(self.Y_val, self.forward(self.X_val))\n",
    "            self.validation_losses_history.append(validation_loss)\n",
    "            validation_accuracy = self.accuracy_on_validation()\n",
    "            self.validation_acc_history.append(validation_accuracy)\n",
    "            if iteration_number > min_iterations and early_stopping:\n",
    "                if validation_loss + early_stopping_delta < best_validation_loss:\n",
    "                    best_validation_loss = validation_loss\n",
    "                    cpt_patience = 0\n",
    "                else:\n",
    "                    cpt_patience += 1\n",
    "            if verbose:\n",
    "                msg = \"iteration number: {0}\\t training loss: {1:.4f}\\t\" + \\\n",
    "                \"validation loss: {2:.4f}\\t validation accuracy: {3:.4f}\"\n",
    "                print(msg.format(iteration_number, \n",
    "                                 training_loss,\n",
    "                                 validation_loss,\n",
    "                                 validation_accuracy))\n",
    "            if cpt_patience >= early_stopping_lookbehind:\n",
    "                break\n",
    "    \n",
    "    # Return the predicted class once the MLP has been trained\n",
    "    def predict(self, X, normalize=True):\n",
    "        if normalize:\n",
    "            X = self.scaler.transform(X)\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "        \n",
    "    \"\"\"\n",
    "    Metrics and plots\n",
    "    \"\"\"\n",
    "    def accuracy_on_train(self):\n",
    "        return (self.predict(self.X_tr, normalize=False) == self.Y_tr).mean()\n",
    "\n",
    "    def accuracy_on_validation(self):\n",
    "        return (self.predict(self.X_val, normalize=False) == self.Y_val).mean()\n",
    "\n",
    "    def plot_loss_history(self, add_to_title=None):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(range(len(self.training_losses_history)), \n",
    "                 self.training_losses_history, label='Training loss evolution')\n",
    "        plt.plot(range(len(self.validation_losses_history)),\n",
    "                 self.validation_losses_history, label='Validation loss evolution')\n",
    "        plt.legend(fontsize=15)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel(\"iteration number\", fontsize=15)\n",
    "        plt.ylabel(\"Cross entropy loss\", fontsize=15)\n",
    "        base_title = \"Cross entropy loss evolution during training\"\n",
    "        if not self.dropout:\n",
    "            base_title += \", no dropout penalization\"\n",
    "        else:\n",
    "            base_title += \", {:.2f} dropout penalization\"\n",
    "            base_title = base_title.format(self.dropout_rate)\n",
    "        title = base_title + \", \" + add_to_title if add_to_title else base_title\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_validation_prediction(self, sample_id):\n",
    "        fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "        classes = np.unique(self.Y_tr)\n",
    "        dim = np.sqrt(self.X_val.shape[1]).astype(int)\n",
    "        ax0.imshow(self.scaler.inverse_transform([self.X_val[sample_id]]).reshape(dim, dim), cmap=plt.cm.gray_r,\n",
    "                   interpolation='nearest')\n",
    "        ax0.set_title(\"True image label: %d\" % self.Y_val[sample_id]);\n",
    "\n",
    "        ax1.bar(classes, self.one_hot(len(classes), self.Y_val[sample_id]), label='true')\n",
    "        ax1.bar(classes, self.forward(self.X_val[sample_id]), label='prediction', color=\"red\")\n",
    "        ax1.set_xticks(classes)\n",
    "        prediction = self.predict(self.X_val[sample_id], normalize=False)\n",
    "        ax1.set_title('Output probabilities (prediction: %d)' % prediction)\n",
    "        ax1.set_xlabel('Digit class')\n",
    "        ax1.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Standard MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1\t training loss: 2.3026\tvalidation loss: 2.3025\t validation accuracy: 0.1178\n",
      "iteration number: 2\t training loss: 2.3026\tvalidation loss: 2.3025\t validation accuracy: 0.1178\n",
      "iteration number: 3\t training loss: 2.3026\tvalidation loss: 2.3024\t validation accuracy: 0.1178\n",
      "iteration number: 4\t training loss: 2.3024\tvalidation loss: 2.3029\t validation accuracy: 0.0844\n",
      "iteration number: 5\t training loss: 2.3025\tvalidation loss: 2.3026\t validation accuracy: 0.0844\n",
      "iteration number: 6\t training loss: 2.3025\tvalidation loss: 2.3026\t validation accuracy: 0.1111\n",
      "iteration number: 7\t training loss: 2.3025\tvalidation loss: 2.3027\t validation accuracy: 0.0844\n",
      "iteration number: 8\t training loss: 2.3025\tvalidation loss: 2.3029\t validation accuracy: 0.0844\n",
      "iteration number: 9\t training loss: 2.3024\tvalidation loss: 2.3029\t validation accuracy: 0.0844\n",
      "iteration number: 10\t training loss: 2.3024\tvalidation loss: 2.3028\t validation accuracy: 0.0844\n",
      "iteration number: 11\t training loss: 2.3023\tvalidation loss: 2.3030\t validation accuracy: 0.0844\n",
      "iteration number: 12\t training loss: 2.3022\tvalidation loss: 2.3032\t validation accuracy: 0.0844\n",
      "iteration number: 13\t training loss: 2.3021\tvalidation loss: 2.3032\t validation accuracy: 0.0844\n",
      "iteration number: 14\t training loss: 2.3021\tvalidation loss: 2.3032\t validation accuracy: 0.0844\n",
      "iteration number: 15\t training loss: 2.3021\tvalidation loss: 2.3033\t validation accuracy: 0.0844\n",
      "iteration number: 16\t training loss: 2.3020\tvalidation loss: 2.3032\t validation accuracy: 0.0844\n",
      "iteration number: 17\t training loss: 2.3020\tvalidation loss: 2.3033\t validation accuracy: 0.0844\n",
      "iteration number: 18\t training loss: 2.3021\tvalidation loss: 2.3030\t validation accuracy: 0.0844\n",
      "iteration number: 19\t training loss: 2.3020\tvalidation loss: 2.3029\t validation accuracy: 0.0911\n",
      "iteration number: 20\t training loss: 2.3020\tvalidation loss: 2.3029\t validation accuracy: 0.0844\n",
      "iteration number: 21\t training loss: 2.3020\tvalidation loss: 2.3026\t validation accuracy: 0.0844\n",
      "iteration number: 22\t training loss: 2.3019\tvalidation loss: 2.3028\t validation accuracy: 0.0844\n",
      "iteration number: 23\t training loss: 2.3019\tvalidation loss: 2.3027\t validation accuracy: 0.0844\n",
      "iteration number: 24\t training loss: 2.3019\tvalidation loss: 2.3026\t validation accuracy: 0.0844\n",
      "iteration number: 25\t training loss: 2.3018\tvalidation loss: 2.3025\t validation accuracy: 0.0844\n",
      "iteration number: 26\t training loss: 2.3018\tvalidation loss: 2.3025\t validation accuracy: 0.0844\n",
      "iteration number: 27\t training loss: 2.3018\tvalidation loss: 2.3023\t validation accuracy: 0.0844\n",
      "iteration number: 28\t training loss: 2.3017\tvalidation loss: 2.3023\t validation accuracy: 0.0844\n",
      "iteration number: 29\t training loss: 2.3017\tvalidation loss: 2.3020\t validation accuracy: 0.0844\n",
      "iteration number: 30\t training loss: 2.3016\tvalidation loss: 2.3020\t validation accuracy: 0.0844\n",
      "iteration number: 31\t training loss: 2.3016\tvalidation loss: 2.3018\t validation accuracy: 0.0844\n",
      "iteration number: 32\t training loss: 2.3015\tvalidation loss: 2.3017\t validation accuracy: 0.1067\n",
      "iteration number: 33\t training loss: 2.3014\tvalidation loss: 2.3019\t validation accuracy: 0.1200\n",
      "iteration number: 34\t training loss: 2.3012\tvalidation loss: 2.3022\t validation accuracy: 0.0778\n",
      "iteration number: 35\t training loss: 2.3011\tvalidation loss: 2.3023\t validation accuracy: 0.0778\n",
      "iteration number: 36\t training loss: 2.3010\tvalidation loss: 2.3022\t validation accuracy: 0.0778\n",
      "iteration number: 37\t training loss: 2.3009\tvalidation loss: 2.3020\t validation accuracy: 0.0778\n",
      "iteration number: 38\t training loss: 2.3009\tvalidation loss: 2.3018\t validation accuracy: 0.1400\n",
      "iteration number: 39\t training loss: 2.3008\tvalidation loss: 2.3018\t validation accuracy: 0.1089\n",
      "iteration number: 40\t training loss: 2.3008\tvalidation loss: 2.3016\t validation accuracy: 0.1089\n",
      "iteration number: 41\t training loss: 2.3007\tvalidation loss: 2.3015\t validation accuracy: 0.1089\n",
      "iteration number: 42\t training loss: 2.3006\tvalidation loss: 2.3017\t validation accuracy: 0.1089\n",
      "iteration number: 43\t training loss: 2.3005\tvalidation loss: 2.3018\t validation accuracy: 0.1089\n",
      "iteration number: 44\t training loss: 2.3005\tvalidation loss: 2.3014\t validation accuracy: 0.1089\n",
      "iteration number: 45\t training loss: 2.3004\tvalidation loss: 2.3016\t validation accuracy: 0.1089\n",
      "iteration number: 46\t training loss: 2.3003\tvalidation loss: 2.3015\t validation accuracy: 0.1089\n",
      "iteration number: 47\t training loss: 2.3003\tvalidation loss: 2.3015\t validation accuracy: 0.1089\n",
      "iteration number: 48\t training loss: 2.3002\tvalidation loss: 2.3015\t validation accuracy: 0.1178\n",
      "iteration number: 49\t training loss: 2.3000\tvalidation loss: 2.3015\t validation accuracy: 0.1578\n",
      "iteration number: 50\t training loss: 2.2999\tvalidation loss: 2.3014\t validation accuracy: 0.1800\n",
      "iteration number: 51\t training loss: 2.2998\tvalidation loss: 2.3011\t validation accuracy: 0.0800\n",
      "iteration number: 52\t training loss: 2.2997\tvalidation loss: 2.3011\t validation accuracy: 0.0778\n",
      "iteration number: 53\t training loss: 2.2996\tvalidation loss: 2.3009\t validation accuracy: 0.0778\n",
      "iteration number: 54\t training loss: 2.2994\tvalidation loss: 2.3011\t validation accuracy: 0.0778\n",
      "iteration number: 55\t training loss: 2.2993\tvalidation loss: 2.3009\t validation accuracy: 0.0778\n",
      "iteration number: 56\t training loss: 2.2991\tvalidation loss: 2.3011\t validation accuracy: 0.0778\n",
      "iteration number: 57\t training loss: 2.2990\tvalidation loss: 2.3010\t validation accuracy: 0.0778\n",
      "iteration number: 58\t training loss: 2.2989\tvalidation loss: 2.3011\t validation accuracy: 0.0778\n",
      "iteration number: 59\t training loss: 2.2988\tvalidation loss: 2.3010\t validation accuracy: 0.0778\n",
      "iteration number: 60\t training loss: 2.2986\tvalidation loss: 2.3010\t validation accuracy: 0.0778\n",
      "iteration number: 61\t training loss: 2.2984\tvalidation loss: 2.3006\t validation accuracy: 0.0778\n",
      "iteration number: 62\t training loss: 2.2983\tvalidation loss: 2.3006\t validation accuracy: 0.0778\n",
      "iteration number: 63\t training loss: 2.2981\tvalidation loss: 2.3007\t validation accuracy: 0.0778\n",
      "iteration number: 64\t training loss: 2.2979\tvalidation loss: 2.3007\t validation accuracy: 0.0778\n",
      "iteration number: 65\t training loss: 2.2977\tvalidation loss: 2.3004\t validation accuracy: 0.0778\n",
      "iteration number: 66\t training loss: 2.2975\tvalidation loss: 2.3004\t validation accuracy: 0.0778\n",
      "iteration number: 67\t training loss: 2.2973\tvalidation loss: 2.3005\t validation accuracy: 0.0778\n",
      "iteration number: 68\t training loss: 2.2970\tvalidation loss: 2.3007\t validation accuracy: 0.0778\n",
      "iteration number: 69\t training loss: 2.2968\tvalidation loss: 2.3006\t validation accuracy: 0.0778\n",
      "iteration number: 70\t training loss: 2.2966\tvalidation loss: 2.3006\t validation accuracy: 0.0778\n",
      "iteration number: 71\t training loss: 2.2963\tvalidation loss: 2.3005\t validation accuracy: 0.0778\n",
      "iteration number: 72\t training loss: 2.2961\tvalidation loss: 2.3003\t validation accuracy: 0.0778\n",
      "iteration number: 73\t training loss: 2.2959\tvalidation loss: 2.3002\t validation accuracy: 0.0778\n",
      "iteration number: 74\t training loss: 2.2957\tvalidation loss: 2.2998\t validation accuracy: 0.0778\n",
      "iteration number: 75\t training loss: 2.2954\tvalidation loss: 2.2995\t validation accuracy: 0.0778\n",
      "iteration number: 76\t training loss: 2.2951\tvalidation loss: 2.2997\t validation accuracy: 0.0778\n",
      "iteration number: 77\t training loss: 2.2947\tvalidation loss: 2.2996\t validation accuracy: 0.0778\n",
      "iteration number: 78\t training loss: 2.2944\tvalidation loss: 2.2994\t validation accuracy: 0.0778\n",
      "iteration number: 79\t training loss: 2.2941\tvalidation loss: 2.2994\t validation accuracy: 0.0778\n",
      "iteration number: 80\t training loss: 2.2937\tvalidation loss: 2.2993\t validation accuracy: 0.0778\n",
      "iteration number: 81\t training loss: 2.2934\tvalidation loss: 2.2988\t validation accuracy: 0.0778\n",
      "iteration number: 82\t training loss: 2.2930\tvalidation loss: 2.2985\t validation accuracy: 0.0778\n",
      "iteration number: 83\t training loss: 2.2927\tvalidation loss: 2.2980\t validation accuracy: 0.0778\n",
      "iteration number: 84\t training loss: 2.2923\tvalidation loss: 2.2978\t validation accuracy: 0.0778\n",
      "iteration number: 85\t training loss: 2.2920\tvalidation loss: 2.2974\t validation accuracy: 0.0778\n",
      "iteration number: 86\t training loss: 2.2916\tvalidation loss: 2.2968\t validation accuracy: 0.0778\n",
      "iteration number: 87\t training loss: 2.2912\tvalidation loss: 2.2965\t validation accuracy: 0.0778\n",
      "iteration number: 88\t training loss: 2.2907\tvalidation loss: 2.2957\t validation accuracy: 0.0778\n",
      "iteration number: 89\t training loss: 2.2902\tvalidation loss: 2.2953\t validation accuracy: 0.0778\n",
      "iteration number: 90\t training loss: 2.2897\tvalidation loss: 2.2952\t validation accuracy: 0.0778\n",
      "iteration number: 91\t training loss: 2.2892\tvalidation loss: 2.2950\t validation accuracy: 0.0778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 92\t training loss: 2.2888\tvalidation loss: 2.2942\t validation accuracy: 0.0778\n",
      "iteration number: 93\t training loss: 2.2883\tvalidation loss: 2.2935\t validation accuracy: 0.0778\n",
      "iteration number: 94\t training loss: 2.2877\tvalidation loss: 2.2929\t validation accuracy: 0.0778\n",
      "iteration number: 95\t training loss: 2.2870\tvalidation loss: 2.2928\t validation accuracy: 0.0778\n",
      "iteration number: 96\t training loss: 2.2865\tvalidation loss: 2.2922\t validation accuracy: 0.0778\n",
      "iteration number: 97\t training loss: 2.2860\tvalidation loss: 2.2914\t validation accuracy: 0.0778\n",
      "iteration number: 98\t training loss: 2.2853\tvalidation loss: 2.2909\t validation accuracy: 0.0778\n",
      "iteration number: 99\t training loss: 2.2846\tvalidation loss: 2.2906\t validation accuracy: 0.0778\n",
      "iteration number: 100\t training loss: 2.2840\tvalidation loss: 2.2901\t validation accuracy: 0.0778\n",
      "iteration number: 101\t training loss: 2.2833\tvalidation loss: 2.2893\t validation accuracy: 0.0778\n",
      "iteration number: 102\t training loss: 2.2825\tvalidation loss: 2.2888\t validation accuracy: 0.0778\n",
      "iteration number: 103\t training loss: 2.2818\tvalidation loss: 2.2879\t validation accuracy: 0.0778\n",
      "iteration number: 104\t training loss: 2.2809\tvalidation loss: 2.2872\t validation accuracy: 0.0778\n",
      "iteration number: 105\t training loss: 2.2801\tvalidation loss: 2.2863\t validation accuracy: 0.0778\n",
      "iteration number: 106\t training loss: 2.2793\tvalidation loss: 2.2858\t validation accuracy: 0.0778\n",
      "iteration number: 107\t training loss: 2.2783\tvalidation loss: 2.2852\t validation accuracy: 0.0778\n",
      "iteration number: 108\t training loss: 2.2772\tvalidation loss: 2.2845\t validation accuracy: 0.0778\n",
      "iteration number: 109\t training loss: 2.2761\tvalidation loss: 2.2837\t validation accuracy: 0.0778\n",
      "iteration number: 110\t training loss: 2.2751\tvalidation loss: 2.2829\t validation accuracy: 0.0800\n",
      "iteration number: 111\t training loss: 2.2740\tvalidation loss: 2.2815\t validation accuracy: 0.0844\n",
      "iteration number: 112\t training loss: 2.2727\tvalidation loss: 2.2807\t validation accuracy: 0.1111\n",
      "iteration number: 113\t training loss: 2.2715\tvalidation loss: 2.2797\t validation accuracy: 0.0933\n",
      "iteration number: 114\t training loss: 2.2701\tvalidation loss: 2.2788\t validation accuracy: 0.0889\n",
      "iteration number: 115\t training loss: 2.2687\tvalidation loss: 2.2777\t validation accuracy: 0.0978\n",
      "iteration number: 116\t training loss: 2.2673\tvalidation loss: 2.2765\t validation accuracy: 0.1111\n",
      "iteration number: 117\t training loss: 2.2659\tvalidation loss: 2.2758\t validation accuracy: 0.1111\n",
      "iteration number: 118\t training loss: 2.2643\tvalidation loss: 2.2745\t validation accuracy: 0.1044\n",
      "iteration number: 119\t training loss: 2.2627\tvalidation loss: 2.2734\t validation accuracy: 0.0889\n",
      "iteration number: 120\t training loss: 2.2614\tvalidation loss: 2.2717\t validation accuracy: 0.0956\n",
      "iteration number: 121\t training loss: 2.2597\tvalidation loss: 2.2704\t validation accuracy: 0.1111\n",
      "iteration number: 122\t training loss: 2.2580\tvalidation loss: 2.2692\t validation accuracy: 0.1289\n",
      "iteration number: 123\t training loss: 2.2564\tvalidation loss: 2.2680\t validation accuracy: 0.1333\n",
      "iteration number: 124\t training loss: 2.2546\tvalidation loss: 2.2660\t validation accuracy: 0.1333\n",
      "iteration number: 125\t training loss: 2.2528\tvalidation loss: 2.2639\t validation accuracy: 0.1333\n",
      "iteration number: 126\t training loss: 2.2510\tvalidation loss: 2.2615\t validation accuracy: 0.1333\n",
      "iteration number: 127\t training loss: 2.2490\tvalidation loss: 2.2598\t validation accuracy: 0.1378\n",
      "iteration number: 128\t training loss: 2.2469\tvalidation loss: 2.2578\t validation accuracy: 0.1356\n",
      "iteration number: 129\t training loss: 2.2445\tvalidation loss: 2.2556\t validation accuracy: 0.1400\n",
      "iteration number: 130\t training loss: 2.2422\tvalidation loss: 2.2534\t validation accuracy: 0.1533\n",
      "iteration number: 131\t training loss: 2.2398\tvalidation loss: 2.2517\t validation accuracy: 0.1533\n",
      "iteration number: 132\t training loss: 2.2374\tvalidation loss: 2.2493\t validation accuracy: 0.1556\n",
      "iteration number: 133\t training loss: 2.2352\tvalidation loss: 2.2471\t validation accuracy: 0.1556\n",
      "iteration number: 134\t training loss: 2.2326\tvalidation loss: 2.2445\t validation accuracy: 0.1644\n",
      "iteration number: 135\t training loss: 2.2303\tvalidation loss: 2.2420\t validation accuracy: 0.1733\n",
      "iteration number: 136\t training loss: 2.2274\tvalidation loss: 2.2392\t validation accuracy: 0.1800\n",
      "iteration number: 137\t training loss: 2.2245\tvalidation loss: 2.2366\t validation accuracy: 0.1911\n",
      "iteration number: 138\t training loss: 2.2210\tvalidation loss: 2.2341\t validation accuracy: 0.1778\n",
      "iteration number: 139\t training loss: 2.2175\tvalidation loss: 2.2312\t validation accuracy: 0.1733\n",
      "iteration number: 140\t training loss: 2.2154\tvalidation loss: 2.2282\t validation accuracy: 0.2044\n",
      "iteration number: 141\t training loss: 2.2120\tvalidation loss: 2.2252\t validation accuracy: 0.1889\n",
      "iteration number: 142\t training loss: 2.2084\tvalidation loss: 2.2223\t validation accuracy: 0.2089\n",
      "iteration number: 143\t training loss: 2.2048\tvalidation loss: 2.2190\t validation accuracy: 0.2156\n",
      "iteration number: 144\t training loss: 2.2009\tvalidation loss: 2.2151\t validation accuracy: 0.2333\n",
      "iteration number: 145\t training loss: 2.1971\tvalidation loss: 2.2116\t validation accuracy: 0.2444\n",
      "iteration number: 146\t training loss: 2.1930\tvalidation loss: 2.2082\t validation accuracy: 0.2400\n",
      "iteration number: 147\t training loss: 2.1895\tvalidation loss: 2.2048\t validation accuracy: 0.2556\n",
      "iteration number: 148\t training loss: 2.1852\tvalidation loss: 2.2014\t validation accuracy: 0.2511\n",
      "iteration number: 149\t training loss: 2.1810\tvalidation loss: 2.1980\t validation accuracy: 0.2533\n",
      "iteration number: 150\t training loss: 2.1756\tvalidation loss: 2.1939\t validation accuracy: 0.2422\n",
      "iteration number: 151\t training loss: 2.1709\tvalidation loss: 2.1898\t validation accuracy: 0.2356\n",
      "iteration number: 152\t training loss: 2.1661\tvalidation loss: 2.1863\t validation accuracy: 0.2267\n",
      "iteration number: 153\t training loss: 2.1610\tvalidation loss: 2.1812\t validation accuracy: 0.2156\n",
      "iteration number: 154\t training loss: 2.1561\tvalidation loss: 2.1766\t validation accuracy: 0.2244\n",
      "iteration number: 155\t training loss: 2.1509\tvalidation loss: 2.1724\t validation accuracy: 0.2067\n",
      "iteration number: 156\t training loss: 2.1461\tvalidation loss: 2.1671\t validation accuracy: 0.2067\n",
      "iteration number: 157\t training loss: 2.1406\tvalidation loss: 2.1618\t validation accuracy: 0.2311\n",
      "iteration number: 158\t training loss: 2.1347\tvalidation loss: 2.1561\t validation accuracy: 0.2156\n",
      "iteration number: 159\t training loss: 2.1296\tvalidation loss: 2.1509\t validation accuracy: 0.2356\n",
      "iteration number: 160\t training loss: 2.1236\tvalidation loss: 2.1443\t validation accuracy: 0.2800\n",
      "iteration number: 161\t training loss: 2.1179\tvalidation loss: 2.1380\t validation accuracy: 0.2889\n",
      "iteration number: 162\t training loss: 2.1107\tvalidation loss: 2.1317\t validation accuracy: 0.3111\n",
      "iteration number: 163\t training loss: 2.1036\tvalidation loss: 2.1258\t validation accuracy: 0.2844\n",
      "iteration number: 164\t training loss: 2.0961\tvalidation loss: 2.1201\t validation accuracy: 0.2578\n",
      "iteration number: 165\t training loss: 2.0891\tvalidation loss: 2.1136\t validation accuracy: 0.2778\n",
      "iteration number: 166\t training loss: 2.0828\tvalidation loss: 2.1071\t validation accuracy: 0.3067\n",
      "iteration number: 167\t training loss: 2.0760\tvalidation loss: 2.1002\t validation accuracy: 0.3156\n",
      "iteration number: 168\t training loss: 2.0681\tvalidation loss: 2.0946\t validation accuracy: 0.2800\n",
      "iteration number: 169\t training loss: 2.0611\tvalidation loss: 2.0868\t validation accuracy: 0.3111\n",
      "iteration number: 170\t training loss: 2.0536\tvalidation loss: 2.0805\t validation accuracy: 0.3067\n",
      "iteration number: 171\t training loss: 2.0461\tvalidation loss: 2.0743\t validation accuracy: 0.3200\n",
      "iteration number: 172\t training loss: 2.0393\tvalidation loss: 2.0675\t validation accuracy: 0.3489\n",
      "iteration number: 173\t training loss: 2.0313\tvalidation loss: 2.0597\t validation accuracy: 0.3533\n",
      "iteration number: 174\t training loss: 2.0235\tvalidation loss: 2.0520\t validation accuracy: 0.3511\n",
      "iteration number: 175\t training loss: 2.0161\tvalidation loss: 2.0454\t validation accuracy: 0.3733\n",
      "iteration number: 176\t training loss: 2.0085\tvalidation loss: 2.0376\t validation accuracy: 0.3822\n",
      "iteration number: 177\t training loss: 2.0009\tvalidation loss: 2.0293\t validation accuracy: 0.3889\n",
      "iteration number: 178\t training loss: 1.9910\tvalidation loss: 2.0213\t validation accuracy: 0.3756\n",
      "iteration number: 179\t training loss: 1.9820\tvalidation loss: 2.0108\t validation accuracy: 0.4133\n",
      "iteration number: 180\t training loss: 1.9733\tvalidation loss: 2.0026\t validation accuracy: 0.3956\n",
      "iteration number: 181\t training loss: 1.9632\tvalidation loss: 1.9945\t validation accuracy: 0.3756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 182\t training loss: 1.9536\tvalidation loss: 1.9865\t validation accuracy: 0.3600\n",
      "iteration number: 183\t training loss: 1.9441\tvalidation loss: 1.9783\t validation accuracy: 0.4000\n",
      "iteration number: 184\t training loss: 1.9339\tvalidation loss: 1.9690\t validation accuracy: 0.3756\n",
      "iteration number: 185\t training loss: 1.9245\tvalidation loss: 1.9613\t validation accuracy: 0.3578\n",
      "iteration number: 186\t training loss: 1.9139\tvalidation loss: 1.9512\t validation accuracy: 0.3689\n",
      "iteration number: 187\t training loss: 1.9036\tvalidation loss: 1.9410\t validation accuracy: 0.3733\n",
      "iteration number: 188\t training loss: 1.8934\tvalidation loss: 1.9301\t validation accuracy: 0.3822\n",
      "iteration number: 189\t training loss: 1.8851\tvalidation loss: 1.9201\t validation accuracy: 0.4200\n",
      "iteration number: 190\t training loss: 1.8742\tvalidation loss: 1.9099\t validation accuracy: 0.4156\n",
      "iteration number: 191\t training loss: 1.8653\tvalidation loss: 1.9004\t validation accuracy: 0.4200\n",
      "iteration number: 192\t training loss: 1.8563\tvalidation loss: 1.8919\t validation accuracy: 0.4600\n",
      "iteration number: 193\t training loss: 1.8466\tvalidation loss: 1.8811\t validation accuracy: 0.5000\n",
      "iteration number: 194\t training loss: 1.8366\tvalidation loss: 1.8706\t validation accuracy: 0.5044\n",
      "iteration number: 195\t training loss: 1.8254\tvalidation loss: 1.8602\t validation accuracy: 0.5422\n",
      "iteration number: 196\t training loss: 1.8108\tvalidation loss: 1.8480\t validation accuracy: 0.5156\n",
      "iteration number: 197\t training loss: 1.8014\tvalidation loss: 1.8389\t validation accuracy: 0.5622\n",
      "iteration number: 198\t training loss: 1.7902\tvalidation loss: 1.8282\t validation accuracy: 0.5533\n",
      "iteration number: 199\t training loss: 1.7782\tvalidation loss: 1.8171\t validation accuracy: 0.5822\n",
      "iteration number: 200\t training loss: 1.7669\tvalidation loss: 1.8055\t validation accuracy: 0.5489\n",
      "iteration number: 201\t training loss: 1.7551\tvalidation loss: 1.7941\t validation accuracy: 0.5622\n",
      "iteration number: 202\t training loss: 1.7436\tvalidation loss: 1.7844\t validation accuracy: 0.5533\n",
      "iteration number: 203\t training loss: 1.7325\tvalidation loss: 1.7749\t validation accuracy: 0.5111\n",
      "iteration number: 204\t training loss: 1.7193\tvalidation loss: 1.7637\t validation accuracy: 0.5067\n",
      "iteration number: 205\t training loss: 1.7074\tvalidation loss: 1.7503\t validation accuracy: 0.5422\n",
      "iteration number: 206\t training loss: 1.6955\tvalidation loss: 1.7400\t validation accuracy: 0.5111\n",
      "iteration number: 207\t training loss: 1.6840\tvalidation loss: 1.7268\t validation accuracy: 0.5511\n",
      "iteration number: 208\t training loss: 1.6728\tvalidation loss: 1.7156\t validation accuracy: 0.5311\n",
      "iteration number: 209\t training loss: 1.6595\tvalidation loss: 1.6996\t validation accuracy: 0.5467\n",
      "iteration number: 210\t training loss: 1.6478\tvalidation loss: 1.6907\t validation accuracy: 0.5444\n",
      "iteration number: 211\t training loss: 1.6358\tvalidation loss: 1.6785\t validation accuracy: 0.5356\n",
      "iteration number: 212\t training loss: 1.6243\tvalidation loss: 1.6624\t validation accuracy: 0.5244\n",
      "iteration number: 213\t training loss: 1.6124\tvalidation loss: 1.6510\t validation accuracy: 0.5400\n",
      "iteration number: 214\t training loss: 1.5999\tvalidation loss: 1.6389\t validation accuracy: 0.5667\n",
      "iteration number: 215\t training loss: 1.5892\tvalidation loss: 1.6308\t validation accuracy: 0.5889\n",
      "iteration number: 216\t training loss: 1.5784\tvalidation loss: 1.6221\t validation accuracy: 0.5644\n",
      "iteration number: 217\t training loss: 1.5657\tvalidation loss: 1.6072\t validation accuracy: 0.5933\n",
      "iteration number: 218\t training loss: 1.5534\tvalidation loss: 1.5968\t validation accuracy: 0.5867\n",
      "iteration number: 219\t training loss: 1.5417\tvalidation loss: 1.5843\t validation accuracy: 0.5911\n",
      "iteration number: 220\t training loss: 1.5305\tvalidation loss: 1.5719\t validation accuracy: 0.5867\n",
      "iteration number: 221\t training loss: 1.5190\tvalidation loss: 1.5586\t validation accuracy: 0.6089\n",
      "iteration number: 222\t training loss: 1.5077\tvalidation loss: 1.5488\t validation accuracy: 0.5800\n",
      "iteration number: 223\t training loss: 1.4964\tvalidation loss: 1.5342\t validation accuracy: 0.5911\n",
      "iteration number: 224\t training loss: 1.4855\tvalidation loss: 1.5227\t validation accuracy: 0.6022\n",
      "iteration number: 225\t training loss: 1.4736\tvalidation loss: 1.5148\t validation accuracy: 0.6000\n",
      "iteration number: 226\t training loss: 1.4627\tvalidation loss: 1.5061\t validation accuracy: 0.6000\n",
      "iteration number: 227\t training loss: 1.4507\tvalidation loss: 1.4929\t validation accuracy: 0.6067\n",
      "iteration number: 228\t training loss: 1.4390\tvalidation loss: 1.4841\t validation accuracy: 0.6067\n",
      "iteration number: 229\t training loss: 1.4297\tvalidation loss: 1.4714\t validation accuracy: 0.6178\n",
      "iteration number: 230\t training loss: 1.4201\tvalidation loss: 1.4589\t validation accuracy: 0.6733\n",
      "iteration number: 231\t training loss: 1.4084\tvalidation loss: 1.4468\t validation accuracy: 0.6733\n",
      "iteration number: 232\t training loss: 1.3966\tvalidation loss: 1.4361\t validation accuracy: 0.6867\n",
      "iteration number: 233\t training loss: 1.3861\tvalidation loss: 1.4221\t validation accuracy: 0.6622\n",
      "iteration number: 234\t training loss: 1.3765\tvalidation loss: 1.4119\t validation accuracy: 0.6800\n",
      "iteration number: 235\t training loss: 1.3703\tvalidation loss: 1.4087\t validation accuracy: 0.6600\n",
      "iteration number: 236\t training loss: 1.3566\tvalidation loss: 1.3936\t validation accuracy: 0.6733\n",
      "iteration number: 237\t training loss: 1.3468\tvalidation loss: 1.3820\t validation accuracy: 0.6933\n",
      "iteration number: 238\t training loss: 1.3342\tvalidation loss: 1.3708\t validation accuracy: 0.6956\n",
      "iteration number: 239\t training loss: 1.3212\tvalidation loss: 1.3583\t validation accuracy: 0.6933\n",
      "iteration number: 240\t training loss: 1.3115\tvalidation loss: 1.3487\t validation accuracy: 0.7200\n",
      "iteration number: 241\t training loss: 1.3027\tvalidation loss: 1.3370\t validation accuracy: 0.7356\n",
      "iteration number: 242\t training loss: 1.2918\tvalidation loss: 1.3249\t validation accuracy: 0.7378\n",
      "iteration number: 243\t training loss: 1.2830\tvalidation loss: 1.3157\t validation accuracy: 0.7267\n",
      "iteration number: 244\t training loss: 1.2695\tvalidation loss: 1.3010\t validation accuracy: 0.7044\n",
      "iteration number: 245\t training loss: 1.2600\tvalidation loss: 1.2923\t validation accuracy: 0.7156\n",
      "iteration number: 246\t training loss: 1.2536\tvalidation loss: 1.2861\t validation accuracy: 0.7133\n",
      "iteration number: 247\t training loss: 1.2409\tvalidation loss: 1.2749\t validation accuracy: 0.6978\n",
      "iteration number: 248\t training loss: 1.2312\tvalidation loss: 1.2614\t validation accuracy: 0.7089\n",
      "iteration number: 249\t training loss: 1.2199\tvalidation loss: 1.2508\t validation accuracy: 0.7133\n",
      "iteration number: 250\t training loss: 1.2096\tvalidation loss: 1.2404\t validation accuracy: 0.7156\n",
      "iteration number: 251\t training loss: 1.2006\tvalidation loss: 1.2328\t validation accuracy: 0.7422\n",
      "iteration number: 252\t training loss: 1.1927\tvalidation loss: 1.2250\t validation accuracy: 0.7511\n",
      "iteration number: 253\t training loss: 1.1825\tvalidation loss: 1.2147\t validation accuracy: 0.7356\n",
      "iteration number: 254\t training loss: 1.1747\tvalidation loss: 1.2073\t validation accuracy: 0.7711\n",
      "iteration number: 255\t training loss: 1.1664\tvalidation loss: 1.1987\t validation accuracy: 0.7289\n",
      "iteration number: 256\t training loss: 1.1578\tvalidation loss: 1.1923\t validation accuracy: 0.7222\n",
      "iteration number: 257\t training loss: 1.1487\tvalidation loss: 1.1834\t validation accuracy: 0.7311\n",
      "iteration number: 258\t training loss: 1.1423\tvalidation loss: 1.1813\t validation accuracy: 0.7533\n",
      "iteration number: 259\t training loss: 1.1349\tvalidation loss: 1.1690\t validation accuracy: 0.7644\n",
      "iteration number: 260\t training loss: 1.1260\tvalidation loss: 1.1584\t validation accuracy: 0.7556\n",
      "iteration number: 261\t training loss: 1.1156\tvalidation loss: 1.1471\t validation accuracy: 0.7400\n",
      "iteration number: 262\t training loss: 1.1076\tvalidation loss: 1.1403\t validation accuracy: 0.7444\n",
      "iteration number: 263\t training loss: 1.1024\tvalidation loss: 1.1322\t validation accuracy: 0.7489\n",
      "iteration number: 264\t training loss: 1.0926\tvalidation loss: 1.1267\t validation accuracy: 0.7178\n",
      "iteration number: 265\t training loss: 1.0842\tvalidation loss: 1.1154\t validation accuracy: 0.7200\n",
      "iteration number: 266\t training loss: 1.0764\tvalidation loss: 1.1052\t validation accuracy: 0.7222\n",
      "iteration number: 267\t training loss: 1.0703\tvalidation loss: 1.0992\t validation accuracy: 0.7578\n",
      "iteration number: 268\t training loss: 1.0636\tvalidation loss: 1.0921\t validation accuracy: 0.7711\n",
      "iteration number: 269\t training loss: 1.0487\tvalidation loss: 1.0783\t validation accuracy: 0.7622\n",
      "iteration number: 270\t training loss: 1.0414\tvalidation loss: 1.0723\t validation accuracy: 0.7644\n",
      "iteration number: 271\t training loss: 1.0357\tvalidation loss: 1.0724\t validation accuracy: 0.7689\n",
      "iteration number: 272\t training loss: 1.0250\tvalidation loss: 1.0548\t validation accuracy: 0.8044\n",
      "iteration number: 273\t training loss: 1.0207\tvalidation loss: 1.0546\t validation accuracy: 0.7978\n",
      "iteration number: 274\t training loss: 1.0132\tvalidation loss: 1.0488\t validation accuracy: 0.8044\n",
      "iteration number: 275\t training loss: 1.0097\tvalidation loss: 1.0505\t validation accuracy: 0.7911\n",
      "iteration number: 276\t training loss: 1.0003\tvalidation loss: 1.0398\t validation accuracy: 0.7800\n",
      "iteration number: 277\t training loss: 0.9924\tvalidation loss: 1.0279\t validation accuracy: 0.8022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 278\t training loss: 0.9845\tvalidation loss: 1.0189\t validation accuracy: 0.7911\n",
      "iteration number: 279\t training loss: 0.9792\tvalidation loss: 1.0060\t validation accuracy: 0.7978\n",
      "iteration number: 280\t training loss: 0.9700\tvalidation loss: 1.0005\t validation accuracy: 0.8022\n",
      "iteration number: 281\t training loss: 0.9648\tvalidation loss: 0.9914\t validation accuracy: 0.8289\n",
      "iteration number: 282\t training loss: 0.9566\tvalidation loss: 0.9821\t validation accuracy: 0.8089\n",
      "iteration number: 283\t training loss: 0.9522\tvalidation loss: 0.9833\t validation accuracy: 0.7644\n",
      "iteration number: 284\t training loss: 0.9489\tvalidation loss: 0.9768\t validation accuracy: 0.7867\n",
      "iteration number: 285\t training loss: 0.9365\tvalidation loss: 0.9708\t validation accuracy: 0.7956\n",
      "iteration number: 286\t training loss: 0.9360\tvalidation loss: 0.9672\t validation accuracy: 0.8244\n",
      "iteration number: 287\t training loss: 0.9313\tvalidation loss: 0.9617\t validation accuracy: 0.8289\n",
      "iteration number: 288\t training loss: 0.9202\tvalidation loss: 0.9487\t validation accuracy: 0.8289\n",
      "iteration number: 289\t training loss: 0.9133\tvalidation loss: 0.9498\t validation accuracy: 0.8089\n",
      "iteration number: 290\t training loss: 0.9047\tvalidation loss: 0.9378\t validation accuracy: 0.8067\n",
      "iteration number: 291\t training loss: 0.9016\tvalidation loss: 0.9363\t validation accuracy: 0.8200\n",
      "iteration number: 292\t training loss: 0.8989\tvalidation loss: 0.9354\t validation accuracy: 0.7978\n",
      "iteration number: 293\t training loss: 0.8912\tvalidation loss: 0.9281\t validation accuracy: 0.8156\n",
      "iteration number: 294\t training loss: 0.8883\tvalidation loss: 0.9260\t validation accuracy: 0.8089\n",
      "iteration number: 295\t training loss: 0.8751\tvalidation loss: 0.9055\t validation accuracy: 0.8111\n",
      "iteration number: 296\t training loss: 0.8688\tvalidation loss: 0.8914\t validation accuracy: 0.8200\n",
      "iteration number: 297\t training loss: 0.8642\tvalidation loss: 0.8871\t validation accuracy: 0.8200\n",
      "iteration number: 298\t training loss: 0.8575\tvalidation loss: 0.8783\t validation accuracy: 0.8178\n",
      "iteration number: 299\t training loss: 0.8542\tvalidation loss: 0.8689\t validation accuracy: 0.8178\n",
      "iteration number: 300\t training loss: 0.8473\tvalidation loss: 0.8670\t validation accuracy: 0.8133\n",
      "iteration number: 301\t training loss: 0.8420\tvalidation loss: 0.8633\t validation accuracy: 0.8067\n",
      "iteration number: 302\t training loss: 0.8404\tvalidation loss: 0.8670\t validation accuracy: 0.7933\n",
      "iteration number: 303\t training loss: 0.8311\tvalidation loss: 0.8534\t validation accuracy: 0.8133\n",
      "iteration number: 304\t training loss: 0.8288\tvalidation loss: 0.8457\t validation accuracy: 0.8133\n",
      "iteration number: 305\t training loss: 0.8217\tvalidation loss: 0.8402\t validation accuracy: 0.8222\n",
      "iteration number: 306\t training loss: 0.8171\tvalidation loss: 0.8401\t validation accuracy: 0.8289\n",
      "iteration number: 307\t training loss: 0.8121\tvalidation loss: 0.8371\t validation accuracy: 0.8333\n",
      "iteration number: 308\t training loss: 0.8045\tvalidation loss: 0.8326\t validation accuracy: 0.8356\n",
      "iteration number: 309\t training loss: 0.8016\tvalidation loss: 0.8326\t validation accuracy: 0.8111\n",
      "iteration number: 310\t training loss: 0.8004\tvalidation loss: 0.8320\t validation accuracy: 0.7867\n",
      "iteration number: 311\t training loss: 0.7908\tvalidation loss: 0.8172\t validation accuracy: 0.8133\n",
      "iteration number: 312\t training loss: 0.7886\tvalidation loss: 0.8191\t validation accuracy: 0.8000\n",
      "iteration number: 313\t training loss: 0.7834\tvalidation loss: 0.8188\t validation accuracy: 0.8133\n",
      "iteration number: 314\t training loss: 0.7804\tvalidation loss: 0.8088\t validation accuracy: 0.7956\n",
      "iteration number: 315\t training loss: 0.7729\tvalidation loss: 0.7929\t validation accuracy: 0.8200\n",
      "iteration number: 316\t training loss: 0.7685\tvalidation loss: 0.7937\t validation accuracy: 0.7911\n",
      "iteration number: 317\t training loss: 0.7608\tvalidation loss: 0.7826\t validation accuracy: 0.8156\n",
      "iteration number: 318\t training loss: 0.7571\tvalidation loss: 0.7836\t validation accuracy: 0.8289\n",
      "iteration number: 319\t training loss: 0.7528\tvalidation loss: 0.7763\t validation accuracy: 0.8111\n",
      "iteration number: 320\t training loss: 0.7532\tvalidation loss: 0.7772\t validation accuracy: 0.7911\n",
      "iteration number: 321\t training loss: 0.7503\tvalidation loss: 0.7748\t validation accuracy: 0.8022\n",
      "iteration number: 322\t training loss: 0.7406\tvalidation loss: 0.7632\t validation accuracy: 0.8133\n",
      "iteration number: 323\t training loss: 0.7358\tvalidation loss: 0.7547\t validation accuracy: 0.8222\n",
      "iteration number: 324\t training loss: 0.7344\tvalidation loss: 0.7538\t validation accuracy: 0.8089\n",
      "iteration number: 325\t training loss: 0.7306\tvalidation loss: 0.7490\t validation accuracy: 0.8178\n",
      "iteration number: 326\t training loss: 0.7260\tvalidation loss: 0.7424\t validation accuracy: 0.8200\n",
      "iteration number: 327\t training loss: 0.7212\tvalidation loss: 0.7382\t validation accuracy: 0.8311\n",
      "iteration number: 328\t training loss: 0.7159\tvalidation loss: 0.7375\t validation accuracy: 0.8356\n",
      "iteration number: 329\t training loss: 0.7126\tvalidation loss: 0.7382\t validation accuracy: 0.8400\n",
      "iteration number: 330\t training loss: 0.7105\tvalidation loss: 0.7383\t validation accuracy: 0.8356\n",
      "iteration number: 331\t training loss: 0.7064\tvalidation loss: 0.7315\t validation accuracy: 0.8400\n",
      "iteration number: 332\t training loss: 0.7012\tvalidation loss: 0.7300\t validation accuracy: 0.8489\n",
      "iteration number: 333\t training loss: 0.6983\tvalidation loss: 0.7278\t validation accuracy: 0.8311\n",
      "iteration number: 334\t training loss: 0.6923\tvalidation loss: 0.7098\t validation accuracy: 0.8400\n",
      "iteration number: 335\t training loss: 0.6905\tvalidation loss: 0.7064\t validation accuracy: 0.8467\n",
      "iteration number: 336\t training loss: 0.6862\tvalidation loss: 0.7025\t validation accuracy: 0.8378\n",
      "iteration number: 337\t training loss: 0.6821\tvalidation loss: 0.7010\t validation accuracy: 0.8356\n",
      "iteration number: 338\t training loss: 0.6775\tvalidation loss: 0.6980\t validation accuracy: 0.8333\n",
      "iteration number: 339\t training loss: 0.6758\tvalidation loss: 0.6930\t validation accuracy: 0.8356\n",
      "iteration number: 340\t training loss: 0.6732\tvalidation loss: 0.6846\t validation accuracy: 0.8489\n",
      "iteration number: 341\t training loss: 0.6659\tvalidation loss: 0.6834\t validation accuracy: 0.8422\n",
      "iteration number: 342\t training loss: 0.6618\tvalidation loss: 0.6802\t validation accuracy: 0.8533\n",
      "iteration number: 343\t training loss: 0.6580\tvalidation loss: 0.6763\t validation accuracy: 0.8444\n",
      "iteration number: 344\t training loss: 0.6567\tvalidation loss: 0.6751\t validation accuracy: 0.8533\n",
      "iteration number: 345\t training loss: 0.6506\tvalidation loss: 0.6752\t validation accuracy: 0.8556\n",
      "iteration number: 346\t training loss: 0.6479\tvalidation loss: 0.6693\t validation accuracy: 0.8600\n",
      "iteration number: 347\t training loss: 0.6493\tvalidation loss: 0.6759\t validation accuracy: 0.8400\n",
      "iteration number: 348\t training loss: 0.6452\tvalidation loss: 0.6694\t validation accuracy: 0.8400\n",
      "iteration number: 349\t training loss: 0.6503\tvalidation loss: 0.6723\t validation accuracy: 0.8244\n",
      "iteration number: 350\t training loss: 0.6404\tvalidation loss: 0.6621\t validation accuracy: 0.8422\n",
      "iteration number: 351\t training loss: 0.6346\tvalidation loss: 0.6482\t validation accuracy: 0.8511\n",
      "iteration number: 352\t training loss: 0.6310\tvalidation loss: 0.6513\t validation accuracy: 0.8489\n",
      "iteration number: 353\t training loss: 0.6304\tvalidation loss: 0.6595\t validation accuracy: 0.8556\n",
      "iteration number: 354\t training loss: 0.6253\tvalidation loss: 0.6466\t validation accuracy: 0.8556\n",
      "iteration number: 355\t training loss: 0.6222\tvalidation loss: 0.6515\t validation accuracy: 0.8356\n",
      "iteration number: 356\t training loss: 0.6182\tvalidation loss: 0.6475\t validation accuracy: 0.8400\n",
      "iteration number: 357\t training loss: 0.6190\tvalidation loss: 0.6535\t validation accuracy: 0.8289\n",
      "iteration number: 358\t training loss: 0.6120\tvalidation loss: 0.6346\t validation accuracy: 0.8600\n",
      "iteration number: 359\t training loss: 0.6094\tvalidation loss: 0.6234\t validation accuracy: 0.8556\n",
      "iteration number: 360\t training loss: 0.6093\tvalidation loss: 0.6229\t validation accuracy: 0.8556\n",
      "iteration number: 361\t training loss: 0.6046\tvalidation loss: 0.6174\t validation accuracy: 0.8578\n",
      "iteration number: 362\t training loss: 0.6062\tvalidation loss: 0.6177\t validation accuracy: 0.8578\n",
      "iteration number: 363\t training loss: 0.6016\tvalidation loss: 0.6242\t validation accuracy: 0.8667\n",
      "iteration number: 364\t training loss: 0.5984\tvalidation loss: 0.6221\t validation accuracy: 0.8667\n",
      "iteration number: 365\t training loss: 0.5931\tvalidation loss: 0.6180\t validation accuracy: 0.8533\n",
      "iteration number: 366\t training loss: 0.5869\tvalidation loss: 0.6014\t validation accuracy: 0.8556\n",
      "iteration number: 367\t training loss: 0.5844\tvalidation loss: 0.6057\t validation accuracy: 0.8622\n",
      "iteration number: 368\t training loss: 0.5838\tvalidation loss: 0.6095\t validation accuracy: 0.8467\n",
      "iteration number: 369\t training loss: 0.5814\tvalidation loss: 0.6037\t validation accuracy: 0.8489\n",
      "iteration number: 370\t training loss: 0.5783\tvalidation loss: 0.6043\t validation accuracy: 0.8533\n",
      "iteration number: 371\t training loss: 0.5769\tvalidation loss: 0.6041\t validation accuracy: 0.8622\n",
      "iteration number: 372\t training loss: 0.5721\tvalidation loss: 0.5904\t validation accuracy: 0.8689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 373\t training loss: 0.5686\tvalidation loss: 0.5798\t validation accuracy: 0.8667\n",
      "iteration number: 374\t training loss: 0.5706\tvalidation loss: 0.5812\t validation accuracy: 0.8711\n",
      "iteration number: 375\t training loss: 0.5688\tvalidation loss: 0.5813\t validation accuracy: 0.8733\n",
      "iteration number: 376\t training loss: 0.5657\tvalidation loss: 0.5813\t validation accuracy: 0.8733\n",
      "iteration number: 377\t training loss: 0.5585\tvalidation loss: 0.5702\t validation accuracy: 0.8667\n",
      "iteration number: 378\t training loss: 0.5552\tvalidation loss: 0.5752\t validation accuracy: 0.8644\n",
      "iteration number: 379\t training loss: 0.5546\tvalidation loss: 0.5744\t validation accuracy: 0.8667\n",
      "iteration number: 380\t training loss: 0.5520\tvalidation loss: 0.5739\t validation accuracy: 0.8733\n",
      "iteration number: 381\t training loss: 0.5483\tvalidation loss: 0.5648\t validation accuracy: 0.8711\n",
      "iteration number: 382\t training loss: 0.5465\tvalidation loss: 0.5617\t validation accuracy: 0.8822\n",
      "iteration number: 383\t training loss: 0.5447\tvalidation loss: 0.5622\t validation accuracy: 0.8844\n",
      "iteration number: 384\t training loss: 0.5428\tvalidation loss: 0.5635\t validation accuracy: 0.8689\n",
      "iteration number: 385\t training loss: 0.5398\tvalidation loss: 0.5579\t validation accuracy: 0.8711\n",
      "iteration number: 386\t training loss: 0.5374\tvalidation loss: 0.5604\t validation accuracy: 0.8756\n",
      "iteration number: 387\t training loss: 0.5339\tvalidation loss: 0.5539\t validation accuracy: 0.8889\n",
      "iteration number: 388\t training loss: 0.5327\tvalidation loss: 0.5497\t validation accuracy: 0.8911\n",
      "iteration number: 389\t training loss: 0.5330\tvalidation loss: 0.5454\t validation accuracy: 0.8978\n",
      "iteration number: 390\t training loss: 0.5281\tvalidation loss: 0.5415\t validation accuracy: 0.8822\n",
      "iteration number: 391\t training loss: 0.5243\tvalidation loss: 0.5382\t validation accuracy: 0.8867\n",
      "iteration number: 392\t training loss: 0.5206\tvalidation loss: 0.5358\t validation accuracy: 0.8822\n",
      "iteration number: 393\t training loss: 0.5212\tvalidation loss: 0.5405\t validation accuracy: 0.8800\n",
      "iteration number: 394\t training loss: 0.5211\tvalidation loss: 0.5412\t validation accuracy: 0.8800\n",
      "iteration number: 395\t training loss: 0.5153\tvalidation loss: 0.5318\t validation accuracy: 0.8822\n",
      "iteration number: 396\t training loss: 0.5153\tvalidation loss: 0.5330\t validation accuracy: 0.8844\n",
      "iteration number: 397\t training loss: 0.5109\tvalidation loss: 0.5296\t validation accuracy: 0.8956\n",
      "iteration number: 398\t training loss: 0.5087\tvalidation loss: 0.5193\t validation accuracy: 0.8933\n",
      "iteration number: 399\t training loss: 0.5123\tvalidation loss: 0.5209\t validation accuracy: 0.8911\n",
      "iteration number: 400\t training loss: 0.5121\tvalidation loss: 0.5163\t validation accuracy: 0.8956\n",
      "iteration number: 401\t training loss: 0.5099\tvalidation loss: 0.5152\t validation accuracy: 0.8933\n",
      "iteration number: 402\t training loss: 0.5120\tvalidation loss: 0.5229\t validation accuracy: 0.8911\n",
      "iteration number: 403\t training loss: 0.5053\tvalidation loss: 0.5101\t validation accuracy: 0.8956\n",
      "iteration number: 404\t training loss: 0.5095\tvalidation loss: 0.5094\t validation accuracy: 0.8933\n",
      "iteration number: 405\t training loss: 0.4999\tvalidation loss: 0.5060\t validation accuracy: 0.8978\n",
      "iteration number: 406\t training loss: 0.4934\tvalidation loss: 0.5053\t validation accuracy: 0.8978\n",
      "iteration number: 407\t training loss: 0.4925\tvalidation loss: 0.5066\t validation accuracy: 0.9089\n",
      "iteration number: 408\t training loss: 0.4894\tvalidation loss: 0.5116\t validation accuracy: 0.8844\n",
      "iteration number: 409\t training loss: 0.4880\tvalidation loss: 0.5071\t validation accuracy: 0.8822\n",
      "iteration number: 410\t training loss: 0.4843\tvalidation loss: 0.4996\t validation accuracy: 0.9000\n",
      "iteration number: 411\t training loss: 0.4845\tvalidation loss: 0.4965\t validation accuracy: 0.8956\n",
      "iteration number: 412\t training loss: 0.4835\tvalidation loss: 0.5013\t validation accuracy: 0.8889\n",
      "iteration number: 413\t training loss: 0.4821\tvalidation loss: 0.4990\t validation accuracy: 0.8933\n",
      "iteration number: 414\t training loss: 0.4808\tvalidation loss: 0.4961\t validation accuracy: 0.8844\n",
      "iteration number: 415\t training loss: 0.4802\tvalidation loss: 0.4978\t validation accuracy: 0.8822\n",
      "iteration number: 416\t training loss: 0.4766\tvalidation loss: 0.4960\t validation accuracy: 0.9000\n",
      "iteration number: 417\t training loss: 0.4735\tvalidation loss: 0.4900\t validation accuracy: 0.8933\n",
      "iteration number: 418\t training loss: 0.4701\tvalidation loss: 0.4844\t validation accuracy: 0.8844\n",
      "iteration number: 419\t training loss: 0.4688\tvalidation loss: 0.4872\t validation accuracy: 0.8911\n",
      "iteration number: 420\t training loss: 0.4663\tvalidation loss: 0.4790\t validation accuracy: 0.9089\n",
      "iteration number: 421\t training loss: 0.4646\tvalidation loss: 0.4798\t validation accuracy: 0.9022\n",
      "iteration number: 422\t training loss: 0.4621\tvalidation loss: 0.4825\t validation accuracy: 0.8911\n",
      "iteration number: 423\t training loss: 0.4620\tvalidation loss: 0.4840\t validation accuracy: 0.8911\n",
      "iteration number: 424\t training loss: 0.4643\tvalidation loss: 0.4898\t validation accuracy: 0.8933\n",
      "iteration number: 425\t training loss: 0.4578\tvalidation loss: 0.4724\t validation accuracy: 0.9156\n",
      "iteration number: 426\t training loss: 0.4562\tvalidation loss: 0.4673\t validation accuracy: 0.9000\n",
      "iteration number: 427\t training loss: 0.4574\tvalidation loss: 0.4693\t validation accuracy: 0.9022\n",
      "iteration number: 428\t training loss: 0.4549\tvalidation loss: 0.4619\t validation accuracy: 0.9044\n",
      "iteration number: 429\t training loss: 0.4528\tvalidation loss: 0.4641\t validation accuracy: 0.9000\n",
      "iteration number: 430\t training loss: 0.4512\tvalidation loss: 0.4647\t validation accuracy: 0.8933\n",
      "iteration number: 431\t training loss: 0.4555\tvalidation loss: 0.4642\t validation accuracy: 0.8867\n",
      "iteration number: 432\t training loss: 0.4539\tvalidation loss: 0.4597\t validation accuracy: 0.8933\n",
      "iteration number: 433\t training loss: 0.4535\tvalidation loss: 0.4613\t validation accuracy: 0.8844\n",
      "iteration number: 434\t training loss: 0.4466\tvalidation loss: 0.4550\t validation accuracy: 0.8978\n",
      "iteration number: 435\t training loss: 0.4405\tvalidation loss: 0.4523\t validation accuracy: 0.9067\n",
      "iteration number: 436\t training loss: 0.4407\tvalidation loss: 0.4561\t validation accuracy: 0.9022\n",
      "iteration number: 437\t training loss: 0.4423\tvalidation loss: 0.4628\t validation accuracy: 0.9022\n",
      "iteration number: 438\t training loss: 0.4406\tvalidation loss: 0.4614\t validation accuracy: 0.8956\n",
      "iteration number: 439\t training loss: 0.4428\tvalidation loss: 0.4657\t validation accuracy: 0.8911\n",
      "iteration number: 440\t training loss: 0.4388\tvalidation loss: 0.4551\t validation accuracy: 0.9022\n",
      "iteration number: 441\t training loss: 0.4329\tvalidation loss: 0.4471\t validation accuracy: 0.9111\n",
      "iteration number: 442\t training loss: 0.4284\tvalidation loss: 0.4380\t validation accuracy: 0.9111\n",
      "iteration number: 443\t training loss: 0.4298\tvalidation loss: 0.4314\t validation accuracy: 0.9044\n",
      "iteration number: 444\t training loss: 0.4262\tvalidation loss: 0.4336\t validation accuracy: 0.9044\n",
      "iteration number: 445\t training loss: 0.4272\tvalidation loss: 0.4416\t validation accuracy: 0.8933\n",
      "iteration number: 446\t training loss: 0.4306\tvalidation loss: 0.4345\t validation accuracy: 0.8956\n",
      "iteration number: 447\t training loss: 0.4250\tvalidation loss: 0.4327\t validation accuracy: 0.9089\n",
      "iteration number: 448\t training loss: 0.4258\tvalidation loss: 0.4342\t validation accuracy: 0.9000\n",
      "iteration number: 449\t training loss: 0.4238\tvalidation loss: 0.4364\t validation accuracy: 0.9044\n",
      "iteration number: 450\t training loss: 0.4180\tvalidation loss: 0.4311\t validation accuracy: 0.9089\n",
      "iteration number: 451\t training loss: 0.4162\tvalidation loss: 0.4340\t validation accuracy: 0.9133\n",
      "iteration number: 452\t training loss: 0.4170\tvalidation loss: 0.4300\t validation accuracy: 0.9089\n",
      "iteration number: 453\t training loss: 0.4151\tvalidation loss: 0.4216\t validation accuracy: 0.9244\n",
      "iteration number: 454\t training loss: 0.4217\tvalidation loss: 0.4358\t validation accuracy: 0.8889\n",
      "iteration number: 455\t training loss: 0.4149\tvalidation loss: 0.4288\t validation accuracy: 0.9022\n",
      "iteration number: 456\t training loss: 0.4113\tvalidation loss: 0.4287\t validation accuracy: 0.9044\n",
      "iteration number: 457\t training loss: 0.4156\tvalidation loss: 0.4396\t validation accuracy: 0.8956\n",
      "iteration number: 458\t training loss: 0.4112\tvalidation loss: 0.4304\t validation accuracy: 0.9044\n",
      "iteration number: 459\t training loss: 0.4089\tvalidation loss: 0.4224\t validation accuracy: 0.9111\n",
      "iteration number: 460\t training loss: 0.4064\tvalidation loss: 0.4151\t validation accuracy: 0.9089\n",
      "iteration number: 461\t training loss: 0.4021\tvalidation loss: 0.4123\t validation accuracy: 0.9156\n",
      "iteration number: 462\t training loss: 0.4054\tvalidation loss: 0.4164\t validation accuracy: 0.9178\n",
      "iteration number: 463\t training loss: 0.4013\tvalidation loss: 0.4099\t validation accuracy: 0.9178\n",
      "iteration number: 464\t training loss: 0.4002\tvalidation loss: 0.4141\t validation accuracy: 0.9178\n",
      "iteration number: 465\t training loss: 0.4008\tvalidation loss: 0.4096\t validation accuracy: 0.9089\n",
      "iteration number: 466\t training loss: 0.3995\tvalidation loss: 0.4165\t validation accuracy: 0.9044\n",
      "iteration number: 467\t training loss: 0.3966\tvalidation loss: 0.4144\t validation accuracy: 0.9067\n",
      "iteration number: 468\t training loss: 0.3969\tvalidation loss: 0.4161\t validation accuracy: 0.9044\n",
      "iteration number: 469\t training loss: 0.4004\tvalidation loss: 0.4172\t validation accuracy: 0.9022\n",
      "iteration number: 470\t training loss: 0.3975\tvalidation loss: 0.4148\t validation accuracy: 0.9044\n",
      "iteration number: 471\t training loss: 0.3893\tvalidation loss: 0.3997\t validation accuracy: 0.9111\n",
      "iteration number: 472\t training loss: 0.3879\tvalidation loss: 0.3926\t validation accuracy: 0.9222\n",
      "iteration number: 473\t training loss: 0.3878\tvalidation loss: 0.3876\t validation accuracy: 0.9200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 474\t training loss: 0.3875\tvalidation loss: 0.3856\t validation accuracy: 0.9200\n",
      "iteration number: 475\t training loss: 0.3843\tvalidation loss: 0.3912\t validation accuracy: 0.9111\n",
      "iteration number: 476\t training loss: 0.3849\tvalidation loss: 0.3900\t validation accuracy: 0.9067\n",
      "iteration number: 477\t training loss: 0.3842\tvalidation loss: 0.3932\t validation accuracy: 0.9089\n",
      "iteration number: 478\t training loss: 0.3815\tvalidation loss: 0.3907\t validation accuracy: 0.9044\n",
      "iteration number: 479\t training loss: 0.3785\tvalidation loss: 0.3892\t validation accuracy: 0.9244\n",
      "iteration number: 480\t training loss: 0.3791\tvalidation loss: 0.3941\t validation accuracy: 0.9089\n",
      "iteration number: 481\t training loss: 0.3843\tvalidation loss: 0.3908\t validation accuracy: 0.9289\n",
      "iteration number: 482\t training loss: 0.3777\tvalidation loss: 0.3884\t validation accuracy: 0.9289\n",
      "iteration number: 483\t training loss: 0.3781\tvalidation loss: 0.3905\t validation accuracy: 0.9178\n",
      "iteration number: 484\t training loss: 0.3726\tvalidation loss: 0.3814\t validation accuracy: 0.9244\n",
      "iteration number: 485\t training loss: 0.3787\tvalidation loss: 0.3855\t validation accuracy: 0.9156\n",
      "iteration number: 486\t training loss: 0.3737\tvalidation loss: 0.3803\t validation accuracy: 0.9267\n",
      "iteration number: 487\t training loss: 0.3741\tvalidation loss: 0.3833\t validation accuracy: 0.9178\n",
      "iteration number: 488\t training loss: 0.3681\tvalidation loss: 0.3788\t validation accuracy: 0.9222\n",
      "iteration number: 489\t training loss: 0.3735\tvalidation loss: 0.3861\t validation accuracy: 0.9222\n",
      "iteration number: 490\t training loss: 0.3683\tvalidation loss: 0.3833\t validation accuracy: 0.9222\n",
      "iteration number: 491\t training loss: 0.3685\tvalidation loss: 0.3793\t validation accuracy: 0.9111\n",
      "iteration number: 492\t training loss: 0.3697\tvalidation loss: 0.3848\t validation accuracy: 0.9222\n",
      "iteration number: 493\t training loss: 0.3740\tvalidation loss: 0.3913\t validation accuracy: 0.9133\n",
      "iteration number: 494\t training loss: 0.3660\tvalidation loss: 0.3804\t validation accuracy: 0.9133\n",
      "iteration number: 495\t training loss: 0.3657\tvalidation loss: 0.3827\t validation accuracy: 0.9089\n",
      "iteration number: 496\t training loss: 0.3644\tvalidation loss: 0.3831\t validation accuracy: 0.9156\n",
      "iteration number: 497\t training loss: 0.3603\tvalidation loss: 0.3706\t validation accuracy: 0.9222\n",
      "iteration number: 498\t training loss: 0.3571\tvalidation loss: 0.3681\t validation accuracy: 0.9133\n",
      "iteration number: 499\t training loss: 0.3559\tvalidation loss: 0.3624\t validation accuracy: 0.9111\n",
      "iteration number: 500\t training loss: 0.3569\tvalidation loss: 0.3645\t validation accuracy: 0.9178\n",
      "iteration number: 501\t training loss: 0.3574\tvalidation loss: 0.3617\t validation accuracy: 0.9200\n",
      "iteration number: 502\t training loss: 0.3578\tvalidation loss: 0.3635\t validation accuracy: 0.9222\n",
      "iteration number: 503\t training loss: 0.3523\tvalidation loss: 0.3574\t validation accuracy: 0.9200\n",
      "iteration number: 504\t training loss: 0.3569\tvalidation loss: 0.3535\t validation accuracy: 0.9200\n",
      "iteration number: 505\t training loss: 0.3496\tvalidation loss: 0.3568\t validation accuracy: 0.9133\n",
      "iteration number: 506\t training loss: 0.3484\tvalidation loss: 0.3561\t validation accuracy: 0.9222\n",
      "iteration number: 507\t training loss: 0.3492\tvalidation loss: 0.3545\t validation accuracy: 0.9200\n",
      "iteration number: 508\t training loss: 0.3487\tvalidation loss: 0.3532\t validation accuracy: 0.9200\n",
      "iteration number: 509\t training loss: 0.3471\tvalidation loss: 0.3505\t validation accuracy: 0.9178\n",
      "iteration number: 510\t training loss: 0.3443\tvalidation loss: 0.3569\t validation accuracy: 0.9289\n",
      "iteration number: 511\t training loss: 0.3431\tvalidation loss: 0.3518\t validation accuracy: 0.9156\n",
      "iteration number: 512\t training loss: 0.3421\tvalidation loss: 0.3544\t validation accuracy: 0.9222\n",
      "iteration number: 513\t training loss: 0.3415\tvalidation loss: 0.3526\t validation accuracy: 0.9200\n",
      "iteration number: 514\t training loss: 0.3397\tvalidation loss: 0.3454\t validation accuracy: 0.9200\n",
      "iteration number: 515\t training loss: 0.3419\tvalidation loss: 0.3483\t validation accuracy: 0.9244\n",
      "iteration number: 516\t training loss: 0.3379\tvalidation loss: 0.3447\t validation accuracy: 0.9222\n",
      "iteration number: 517\t training loss: 0.3398\tvalidation loss: 0.3449\t validation accuracy: 0.9289\n",
      "iteration number: 518\t training loss: 0.3391\tvalidation loss: 0.3388\t validation accuracy: 0.9178\n",
      "iteration number: 519\t training loss: 0.3362\tvalidation loss: 0.3413\t validation accuracy: 0.9156\n",
      "iteration number: 520\t training loss: 0.3379\tvalidation loss: 0.3407\t validation accuracy: 0.9111\n",
      "iteration number: 521\t training loss: 0.3381\tvalidation loss: 0.3453\t validation accuracy: 0.9111\n",
      "iteration number: 522\t training loss: 0.3342\tvalidation loss: 0.3367\t validation accuracy: 0.9156\n",
      "iteration number: 523\t training loss: 0.3328\tvalidation loss: 0.3383\t validation accuracy: 0.9244\n",
      "iteration number: 524\t training loss: 0.3345\tvalidation loss: 0.3398\t validation accuracy: 0.9289\n",
      "iteration number: 525\t training loss: 0.3370\tvalidation loss: 0.3450\t validation accuracy: 0.9267\n",
      "iteration number: 526\t training loss: 0.3309\tvalidation loss: 0.3359\t validation accuracy: 0.9311\n",
      "iteration number: 527\t training loss: 0.3287\tvalidation loss: 0.3361\t validation accuracy: 0.9356\n",
      "iteration number: 528\t training loss: 0.3267\tvalidation loss: 0.3378\t validation accuracy: 0.9244\n",
      "iteration number: 529\t training loss: 0.3316\tvalidation loss: 0.3526\t validation accuracy: 0.9200\n",
      "iteration number: 530\t training loss: 0.3279\tvalidation loss: 0.3498\t validation accuracy: 0.9200\n",
      "iteration number: 531\t training loss: 0.3333\tvalidation loss: 0.3530\t validation accuracy: 0.9178\n",
      "iteration number: 532\t training loss: 0.3275\tvalidation loss: 0.3423\t validation accuracy: 0.9244\n",
      "iteration number: 533\t training loss: 0.3230\tvalidation loss: 0.3362\t validation accuracy: 0.9356\n",
      "iteration number: 534\t training loss: 0.3230\tvalidation loss: 0.3342\t validation accuracy: 0.9333\n",
      "iteration number: 535\t training loss: 0.3249\tvalidation loss: 0.3327\t validation accuracy: 0.9222\n",
      "iteration number: 536\t training loss: 0.3243\tvalidation loss: 0.3353\t validation accuracy: 0.9267\n",
      "iteration number: 537\t training loss: 0.3212\tvalidation loss: 0.3299\t validation accuracy: 0.9267\n",
      "iteration number: 538\t training loss: 0.3184\tvalidation loss: 0.3241\t validation accuracy: 0.9311\n",
      "iteration number: 539\t training loss: 0.3195\tvalidation loss: 0.3237\t validation accuracy: 0.9333\n",
      "iteration number: 540\t training loss: 0.3197\tvalidation loss: 0.3278\t validation accuracy: 0.9178\n",
      "iteration number: 541\t training loss: 0.3195\tvalidation loss: 0.3264\t validation accuracy: 0.9356\n",
      "iteration number: 542\t training loss: 0.3162\tvalidation loss: 0.3177\t validation accuracy: 0.9222\n",
      "iteration number: 543\t training loss: 0.3152\tvalidation loss: 0.3215\t validation accuracy: 0.9178\n",
      "iteration number: 544\t training loss: 0.3131\tvalidation loss: 0.3169\t validation accuracy: 0.9289\n",
      "iteration number: 545\t training loss: 0.3121\tvalidation loss: 0.3221\t validation accuracy: 0.9311\n",
      "iteration number: 546\t training loss: 0.3146\tvalidation loss: 0.3192\t validation accuracy: 0.9267\n",
      "iteration number: 547\t training loss: 0.3131\tvalidation loss: 0.3205\t validation accuracy: 0.9244\n",
      "iteration number: 548\t training loss: 0.3121\tvalidation loss: 0.3244\t validation accuracy: 0.9156\n",
      "iteration number: 549\t training loss: 0.3118\tvalidation loss: 0.3261\t validation accuracy: 0.9200\n",
      "iteration number: 550\t training loss: 0.3155\tvalidation loss: 0.3249\t validation accuracy: 0.9156\n",
      "iteration number: 551\t training loss: 0.3115\tvalidation loss: 0.3297\t validation accuracy: 0.9178\n",
      "iteration number: 552\t training loss: 0.3085\tvalidation loss: 0.3247\t validation accuracy: 0.9222\n",
      "iteration number: 553\t training loss: 0.3080\tvalidation loss: 0.3173\t validation accuracy: 0.9178\n",
      "iteration number: 554\t training loss: 0.3070\tvalidation loss: 0.3137\t validation accuracy: 0.9222\n",
      "iteration number: 555\t training loss: 0.3040\tvalidation loss: 0.3159\t validation accuracy: 0.9289\n",
      "iteration number: 556\t training loss: 0.3031\tvalidation loss: 0.3130\t validation accuracy: 0.9311\n",
      "iteration number: 557\t training loss: 0.3036\tvalidation loss: 0.3168\t validation accuracy: 0.9267\n",
      "iteration number: 558\t training loss: 0.3071\tvalidation loss: 0.3254\t validation accuracy: 0.9222\n",
      "iteration number: 559\t training loss: 0.3046\tvalidation loss: 0.3218\t validation accuracy: 0.9222\n",
      "iteration number: 560\t training loss: 0.3033\tvalidation loss: 0.3161\t validation accuracy: 0.9289\n",
      "iteration number: 561\t training loss: 0.3013\tvalidation loss: 0.3156\t validation accuracy: 0.9289\n",
      "iteration number: 562\t training loss: 0.2999\tvalidation loss: 0.3074\t validation accuracy: 0.9311\n",
      "iteration number: 563\t training loss: 0.3021\tvalidation loss: 0.3090\t validation accuracy: 0.9289\n",
      "iteration number: 564\t training loss: 0.2989\tvalidation loss: 0.3094\t validation accuracy: 0.9178\n",
      "iteration number: 565\t training loss: 0.2963\tvalidation loss: 0.3073\t validation accuracy: 0.9333\n",
      "iteration number: 566\t training loss: 0.2962\tvalidation loss: 0.3090\t validation accuracy: 0.9378\n",
      "iteration number: 567\t training loss: 0.2972\tvalidation loss: 0.3107\t validation accuracy: 0.9267\n",
      "iteration number: 568\t training loss: 0.2996\tvalidation loss: 0.3153\t validation accuracy: 0.9378\n",
      "iteration number: 569\t training loss: 0.2970\tvalidation loss: 0.3118\t validation accuracy: 0.9333\n",
      "iteration number: 570\t training loss: 0.2994\tvalidation loss: 0.3153\t validation accuracy: 0.9267\n",
      "iteration number: 571\t training loss: 0.2925\tvalidation loss: 0.3041\t validation accuracy: 0.9311\n",
      "iteration number: 572\t training loss: 0.2922\tvalidation loss: 0.3054\t validation accuracy: 0.9356\n",
      "iteration number: 573\t training loss: 0.2910\tvalidation loss: 0.3006\t validation accuracy: 0.9333\n",
      "iteration number: 574\t training loss: 0.2902\tvalidation loss: 0.3012\t validation accuracy: 0.9333\n",
      "iteration number: 575\t training loss: 0.2897\tvalidation loss: 0.2997\t validation accuracy: 0.9289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 576\t training loss: 0.2896\tvalidation loss: 0.2974\t validation accuracy: 0.9244\n",
      "iteration number: 577\t training loss: 0.2909\tvalidation loss: 0.2981\t validation accuracy: 0.9222\n",
      "iteration number: 578\t training loss: 0.2903\tvalidation loss: 0.3025\t validation accuracy: 0.9289\n",
      "iteration number: 579\t training loss: 0.2894\tvalidation loss: 0.3054\t validation accuracy: 0.9311\n",
      "iteration number: 580\t training loss: 0.2887\tvalidation loss: 0.3022\t validation accuracy: 0.9378\n",
      "iteration number: 581\t training loss: 0.2869\tvalidation loss: 0.2996\t validation accuracy: 0.9267\n",
      "iteration number: 582\t training loss: 0.2870\tvalidation loss: 0.2953\t validation accuracy: 0.9289\n",
      "iteration number: 583\t training loss: 0.2839\tvalidation loss: 0.2923\t validation accuracy: 0.9378\n",
      "iteration number: 584\t training loss: 0.2840\tvalidation loss: 0.2923\t validation accuracy: 0.9267\n",
      "iteration number: 585\t training loss: 0.2832\tvalidation loss: 0.2905\t validation accuracy: 0.9267\n",
      "iteration number: 586\t training loss: 0.2848\tvalidation loss: 0.2951\t validation accuracy: 0.9244\n",
      "iteration number: 587\t training loss: 0.2870\tvalidation loss: 0.2940\t validation accuracy: 0.9222\n",
      "iteration number: 588\t training loss: 0.2829\tvalidation loss: 0.2900\t validation accuracy: 0.9378\n",
      "iteration number: 589\t training loss: 0.2844\tvalidation loss: 0.2888\t validation accuracy: 0.9244\n",
      "iteration number: 590\t training loss: 0.2808\tvalidation loss: 0.2899\t validation accuracy: 0.9311\n",
      "iteration number: 591\t training loss: 0.2783\tvalidation loss: 0.2906\t validation accuracy: 0.9422\n",
      "iteration number: 592\t training loss: 0.2776\tvalidation loss: 0.2855\t validation accuracy: 0.9400\n",
      "iteration number: 593\t training loss: 0.2778\tvalidation loss: 0.2888\t validation accuracy: 0.9333\n",
      "iteration number: 594\t training loss: 0.2784\tvalidation loss: 0.2805\t validation accuracy: 0.9378\n",
      "iteration number: 595\t training loss: 0.2770\tvalidation loss: 0.2884\t validation accuracy: 0.9378\n",
      "iteration number: 596\t training loss: 0.2763\tvalidation loss: 0.2847\t validation accuracy: 0.9400\n",
      "iteration number: 597\t training loss: 0.2837\tvalidation loss: 0.2879\t validation accuracy: 0.9400\n",
      "iteration number: 598\t training loss: 0.2781\tvalidation loss: 0.2832\t validation accuracy: 0.9333\n",
      "iteration number: 599\t training loss: 0.2771\tvalidation loss: 0.2788\t validation accuracy: 0.9400\n",
      "iteration number: 600\t training loss: 0.2876\tvalidation loss: 0.2819\t validation accuracy: 0.9333\n",
      "iteration number: 601\t training loss: 0.2774\tvalidation loss: 0.2794\t validation accuracy: 0.9400\n",
      "iteration number: 602\t training loss: 0.2715\tvalidation loss: 0.2837\t validation accuracy: 0.9311\n",
      "iteration number: 603\t training loss: 0.2713\tvalidation loss: 0.2866\t validation accuracy: 0.9333\n",
      "iteration number: 604\t training loss: 0.2706\tvalidation loss: 0.2828\t validation accuracy: 0.9333\n",
      "iteration number: 605\t training loss: 0.2706\tvalidation loss: 0.2847\t validation accuracy: 0.9356\n",
      "iteration number: 606\t training loss: 0.2692\tvalidation loss: 0.2839\t validation accuracy: 0.9356\n",
      "iteration number: 607\t training loss: 0.2692\tvalidation loss: 0.2804\t validation accuracy: 0.9400\n",
      "iteration number: 608\t training loss: 0.2680\tvalidation loss: 0.2826\t validation accuracy: 0.9400\n",
      "iteration number: 609\t training loss: 0.2672\tvalidation loss: 0.2785\t validation accuracy: 0.9333\n",
      "iteration number: 610\t training loss: 0.2698\tvalidation loss: 0.2821\t validation accuracy: 0.9222\n",
      "iteration number: 611\t training loss: 0.2681\tvalidation loss: 0.2793\t validation accuracy: 0.9289\n",
      "iteration number: 612\t training loss: 0.2666\tvalidation loss: 0.2777\t validation accuracy: 0.9267\n",
      "iteration number: 613\t training loss: 0.2656\tvalidation loss: 0.2737\t validation accuracy: 0.9311\n",
      "iteration number: 614\t training loss: 0.2666\tvalidation loss: 0.2822\t validation accuracy: 0.9400\n",
      "iteration number: 615\t training loss: 0.2648\tvalidation loss: 0.2781\t validation accuracy: 0.9356\n",
      "iteration number: 616\t training loss: 0.2640\tvalidation loss: 0.2765\t validation accuracy: 0.9356\n",
      "iteration number: 617\t training loss: 0.2648\tvalidation loss: 0.2847\t validation accuracy: 0.9333\n",
      "iteration number: 618\t training loss: 0.2639\tvalidation loss: 0.2849\t validation accuracy: 0.9333\n",
      "iteration number: 619\t training loss: 0.2632\tvalidation loss: 0.2829\t validation accuracy: 0.9333\n",
      "iteration number: 620\t training loss: 0.2624\tvalidation loss: 0.2787\t validation accuracy: 0.9311\n",
      "iteration number: 621\t training loss: 0.2616\tvalidation loss: 0.2788\t validation accuracy: 0.9289\n",
      "iteration number: 622\t training loss: 0.2612\tvalidation loss: 0.2750\t validation accuracy: 0.9289\n",
      "iteration number: 623\t training loss: 0.2610\tvalidation loss: 0.2735\t validation accuracy: 0.9356\n",
      "iteration number: 624\t training loss: 0.2620\tvalidation loss: 0.2717\t validation accuracy: 0.9333\n",
      "iteration number: 625\t training loss: 0.2598\tvalidation loss: 0.2687\t validation accuracy: 0.9333\n",
      "iteration number: 626\t training loss: 0.2581\tvalidation loss: 0.2696\t validation accuracy: 0.9356\n",
      "iteration number: 627\t training loss: 0.2594\tvalidation loss: 0.2679\t validation accuracy: 0.9267\n",
      "iteration number: 628\t training loss: 0.2572\tvalidation loss: 0.2671\t validation accuracy: 0.9289\n",
      "iteration number: 629\t training loss: 0.2589\tvalidation loss: 0.2625\t validation accuracy: 0.9311\n",
      "iteration number: 630\t training loss: 0.2583\tvalidation loss: 0.2668\t validation accuracy: 0.9289\n",
      "iteration number: 631\t training loss: 0.2618\tvalidation loss: 0.2642\t validation accuracy: 0.9356\n",
      "iteration number: 632\t training loss: 0.2575\tvalidation loss: 0.2631\t validation accuracy: 0.9356\n",
      "iteration number: 633\t training loss: 0.2551\tvalidation loss: 0.2667\t validation accuracy: 0.9311\n",
      "iteration number: 634\t training loss: 0.2533\tvalidation loss: 0.2635\t validation accuracy: 0.9356\n",
      "iteration number: 635\t training loss: 0.2549\tvalidation loss: 0.2604\t validation accuracy: 0.9289\n",
      "iteration number: 636\t training loss: 0.2538\tvalidation loss: 0.2610\t validation accuracy: 0.9311\n",
      "iteration number: 637\t training loss: 0.2524\tvalidation loss: 0.2666\t validation accuracy: 0.9400\n",
      "iteration number: 638\t training loss: 0.2513\tvalidation loss: 0.2637\t validation accuracy: 0.9444\n",
      "iteration number: 639\t training loss: 0.2524\tvalidation loss: 0.2708\t validation accuracy: 0.9311\n",
      "iteration number: 640\t training loss: 0.2526\tvalidation loss: 0.2714\t validation accuracy: 0.9311\n",
      "iteration number: 641\t training loss: 0.2545\tvalidation loss: 0.2781\t validation accuracy: 0.9267\n",
      "iteration number: 642\t training loss: 0.2535\tvalidation loss: 0.2784\t validation accuracy: 0.9311\n",
      "iteration number: 643\t training loss: 0.2506\tvalidation loss: 0.2687\t validation accuracy: 0.9356\n",
      "iteration number: 644\t training loss: 0.2533\tvalidation loss: 0.2750\t validation accuracy: 0.9333\n",
      "iteration number: 645\t training loss: 0.2495\tvalidation loss: 0.2701\t validation accuracy: 0.9333\n",
      "iteration number: 646\t training loss: 0.2509\tvalidation loss: 0.2749\t validation accuracy: 0.9333\n",
      "iteration number: 647\t training loss: 0.2539\tvalidation loss: 0.2789\t validation accuracy: 0.9267\n",
      "iteration number: 648\t training loss: 0.2512\tvalidation loss: 0.2749\t validation accuracy: 0.9267\n",
      "iteration number: 649\t training loss: 0.2479\tvalidation loss: 0.2713\t validation accuracy: 0.9356\n",
      "iteration number: 650\t training loss: 0.2465\tvalidation loss: 0.2661\t validation accuracy: 0.9378\n",
      "iteration number: 651\t training loss: 0.2448\tvalidation loss: 0.2562\t validation accuracy: 0.9378\n",
      "iteration number: 652\t training loss: 0.2465\tvalidation loss: 0.2612\t validation accuracy: 0.9422\n",
      "iteration number: 653\t training loss: 0.2463\tvalidation loss: 0.2589\t validation accuracy: 0.9311\n",
      "iteration number: 654\t training loss: 0.2473\tvalidation loss: 0.2597\t validation accuracy: 0.9289\n",
      "iteration number: 655\t training loss: 0.2438\tvalidation loss: 0.2616\t validation accuracy: 0.9400\n",
      "iteration number: 656\t training loss: 0.2441\tvalidation loss: 0.2568\t validation accuracy: 0.9422\n",
      "iteration number: 657\t training loss: 0.2450\tvalidation loss: 0.2523\t validation accuracy: 0.9333\n",
      "iteration number: 658\t training loss: 0.2470\tvalidation loss: 0.2590\t validation accuracy: 0.9467\n",
      "iteration number: 659\t training loss: 0.2407\tvalidation loss: 0.2530\t validation accuracy: 0.9422\n",
      "iteration number: 660\t training loss: 0.2419\tvalidation loss: 0.2472\t validation accuracy: 0.9356\n",
      "iteration number: 661\t training loss: 0.2398\tvalidation loss: 0.2552\t validation accuracy: 0.9400\n",
      "iteration number: 662\t training loss: 0.2445\tvalidation loss: 0.2526\t validation accuracy: 0.9422\n",
      "iteration number: 663\t training loss: 0.2413\tvalidation loss: 0.2532\t validation accuracy: 0.9378\n",
      "iteration number: 664\t training loss: 0.2405\tvalidation loss: 0.2621\t validation accuracy: 0.9333\n",
      "iteration number: 665\t training loss: 0.2400\tvalidation loss: 0.2579\t validation accuracy: 0.9422\n",
      "iteration number: 666\t training loss: 0.2444\tvalidation loss: 0.2732\t validation accuracy: 0.9267\n",
      "iteration number: 667\t training loss: 0.2429\tvalidation loss: 0.2693\t validation accuracy: 0.9267\n",
      "iteration number: 668\t training loss: 0.2427\tvalidation loss: 0.2682\t validation accuracy: 0.9311\n",
      "iteration number: 669\t training loss: 0.2387\tvalidation loss: 0.2562\t validation accuracy: 0.9333\n",
      "iteration number: 670\t training loss: 0.2423\tvalidation loss: 0.2586\t validation accuracy: 0.9289\n",
      "iteration number: 671\t training loss: 0.2405\tvalidation loss: 0.2543\t validation accuracy: 0.9289\n",
      "iteration number: 672\t training loss: 0.2378\tvalidation loss: 0.2594\t validation accuracy: 0.9356\n",
      "iteration number: 673\t training loss: 0.2346\tvalidation loss: 0.2467\t validation accuracy: 0.9422\n",
      "iteration number: 674\t training loss: 0.2348\tvalidation loss: 0.2487\t validation accuracy: 0.9400\n",
      "iteration number: 675\t training loss: 0.2359\tvalidation loss: 0.2553\t validation accuracy: 0.9333\n",
      "iteration number: 676\t training loss: 0.2360\tvalidation loss: 0.2498\t validation accuracy: 0.9356\n",
      "iteration number: 677\t training loss: 0.2345\tvalidation loss: 0.2476\t validation accuracy: 0.9378\n",
      "iteration number: 678\t training loss: 0.2346\tvalidation loss: 0.2532\t validation accuracy: 0.9356\n",
      "iteration number: 679\t training loss: 0.2359\tvalidation loss: 0.2530\t validation accuracy: 0.9244\n",
      "iteration number: 680\t training loss: 0.2371\tvalidation loss: 0.2612\t validation accuracy: 0.9289\n",
      "iteration number: 681\t training loss: 0.2369\tvalidation loss: 0.2617\t validation accuracy: 0.9311\n",
      "iteration number: 682\t training loss: 0.2321\tvalidation loss: 0.2514\t validation accuracy: 0.9356\n",
      "iteration number: 683\t training loss: 0.2313\tvalidation loss: 0.2502\t validation accuracy: 0.9378\n",
      "iteration number: 684\t training loss: 0.2306\tvalidation loss: 0.2487\t validation accuracy: 0.9400\n",
      "iteration number: 685\t training loss: 0.2324\tvalidation loss: 0.2489\t validation accuracy: 0.9356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 686\t training loss: 0.2298\tvalidation loss: 0.2410\t validation accuracy: 0.9356\n",
      "iteration number: 687\t training loss: 0.2277\tvalidation loss: 0.2408\t validation accuracy: 0.9422\n",
      "iteration number: 688\t training loss: 0.2290\tvalidation loss: 0.2355\t validation accuracy: 0.9467\n",
      "iteration number: 689\t training loss: 0.2273\tvalidation loss: 0.2385\t validation accuracy: 0.9444\n",
      "iteration number: 690\t training loss: 0.2271\tvalidation loss: 0.2412\t validation accuracy: 0.9422\n",
      "iteration number: 691\t training loss: 0.2298\tvalidation loss: 0.2519\t validation accuracy: 0.9356\n",
      "iteration number: 692\t training loss: 0.2280\tvalidation loss: 0.2471\t validation accuracy: 0.9356\n",
      "iteration number: 693\t training loss: 0.2270\tvalidation loss: 0.2449\t validation accuracy: 0.9356\n",
      "iteration number: 694\t training loss: 0.2285\tvalidation loss: 0.2502\t validation accuracy: 0.9356\n",
      "iteration number: 695\t training loss: 0.2252\tvalidation loss: 0.2393\t validation accuracy: 0.9422\n",
      "iteration number: 696\t training loss: 0.2242\tvalidation loss: 0.2331\t validation accuracy: 0.9422\n",
      "iteration number: 697\t training loss: 0.2257\tvalidation loss: 0.2331\t validation accuracy: 0.9467\n",
      "iteration number: 698\t training loss: 0.2291\tvalidation loss: 0.2358\t validation accuracy: 0.9444\n",
      "iteration number: 699\t training loss: 0.2248\tvalidation loss: 0.2312\t validation accuracy: 0.9444\n",
      "iteration number: 700\t training loss: 0.2250\tvalidation loss: 0.2290\t validation accuracy: 0.9422\n",
      "iteration number: 701\t training loss: 0.2255\tvalidation loss: 0.2370\t validation accuracy: 0.9467\n",
      "iteration number: 702\t training loss: 0.2262\tvalidation loss: 0.2323\t validation accuracy: 0.9444\n",
      "iteration number: 703\t training loss: 0.2230\tvalidation loss: 0.2295\t validation accuracy: 0.9422\n",
      "iteration number: 704\t training loss: 0.2230\tvalidation loss: 0.2349\t validation accuracy: 0.9422\n",
      "iteration number: 705\t training loss: 0.2218\tvalidation loss: 0.2415\t validation accuracy: 0.9378\n",
      "iteration number: 706\t training loss: 0.2216\tvalidation loss: 0.2427\t validation accuracy: 0.9356\n",
      "iteration number: 707\t training loss: 0.2198\tvalidation loss: 0.2360\t validation accuracy: 0.9378\n",
      "iteration number: 708\t training loss: 0.2188\tvalidation loss: 0.2289\t validation accuracy: 0.9444\n",
      "iteration number: 709\t training loss: 0.2202\tvalidation loss: 0.2279\t validation accuracy: 0.9400\n",
      "iteration number: 710\t training loss: 0.2192\tvalidation loss: 0.2404\t validation accuracy: 0.9422\n",
      "iteration number: 711\t training loss: 0.2179\tvalidation loss: 0.2377\t validation accuracy: 0.9422\n",
      "iteration number: 712\t training loss: 0.2190\tvalidation loss: 0.2347\t validation accuracy: 0.9311\n",
      "iteration number: 713\t training loss: 0.2194\tvalidation loss: 0.2372\t validation accuracy: 0.9333\n",
      "iteration number: 714\t training loss: 0.2187\tvalidation loss: 0.2355\t validation accuracy: 0.9356\n",
      "iteration number: 715\t training loss: 0.2176\tvalidation loss: 0.2353\t validation accuracy: 0.9400\n",
      "iteration number: 716\t training loss: 0.2181\tvalidation loss: 0.2346\t validation accuracy: 0.9311\n",
      "iteration number: 717\t training loss: 0.2237\tvalidation loss: 0.2458\t validation accuracy: 0.9356\n",
      "iteration number: 718\t training loss: 0.2224\tvalidation loss: 0.2462\t validation accuracy: 0.9333\n",
      "iteration number: 719\t training loss: 0.2255\tvalidation loss: 0.2531\t validation accuracy: 0.9333\n",
      "iteration number: 720\t training loss: 0.2228\tvalidation loss: 0.2455\t validation accuracy: 0.9267\n",
      "iteration number: 721\t training loss: 0.2197\tvalidation loss: 0.2411\t validation accuracy: 0.9356\n",
      "iteration number: 722\t training loss: 0.2133\tvalidation loss: 0.2296\t validation accuracy: 0.9444\n",
      "iteration number: 723\t training loss: 0.2140\tvalidation loss: 0.2310\t validation accuracy: 0.9400\n",
      "iteration number: 724\t training loss: 0.2135\tvalidation loss: 0.2337\t validation accuracy: 0.9400\n",
      "iteration number: 725\t training loss: 0.2141\tvalidation loss: 0.2295\t validation accuracy: 0.9422\n",
      "iteration number: 726\t training loss: 0.2133\tvalidation loss: 0.2323\t validation accuracy: 0.9400\n",
      "iteration number: 727\t training loss: 0.2115\tvalidation loss: 0.2253\t validation accuracy: 0.9422\n",
      "iteration number: 728\t training loss: 0.2130\tvalidation loss: 0.2233\t validation accuracy: 0.9467\n",
      "iteration number: 729\t training loss: 0.2119\tvalidation loss: 0.2208\t validation accuracy: 0.9444\n",
      "iteration number: 730\t training loss: 0.2112\tvalidation loss: 0.2279\t validation accuracy: 0.9422\n",
      "iteration number: 731\t training loss: 0.2121\tvalidation loss: 0.2256\t validation accuracy: 0.9356\n",
      "iteration number: 732\t training loss: 0.2121\tvalidation loss: 0.2225\t validation accuracy: 0.9311\n",
      "iteration number: 733\t training loss: 0.2139\tvalidation loss: 0.2223\t validation accuracy: 0.9311\n",
      "iteration number: 734\t training loss: 0.2145\tvalidation loss: 0.2203\t validation accuracy: 0.9333\n",
      "iteration number: 735\t training loss: 0.2120\tvalidation loss: 0.2218\t validation accuracy: 0.9467\n",
      "iteration number: 736\t training loss: 0.2123\tvalidation loss: 0.2223\t validation accuracy: 0.9467\n",
      "iteration number: 737\t training loss: 0.2115\tvalidation loss: 0.2211\t validation accuracy: 0.9489\n",
      "iteration number: 738\t training loss: 0.2094\tvalidation loss: 0.2180\t validation accuracy: 0.9467\n",
      "iteration number: 739\t training loss: 0.2089\tvalidation loss: 0.2178\t validation accuracy: 0.9444\n",
      "iteration number: 740\t training loss: 0.2101\tvalidation loss: 0.2292\t validation accuracy: 0.9378\n",
      "iteration number: 741\t training loss: 0.2080\tvalidation loss: 0.2229\t validation accuracy: 0.9422\n",
      "iteration number: 742\t training loss: 0.2082\tvalidation loss: 0.2304\t validation accuracy: 0.9378\n",
      "iteration number: 743\t training loss: 0.2090\tvalidation loss: 0.2343\t validation accuracy: 0.9333\n",
      "iteration number: 744\t training loss: 0.2074\tvalidation loss: 0.2307\t validation accuracy: 0.9400\n",
      "iteration number: 745\t training loss: 0.2059\tvalidation loss: 0.2250\t validation accuracy: 0.9400\n",
      "iteration number: 746\t training loss: 0.2073\tvalidation loss: 0.2267\t validation accuracy: 0.9356\n",
      "iteration number: 747\t training loss: 0.2064\tvalidation loss: 0.2209\t validation accuracy: 0.9467\n",
      "iteration number: 748\t training loss: 0.2070\tvalidation loss: 0.2297\t validation accuracy: 0.9356\n",
      "iteration number: 749\t training loss: 0.2058\tvalidation loss: 0.2288\t validation accuracy: 0.9400\n",
      "iteration number: 750\t training loss: 0.2055\tvalidation loss: 0.2263\t validation accuracy: 0.9378\n",
      "iteration number: 751\t training loss: 0.2063\tvalidation loss: 0.2255\t validation accuracy: 0.9333\n",
      "iteration number: 752\t training loss: 0.2060\tvalidation loss: 0.2231\t validation accuracy: 0.9444\n",
      "iteration number: 753\t training loss: 0.2035\tvalidation loss: 0.2232\t validation accuracy: 0.9422\n",
      "iteration number: 754\t training loss: 0.2028\tvalidation loss: 0.2227\t validation accuracy: 0.9422\n",
      "iteration number: 755\t training loss: 0.2031\tvalidation loss: 0.2188\t validation accuracy: 0.9467\n",
      "iteration number: 756\t training loss: 0.2034\tvalidation loss: 0.2198\t validation accuracy: 0.9467\n",
      "iteration number: 757\t training loss: 0.2024\tvalidation loss: 0.2135\t validation accuracy: 0.9444\n",
      "iteration number: 758\t training loss: 0.2017\tvalidation loss: 0.2141\t validation accuracy: 0.9422\n",
      "iteration number: 759\t training loss: 0.2038\tvalidation loss: 0.2104\t validation accuracy: 0.9467\n",
      "iteration number: 760\t training loss: 0.2007\tvalidation loss: 0.2113\t validation accuracy: 0.9422\n",
      "iteration number: 761\t training loss: 0.2013\tvalidation loss: 0.2175\t validation accuracy: 0.9400\n",
      "iteration number: 762\t training loss: 0.2021\tvalidation loss: 0.2178\t validation accuracy: 0.9356\n",
      "iteration number: 763\t training loss: 0.1989\tvalidation loss: 0.2173\t validation accuracy: 0.9422\n",
      "iteration number: 764\t training loss: 0.1993\tvalidation loss: 0.2164\t validation accuracy: 0.9400\n",
      "iteration number: 765\t training loss: 0.2010\tvalidation loss: 0.2182\t validation accuracy: 0.9356\n",
      "iteration number: 766\t training loss: 0.2010\tvalidation loss: 0.2149\t validation accuracy: 0.9378\n",
      "iteration number: 767\t training loss: 0.2017\tvalidation loss: 0.2118\t validation accuracy: 0.9400\n",
      "iteration number: 768\t training loss: 0.2028\tvalidation loss: 0.2095\t validation accuracy: 0.9400\n",
      "iteration number: 769\t training loss: 0.2033\tvalidation loss: 0.2085\t validation accuracy: 0.9400\n",
      "iteration number: 770\t training loss: 0.2010\tvalidation loss: 0.2102\t validation accuracy: 0.9378\n",
      "iteration number: 771\t training loss: 0.1999\tvalidation loss: 0.2115\t validation accuracy: 0.9378\n",
      "iteration number: 772\t training loss: 0.1994\tvalidation loss: 0.2099\t validation accuracy: 0.9422\n",
      "iteration number: 773\t training loss: 0.1984\tvalidation loss: 0.2085\t validation accuracy: 0.9422\n",
      "iteration number: 774\t training loss: 0.1986\tvalidation loss: 0.2103\t validation accuracy: 0.9444\n",
      "iteration number: 775\t training loss: 0.1994\tvalidation loss: 0.2190\t validation accuracy: 0.9444\n",
      "iteration number: 776\t training loss: 0.1968\tvalidation loss: 0.2148\t validation accuracy: 0.9422\n",
      "iteration number: 777\t training loss: 0.1978\tvalidation loss: 0.2091\t validation accuracy: 0.9467\n",
      "iteration number: 778\t training loss: 0.1963\tvalidation loss: 0.2067\t validation accuracy: 0.9511\n",
      "iteration number: 779\t training loss: 0.1978\tvalidation loss: 0.2122\t validation accuracy: 0.9511\n",
      "iteration number: 780\t training loss: 0.1949\tvalidation loss: 0.2130\t validation accuracy: 0.9378\n",
      "iteration number: 781\t training loss: 0.1950\tvalidation loss: 0.2096\t validation accuracy: 0.9378\n",
      "iteration number: 782\t training loss: 0.1960\tvalidation loss: 0.2130\t validation accuracy: 0.9422\n",
      "iteration number: 783\t training loss: 0.1966\tvalidation loss: 0.2108\t validation accuracy: 0.9400\n",
      "iteration number: 784\t training loss: 0.1975\tvalidation loss: 0.2089\t validation accuracy: 0.9356\n",
      "iteration number: 785\t training loss: 0.2003\tvalidation loss: 0.2024\t validation accuracy: 0.9422\n",
      "iteration number: 786\t training loss: 0.1950\tvalidation loss: 0.2061\t validation accuracy: 0.9400\n",
      "iteration number: 787\t training loss: 0.1949\tvalidation loss: 0.2005\t validation accuracy: 0.9422\n",
      "iteration number: 788\t training loss: 0.1947\tvalidation loss: 0.2072\t validation accuracy: 0.9378\n",
      "iteration number: 789\t training loss: 0.1918\tvalidation loss: 0.2008\t validation accuracy: 0.9467\n",
      "iteration number: 790\t training loss: 0.1925\tvalidation loss: 0.2019\t validation accuracy: 0.9422\n",
      "iteration number: 791\t training loss: 0.1945\tvalidation loss: 0.1978\t validation accuracy: 0.9533\n",
      "iteration number: 792\t training loss: 0.1908\tvalidation loss: 0.2006\t validation accuracy: 0.9489\n",
      "iteration number: 793\t training loss: 0.1911\tvalidation loss: 0.1987\t validation accuracy: 0.9489\n",
      "iteration number: 794\t training loss: 0.1909\tvalidation loss: 0.2076\t validation accuracy: 0.9444\n",
      "iteration number: 795\t training loss: 0.1903\tvalidation loss: 0.2070\t validation accuracy: 0.9422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 796\t training loss: 0.1906\tvalidation loss: 0.2100\t validation accuracy: 0.9422\n",
      "iteration number: 797\t training loss: 0.1895\tvalidation loss: 0.2089\t validation accuracy: 0.9422\n",
      "iteration number: 798\t training loss: 0.1888\tvalidation loss: 0.2061\t validation accuracy: 0.9400\n",
      "iteration number: 799\t training loss: 0.1874\tvalidation loss: 0.2045\t validation accuracy: 0.9467\n",
      "iteration number: 800\t training loss: 0.1890\tvalidation loss: 0.2034\t validation accuracy: 0.9489\n",
      "iteration number: 801\t training loss: 0.1881\tvalidation loss: 0.2071\t validation accuracy: 0.9489\n",
      "iteration number: 802\t training loss: 0.1881\tvalidation loss: 0.2048\t validation accuracy: 0.9467\n",
      "iteration number: 803\t training loss: 0.1886\tvalidation loss: 0.2119\t validation accuracy: 0.9444\n",
      "iteration number: 804\t training loss: 0.1889\tvalidation loss: 0.2022\t validation accuracy: 0.9467\n",
      "iteration number: 805\t training loss: 0.1943\tvalidation loss: 0.2053\t validation accuracy: 0.9467\n",
      "iteration number: 806\t training loss: 0.1894\tvalidation loss: 0.1984\t validation accuracy: 0.9533\n",
      "iteration number: 807\t training loss: 0.1927\tvalidation loss: 0.1958\t validation accuracy: 0.9533\n",
      "iteration number: 808\t training loss: 0.1924\tvalidation loss: 0.1929\t validation accuracy: 0.9533\n",
      "iteration number: 809\t training loss: 0.1867\tvalidation loss: 0.1934\t validation accuracy: 0.9533\n",
      "iteration number: 810\t training loss: 0.1871\tvalidation loss: 0.1948\t validation accuracy: 0.9489\n",
      "iteration number: 811\t training loss: 0.1869\tvalidation loss: 0.1927\t validation accuracy: 0.9489\n",
      "iteration number: 812\t training loss: 0.1854\tvalidation loss: 0.1973\t validation accuracy: 0.9444\n",
      "iteration number: 813\t training loss: 0.1844\tvalidation loss: 0.1942\t validation accuracy: 0.9511\n",
      "iteration number: 814\t training loss: 0.1840\tvalidation loss: 0.1956\t validation accuracy: 0.9489\n",
      "iteration number: 815\t training loss: 0.1841\tvalidation loss: 0.1960\t validation accuracy: 0.9511\n",
      "iteration number: 816\t training loss: 0.1847\tvalidation loss: 0.2012\t validation accuracy: 0.9444\n",
      "iteration number: 817\t training loss: 0.1838\tvalidation loss: 0.1955\t validation accuracy: 0.9489\n",
      "iteration number: 818\t training loss: 0.1835\tvalidation loss: 0.2034\t validation accuracy: 0.9422\n",
      "iteration number: 819\t training loss: 0.1837\tvalidation loss: 0.2037\t validation accuracy: 0.9467\n",
      "iteration number: 820\t training loss: 0.1838\tvalidation loss: 0.2003\t validation accuracy: 0.9489\n",
      "iteration number: 821\t training loss: 0.1833\tvalidation loss: 0.1999\t validation accuracy: 0.9467\n",
      "iteration number: 822\t training loss: 0.1838\tvalidation loss: 0.2076\t validation accuracy: 0.9467\n",
      "iteration number: 823\t training loss: 0.1852\tvalidation loss: 0.2048\t validation accuracy: 0.9422\n",
      "iteration number: 824\t training loss: 0.1823\tvalidation loss: 0.2035\t validation accuracy: 0.9444\n",
      "iteration number: 825\t training loss: 0.1828\tvalidation loss: 0.2043\t validation accuracy: 0.9400\n",
      "iteration number: 826\t training loss: 0.1813\tvalidation loss: 0.2017\t validation accuracy: 0.9422\n",
      "iteration number: 827\t training loss: 0.1807\tvalidation loss: 0.1982\t validation accuracy: 0.9467\n",
      "iteration number: 828\t training loss: 0.1803\tvalidation loss: 0.1990\t validation accuracy: 0.9444\n",
      "iteration number: 829\t training loss: 0.1806\tvalidation loss: 0.1970\t validation accuracy: 0.9467\n",
      "iteration number: 830\t training loss: 0.1820\tvalidation loss: 0.1938\t validation accuracy: 0.9489\n",
      "iteration number: 831\t training loss: 0.1844\tvalidation loss: 0.1927\t validation accuracy: 0.9533\n",
      "iteration number: 832\t training loss: 0.1818\tvalidation loss: 0.1966\t validation accuracy: 0.9444\n",
      "iteration number: 833\t training loss: 0.1814\tvalidation loss: 0.1981\t validation accuracy: 0.9422\n",
      "iteration number: 834\t training loss: 0.1793\tvalidation loss: 0.1965\t validation accuracy: 0.9422\n",
      "iteration number: 835\t training loss: 0.1784\tvalidation loss: 0.1975\t validation accuracy: 0.9444\n",
      "iteration number: 836\t training loss: 0.1777\tvalidation loss: 0.1989\t validation accuracy: 0.9444\n",
      "iteration number: 837\t training loss: 0.1775\tvalidation loss: 0.1985\t validation accuracy: 0.9444\n",
      "iteration number: 838\t training loss: 0.1783\tvalidation loss: 0.2000\t validation accuracy: 0.9467\n",
      "iteration number: 839\t training loss: 0.1795\tvalidation loss: 0.1997\t validation accuracy: 0.9467\n",
      "iteration number: 840\t training loss: 0.1783\tvalidation loss: 0.2003\t validation accuracy: 0.9467\n",
      "iteration number: 841\t training loss: 0.1778\tvalidation loss: 0.1986\t validation accuracy: 0.9467\n",
      "iteration number: 842\t training loss: 0.1765\tvalidation loss: 0.1949\t validation accuracy: 0.9467\n",
      "iteration number: 843\t training loss: 0.1786\tvalidation loss: 0.1988\t validation accuracy: 0.9444\n",
      "iteration number: 844\t training loss: 0.1777\tvalidation loss: 0.2005\t validation accuracy: 0.9444\n",
      "iteration number: 845\t training loss: 0.1796\tvalidation loss: 0.1958\t validation accuracy: 0.9511\n",
      "iteration number: 846\t training loss: 0.1774\tvalidation loss: 0.1922\t validation accuracy: 0.9533\n",
      "iteration number: 847\t training loss: 0.1774\tvalidation loss: 0.1922\t validation accuracy: 0.9533\n",
      "iteration number: 848\t training loss: 0.1781\tvalidation loss: 0.1910\t validation accuracy: 0.9467\n",
      "iteration number: 849\t training loss: 0.1806\tvalidation loss: 0.1882\t validation accuracy: 0.9489\n",
      "iteration number: 850\t training loss: 0.1785\tvalidation loss: 0.1857\t validation accuracy: 0.9489\n",
      "iteration number: 851\t training loss: 0.1768\tvalidation loss: 0.1865\t validation accuracy: 0.9511\n",
      "iteration number: 852\t training loss: 0.1749\tvalidation loss: 0.1853\t validation accuracy: 0.9556\n",
      "iteration number: 853\t training loss: 0.1749\tvalidation loss: 0.1856\t validation accuracy: 0.9489\n",
      "iteration number: 854\t training loss: 0.1738\tvalidation loss: 0.1871\t validation accuracy: 0.9467\n",
      "iteration number: 855\t training loss: 0.1735\tvalidation loss: 0.1858\t validation accuracy: 0.9578\n",
      "iteration number: 856\t training loss: 0.1748\tvalidation loss: 0.1857\t validation accuracy: 0.9467\n",
      "iteration number: 857\t training loss: 0.1746\tvalidation loss: 0.1902\t validation accuracy: 0.9489\n",
      "iteration number: 858\t training loss: 0.1751\tvalidation loss: 0.1942\t validation accuracy: 0.9489\n",
      "iteration number: 859\t training loss: 0.1731\tvalidation loss: 0.1866\t validation accuracy: 0.9533\n",
      "iteration number: 860\t training loss: 0.1733\tvalidation loss: 0.1902\t validation accuracy: 0.9533\n",
      "iteration number: 861\t training loss: 0.1726\tvalidation loss: 0.1910\t validation accuracy: 0.9489\n",
      "iteration number: 862\t training loss: 0.1714\tvalidation loss: 0.1859\t validation accuracy: 0.9533\n",
      "iteration number: 863\t training loss: 0.1718\tvalidation loss: 0.1850\t validation accuracy: 0.9511\n",
      "iteration number: 864\t training loss: 0.1721\tvalidation loss: 0.1898\t validation accuracy: 0.9489\n",
      "iteration number: 865\t training loss: 0.1722\tvalidation loss: 0.1904\t validation accuracy: 0.9533\n",
      "iteration number: 866\t training loss: 0.1716\tvalidation loss: 0.1871\t validation accuracy: 0.9511\n",
      "iteration number: 867\t training loss: 0.1724\tvalidation loss: 0.1858\t validation accuracy: 0.9533\n",
      "iteration number: 868\t training loss: 0.1754\tvalidation loss: 0.1818\t validation accuracy: 0.9578\n",
      "iteration number: 869\t training loss: 0.1715\tvalidation loss: 0.1843\t validation accuracy: 0.9533\n",
      "iteration number: 870\t training loss: 0.1719\tvalidation loss: 0.1834\t validation accuracy: 0.9533\n",
      "iteration number: 871\t training loss: 0.1722\tvalidation loss: 0.1850\t validation accuracy: 0.9511\n",
      "iteration number: 872\t training loss: 0.1703\tvalidation loss: 0.1835\t validation accuracy: 0.9556\n",
      "iteration number: 873\t training loss: 0.1711\tvalidation loss: 0.1837\t validation accuracy: 0.9511\n",
      "iteration number: 874\t training loss: 0.1721\tvalidation loss: 0.1830\t validation accuracy: 0.9511\n",
      "iteration number: 875\t training loss: 0.1722\tvalidation loss: 0.1821\t validation accuracy: 0.9511\n",
      "iteration number: 876\t training loss: 0.1693\tvalidation loss: 0.1842\t validation accuracy: 0.9533\n",
      "iteration number: 877\t training loss: 0.1687\tvalidation loss: 0.1850\t validation accuracy: 0.9489\n",
      "iteration number: 878\t training loss: 0.1691\tvalidation loss: 0.1823\t validation accuracy: 0.9489\n",
      "iteration number: 879\t training loss: 0.1695\tvalidation loss: 0.1818\t validation accuracy: 0.9578\n",
      "iteration number: 880\t training loss: 0.1704\tvalidation loss: 0.1806\t validation accuracy: 0.9622\n",
      "iteration number: 881\t training loss: 0.1693\tvalidation loss: 0.1806\t validation accuracy: 0.9578\n",
      "iteration number: 882\t training loss: 0.1693\tvalidation loss: 0.1792\t validation accuracy: 0.9622\n",
      "iteration number: 883\t training loss: 0.1676\tvalidation loss: 0.1828\t validation accuracy: 0.9533\n",
      "iteration number: 884\t training loss: 0.1690\tvalidation loss: 0.1831\t validation accuracy: 0.9467\n",
      "iteration number: 885\t training loss: 0.1684\tvalidation loss: 0.1806\t validation accuracy: 0.9511\n",
      "iteration number: 886\t training loss: 0.1672\tvalidation loss: 0.1795\t validation accuracy: 0.9533\n",
      "iteration number: 887\t training loss: 0.1661\tvalidation loss: 0.1831\t validation accuracy: 0.9556\n",
      "iteration number: 888\t training loss: 0.1664\tvalidation loss: 0.1821\t validation accuracy: 0.9578\n",
      "iteration number: 889\t training loss: 0.1672\tvalidation loss: 0.1804\t validation accuracy: 0.9578\n",
      "iteration number: 890\t training loss: 0.1666\tvalidation loss: 0.1801\t validation accuracy: 0.9533\n",
      "iteration number: 891\t training loss: 0.1682\tvalidation loss: 0.1813\t validation accuracy: 0.9578\n",
      "iteration number: 892\t training loss: 0.1724\tvalidation loss: 0.1811\t validation accuracy: 0.9600\n",
      "iteration number: 893\t training loss: 0.1669\tvalidation loss: 0.1850\t validation accuracy: 0.9556\n",
      "iteration number: 894\t training loss: 0.1662\tvalidation loss: 0.1795\t validation accuracy: 0.9511\n",
      "iteration number: 895\t training loss: 0.1662\tvalidation loss: 0.1779\t validation accuracy: 0.9578\n",
      "iteration number: 896\t training loss: 0.1655\tvalidation loss: 0.1803\t validation accuracy: 0.9556\n",
      "iteration number: 897\t training loss: 0.1654\tvalidation loss: 0.1841\t validation accuracy: 0.9556\n",
      "iteration number: 898\t training loss: 0.1654\tvalidation loss: 0.1819\t validation accuracy: 0.9533\n",
      "iteration number: 899\t training loss: 0.1642\tvalidation loss: 0.1803\t validation accuracy: 0.9533\n",
      "iteration number: 900\t training loss: 0.1641\tvalidation loss: 0.1774\t validation accuracy: 0.9511\n",
      "iteration number: 901\t training loss: 0.1647\tvalidation loss: 0.1767\t validation accuracy: 0.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 902\t training loss: 0.1647\tvalidation loss: 0.1763\t validation accuracy: 0.9556\n",
      "iteration number: 903\t training loss: 0.1657\tvalidation loss: 0.1816\t validation accuracy: 0.9467\n",
      "iteration number: 904\t training loss: 0.1629\tvalidation loss: 0.1800\t validation accuracy: 0.9578\n",
      "iteration number: 905\t training loss: 0.1620\tvalidation loss: 0.1828\t validation accuracy: 0.9533\n",
      "iteration number: 906\t training loss: 0.1615\tvalidation loss: 0.1774\t validation accuracy: 0.9600\n",
      "iteration number: 907\t training loss: 0.1616\tvalidation loss: 0.1779\t validation accuracy: 0.9578\n",
      "iteration number: 908\t training loss: 0.1623\tvalidation loss: 0.1768\t validation accuracy: 0.9578\n",
      "iteration number: 909\t training loss: 0.1626\tvalidation loss: 0.1793\t validation accuracy: 0.9556\n",
      "iteration number: 910\t training loss: 0.1629\tvalidation loss: 0.1807\t validation accuracy: 0.9556\n",
      "iteration number: 911\t training loss: 0.1647\tvalidation loss: 0.1878\t validation accuracy: 0.9489\n",
      "iteration number: 912\t training loss: 0.1641\tvalidation loss: 0.1855\t validation accuracy: 0.9489\n",
      "iteration number: 913\t training loss: 0.1628\tvalidation loss: 0.1811\t validation accuracy: 0.9578\n",
      "iteration number: 914\t training loss: 0.1639\tvalidation loss: 0.1839\t validation accuracy: 0.9467\n",
      "iteration number: 915\t training loss: 0.1616\tvalidation loss: 0.1858\t validation accuracy: 0.9533\n",
      "iteration number: 916\t training loss: 0.1631\tvalidation loss: 0.1880\t validation accuracy: 0.9489\n",
      "iteration number: 917\t training loss: 0.1615\tvalidation loss: 0.1809\t validation accuracy: 0.9578\n",
      "iteration number: 918\t training loss: 0.1611\tvalidation loss: 0.1820\t validation accuracy: 0.9556\n",
      "iteration number: 919\t training loss: 0.1627\tvalidation loss: 0.1839\t validation accuracy: 0.9511\n",
      "iteration number: 920\t training loss: 0.1614\tvalidation loss: 0.1851\t validation accuracy: 0.9489\n",
      "iteration number: 921\t training loss: 0.1592\tvalidation loss: 0.1796\t validation accuracy: 0.9600\n",
      "iteration number: 922\t training loss: 0.1593\tvalidation loss: 0.1815\t validation accuracy: 0.9578\n",
      "iteration number: 923\t training loss: 0.1610\tvalidation loss: 0.1852\t validation accuracy: 0.9556\n",
      "iteration number: 924\t training loss: 0.1602\tvalidation loss: 0.1770\t validation accuracy: 0.9533\n",
      "iteration number: 925\t training loss: 0.1599\tvalidation loss: 0.1765\t validation accuracy: 0.9578\n",
      "iteration number: 926\t training loss: 0.1605\tvalidation loss: 0.1810\t validation accuracy: 0.9489\n",
      "iteration number: 927\t training loss: 0.1614\tvalidation loss: 0.1805\t validation accuracy: 0.9467\n",
      "iteration number: 928\t training loss: 0.1600\tvalidation loss: 0.1791\t validation accuracy: 0.9511\n",
      "iteration number: 929\t training loss: 0.1606\tvalidation loss: 0.1844\t validation accuracy: 0.9511\n",
      "iteration number: 930\t training loss: 0.1592\tvalidation loss: 0.1842\t validation accuracy: 0.9511\n",
      "iteration number: 931\t training loss: 0.1587\tvalidation loss: 0.1840\t validation accuracy: 0.9533\n",
      "iteration number: 932\t training loss: 0.1572\tvalidation loss: 0.1799\t validation accuracy: 0.9556\n",
      "iteration number: 933\t training loss: 0.1576\tvalidation loss: 0.1788\t validation accuracy: 0.9533\n",
      "iteration number: 934\t training loss: 0.1578\tvalidation loss: 0.1748\t validation accuracy: 0.9533\n",
      "iteration number: 935\t training loss: 0.1572\tvalidation loss: 0.1712\t validation accuracy: 0.9556\n",
      "iteration number: 936\t training loss: 0.1564\tvalidation loss: 0.1708\t validation accuracy: 0.9533\n",
      "iteration number: 937\t training loss: 0.1573\tvalidation loss: 0.1710\t validation accuracy: 0.9600\n",
      "iteration number: 938\t training loss: 0.1561\tvalidation loss: 0.1725\t validation accuracy: 0.9556\n",
      "iteration number: 939\t training loss: 0.1551\tvalidation loss: 0.1746\t validation accuracy: 0.9556\n",
      "iteration number: 940\t training loss: 0.1550\tvalidation loss: 0.1758\t validation accuracy: 0.9556\n",
      "iteration number: 941\t training loss: 0.1552\tvalidation loss: 0.1758\t validation accuracy: 0.9533\n",
      "iteration number: 942\t training loss: 0.1548\tvalidation loss: 0.1706\t validation accuracy: 0.9556\n",
      "iteration number: 943\t training loss: 0.1550\tvalidation loss: 0.1760\t validation accuracy: 0.9556\n",
      "iteration number: 944\t training loss: 0.1554\tvalidation loss: 0.1749\t validation accuracy: 0.9533\n",
      "iteration number: 945\t training loss: 0.1543\tvalidation loss: 0.1811\t validation accuracy: 0.9556\n",
      "iteration number: 946\t training loss: 0.1565\tvalidation loss: 0.1895\t validation accuracy: 0.9467\n",
      "iteration number: 947\t training loss: 0.1534\tvalidation loss: 0.1786\t validation accuracy: 0.9511\n",
      "iteration number: 948\t training loss: 0.1535\tvalidation loss: 0.1778\t validation accuracy: 0.9533\n",
      "iteration number: 949\t training loss: 0.1549\tvalidation loss: 0.1803\t validation accuracy: 0.9489\n",
      "iteration number: 950\t training loss: 0.1550\tvalidation loss: 0.1834\t validation accuracy: 0.9489\n",
      "iteration number: 951\t training loss: 0.1550\tvalidation loss: 0.1847\t validation accuracy: 0.9467\n",
      "iteration number: 952\t training loss: 0.1553\tvalidation loss: 0.1829\t validation accuracy: 0.9511\n",
      "iteration number: 953\t training loss: 0.1548\tvalidation loss: 0.1771\t validation accuracy: 0.9556\n",
      "iteration number: 954\t training loss: 0.1531\tvalidation loss: 0.1790\t validation accuracy: 0.9533\n",
      "iteration number: 955\t training loss: 0.1539\tvalidation loss: 0.1821\t validation accuracy: 0.9489\n",
      "iteration number: 956\t training loss: 0.1541\tvalidation loss: 0.1763\t validation accuracy: 0.9556\n",
      "iteration number: 957\t training loss: 0.1521\tvalidation loss: 0.1791\t validation accuracy: 0.9511\n",
      "iteration number: 958\t training loss: 0.1522\tvalidation loss: 0.1779\t validation accuracy: 0.9511\n",
      "iteration number: 959\t training loss: 0.1514\tvalidation loss: 0.1764\t validation accuracy: 0.9489\n",
      "iteration number: 960\t training loss: 0.1512\tvalidation loss: 0.1717\t validation accuracy: 0.9556\n",
      "iteration number: 961\t training loss: 0.1521\tvalidation loss: 0.1756\t validation accuracy: 0.9578\n",
      "iteration number: 962\t training loss: 0.1526\tvalidation loss: 0.1684\t validation accuracy: 0.9533\n",
      "iteration number: 963\t training loss: 0.1518\tvalidation loss: 0.1693\t validation accuracy: 0.9578\n",
      "iteration number: 964\t training loss: 0.1520\tvalidation loss: 0.1695\t validation accuracy: 0.9511\n",
      "iteration number: 965\t training loss: 0.1502\tvalidation loss: 0.1743\t validation accuracy: 0.9489\n",
      "iteration number: 966\t training loss: 0.1503\tvalidation loss: 0.1744\t validation accuracy: 0.9489\n",
      "iteration number: 967\t training loss: 0.1523\tvalidation loss: 0.1749\t validation accuracy: 0.9489\n",
      "iteration number: 968\t training loss: 0.1559\tvalidation loss: 0.1791\t validation accuracy: 0.9533\n",
      "iteration number: 969\t training loss: 0.1517\tvalidation loss: 0.1734\t validation accuracy: 0.9489\n",
      "iteration number: 970\t training loss: 0.1513\tvalidation loss: 0.1698\t validation accuracy: 0.9511\n",
      "iteration number: 971\t training loss: 0.1509\tvalidation loss: 0.1801\t validation accuracy: 0.9467\n",
      "iteration number: 972\t training loss: 0.1520\tvalidation loss: 0.1822\t validation accuracy: 0.9422\n",
      "iteration number: 973\t training loss: 0.1509\tvalidation loss: 0.1835\t validation accuracy: 0.9444\n",
      "iteration number: 974\t training loss: 0.1507\tvalidation loss: 0.1831\t validation accuracy: 0.9511\n",
      "iteration number: 975\t training loss: 0.1503\tvalidation loss: 0.1799\t validation accuracy: 0.9511\n",
      "iteration number: 976\t training loss: 0.1494\tvalidation loss: 0.1769\t validation accuracy: 0.9533\n",
      "iteration number: 977\t training loss: 0.1497\tvalidation loss: 0.1806\t validation accuracy: 0.9511\n",
      "iteration number: 978\t training loss: 0.1505\tvalidation loss: 0.1799\t validation accuracy: 0.9467\n",
      "iteration number: 979\t training loss: 0.1483\tvalidation loss: 0.1716\t validation accuracy: 0.9556\n",
      "iteration number: 980\t training loss: 0.1505\tvalidation loss: 0.1700\t validation accuracy: 0.9533\n",
      "iteration number: 981\t training loss: 0.1499\tvalidation loss: 0.1682\t validation accuracy: 0.9556\n",
      "iteration number: 982\t training loss: 0.1477\tvalidation loss: 0.1694\t validation accuracy: 0.9533\n",
      "iteration number: 983\t training loss: 0.1475\tvalidation loss: 0.1687\t validation accuracy: 0.9556\n",
      "iteration number: 984\t training loss: 0.1501\tvalidation loss: 0.1758\t validation accuracy: 0.9489\n",
      "iteration number: 985\t training loss: 0.1475\tvalidation loss: 0.1721\t validation accuracy: 0.9533\n",
      "iteration number: 986\t training loss: 0.1474\tvalidation loss: 0.1725\t validation accuracy: 0.9533\n",
      "iteration number: 987\t training loss: 0.1468\tvalidation loss: 0.1678\t validation accuracy: 0.9556\n",
      "iteration number: 988\t training loss: 0.1469\tvalidation loss: 0.1727\t validation accuracy: 0.9578\n",
      "iteration number: 989\t training loss: 0.1481\tvalidation loss: 0.1798\t validation accuracy: 0.9444\n",
      "iteration number: 990\t training loss: 0.1475\tvalidation loss: 0.1796\t validation accuracy: 0.9489\n",
      "iteration number: 991\t training loss: 0.1461\tvalidation loss: 0.1754\t validation accuracy: 0.9556\n",
      "iteration number: 992\t training loss: 0.1483\tvalidation loss: 0.1828\t validation accuracy: 0.9489\n",
      "iteration number: 993\t training loss: 0.1477\tvalidation loss: 0.1794\t validation accuracy: 0.9511\n",
      "iteration number: 994\t training loss: 0.1476\tvalidation loss: 0.1769\t validation accuracy: 0.9578\n",
      "iteration number: 995\t training loss: 0.1465\tvalidation loss: 0.1787\t validation accuracy: 0.9489\n",
      "iteration number: 996\t training loss: 0.1499\tvalidation loss: 0.1854\t validation accuracy: 0.9467\n",
      "iteration number: 997\t training loss: 0.1468\tvalidation loss: 0.1753\t validation accuracy: 0.9533\n",
      "iteration number: 998\t training loss: 0.1451\tvalidation loss: 0.1710\t validation accuracy: 0.9533\n",
      "iteration number: 999\t training loss: 0.1443\tvalidation loss: 0.1733\t validation accuracy: 0.9533\n",
      "iteration number: 1000\t training loss: 0.1455\tvalidation loss: 0.1752\t validation accuracy: 0.9489\n",
      "iteration number: 1001\t training loss: 0.1441\tvalidation loss: 0.1723\t validation accuracy: 0.9511\n",
      "iteration number: 1002\t training loss: 0.1438\tvalidation loss: 0.1737\t validation accuracy: 0.9533\n",
      "iteration number: 1003\t training loss: 0.1432\tvalidation loss: 0.1728\t validation accuracy: 0.9533\n",
      "iteration number: 1004\t training loss: 0.1444\tvalidation loss: 0.1727\t validation accuracy: 0.9556\n",
      "iteration number: 1005\t training loss: 0.1437\tvalidation loss: 0.1696\t validation accuracy: 0.9578\n",
      "iteration number: 1006\t training loss: 0.1439\tvalidation loss: 0.1710\t validation accuracy: 0.9578\n",
      "iteration number: 1007\t training loss: 0.1431\tvalidation loss: 0.1681\t validation accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1008\t training loss: 0.1438\tvalidation loss: 0.1656\t validation accuracy: 0.9556\n",
      "iteration number: 1009\t training loss: 0.1425\tvalidation loss: 0.1643\t validation accuracy: 0.9556\n",
      "iteration number: 1010\t training loss: 0.1421\tvalidation loss: 0.1648\t validation accuracy: 0.9600\n",
      "iteration number: 1011\t training loss: 0.1421\tvalidation loss: 0.1668\t validation accuracy: 0.9556\n",
      "iteration number: 1012\t training loss: 0.1425\tvalidation loss: 0.1742\t validation accuracy: 0.9578\n",
      "iteration number: 1013\t training loss: 0.1425\tvalidation loss: 0.1758\t validation accuracy: 0.9533\n",
      "iteration number: 1014\t training loss: 0.1422\tvalidation loss: 0.1708\t validation accuracy: 0.9533\n",
      "iteration number: 1015\t training loss: 0.1422\tvalidation loss: 0.1676\t validation accuracy: 0.9533\n",
      "iteration number: 1016\t training loss: 0.1413\tvalidation loss: 0.1671\t validation accuracy: 0.9578\n",
      "iteration number: 1017\t training loss: 0.1424\tvalidation loss: 0.1660\t validation accuracy: 0.9600\n",
      "iteration number: 1018\t training loss: 0.1435\tvalidation loss: 0.1639\t validation accuracy: 0.9622\n",
      "iteration number: 1019\t training loss: 0.1422\tvalidation loss: 0.1658\t validation accuracy: 0.9578\n",
      "iteration number: 1020\t training loss: 0.1420\tvalidation loss: 0.1625\t validation accuracy: 0.9600\n",
      "iteration number: 1021\t training loss: 0.1428\tvalidation loss: 0.1683\t validation accuracy: 0.9578\n",
      "iteration number: 1022\t training loss: 0.1421\tvalidation loss: 0.1711\t validation accuracy: 0.9578\n",
      "iteration number: 1023\t training loss: 0.1441\tvalidation loss: 0.1756\t validation accuracy: 0.9533\n",
      "iteration number: 1024\t training loss: 0.1415\tvalidation loss: 0.1727\t validation accuracy: 0.9578\n",
      "iteration number: 1025\t training loss: 0.1410\tvalidation loss: 0.1688\t validation accuracy: 0.9533\n",
      "iteration number: 1026\t training loss: 0.1421\tvalidation loss: 0.1663\t validation accuracy: 0.9600\n",
      "iteration number: 1027\t training loss: 0.1409\tvalidation loss: 0.1634\t validation accuracy: 0.9578\n",
      "iteration number: 1028\t training loss: 0.1406\tvalidation loss: 0.1619\t validation accuracy: 0.9578\n",
      "iteration number: 1029\t training loss: 0.1438\tvalidation loss: 0.1644\t validation accuracy: 0.9556\n",
      "iteration number: 1030\t training loss: 0.1458\tvalidation loss: 0.1688\t validation accuracy: 0.9511\n",
      "iteration number: 1031\t training loss: 0.1407\tvalidation loss: 0.1677\t validation accuracy: 0.9533\n",
      "iteration number: 1032\t training loss: 0.1400\tvalidation loss: 0.1687\t validation accuracy: 0.9578\n",
      "iteration number: 1033\t training loss: 0.1391\tvalidation loss: 0.1688\t validation accuracy: 0.9556\n",
      "iteration number: 1034\t training loss: 0.1389\tvalidation loss: 0.1701\t validation accuracy: 0.9556\n",
      "iteration number: 1035\t training loss: 0.1400\tvalidation loss: 0.1717\t validation accuracy: 0.9578\n",
      "iteration number: 1036\t training loss: 0.1387\tvalidation loss: 0.1672\t validation accuracy: 0.9578\n",
      "iteration number: 1037\t training loss: 0.1406\tvalidation loss: 0.1634\t validation accuracy: 0.9578\n",
      "iteration number: 1038\t training loss: 0.1400\tvalidation loss: 0.1593\t validation accuracy: 0.9556\n",
      "iteration number: 1039\t training loss: 0.1407\tvalidation loss: 0.1597\t validation accuracy: 0.9600\n",
      "iteration number: 1040\t training loss: 0.1379\tvalidation loss: 0.1639\t validation accuracy: 0.9622\n",
      "iteration number: 1041\t training loss: 0.1381\tvalidation loss: 0.1630\t validation accuracy: 0.9578\n",
      "iteration number: 1042\t training loss: 0.1465\tvalidation loss: 0.1659\t validation accuracy: 0.9644\n",
      "iteration number: 1043\t training loss: 0.1431\tvalidation loss: 0.1622\t validation accuracy: 0.9667\n",
      "iteration number: 1044\t training loss: 0.1469\tvalidation loss: 0.1636\t validation accuracy: 0.9622\n",
      "iteration number: 1045\t training loss: 0.1398\tvalidation loss: 0.1649\t validation accuracy: 0.9600\n",
      "iteration number: 1046\t training loss: 0.1388\tvalidation loss: 0.1705\t validation accuracy: 0.9533\n",
      "iteration number: 1047\t training loss: 0.1378\tvalidation loss: 0.1670\t validation accuracy: 0.9556\n",
      "iteration number: 1048\t training loss: 0.1375\tvalidation loss: 0.1640\t validation accuracy: 0.9556\n",
      "iteration number: 1049\t training loss: 0.1380\tvalidation loss: 0.1664\t validation accuracy: 0.9556\n",
      "iteration number: 1050\t training loss: 0.1367\tvalidation loss: 0.1650\t validation accuracy: 0.9533\n",
      "iteration number: 1051\t training loss: 0.1362\tvalidation loss: 0.1675\t validation accuracy: 0.9511\n",
      "iteration number: 1052\t training loss: 0.1370\tvalidation loss: 0.1690\t validation accuracy: 0.9489\n",
      "iteration number: 1053\t training loss: 0.1391\tvalidation loss: 0.1653\t validation accuracy: 0.9556\n",
      "iteration number: 1054\t training loss: 0.1383\tvalidation loss: 0.1713\t validation accuracy: 0.9467\n",
      "iteration number: 1055\t training loss: 0.1383\tvalidation loss: 0.1704\t validation accuracy: 0.9511\n",
      "iteration number: 1056\t training loss: 0.1373\tvalidation loss: 0.1646\t validation accuracy: 0.9578\n",
      "iteration number: 1057\t training loss: 0.1369\tvalidation loss: 0.1686\t validation accuracy: 0.9511\n",
      "iteration number: 1058\t training loss: 0.1365\tvalidation loss: 0.1662\t validation accuracy: 0.9533\n",
      "iteration number: 1059\t training loss: 0.1375\tvalidation loss: 0.1671\t validation accuracy: 0.9600\n",
      "iteration number: 1060\t training loss: 0.1389\tvalidation loss: 0.1670\t validation accuracy: 0.9533\n",
      "iteration number: 1061\t training loss: 0.1375\tvalidation loss: 0.1680\t validation accuracy: 0.9511\n",
      "iteration number: 1062\t training loss: 0.1366\tvalidation loss: 0.1660\t validation accuracy: 0.9556\n",
      "iteration number: 1063\t training loss: 0.1359\tvalidation loss: 0.1623\t validation accuracy: 0.9556\n",
      "iteration number: 1064\t training loss: 0.1370\tvalidation loss: 0.1617\t validation accuracy: 0.9578\n",
      "iteration number: 1065\t training loss: 0.1358\tvalidation loss: 0.1639\t validation accuracy: 0.9578\n",
      "iteration number: 1066\t training loss: 0.1354\tvalidation loss: 0.1597\t validation accuracy: 0.9556\n",
      "iteration number: 1067\t training loss: 0.1348\tvalidation loss: 0.1606\t validation accuracy: 0.9578\n",
      "iteration number: 1068\t training loss: 0.1352\tvalidation loss: 0.1612\t validation accuracy: 0.9578\n",
      "iteration number: 1069\t training loss: 0.1373\tvalidation loss: 0.1589\t validation accuracy: 0.9600\n",
      "iteration number: 1070\t training loss: 0.1359\tvalidation loss: 0.1599\t validation accuracy: 0.9600\n",
      "iteration number: 1071\t training loss: 0.1367\tvalidation loss: 0.1597\t validation accuracy: 0.9689\n",
      "iteration number: 1072\t training loss: 0.1369\tvalidation loss: 0.1596\t validation accuracy: 0.9667\n",
      "iteration number: 1073\t training loss: 0.1385\tvalidation loss: 0.1601\t validation accuracy: 0.9711\n",
      "iteration number: 1074\t training loss: 0.1356\tvalidation loss: 0.1568\t validation accuracy: 0.9689\n",
      "iteration number: 1075\t training loss: 0.1324\tvalidation loss: 0.1590\t validation accuracy: 0.9600\n",
      "iteration number: 1076\t training loss: 0.1320\tvalidation loss: 0.1614\t validation accuracy: 0.9556\n",
      "iteration number: 1077\t training loss: 0.1331\tvalidation loss: 0.1677\t validation accuracy: 0.9600\n",
      "iteration number: 1078\t training loss: 0.1337\tvalidation loss: 0.1699\t validation accuracy: 0.9578\n",
      "iteration number: 1079\t training loss: 0.1342\tvalidation loss: 0.1689\t validation accuracy: 0.9578\n",
      "iteration number: 1080\t training loss: 0.1334\tvalidation loss: 0.1665\t validation accuracy: 0.9578\n",
      "iteration number: 1081\t training loss: 0.1317\tvalidation loss: 0.1611\t validation accuracy: 0.9578\n",
      "iteration number: 1082\t training loss: 0.1316\tvalidation loss: 0.1593\t validation accuracy: 0.9600\n",
      "iteration number: 1083\t training loss: 0.1321\tvalidation loss: 0.1590\t validation accuracy: 0.9600\n",
      "iteration number: 1084\t training loss: 0.1322\tvalidation loss: 0.1611\t validation accuracy: 0.9578\n",
      "iteration number: 1085\t training loss: 0.1320\tvalidation loss: 0.1653\t validation accuracy: 0.9511\n",
      "iteration number: 1086\t training loss: 0.1318\tvalidation loss: 0.1675\t validation accuracy: 0.9511\n",
      "iteration number: 1087\t training loss: 0.1331\tvalidation loss: 0.1657\t validation accuracy: 0.9511\n",
      "iteration number: 1088\t training loss: 0.1325\tvalidation loss: 0.1639\t validation accuracy: 0.9533\n",
      "iteration number: 1089\t training loss: 0.1334\tvalidation loss: 0.1572\t validation accuracy: 0.9622\n",
      "iteration number: 1090\t training loss: 0.1310\tvalidation loss: 0.1655\t validation accuracy: 0.9556\n",
      "iteration number: 1091\t training loss: 0.1305\tvalidation loss: 0.1626\t validation accuracy: 0.9556\n",
      "iteration number: 1092\t training loss: 0.1307\tvalidation loss: 0.1664\t validation accuracy: 0.9533\n",
      "iteration number: 1093\t training loss: 0.1302\tvalidation loss: 0.1640\t validation accuracy: 0.9556\n",
      "iteration number: 1094\t training loss: 0.1302\tvalidation loss: 0.1644\t validation accuracy: 0.9533\n",
      "iteration number: 1095\t training loss: 0.1300\tvalidation loss: 0.1574\t validation accuracy: 0.9622\n",
      "iteration number: 1096\t training loss: 0.1311\tvalidation loss: 0.1563\t validation accuracy: 0.9667\n",
      "iteration number: 1097\t training loss: 0.1307\tvalidation loss: 0.1591\t validation accuracy: 0.9600\n",
      "iteration number: 1098\t training loss: 0.1341\tvalidation loss: 0.1626\t validation accuracy: 0.9489\n",
      "iteration number: 1099\t training loss: 0.1329\tvalidation loss: 0.1596\t validation accuracy: 0.9600\n",
      "iteration number: 1100\t training loss: 0.1302\tvalidation loss: 0.1578\t validation accuracy: 0.9578\n",
      "iteration number: 1101\t training loss: 0.1294\tvalidation loss: 0.1606\t validation accuracy: 0.9556\n",
      "iteration number: 1102\t training loss: 0.1302\tvalidation loss: 0.1621\t validation accuracy: 0.9556\n",
      "iteration number: 1103\t training loss: 0.1289\tvalidation loss: 0.1600\t validation accuracy: 0.9556\n",
      "iteration number: 1104\t training loss: 0.1291\tvalidation loss: 0.1601\t validation accuracy: 0.9556\n",
      "iteration number: 1105\t training loss: 0.1305\tvalidation loss: 0.1578\t validation accuracy: 0.9556\n",
      "iteration number: 1106\t training loss: 0.1294\tvalidation loss: 0.1579\t validation accuracy: 0.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1107\t training loss: 0.1309\tvalidation loss: 0.1593\t validation accuracy: 0.9578\n",
      "iteration number: 1108\t training loss: 0.1282\tvalidation loss: 0.1608\t validation accuracy: 0.9533\n",
      "iteration number: 1109\t training loss: 0.1293\tvalidation loss: 0.1655\t validation accuracy: 0.9511\n",
      "iteration number: 1110\t training loss: 0.1303\tvalidation loss: 0.1695\t validation accuracy: 0.9467\n",
      "iteration number: 1111\t training loss: 0.1298\tvalidation loss: 0.1686\t validation accuracy: 0.9511\n",
      "iteration number: 1112\t training loss: 0.1283\tvalidation loss: 0.1635\t validation accuracy: 0.9511\n",
      "iteration number: 1113\t training loss: 0.1291\tvalidation loss: 0.1629\t validation accuracy: 0.9556\n",
      "iteration number: 1114\t training loss: 0.1281\tvalidation loss: 0.1572\t validation accuracy: 0.9556\n",
      "iteration number: 1115\t training loss: 0.1282\tvalidation loss: 0.1583\t validation accuracy: 0.9600\n",
      "iteration number: 1116\t training loss: 0.1283\tvalidation loss: 0.1568\t validation accuracy: 0.9622\n",
      "iteration number: 1117\t training loss: 0.1279\tvalidation loss: 0.1581\t validation accuracy: 0.9622\n",
      "iteration number: 1118\t training loss: 0.1276\tvalidation loss: 0.1642\t validation accuracy: 0.9489\n",
      "iteration number: 1119\t training loss: 0.1283\tvalidation loss: 0.1617\t validation accuracy: 0.9533\n",
      "iteration number: 1120\t training loss: 0.1276\tvalidation loss: 0.1606\t validation accuracy: 0.9578\n",
      "iteration number: 1121\t training loss: 0.1266\tvalidation loss: 0.1595\t validation accuracy: 0.9600\n",
      "iteration number: 1122\t training loss: 0.1271\tvalidation loss: 0.1596\t validation accuracy: 0.9600\n",
      "iteration number: 1123\t training loss: 0.1264\tvalidation loss: 0.1544\t validation accuracy: 0.9622\n",
      "iteration number: 1124\t training loss: 0.1269\tvalidation loss: 0.1593\t validation accuracy: 0.9600\n",
      "iteration number: 1125\t training loss: 0.1260\tvalidation loss: 0.1571\t validation accuracy: 0.9600\n",
      "iteration number: 1126\t training loss: 0.1258\tvalidation loss: 0.1587\t validation accuracy: 0.9556\n",
      "iteration number: 1127\t training loss: 0.1263\tvalidation loss: 0.1596\t validation accuracy: 0.9556\n",
      "iteration number: 1128\t training loss: 0.1275\tvalidation loss: 0.1664\t validation accuracy: 0.9511\n",
      "iteration number: 1129\t training loss: 0.1276\tvalidation loss: 0.1654\t validation accuracy: 0.9533\n",
      "iteration number: 1130\t training loss: 0.1273\tvalidation loss: 0.1637\t validation accuracy: 0.9556\n",
      "iteration number: 1131\t training loss: 0.1274\tvalidation loss: 0.1621\t validation accuracy: 0.9533\n",
      "iteration number: 1132\t training loss: 0.1259\tvalidation loss: 0.1583\t validation accuracy: 0.9622\n",
      "iteration number: 1133\t training loss: 0.1255\tvalidation loss: 0.1566\t validation accuracy: 0.9600\n",
      "iteration number: 1134\t training loss: 0.1251\tvalidation loss: 0.1519\t validation accuracy: 0.9622\n",
      "iteration number: 1135\t training loss: 0.1262\tvalidation loss: 0.1540\t validation accuracy: 0.9644\n",
      "iteration number: 1136\t training loss: 0.1253\tvalidation loss: 0.1499\t validation accuracy: 0.9644\n",
      "iteration number: 1137\t training loss: 0.1253\tvalidation loss: 0.1510\t validation accuracy: 0.9644\n",
      "iteration number: 1138\t training loss: 0.1254\tvalidation loss: 0.1473\t validation accuracy: 0.9644\n",
      "iteration number: 1139\t training loss: 0.1255\tvalidation loss: 0.1562\t validation accuracy: 0.9622\n",
      "iteration number: 1140\t training loss: 0.1263\tvalidation loss: 0.1587\t validation accuracy: 0.9600\n",
      "iteration number: 1141\t training loss: 0.1246\tvalidation loss: 0.1532\t validation accuracy: 0.9644\n",
      "iteration number: 1142\t training loss: 0.1256\tvalidation loss: 0.1510\t validation accuracy: 0.9644\n",
      "iteration number: 1143\t training loss: 0.1265\tvalidation loss: 0.1525\t validation accuracy: 0.9644\n",
      "iteration number: 1144\t training loss: 0.1266\tvalidation loss: 0.1516\t validation accuracy: 0.9644\n",
      "iteration number: 1145\t training loss: 0.1269\tvalidation loss: 0.1526\t validation accuracy: 0.9667\n",
      "iteration number: 1146\t training loss: 0.1259\tvalidation loss: 0.1485\t validation accuracy: 0.9667\n",
      "iteration number: 1147\t training loss: 0.1262\tvalidation loss: 0.1525\t validation accuracy: 0.9644\n",
      "iteration number: 1148\t training loss: 0.1266\tvalidation loss: 0.1516\t validation accuracy: 0.9622\n",
      "iteration number: 1149\t training loss: 0.1263\tvalidation loss: 0.1504\t validation accuracy: 0.9644\n",
      "iteration number: 1150\t training loss: 0.1238\tvalidation loss: 0.1487\t validation accuracy: 0.9644\n",
      "iteration number: 1151\t training loss: 0.1241\tvalidation loss: 0.1509\t validation accuracy: 0.9644\n",
      "iteration number: 1152\t training loss: 0.1239\tvalidation loss: 0.1485\t validation accuracy: 0.9667\n",
      "iteration number: 1153\t training loss: 0.1228\tvalidation loss: 0.1516\t validation accuracy: 0.9622\n",
      "iteration number: 1154\t training loss: 0.1233\tvalidation loss: 0.1515\t validation accuracy: 0.9622\n",
      "iteration number: 1155\t training loss: 0.1231\tvalidation loss: 0.1548\t validation accuracy: 0.9622\n",
      "iteration number: 1156\t training loss: 0.1227\tvalidation loss: 0.1559\t validation accuracy: 0.9622\n",
      "iteration number: 1157\t training loss: 0.1226\tvalidation loss: 0.1578\t validation accuracy: 0.9600\n",
      "iteration number: 1158\t training loss: 0.1234\tvalidation loss: 0.1569\t validation accuracy: 0.9578\n",
      "iteration number: 1159\t training loss: 0.1225\tvalidation loss: 0.1548\t validation accuracy: 0.9622\n",
      "iteration number: 1160\t training loss: 0.1232\tvalidation loss: 0.1581\t validation accuracy: 0.9622\n",
      "iteration number: 1161\t training loss: 0.1243\tvalidation loss: 0.1544\t validation accuracy: 0.9578\n",
      "iteration number: 1162\t training loss: 0.1244\tvalidation loss: 0.1591\t validation accuracy: 0.9600\n",
      "iteration number: 1163\t training loss: 0.1264\tvalidation loss: 0.1604\t validation accuracy: 0.9600\n",
      "iteration number: 1164\t training loss: 0.1263\tvalidation loss: 0.1639\t validation accuracy: 0.9578\n",
      "iteration number: 1165\t training loss: 0.1237\tvalidation loss: 0.1579\t validation accuracy: 0.9622\n",
      "iteration number: 1166\t training loss: 0.1218\tvalidation loss: 0.1542\t validation accuracy: 0.9600\n",
      "iteration number: 1167\t training loss: 0.1225\tvalidation loss: 0.1520\t validation accuracy: 0.9622\n",
      "iteration number: 1168\t training loss: 0.1221\tvalidation loss: 0.1517\t validation accuracy: 0.9622\n",
      "iteration number: 1169\t training loss: 0.1233\tvalidation loss: 0.1583\t validation accuracy: 0.9622\n",
      "iteration number: 1170\t training loss: 0.1252\tvalidation loss: 0.1597\t validation accuracy: 0.9556\n",
      "iteration number: 1171\t training loss: 0.1263\tvalidation loss: 0.1610\t validation accuracy: 0.9578\n",
      "iteration number: 1172\t training loss: 0.1234\tvalidation loss: 0.1524\t validation accuracy: 0.9600\n",
      "iteration number: 1173\t training loss: 0.1229\tvalidation loss: 0.1523\t validation accuracy: 0.9644\n",
      "iteration number: 1174\t training loss: 0.1210\tvalidation loss: 0.1485\t validation accuracy: 0.9644\n",
      "iteration number: 1175\t training loss: 0.1214\tvalidation loss: 0.1504\t validation accuracy: 0.9644\n",
      "iteration number: 1176\t training loss: 0.1206\tvalidation loss: 0.1497\t validation accuracy: 0.9644\n",
      "iteration number: 1177\t training loss: 0.1206\tvalidation loss: 0.1483\t validation accuracy: 0.9644\n",
      "iteration number: 1178\t training loss: 0.1204\tvalidation loss: 0.1461\t validation accuracy: 0.9644\n",
      "iteration number: 1179\t training loss: 0.1201\tvalidation loss: 0.1457\t validation accuracy: 0.9644\n",
      "iteration number: 1180\t training loss: 0.1207\tvalidation loss: 0.1466\t validation accuracy: 0.9644\n",
      "iteration number: 1181\t training loss: 0.1203\tvalidation loss: 0.1517\t validation accuracy: 0.9600\n",
      "iteration number: 1182\t training loss: 0.1234\tvalidation loss: 0.1623\t validation accuracy: 0.9600\n",
      "iteration number: 1183\t training loss: 0.1216\tvalidation loss: 0.1570\t validation accuracy: 0.9578\n",
      "iteration number: 1184\t training loss: 0.1225\tvalidation loss: 0.1620\t validation accuracy: 0.9578\n",
      "iteration number: 1185\t training loss: 0.1207\tvalidation loss: 0.1560\t validation accuracy: 0.9600\n",
      "iteration number: 1186\t training loss: 0.1208\tvalidation loss: 0.1569\t validation accuracy: 0.9578\n",
      "iteration number: 1187\t training loss: 0.1212\tvalidation loss: 0.1569\t validation accuracy: 0.9533\n",
      "iteration number: 1188\t training loss: 0.1197\tvalidation loss: 0.1560\t validation accuracy: 0.9511\n",
      "iteration number: 1189\t training loss: 0.1201\tvalidation loss: 0.1560\t validation accuracy: 0.9533\n",
      "iteration number: 1190\t training loss: 0.1228\tvalidation loss: 0.1621\t validation accuracy: 0.9533\n",
      "iteration number: 1191\t training loss: 0.1208\tvalidation loss: 0.1605\t validation accuracy: 0.9533\n",
      "iteration number: 1192\t training loss: 0.1200\tvalidation loss: 0.1578\t validation accuracy: 0.9533\n",
      "iteration number: 1193\t training loss: 0.1189\tvalidation loss: 0.1510\t validation accuracy: 0.9578\n",
      "iteration number: 1194\t training loss: 0.1188\tvalidation loss: 0.1530\t validation accuracy: 0.9600\n",
      "iteration number: 1195\t training loss: 0.1195\tvalidation loss: 0.1520\t validation accuracy: 0.9622\n",
      "iteration number: 1196\t training loss: 0.1187\tvalidation loss: 0.1534\t validation accuracy: 0.9600\n",
      "iteration number: 1197\t training loss: 0.1191\tvalidation loss: 0.1528\t validation accuracy: 0.9622\n",
      "iteration number: 1198\t training loss: 0.1187\tvalidation loss: 0.1528\t validation accuracy: 0.9600\n",
      "iteration number: 1199\t training loss: 0.1184\tvalidation loss: 0.1523\t validation accuracy: 0.9600\n",
      "iteration number: 1200\t training loss: 0.1183\tvalidation loss: 0.1548\t validation accuracy: 0.9600\n",
      "iteration number: 1201\t training loss: 0.1180\tvalidation loss: 0.1490\t validation accuracy: 0.9622\n",
      "iteration number: 1202\t training loss: 0.1214\tvalidation loss: 0.1471\t validation accuracy: 0.9622\n",
      "iteration number: 1203\t training loss: 0.1186\tvalidation loss: 0.1496\t validation accuracy: 0.9600\n",
      "iteration number: 1204\t training loss: 0.1173\tvalidation loss: 0.1520\t validation accuracy: 0.9600\n",
      "iteration number: 1205\t training loss: 0.1174\tvalidation loss: 0.1512\t validation accuracy: 0.9600\n",
      "iteration number: 1206\t training loss: 0.1169\tvalidation loss: 0.1495\t validation accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1207\t training loss: 0.1168\tvalidation loss: 0.1506\t validation accuracy: 0.9622\n",
      "iteration number: 1208\t training loss: 0.1172\tvalidation loss: 0.1510\t validation accuracy: 0.9600\n",
      "iteration number: 1209\t training loss: 0.1187\tvalidation loss: 0.1533\t validation accuracy: 0.9578\n",
      "iteration number: 1210\t training loss: 0.1181\tvalidation loss: 0.1536\t validation accuracy: 0.9600\n",
      "iteration number: 1211\t training loss: 0.1171\tvalidation loss: 0.1496\t validation accuracy: 0.9644\n",
      "iteration number: 1212\t training loss: 0.1176\tvalidation loss: 0.1517\t validation accuracy: 0.9622\n",
      "iteration number: 1213\t training loss: 0.1175\tvalidation loss: 0.1529\t validation accuracy: 0.9622\n",
      "iteration number: 1214\t training loss: 0.1169\tvalidation loss: 0.1487\t validation accuracy: 0.9644\n",
      "iteration number: 1215\t training loss: 0.1173\tvalidation loss: 0.1503\t validation accuracy: 0.9644\n",
      "iteration number: 1216\t training loss: 0.1162\tvalidation loss: 0.1480\t validation accuracy: 0.9644\n",
      "iteration number: 1217\t training loss: 0.1184\tvalidation loss: 0.1438\t validation accuracy: 0.9622\n",
      "iteration number: 1218\t training loss: 0.1172\tvalidation loss: 0.1480\t validation accuracy: 0.9622\n",
      "iteration number: 1219\t training loss: 0.1198\tvalidation loss: 0.1466\t validation accuracy: 0.9667\n",
      "iteration number: 1220\t training loss: 0.1180\tvalidation loss: 0.1454\t validation accuracy: 0.9622\n",
      "iteration number: 1221\t training loss: 0.1179\tvalidation loss: 0.1433\t validation accuracy: 0.9622\n",
      "iteration number: 1222\t training loss: 0.1163\tvalidation loss: 0.1423\t validation accuracy: 0.9644\n",
      "iteration number: 1223\t training loss: 0.1153\tvalidation loss: 0.1488\t validation accuracy: 0.9622\n",
      "iteration number: 1224\t training loss: 0.1182\tvalidation loss: 0.1522\t validation accuracy: 0.9578\n",
      "iteration number: 1225\t training loss: 0.1167\tvalidation loss: 0.1480\t validation accuracy: 0.9622\n",
      "iteration number: 1226\t training loss: 0.1166\tvalidation loss: 0.1480\t validation accuracy: 0.9622\n",
      "iteration number: 1227\t training loss: 0.1165\tvalidation loss: 0.1488\t validation accuracy: 0.9622\n",
      "iteration number: 1228\t training loss: 0.1162\tvalidation loss: 0.1471\t validation accuracy: 0.9622\n",
      "iteration number: 1229\t training loss: 0.1158\tvalidation loss: 0.1426\t validation accuracy: 0.9644\n",
      "iteration number: 1230\t training loss: 0.1184\tvalidation loss: 0.1419\t validation accuracy: 0.9667\n",
      "iteration number: 1231\t training loss: 0.1197\tvalidation loss: 0.1402\t validation accuracy: 0.9644\n",
      "iteration number: 1232\t training loss: 0.1172\tvalidation loss: 0.1408\t validation accuracy: 0.9667\n",
      "iteration number: 1233\t training loss: 0.1166\tvalidation loss: 0.1413\t validation accuracy: 0.9667\n",
      "iteration number: 1234\t training loss: 0.1161\tvalidation loss: 0.1425\t validation accuracy: 0.9667\n",
      "iteration number: 1235\t training loss: 0.1151\tvalidation loss: 0.1453\t validation accuracy: 0.9622\n",
      "iteration number: 1236\t training loss: 0.1159\tvalidation loss: 0.1454\t validation accuracy: 0.9644\n",
      "iteration number: 1237\t training loss: 0.1157\tvalidation loss: 0.1465\t validation accuracy: 0.9622\n",
      "iteration number: 1238\t training loss: 0.1154\tvalidation loss: 0.1467\t validation accuracy: 0.9622\n",
      "iteration number: 1239\t training loss: 0.1163\tvalidation loss: 0.1531\t validation accuracy: 0.9600\n",
      "iteration number: 1240\t training loss: 0.1175\tvalidation loss: 0.1552\t validation accuracy: 0.9622\n",
      "iteration number: 1241\t training loss: 0.1170\tvalidation loss: 0.1555\t validation accuracy: 0.9622\n",
      "iteration number: 1242\t training loss: 0.1183\tvalidation loss: 0.1460\t validation accuracy: 0.9600\n",
      "iteration number: 1243\t training loss: 0.1157\tvalidation loss: 0.1461\t validation accuracy: 0.9622\n",
      "iteration number: 1244\t training loss: 0.1145\tvalidation loss: 0.1503\t validation accuracy: 0.9622\n",
      "iteration number: 1245\t training loss: 0.1142\tvalidation loss: 0.1486\t validation accuracy: 0.9622\n",
      "iteration number: 1246\t training loss: 0.1155\tvalidation loss: 0.1511\t validation accuracy: 0.9622\n",
      "iteration number: 1247\t training loss: 0.1160\tvalidation loss: 0.1546\t validation accuracy: 0.9600\n",
      "iteration number: 1248\t training loss: 0.1156\tvalidation loss: 0.1528\t validation accuracy: 0.9600\n",
      "iteration number: 1249\t training loss: 0.1144\tvalidation loss: 0.1508\t validation accuracy: 0.9622\n",
      "iteration number: 1250\t training loss: 0.1127\tvalidation loss: 0.1471\t validation accuracy: 0.9644\n",
      "iteration number: 1251\t training loss: 0.1129\tvalidation loss: 0.1454\t validation accuracy: 0.9644\n",
      "iteration number: 1252\t training loss: 0.1139\tvalidation loss: 0.1463\t validation accuracy: 0.9622\n",
      "iteration number: 1253\t training loss: 0.1138\tvalidation loss: 0.1475\t validation accuracy: 0.9600\n",
      "iteration number: 1254\t training loss: 0.1156\tvalidation loss: 0.1543\t validation accuracy: 0.9600\n",
      "iteration number: 1255\t training loss: 0.1156\tvalidation loss: 0.1587\t validation accuracy: 0.9578\n",
      "iteration number: 1256\t training loss: 0.1136\tvalidation loss: 0.1542\t validation accuracy: 0.9600\n",
      "iteration number: 1257\t training loss: 0.1128\tvalidation loss: 0.1496\t validation accuracy: 0.9600\n",
      "iteration number: 1258\t training loss: 0.1128\tvalidation loss: 0.1501\t validation accuracy: 0.9556\n",
      "iteration number: 1259\t training loss: 0.1125\tvalidation loss: 0.1502\t validation accuracy: 0.9578\n",
      "iteration number: 1260\t training loss: 0.1119\tvalidation loss: 0.1456\t validation accuracy: 0.9600\n",
      "iteration number: 1261\t training loss: 0.1141\tvalidation loss: 0.1477\t validation accuracy: 0.9667\n",
      "iteration number: 1262\t training loss: 0.1155\tvalidation loss: 0.1486\t validation accuracy: 0.9622\n",
      "iteration number: 1263\t training loss: 0.1146\tvalidation loss: 0.1502\t validation accuracy: 0.9600\n",
      "iteration number: 1264\t training loss: 0.1119\tvalidation loss: 0.1432\t validation accuracy: 0.9644\n",
      "iteration number: 1265\t training loss: 0.1135\tvalidation loss: 0.1394\t validation accuracy: 0.9667\n",
      "iteration number: 1266\t training loss: 0.1128\tvalidation loss: 0.1404\t validation accuracy: 0.9644\n",
      "iteration number: 1267\t training loss: 0.1124\tvalidation loss: 0.1469\t validation accuracy: 0.9622\n",
      "iteration number: 1268\t training loss: 0.1111\tvalidation loss: 0.1457\t validation accuracy: 0.9600\n",
      "iteration number: 1269\t training loss: 0.1116\tvalidation loss: 0.1444\t validation accuracy: 0.9600\n",
      "iteration number: 1270\t training loss: 0.1118\tvalidation loss: 0.1406\t validation accuracy: 0.9644\n",
      "iteration number: 1271\t training loss: 0.1114\tvalidation loss: 0.1406\t validation accuracy: 0.9644\n",
      "iteration number: 1272\t training loss: 0.1117\tvalidation loss: 0.1393\t validation accuracy: 0.9644\n",
      "iteration number: 1273\t training loss: 0.1134\tvalidation loss: 0.1382\t validation accuracy: 0.9711\n",
      "iteration number: 1274\t training loss: 0.1126\tvalidation loss: 0.1382\t validation accuracy: 0.9689\n",
      "iteration number: 1275\t training loss: 0.1110\tvalidation loss: 0.1393\t validation accuracy: 0.9689\n",
      "iteration number: 1276\t training loss: 0.1104\tvalidation loss: 0.1413\t validation accuracy: 0.9600\n",
      "iteration number: 1277\t training loss: 0.1101\tvalidation loss: 0.1408\t validation accuracy: 0.9667\n",
      "iteration number: 1278\t training loss: 0.1101\tvalidation loss: 0.1411\t validation accuracy: 0.9644\n",
      "iteration number: 1279\t training loss: 0.1101\tvalidation loss: 0.1427\t validation accuracy: 0.9578\n",
      "iteration number: 1280\t training loss: 0.1109\tvalidation loss: 0.1436\t validation accuracy: 0.9600\n",
      "iteration number: 1281\t training loss: 0.1109\tvalidation loss: 0.1410\t validation accuracy: 0.9622\n",
      "iteration number: 1282\t training loss: 0.1102\tvalidation loss: 0.1415\t validation accuracy: 0.9578\n",
      "iteration number: 1283\t training loss: 0.1092\tvalidation loss: 0.1400\t validation accuracy: 0.9689\n",
      "iteration number: 1284\t training loss: 0.1092\tvalidation loss: 0.1406\t validation accuracy: 0.9667\n",
      "iteration number: 1285\t training loss: 0.1099\tvalidation loss: 0.1404\t validation accuracy: 0.9644\n",
      "iteration number: 1286\t training loss: 0.1106\tvalidation loss: 0.1450\t validation accuracy: 0.9622\n",
      "iteration number: 1287\t training loss: 0.1089\tvalidation loss: 0.1416\t validation accuracy: 0.9622\n",
      "iteration number: 1288\t training loss: 0.1091\tvalidation loss: 0.1460\t validation accuracy: 0.9600\n",
      "iteration number: 1289\t training loss: 0.1089\tvalidation loss: 0.1464\t validation accuracy: 0.9578\n",
      "iteration number: 1290\t training loss: 0.1101\tvalidation loss: 0.1475\t validation accuracy: 0.9600\n",
      "iteration number: 1291\t training loss: 0.1091\tvalidation loss: 0.1455\t validation accuracy: 0.9600\n",
      "iteration number: 1292\t training loss: 0.1087\tvalidation loss: 0.1464\t validation accuracy: 0.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1293\t training loss: 0.1086\tvalidation loss: 0.1434\t validation accuracy: 0.9644\n",
      "iteration number: 1294\t training loss: 0.1084\tvalidation loss: 0.1466\t validation accuracy: 0.9578\n",
      "iteration number: 1295\t training loss: 0.1087\tvalidation loss: 0.1504\t validation accuracy: 0.9578\n",
      "iteration number: 1296\t training loss: 0.1104\tvalidation loss: 0.1547\t validation accuracy: 0.9578\n",
      "iteration number: 1297\t training loss: 0.1113\tvalidation loss: 0.1568\t validation accuracy: 0.9556\n",
      "iteration number: 1298\t training loss: 0.1094\tvalidation loss: 0.1498\t validation accuracy: 0.9556\n",
      "iteration number: 1299\t training loss: 0.1090\tvalidation loss: 0.1465\t validation accuracy: 0.9600\n",
      "iteration number: 1300\t training loss: 0.1093\tvalidation loss: 0.1493\t validation accuracy: 0.9600\n",
      "iteration number: 1301\t training loss: 0.1092\tvalidation loss: 0.1485\t validation accuracy: 0.9600\n",
      "iteration number: 1302\t training loss: 0.1083\tvalidation loss: 0.1450\t validation accuracy: 0.9578\n",
      "iteration number: 1303\t training loss: 0.1073\tvalidation loss: 0.1422\t validation accuracy: 0.9622\n",
      "iteration number: 1304\t training loss: 0.1072\tvalidation loss: 0.1421\t validation accuracy: 0.9622\n",
      "iteration number: 1305\t training loss: 0.1079\tvalidation loss: 0.1425\t validation accuracy: 0.9600\n",
      "iteration number: 1306\t training loss: 0.1082\tvalidation loss: 0.1435\t validation accuracy: 0.9600\n",
      "iteration number: 1307\t training loss: 0.1079\tvalidation loss: 0.1424\t validation accuracy: 0.9600\n",
      "iteration number: 1308\t training loss: 0.1082\tvalidation loss: 0.1387\t validation accuracy: 0.9644\n",
      "iteration number: 1309\t training loss: 0.1077\tvalidation loss: 0.1446\t validation accuracy: 0.9622\n",
      "iteration number: 1310\t training loss: 0.1085\tvalidation loss: 0.1450\t validation accuracy: 0.9622\n",
      "iteration number: 1311\t training loss: 0.1094\tvalidation loss: 0.1491\t validation accuracy: 0.9622\n",
      "iteration number: 1312\t training loss: 0.1098\tvalidation loss: 0.1497\t validation accuracy: 0.9622\n",
      "iteration number: 1313\t training loss: 0.1083\tvalidation loss: 0.1461\t validation accuracy: 0.9622\n",
      "iteration number: 1314\t training loss: 0.1066\tvalidation loss: 0.1419\t validation accuracy: 0.9644\n",
      "iteration number: 1315\t training loss: 0.1072\tvalidation loss: 0.1455\t validation accuracy: 0.9622\n",
      "iteration number: 1316\t training loss: 0.1076\tvalidation loss: 0.1432\t validation accuracy: 0.9622\n",
      "iteration number: 1317\t training loss: 0.1080\tvalidation loss: 0.1427\t validation accuracy: 0.9644\n",
      "iteration number: 1318\t training loss: 0.1069\tvalidation loss: 0.1467\t validation accuracy: 0.9600\n",
      "iteration number: 1319\t training loss: 0.1071\tvalidation loss: 0.1422\t validation accuracy: 0.9644\n",
      "iteration number: 1320\t training loss: 0.1065\tvalidation loss: 0.1473\t validation accuracy: 0.9622\n",
      "iteration number: 1321\t training loss: 0.1068\tvalidation loss: 0.1485\t validation accuracy: 0.9600\n",
      "iteration number: 1322\t training loss: 0.1068\tvalidation loss: 0.1465\t validation accuracy: 0.9600\n",
      "iteration number: 1323\t training loss: 0.1070\tvalidation loss: 0.1402\t validation accuracy: 0.9622\n",
      "iteration number: 1324\t training loss: 0.1086\tvalidation loss: 0.1418\t validation accuracy: 0.9600\n",
      "iteration number: 1325\t training loss: 0.1068\tvalidation loss: 0.1438\t validation accuracy: 0.9600\n",
      "iteration number: 1326\t training loss: 0.1068\tvalidation loss: 0.1407\t validation accuracy: 0.9622\n",
      "iteration number: 1327\t training loss: 0.1072\tvalidation loss: 0.1427\t validation accuracy: 0.9600\n",
      "iteration number: 1328\t training loss: 0.1069\tvalidation loss: 0.1447\t validation accuracy: 0.9622\n",
      "iteration number: 1329\t training loss: 0.1080\tvalidation loss: 0.1481\t validation accuracy: 0.9600\n",
      "iteration number: 1330\t training loss: 0.1096\tvalidation loss: 0.1529\t validation accuracy: 0.9578\n",
      "iteration number: 1331\t training loss: 0.1086\tvalidation loss: 0.1520\t validation accuracy: 0.9578\n",
      "iteration number: 1332\t training loss: 0.1087\tvalidation loss: 0.1492\t validation accuracy: 0.9578\n",
      "iteration number: 1333\t training loss: 0.1084\tvalidation loss: 0.1501\t validation accuracy: 0.9578\n",
      "iteration number: 1334\t training loss: 0.1080\tvalidation loss: 0.1463\t validation accuracy: 0.9578\n",
      "iteration number: 1335\t training loss: 0.1069\tvalidation loss: 0.1458\t validation accuracy: 0.9556\n",
      "iteration number: 1336\t training loss: 0.1069\tvalidation loss: 0.1476\t validation accuracy: 0.9578\n",
      "iteration number: 1337\t training loss: 0.1062\tvalidation loss: 0.1461\t validation accuracy: 0.9600\n",
      "iteration number: 1338\t training loss: 0.1073\tvalidation loss: 0.1440\t validation accuracy: 0.9578\n",
      "iteration number: 1339\t training loss: 0.1076\tvalidation loss: 0.1424\t validation accuracy: 0.9600\n",
      "iteration number: 1340\t training loss: 0.1069\tvalidation loss: 0.1353\t validation accuracy: 0.9644\n",
      "iteration number: 1341\t training loss: 0.1055\tvalidation loss: 0.1387\t validation accuracy: 0.9600\n",
      "iteration number: 1342\t training loss: 0.1053\tvalidation loss: 0.1384\t validation accuracy: 0.9600\n",
      "iteration number: 1343\t training loss: 0.1055\tvalidation loss: 0.1418\t validation accuracy: 0.9600\n",
      "iteration number: 1344\t training loss: 0.1066\tvalidation loss: 0.1462\t validation accuracy: 0.9600\n",
      "iteration number: 1345\t training loss: 0.1074\tvalidation loss: 0.1472\t validation accuracy: 0.9578\n",
      "iteration number: 1346\t training loss: 0.1089\tvalidation loss: 0.1523\t validation accuracy: 0.9556\n",
      "iteration number: 1347\t training loss: 0.1093\tvalidation loss: 0.1483\t validation accuracy: 0.9600\n",
      "iteration number: 1348\t training loss: 0.1079\tvalidation loss: 0.1440\t validation accuracy: 0.9622\n",
      "iteration number: 1349\t training loss: 0.1090\tvalidation loss: 0.1504\t validation accuracy: 0.9578\n",
      "iteration number: 1350\t training loss: 0.1067\tvalidation loss: 0.1452\t validation accuracy: 0.9600\n",
      "iteration number: 1351\t training loss: 0.1073\tvalidation loss: 0.1484\t validation accuracy: 0.9578\n",
      "iteration number: 1352\t training loss: 0.1079\tvalidation loss: 0.1359\t validation accuracy: 0.9667\n",
      "iteration number: 1353\t training loss: 0.1047\tvalidation loss: 0.1373\t validation accuracy: 0.9644\n",
      "iteration number: 1354\t training loss: 0.1057\tvalidation loss: 0.1396\t validation accuracy: 0.9622\n",
      "iteration number: 1355\t training loss: 0.1061\tvalidation loss: 0.1440\t validation accuracy: 0.9622\n",
      "iteration number: 1356\t training loss: 0.1038\tvalidation loss: 0.1388\t validation accuracy: 0.9644\n",
      "iteration number: 1357\t training loss: 0.1035\tvalidation loss: 0.1379\t validation accuracy: 0.9644\n",
      "iteration number: 1358\t training loss: 0.1033\tvalidation loss: 0.1372\t validation accuracy: 0.9644\n",
      "iteration number: 1359\t training loss: 0.1028\tvalidation loss: 0.1346\t validation accuracy: 0.9644\n",
      "iteration number: 1360\t training loss: 0.1029\tvalidation loss: 0.1385\t validation accuracy: 0.9622\n",
      "iteration number: 1361\t training loss: 0.1027\tvalidation loss: 0.1354\t validation accuracy: 0.9622\n",
      "iteration number: 1362\t training loss: 0.1024\tvalidation loss: 0.1366\t validation accuracy: 0.9644\n",
      "iteration number: 1363\t training loss: 0.1027\tvalidation loss: 0.1405\t validation accuracy: 0.9600\n",
      "iteration number: 1364\t training loss: 0.1030\tvalidation loss: 0.1379\t validation accuracy: 0.9622\n",
      "iteration number: 1365\t training loss: 0.1026\tvalidation loss: 0.1380\t validation accuracy: 0.9667\n",
      "iteration number: 1366\t training loss: 0.1029\tvalidation loss: 0.1399\t validation accuracy: 0.9644\n",
      "iteration number: 1367\t training loss: 0.1034\tvalidation loss: 0.1390\t validation accuracy: 0.9667\n",
      "iteration number: 1368\t training loss: 0.1049\tvalidation loss: 0.1407\t validation accuracy: 0.9644\n",
      "iteration number: 1369\t training loss: 0.1036\tvalidation loss: 0.1378\t validation accuracy: 0.9667\n",
      "iteration number: 1370\t training loss: 0.1024\tvalidation loss: 0.1389\t validation accuracy: 0.9644\n",
      "iteration number: 1371\t training loss: 0.1034\tvalidation loss: 0.1440\t validation accuracy: 0.9600\n",
      "iteration number: 1372\t training loss: 0.1021\tvalidation loss: 0.1396\t validation accuracy: 0.9600\n",
      "iteration number: 1373\t training loss: 0.1024\tvalidation loss: 0.1373\t validation accuracy: 0.9622\n",
      "iteration number: 1374\t training loss: 0.1048\tvalidation loss: 0.1429\t validation accuracy: 0.9578\n",
      "iteration number: 1375\t training loss: 0.1041\tvalidation loss: 0.1415\t validation accuracy: 0.9600\n",
      "iteration number: 1376\t training loss: 0.1027\tvalidation loss: 0.1371\t validation accuracy: 0.9600\n",
      "iteration number: 1377\t training loss: 0.1023\tvalidation loss: 0.1397\t validation accuracy: 0.9578\n",
      "iteration number: 1378\t training loss: 0.1031\tvalidation loss: 0.1420\t validation accuracy: 0.9578\n",
      "iteration number: 1379\t training loss: 0.1030\tvalidation loss: 0.1432\t validation accuracy: 0.9578\n",
      "iteration number: 1380\t training loss: 0.1042\tvalidation loss: 0.1447\t validation accuracy: 0.9600\n",
      "iteration number: 1381\t training loss: 0.1040\tvalidation loss: 0.1455\t validation accuracy: 0.9578\n",
      "iteration number: 1382\t training loss: 0.1017\tvalidation loss: 0.1404\t validation accuracy: 0.9578\n",
      "iteration number: 1383\t training loss: 0.1021\tvalidation loss: 0.1368\t validation accuracy: 0.9578\n",
      "iteration number: 1384\t training loss: 0.1029\tvalidation loss: 0.1378\t validation accuracy: 0.9600\n",
      "iteration number: 1385\t training loss: 0.1026\tvalidation loss: 0.1395\t validation accuracy: 0.9578\n",
      "iteration number: 1386\t training loss: 0.1038\tvalidation loss: 0.1370\t validation accuracy: 0.9578\n",
      "iteration number: 1387\t training loss: 0.1024\tvalidation loss: 0.1377\t validation accuracy: 0.9578\n",
      "iteration number: 1388\t training loss: 0.1019\tvalidation loss: 0.1408\t validation accuracy: 0.9578\n",
      "iteration number: 1389\t training loss: 0.1034\tvalidation loss: 0.1483\t validation accuracy: 0.9578\n",
      "iteration number: 1390\t training loss: 0.1025\tvalidation loss: 0.1400\t validation accuracy: 0.9600\n",
      "iteration number: 1391\t training loss: 0.1019\tvalidation loss: 0.1390\t validation accuracy: 0.9600\n",
      "iteration number: 1392\t training loss: 0.1015\tvalidation loss: 0.1380\t validation accuracy: 0.9578\n",
      "iteration number: 1393\t training loss: 0.1016\tvalidation loss: 0.1403\t validation accuracy: 0.9578\n",
      "iteration number: 1394\t training loss: 0.1014\tvalidation loss: 0.1383\t validation accuracy: 0.9578\n",
      "iteration number: 1395\t training loss: 0.1013\tvalidation loss: 0.1401\t validation accuracy: 0.9578\n",
      "iteration number: 1396\t training loss: 0.1006\tvalidation loss: 0.1371\t validation accuracy: 0.9578\n",
      "iteration number: 1397\t training loss: 0.1003\tvalidation loss: 0.1374\t validation accuracy: 0.9578\n",
      "iteration number: 1398\t training loss: 0.1008\tvalidation loss: 0.1370\t validation accuracy: 0.9622\n",
      "iteration number: 1399\t training loss: 0.1010\tvalidation loss: 0.1387\t validation accuracy: 0.9644\n",
      "iteration number: 1400\t training loss: 0.1013\tvalidation loss: 0.1438\t validation accuracy: 0.9600\n",
      "iteration number: 1401\t training loss: 0.1016\tvalidation loss: 0.1422\t validation accuracy: 0.9622\n",
      "iteration number: 1402\t training loss: 0.1011\tvalidation loss: 0.1407\t validation accuracy: 0.9622\n",
      "iteration number: 1403\t training loss: 0.1011\tvalidation loss: 0.1438\t validation accuracy: 0.9600\n",
      "iteration number: 1404\t training loss: 0.1000\tvalidation loss: 0.1410\t validation accuracy: 0.9578\n",
      "iteration number: 1405\t training loss: 0.1000\tvalidation loss: 0.1374\t validation accuracy: 0.9667\n",
      "iteration number: 1406\t training loss: 0.1000\tvalidation loss: 0.1404\t validation accuracy: 0.9578\n",
      "iteration number: 1407\t training loss: 0.1013\tvalidation loss: 0.1397\t validation accuracy: 0.9622\n",
      "iteration number: 1408\t training loss: 0.1004\tvalidation loss: 0.1387\t validation accuracy: 0.9622\n",
      "iteration number: 1409\t training loss: 0.1005\tvalidation loss: 0.1416\t validation accuracy: 0.9600\n",
      "iteration number: 1410\t training loss: 0.0999\tvalidation loss: 0.1418\t validation accuracy: 0.9600\n",
      "iteration number: 1411\t training loss: 0.1010\tvalidation loss: 0.1380\t validation accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1412\t training loss: 0.1029\tvalidation loss: 0.1366\t validation accuracy: 0.9689\n",
      "iteration number: 1413\t training loss: 0.0991\tvalidation loss: 0.1379\t validation accuracy: 0.9622\n",
      "iteration number: 1414\t training loss: 0.0993\tvalidation loss: 0.1382\t validation accuracy: 0.9644\n",
      "iteration number: 1415\t training loss: 0.0996\tvalidation loss: 0.1401\t validation accuracy: 0.9644\n",
      "iteration number: 1416\t training loss: 0.0997\tvalidation loss: 0.1418\t validation accuracy: 0.9600\n",
      "iteration number: 1417\t training loss: 0.0992\tvalidation loss: 0.1411\t validation accuracy: 0.9600\n",
      "iteration number: 1418\t training loss: 0.1002\tvalidation loss: 0.1442\t validation accuracy: 0.9578\n",
      "iteration number: 1419\t training loss: 0.1005\tvalidation loss: 0.1438\t validation accuracy: 0.9600\n",
      "iteration number: 1420\t training loss: 0.0999\tvalidation loss: 0.1450\t validation accuracy: 0.9578\n",
      "iteration number: 1421\t training loss: 0.0999\tvalidation loss: 0.1469\t validation accuracy: 0.9556\n",
      "iteration number: 1422\t training loss: 0.1020\tvalidation loss: 0.1531\t validation accuracy: 0.9556\n",
      "iteration number: 1423\t training loss: 0.1005\tvalidation loss: 0.1514\t validation accuracy: 0.9556\n",
      "iteration number: 1424\t training loss: 0.1013\tvalidation loss: 0.1537\t validation accuracy: 0.9533\n",
      "iteration number: 1425\t training loss: 0.1011\tvalidation loss: 0.1518\t validation accuracy: 0.9533\n",
      "iteration number: 1426\t training loss: 0.1001\tvalidation loss: 0.1495\t validation accuracy: 0.9578\n",
      "iteration number: 1427\t training loss: 0.0997\tvalidation loss: 0.1487\t validation accuracy: 0.9556\n",
      "iteration number: 1428\t training loss: 0.0991\tvalidation loss: 0.1473\t validation accuracy: 0.9578\n",
      "iteration number: 1429\t training loss: 0.0991\tvalidation loss: 0.1432\t validation accuracy: 0.9600\n",
      "iteration number: 1430\t training loss: 0.0987\tvalidation loss: 0.1433\t validation accuracy: 0.9578\n",
      "iteration number: 1431\t training loss: 0.0986\tvalidation loss: 0.1424\t validation accuracy: 0.9578\n",
      "iteration number: 1432\t training loss: 0.0986\tvalidation loss: 0.1433\t validation accuracy: 0.9578\n",
      "iteration number: 1433\t training loss: 0.0994\tvalidation loss: 0.1474\t validation accuracy: 0.9578\n",
      "iteration number: 1434\t training loss: 0.0984\tvalidation loss: 0.1453\t validation accuracy: 0.9578\n",
      "iteration number: 1435\t training loss: 0.0986\tvalidation loss: 0.1467\t validation accuracy: 0.9578\n",
      "iteration number: 1436\t training loss: 0.0992\tvalidation loss: 0.1496\t validation accuracy: 0.9556\n",
      "iteration number: 1437\t training loss: 0.0987\tvalidation loss: 0.1453\t validation accuracy: 0.9600\n",
      "iteration number: 1438\t training loss: 0.0994\tvalidation loss: 0.1396\t validation accuracy: 0.9578\n",
      "iteration number: 1439\t training loss: 0.0982\tvalidation loss: 0.1404\t validation accuracy: 0.9600\n",
      "iteration number: 1440\t training loss: 0.0992\tvalidation loss: 0.1384\t validation accuracy: 0.9600\n",
      "iteration number: 1441\t training loss: 0.0973\tvalidation loss: 0.1402\t validation accuracy: 0.9644\n",
      "iteration number: 1442\t training loss: 0.0973\tvalidation loss: 0.1391\t validation accuracy: 0.9644\n",
      "iteration number: 1443\t training loss: 0.0970\tvalidation loss: 0.1397\t validation accuracy: 0.9644\n",
      "iteration number: 1444\t training loss: 0.0966\tvalidation loss: 0.1392\t validation accuracy: 0.9600\n",
      "iteration number: 1445\t training loss: 0.0968\tvalidation loss: 0.1387\t validation accuracy: 0.9622\n",
      "iteration number: 1446\t training loss: 0.0973\tvalidation loss: 0.1355\t validation accuracy: 0.9667\n",
      "iteration number: 1447\t training loss: 0.0971\tvalidation loss: 0.1404\t validation accuracy: 0.9578\n",
      "iteration number: 1448\t training loss: 0.0988\tvalidation loss: 0.1418\t validation accuracy: 0.9600\n",
      "iteration number: 1449\t training loss: 0.0968\tvalidation loss: 0.1389\t validation accuracy: 0.9622\n",
      "iteration number: 1450\t training loss: 0.0970\tvalidation loss: 0.1389\t validation accuracy: 0.9622\n",
      "iteration number: 1451\t training loss: 0.0970\tvalidation loss: 0.1378\t validation accuracy: 0.9622\n",
      "iteration number: 1452\t training loss: 0.0968\tvalidation loss: 0.1339\t validation accuracy: 0.9622\n",
      "iteration number: 1453\t training loss: 0.0965\tvalidation loss: 0.1329\t validation accuracy: 0.9689\n",
      "iteration number: 1454\t training loss: 0.0971\tvalidation loss: 0.1354\t validation accuracy: 0.9622\n",
      "iteration number: 1455\t training loss: 0.0963\tvalidation loss: 0.1405\t validation accuracy: 0.9578\n",
      "iteration number: 1456\t training loss: 0.0956\tvalidation loss: 0.1369\t validation accuracy: 0.9622\n",
      "iteration number: 1457\t training loss: 0.0961\tvalidation loss: 0.1341\t validation accuracy: 0.9622\n",
      "iteration number: 1458\t training loss: 0.0958\tvalidation loss: 0.1349\t validation accuracy: 0.9600\n",
      "iteration number: 1459\t training loss: 0.0963\tvalidation loss: 0.1344\t validation accuracy: 0.9600\n",
      "iteration number: 1460\t training loss: 0.0959\tvalidation loss: 0.1353\t validation accuracy: 0.9600\n",
      "iteration number: 1461\t training loss: 0.0960\tvalidation loss: 0.1341\t validation accuracy: 0.9644\n",
      "iteration number: 1462\t training loss: 0.0966\tvalidation loss: 0.1352\t validation accuracy: 0.9622\n",
      "iteration number: 1463\t training loss: 0.0967\tvalidation loss: 0.1376\t validation accuracy: 0.9578\n",
      "iteration number: 1464\t training loss: 0.0978\tvalidation loss: 0.1377\t validation accuracy: 0.9600\n",
      "iteration number: 1465\t training loss: 0.0979\tvalidation loss: 0.1367\t validation accuracy: 0.9622\n",
      "iteration number: 1466\t training loss: 0.0969\tvalidation loss: 0.1331\t validation accuracy: 0.9622\n",
      "iteration number: 1467\t training loss: 0.0974\tvalidation loss: 0.1317\t validation accuracy: 0.9644\n",
      "iteration number: 1468\t training loss: 0.0977\tvalidation loss: 0.1353\t validation accuracy: 0.9600\n",
      "iteration number: 1469\t training loss: 0.0960\tvalidation loss: 0.1362\t validation accuracy: 0.9622\n",
      "iteration number: 1470\t training loss: 0.0950\tvalidation loss: 0.1287\t validation accuracy: 0.9667\n",
      "iteration number: 1471\t training loss: 0.0966\tvalidation loss: 0.1267\t validation accuracy: 0.9733\n",
      "iteration number: 1472\t training loss: 0.0966\tvalidation loss: 0.1265\t validation accuracy: 0.9689\n",
      "iteration number: 1473\t training loss: 0.0970\tvalidation loss: 0.1257\t validation accuracy: 0.9689\n",
      "iteration number: 1474\t training loss: 0.0956\tvalidation loss: 0.1297\t validation accuracy: 0.9667\n",
      "iteration number: 1475\t training loss: 0.0946\tvalidation loss: 0.1321\t validation accuracy: 0.9644\n",
      "iteration number: 1476\t training loss: 0.0959\tvalidation loss: 0.1299\t validation accuracy: 0.9644\n",
      "iteration number: 1477\t training loss: 0.0963\tvalidation loss: 0.1294\t validation accuracy: 0.9711\n",
      "iteration number: 1478\t training loss: 0.0961\tvalidation loss: 0.1363\t validation accuracy: 0.9644\n",
      "iteration number: 1479\t training loss: 0.0961\tvalidation loss: 0.1375\t validation accuracy: 0.9644\n",
      "iteration number: 1480\t training loss: 0.0954\tvalidation loss: 0.1359\t validation accuracy: 0.9644\n",
      "iteration number: 1481\t training loss: 0.0957\tvalidation loss: 0.1379\t validation accuracy: 0.9644\n",
      "iteration number: 1482\t training loss: 0.0947\tvalidation loss: 0.1329\t validation accuracy: 0.9644\n",
      "iteration number: 1483\t training loss: 0.0941\tvalidation loss: 0.1340\t validation accuracy: 0.9600\n",
      "iteration number: 1484\t training loss: 0.0938\tvalidation loss: 0.1326\t validation accuracy: 0.9644\n",
      "iteration number: 1485\t training loss: 0.0938\tvalidation loss: 0.1338\t validation accuracy: 0.9600\n",
      "iteration number: 1486\t training loss: 0.0937\tvalidation loss: 0.1332\t validation accuracy: 0.9600\n",
      "iteration number: 1487\t training loss: 0.0947\tvalidation loss: 0.1329\t validation accuracy: 0.9689\n",
      "iteration number: 1488\t training loss: 0.0944\tvalidation loss: 0.1314\t validation accuracy: 0.9667\n",
      "iteration number: 1489\t training loss: 0.0939\tvalidation loss: 0.1343\t validation accuracy: 0.9644\n",
      "iteration number: 1490\t training loss: 0.0943\tvalidation loss: 0.1303\t validation accuracy: 0.9644\n",
      "iteration number: 1491\t training loss: 0.0942\tvalidation loss: 0.1300\t validation accuracy: 0.9644\n",
      "iteration number: 1492\t training loss: 0.0939\tvalidation loss: 0.1328\t validation accuracy: 0.9667\n",
      "iteration number: 1493\t training loss: 0.0935\tvalidation loss: 0.1332\t validation accuracy: 0.9644\n",
      "iteration number: 1494\t training loss: 0.0935\tvalidation loss: 0.1311\t validation accuracy: 0.9667\n",
      "iteration number: 1495\t training loss: 0.0935\tvalidation loss: 0.1310\t validation accuracy: 0.9667\n",
      "iteration number: 1496\t training loss: 0.0930\tvalidation loss: 0.1295\t validation accuracy: 0.9622\n",
      "iteration number: 1497\t training loss: 0.0933\tvalidation loss: 0.1283\t validation accuracy: 0.9667\n",
      "iteration number: 1498\t training loss: 0.0938\tvalidation loss: 0.1285\t validation accuracy: 0.9644\n",
      "iteration number: 1499\t training loss: 0.0932\tvalidation loss: 0.1298\t validation accuracy: 0.9644\n",
      "iteration number: 1500\t training loss: 0.0935\tvalidation loss: 0.1310\t validation accuracy: 0.9622\n",
      "iteration number: 1501\t training loss: 0.0941\tvalidation loss: 0.1326\t validation accuracy: 0.9622\n",
      "iteration number: 1502\t training loss: 0.0934\tvalidation loss: 0.1364\t validation accuracy: 0.9622\n",
      "iteration number: 1503\t training loss: 0.0937\tvalidation loss: 0.1382\t validation accuracy: 0.9600\n",
      "iteration number: 1504\t training loss: 0.0926\tvalidation loss: 0.1381\t validation accuracy: 0.9600\n",
      "iteration number: 1505\t training loss: 0.0922\tvalidation loss: 0.1337\t validation accuracy: 0.9667\n",
      "iteration number: 1506\t training loss: 0.0920\tvalidation loss: 0.1328\t validation accuracy: 0.9667\n",
      "iteration number: 1507\t training loss: 0.0922\tvalidation loss: 0.1326\t validation accuracy: 0.9644\n",
      "iteration number: 1508\t training loss: 0.0923\tvalidation loss: 0.1365\t validation accuracy: 0.9600\n",
      "iteration number: 1509\t training loss: 0.0922\tvalidation loss: 0.1320\t validation accuracy: 0.9622\n",
      "iteration number: 1510\t training loss: 0.0924\tvalidation loss: 0.1354\t validation accuracy: 0.9600\n",
      "iteration number: 1511\t training loss: 0.0918\tvalidation loss: 0.1332\t validation accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1512\t training loss: 0.0921\tvalidation loss: 0.1356\t validation accuracy: 0.9600\n",
      "iteration number: 1513\t training loss: 0.0926\tvalidation loss: 0.1329\t validation accuracy: 0.9600\n",
      "iteration number: 1514\t training loss: 0.0923\tvalidation loss: 0.1313\t validation accuracy: 0.9667\n",
      "iteration number: 1515\t training loss: 0.0926\tvalidation loss: 0.1310\t validation accuracy: 0.9667\n",
      "iteration number: 1516\t training loss: 0.0929\tvalidation loss: 0.1314\t validation accuracy: 0.9667\n",
      "iteration number: 1517\t training loss: 0.0916\tvalidation loss: 0.1295\t validation accuracy: 0.9689\n",
      "iteration number: 1518\t training loss: 0.0917\tvalidation loss: 0.1298\t validation accuracy: 0.9689\n",
      "iteration number: 1519\t training loss: 0.0923\tvalidation loss: 0.1278\t validation accuracy: 0.9689\n",
      "iteration number: 1520\t training loss: 0.0924\tvalidation loss: 0.1267\t validation accuracy: 0.9689\n",
      "iteration number: 1521\t training loss: 0.0923\tvalidation loss: 0.1274\t validation accuracy: 0.9689\n",
      "iteration number: 1522\t training loss: 0.0955\tvalidation loss: 0.1254\t validation accuracy: 0.9667\n",
      "iteration number: 1523\t training loss: 0.0961\tvalidation loss: 0.1264\t validation accuracy: 0.9667\n",
      "iteration number: 1524\t training loss: 0.0937\tvalidation loss: 0.1276\t validation accuracy: 0.9644\n",
      "iteration number: 1525\t training loss: 0.0929\tvalidation loss: 0.1258\t validation accuracy: 0.9689\n",
      "iteration number: 1526\t training loss: 0.0936\tvalidation loss: 0.1254\t validation accuracy: 0.9689\n",
      "iteration number: 1527\t training loss: 0.0942\tvalidation loss: 0.1258\t validation accuracy: 0.9644\n",
      "iteration number: 1528\t training loss: 0.0939\tvalidation loss: 0.1285\t validation accuracy: 0.9667\n",
      "iteration number: 1529\t training loss: 0.0938\tvalidation loss: 0.1295\t validation accuracy: 0.9644\n",
      "iteration number: 1530\t training loss: 0.0929\tvalidation loss: 0.1340\t validation accuracy: 0.9622\n",
      "iteration number: 1531\t training loss: 0.0934\tvalidation loss: 0.1348\t validation accuracy: 0.9622\n",
      "iteration number: 1532\t training loss: 0.0918\tvalidation loss: 0.1351\t validation accuracy: 0.9600\n",
      "iteration number: 1533\t training loss: 0.0917\tvalidation loss: 0.1359\t validation accuracy: 0.9600\n",
      "iteration number: 1534\t training loss: 0.0926\tvalidation loss: 0.1385\t validation accuracy: 0.9600\n",
      "iteration number: 1535\t training loss: 0.0919\tvalidation loss: 0.1364\t validation accuracy: 0.9600\n",
      "iteration number: 1536\t training loss: 0.0914\tvalidation loss: 0.1364\t validation accuracy: 0.9600\n",
      "iteration number: 1537\t training loss: 0.0946\tvalidation loss: 0.1443\t validation accuracy: 0.9556\n",
      "iteration number: 1538\t training loss: 0.0936\tvalidation loss: 0.1418\t validation accuracy: 0.9578\n",
      "iteration number: 1539\t training loss: 0.0928\tvalidation loss: 0.1395\t validation accuracy: 0.9600\n",
      "iteration number: 1540\t training loss: 0.0925\tvalidation loss: 0.1421\t validation accuracy: 0.9600\n",
      "iteration number: 1541\t training loss: 0.0906\tvalidation loss: 0.1362\t validation accuracy: 0.9600\n",
      "iteration number: 1542\t training loss: 0.0914\tvalidation loss: 0.1360\t validation accuracy: 0.9600\n",
      "iteration number: 1543\t training loss: 0.0905\tvalidation loss: 0.1339\t validation accuracy: 0.9644\n",
      "iteration number: 1544\t training loss: 0.0906\tvalidation loss: 0.1354\t validation accuracy: 0.9600\n",
      "iteration number: 1545\t training loss: 0.0906\tvalidation loss: 0.1364\t validation accuracy: 0.9600\n",
      "iteration number: 1546\t training loss: 0.0903\tvalidation loss: 0.1334\t validation accuracy: 0.9622\n",
      "iteration number: 1547\t training loss: 0.0899\tvalidation loss: 0.1346\t validation accuracy: 0.9600\n",
      "iteration number: 1548\t training loss: 0.0909\tvalidation loss: 0.1309\t validation accuracy: 0.9622\n",
      "iteration number: 1549\t training loss: 0.0914\tvalidation loss: 0.1316\t validation accuracy: 0.9622\n",
      "iteration number: 1550\t training loss: 0.0928\tvalidation loss: 0.1332\t validation accuracy: 0.9600\n",
      "iteration number: 1551\t training loss: 0.0918\tvalidation loss: 0.1352\t validation accuracy: 0.9644\n",
      "iteration number: 1552\t training loss: 0.0901\tvalidation loss: 0.1368\t validation accuracy: 0.9578\n",
      "iteration number: 1553\t training loss: 0.0894\tvalidation loss: 0.1335\t validation accuracy: 0.9600\n",
      "iteration number: 1554\t training loss: 0.0895\tvalidation loss: 0.1347\t validation accuracy: 0.9600\n",
      "iteration number: 1555\t training loss: 0.0895\tvalidation loss: 0.1346\t validation accuracy: 0.9600\n",
      "iteration number: 1556\t training loss: 0.0892\tvalidation loss: 0.1315\t validation accuracy: 0.9644\n",
      "iteration number: 1557\t training loss: 0.0894\tvalidation loss: 0.1335\t validation accuracy: 0.9622\n",
      "iteration number: 1558\t training loss: 0.0896\tvalidation loss: 0.1337\t validation accuracy: 0.9622\n",
      "iteration number: 1559\t training loss: 0.0898\tvalidation loss: 0.1318\t validation accuracy: 0.9644\n",
      "iteration number: 1560\t training loss: 0.0896\tvalidation loss: 0.1286\t validation accuracy: 0.9667\n",
      "iteration number: 1561\t training loss: 0.0899\tvalidation loss: 0.1297\t validation accuracy: 0.9667\n",
      "iteration number: 1562\t training loss: 0.0894\tvalidation loss: 0.1293\t validation accuracy: 0.9644\n",
      "iteration number: 1563\t training loss: 0.0903\tvalidation loss: 0.1253\t validation accuracy: 0.9689\n",
      "iteration number: 1564\t training loss: 0.0898\tvalidation loss: 0.1240\t validation accuracy: 0.9689\n",
      "iteration number: 1565\t training loss: 0.0893\tvalidation loss: 0.1259\t validation accuracy: 0.9689\n",
      "iteration number: 1566\t training loss: 0.0894\tvalidation loss: 0.1245\t validation accuracy: 0.9711\n",
      "iteration number: 1567\t training loss: 0.0891\tvalidation loss: 0.1261\t validation accuracy: 0.9689\n",
      "iteration number: 1568\t training loss: 0.0897\tvalidation loss: 0.1288\t validation accuracy: 0.9667\n",
      "iteration number: 1569\t training loss: 0.0900\tvalidation loss: 0.1294\t validation accuracy: 0.9644\n",
      "iteration number: 1570\t training loss: 0.0915\tvalidation loss: 0.1324\t validation accuracy: 0.9622\n",
      "iteration number: 1571\t training loss: 0.0910\tvalidation loss: 0.1325\t validation accuracy: 0.9622\n",
      "iteration number: 1572\t training loss: 0.0928\tvalidation loss: 0.1350\t validation accuracy: 0.9622\n",
      "iteration number: 1573\t training loss: 0.0917\tvalidation loss: 0.1352\t validation accuracy: 0.9600\n",
      "iteration number: 1574\t training loss: 0.0879\tvalidation loss: 0.1298\t validation accuracy: 0.9644\n",
      "iteration number: 1575\t training loss: 0.0886\tvalidation loss: 0.1300\t validation accuracy: 0.9622\n",
      "iteration number: 1576\t training loss: 0.0897\tvalidation loss: 0.1333\t validation accuracy: 0.9578\n",
      "iteration number: 1577\t training loss: 0.0902\tvalidation loss: 0.1344\t validation accuracy: 0.9600\n",
      "iteration number: 1578\t training loss: 0.0888\tvalidation loss: 0.1285\t validation accuracy: 0.9644\n",
      "iteration number: 1579\t training loss: 0.0905\tvalidation loss: 0.1340\t validation accuracy: 0.9556\n",
      "iteration number: 1580\t training loss: 0.0897\tvalidation loss: 0.1334\t validation accuracy: 0.9578\n",
      "iteration number: 1581\t training loss: 0.0889\tvalidation loss: 0.1320\t validation accuracy: 0.9600\n",
      "iteration number: 1582\t training loss: 0.0881\tvalidation loss: 0.1295\t validation accuracy: 0.9622\n",
      "iteration number: 1583\t training loss: 0.0882\tvalidation loss: 0.1259\t validation accuracy: 0.9689\n",
      "iteration number: 1584\t training loss: 0.0892\tvalidation loss: 0.1226\t validation accuracy: 0.9667\n",
      "iteration number: 1585\t training loss: 0.0887\tvalidation loss: 0.1227\t validation accuracy: 0.9689\n",
      "iteration number: 1586\t training loss: 0.0886\tvalidation loss: 0.1229\t validation accuracy: 0.9711\n",
      "iteration number: 1587\t training loss: 0.0890\tvalidation loss: 0.1229\t validation accuracy: 0.9733\n",
      "iteration number: 1588\t training loss: 0.0887\tvalidation loss: 0.1248\t validation accuracy: 0.9733\n",
      "iteration number: 1589\t training loss: 0.0876\tvalidation loss: 0.1287\t validation accuracy: 0.9689\n",
      "iteration number: 1590\t training loss: 0.0874\tvalidation loss: 0.1289\t validation accuracy: 0.9689\n",
      "iteration number: 1591\t training loss: 0.0875\tvalidation loss: 0.1270\t validation accuracy: 0.9689\n",
      "iteration number: 1592\t training loss: 0.0870\tvalidation loss: 0.1266\t validation accuracy: 0.9689\n",
      "iteration number: 1593\t training loss: 0.0872\tvalidation loss: 0.1249\t validation accuracy: 0.9689\n",
      "iteration number: 1594\t training loss: 0.0879\tvalidation loss: 0.1261\t validation accuracy: 0.9689\n",
      "iteration number: 1595\t training loss: 0.0874\tvalidation loss: 0.1238\t validation accuracy: 0.9667\n",
      "iteration number: 1596\t training loss: 0.0880\tvalidation loss: 0.1225\t validation accuracy: 0.9667\n",
      "iteration number: 1597\t training loss: 0.0900\tvalidation loss: 0.1225\t validation accuracy: 0.9667\n",
      "iteration number: 1598\t training loss: 0.0875\tvalidation loss: 0.1257\t validation accuracy: 0.9667\n",
      "iteration number: 1599\t training loss: 0.0879\tvalidation loss: 0.1278\t validation accuracy: 0.9622\n",
      "iteration number: 1600\t training loss: 0.0890\tvalidation loss: 0.1300\t validation accuracy: 0.9644\n",
      "iteration number: 1601\t training loss: 0.0879\tvalidation loss: 0.1316\t validation accuracy: 0.9622\n",
      "iteration number: 1602\t training loss: 0.0881\tvalidation loss: 0.1296\t validation accuracy: 0.9622\n",
      "iteration number: 1603\t training loss: 0.0886\tvalidation loss: 0.1314\t validation accuracy: 0.9644\n",
      "iteration number: 1604\t training loss: 0.0888\tvalidation loss: 0.1307\t validation accuracy: 0.9644\n",
      "iteration number: 1605\t training loss: 0.0885\tvalidation loss: 0.1309\t validation accuracy: 0.9622\n",
      "iteration number: 1606\t training loss: 0.0880\tvalidation loss: 0.1303\t validation accuracy: 0.9622\n",
      "iteration number: 1607\t training loss: 0.0874\tvalidation loss: 0.1324\t validation accuracy: 0.9578\n",
      "iteration number: 1608\t training loss: 0.0869\tvalidation loss: 0.1307\t validation accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1609\t training loss: 0.0866\tvalidation loss: 0.1276\t validation accuracy: 0.9667\n",
      "iteration number: 1610\t training loss: 0.0865\tvalidation loss: 0.1277\t validation accuracy: 0.9644\n",
      "iteration number: 1611\t training loss: 0.0864\tvalidation loss: 0.1274\t validation accuracy: 0.9622\n",
      "iteration number: 1612\t training loss: 0.0888\tvalidation loss: 0.1254\t validation accuracy: 0.9644\n",
      "iteration number: 1613\t training loss: 0.0885\tvalidation loss: 0.1275\t validation accuracy: 0.9644\n",
      "iteration number: 1614\t training loss: 0.0889\tvalidation loss: 0.1258\t validation accuracy: 0.9644\n",
      "iteration number: 1615\t training loss: 0.0897\tvalidation loss: 0.1260\t validation accuracy: 0.9667\n",
      "iteration number: 1616\t training loss: 0.0873\tvalidation loss: 0.1273\t validation accuracy: 0.9667\n",
      "iteration number: 1617\t training loss: 0.0877\tvalidation loss: 0.1278\t validation accuracy: 0.9667\n",
      "iteration number: 1618\t training loss: 0.0880\tvalidation loss: 0.1288\t validation accuracy: 0.9644\n",
      "iteration number: 1619\t training loss: 0.0879\tvalidation loss: 0.1253\t validation accuracy: 0.9644\n",
      "iteration number: 1620\t training loss: 0.0892\tvalidation loss: 0.1255\t validation accuracy: 0.9644\n",
      "iteration number: 1621\t training loss: 0.0938\tvalidation loss: 0.1284\t validation accuracy: 0.9711\n",
      "iteration number: 1622\t training loss: 0.0892\tvalidation loss: 0.1274\t validation accuracy: 0.9667\n",
      "iteration number: 1623\t training loss: 0.0875\tvalidation loss: 0.1258\t validation accuracy: 0.9667\n",
      "iteration number: 1624\t training loss: 0.0867\tvalidation loss: 0.1295\t validation accuracy: 0.9622\n",
      "iteration number: 1625\t training loss: 0.0860\tvalidation loss: 0.1292\t validation accuracy: 0.9644\n",
      "iteration number: 1626\t training loss: 0.0861\tvalidation loss: 0.1286\t validation accuracy: 0.9622\n",
      "iteration number: 1627\t training loss: 0.0862\tvalidation loss: 0.1286\t validation accuracy: 0.9622\n",
      "iteration number: 1628\t training loss: 0.0859\tvalidation loss: 0.1279\t validation accuracy: 0.9622\n",
      "iteration number: 1629\t training loss: 0.0855\tvalidation loss: 0.1305\t validation accuracy: 0.9644\n",
      "iteration number: 1630\t training loss: 0.0866\tvalidation loss: 0.1336\t validation accuracy: 0.9622\n",
      "iteration number: 1631\t training loss: 0.0877\tvalidation loss: 0.1384\t validation accuracy: 0.9600\n",
      "iteration number: 1632\t training loss: 0.0856\tvalidation loss: 0.1328\t validation accuracy: 0.9600\n",
      "iteration number: 1633\t training loss: 0.0851\tvalidation loss: 0.1318\t validation accuracy: 0.9667\n",
      "iteration number: 1634\t training loss: 0.0850\tvalidation loss: 0.1323\t validation accuracy: 0.9644\n",
      "iteration number: 1635\t training loss: 0.0850\tvalidation loss: 0.1317\t validation accuracy: 0.9644\n",
      "iteration number: 1636\t training loss: 0.0852\tvalidation loss: 0.1335\t validation accuracy: 0.9578\n",
      "iteration number: 1637\t training loss: 0.0855\tvalidation loss: 0.1326\t validation accuracy: 0.9622\n",
      "iteration number: 1638\t training loss: 0.0850\tvalidation loss: 0.1321\t validation accuracy: 0.9622\n",
      "iteration number: 1639\t training loss: 0.0846\tvalidation loss: 0.1320\t validation accuracy: 0.9600\n",
      "iteration number: 1640\t training loss: 0.0844\tvalidation loss: 0.1298\t validation accuracy: 0.9600\n",
      "iteration number: 1641\t training loss: 0.0845\tvalidation loss: 0.1289\t validation accuracy: 0.9622\n",
      "iteration number: 1642\t training loss: 0.0852\tvalidation loss: 0.1299\t validation accuracy: 0.9600\n",
      "iteration number: 1643\t training loss: 0.0849\tvalidation loss: 0.1299\t validation accuracy: 0.9600\n",
      "iteration number: 1644\t training loss: 0.0857\tvalidation loss: 0.1307\t validation accuracy: 0.9644\n",
      "iteration number: 1645\t training loss: 0.0863\tvalidation loss: 0.1329\t validation accuracy: 0.9600\n",
      "iteration number: 1646\t training loss: 0.0855\tvalidation loss: 0.1301\t validation accuracy: 0.9600\n",
      "iteration number: 1647\t training loss: 0.0863\tvalidation loss: 0.1326\t validation accuracy: 0.9600\n",
      "iteration number: 1648\t training loss: 0.0871\tvalidation loss: 0.1350\t validation accuracy: 0.9600\n",
      "iteration number: 1649\t training loss: 0.0882\tvalidation loss: 0.1374\t validation accuracy: 0.9622\n",
      "iteration number: 1650\t training loss: 0.0874\tvalidation loss: 0.1359\t validation accuracy: 0.9622\n",
      "iteration number: 1651\t training loss: 0.0861\tvalidation loss: 0.1324\t validation accuracy: 0.9622\n",
      "iteration number: 1652\t training loss: 0.0852\tvalidation loss: 0.1271\t validation accuracy: 0.9711\n",
      "iteration number: 1653\t training loss: 0.0847\tvalidation loss: 0.1257\t validation accuracy: 0.9667\n",
      "iteration number: 1654\t training loss: 0.0849\tvalidation loss: 0.1264\t validation accuracy: 0.9667\n",
      "iteration number: 1655\t training loss: 0.0848\tvalidation loss: 0.1292\t validation accuracy: 0.9600\n",
      "iteration number: 1656\t training loss: 0.0854\tvalidation loss: 0.1326\t validation accuracy: 0.9578\n",
      "iteration number: 1657\t training loss: 0.0848\tvalidation loss: 0.1316\t validation accuracy: 0.9600\n",
      "iteration number: 1658\t training loss: 0.0842\tvalidation loss: 0.1304\t validation accuracy: 0.9600\n",
      "iteration number: 1659\t training loss: 0.0848\tvalidation loss: 0.1301\t validation accuracy: 0.9600\n",
      "iteration number: 1660\t training loss: 0.0840\tvalidation loss: 0.1301\t validation accuracy: 0.9578\n",
      "iteration number: 1661\t training loss: 0.0854\tvalidation loss: 0.1321\t validation accuracy: 0.9600\n",
      "iteration number: 1662\t training loss: 0.0837\tvalidation loss: 0.1283\t validation accuracy: 0.9622\n",
      "iteration number: 1663\t training loss: 0.0837\tvalidation loss: 0.1304\t validation accuracy: 0.9600\n",
      "iteration number: 1664\t training loss: 0.0837\tvalidation loss: 0.1287\t validation accuracy: 0.9600\n",
      "iteration number: 1665\t training loss: 0.0841\tvalidation loss: 0.1269\t validation accuracy: 0.9622\n",
      "iteration number: 1666\t training loss: 0.0834\tvalidation loss: 0.1274\t validation accuracy: 0.9600\n",
      "iteration number: 1667\t training loss: 0.0838\tvalidation loss: 0.1288\t validation accuracy: 0.9600\n",
      "iteration number: 1668\t training loss: 0.0847\tvalidation loss: 0.1315\t validation accuracy: 0.9578\n",
      "iteration number: 1669\t training loss: 0.0841\tvalidation loss: 0.1311\t validation accuracy: 0.9600\n",
      "iteration number: 1670\t training loss: 0.0844\tvalidation loss: 0.1291\t validation accuracy: 0.9622\n",
      "iteration number: 1671\t training loss: 0.0838\tvalidation loss: 0.1276\t validation accuracy: 0.9644\n",
      "iteration number: 1672\t training loss: 0.0852\tvalidation loss: 0.1289\t validation accuracy: 0.9622\n",
      "iteration number: 1673\t training loss: 0.0842\tvalidation loss: 0.1261\t validation accuracy: 0.9622\n",
      "iteration number: 1674\t training loss: 0.0866\tvalidation loss: 0.1277\t validation accuracy: 0.9667\n",
      "iteration number: 1675\t training loss: 0.0856\tvalidation loss: 0.1254\t validation accuracy: 0.9644\n",
      "iteration number: 1676\t training loss: 0.0836\tvalidation loss: 0.1221\t validation accuracy: 0.9644\n",
      "iteration number: 1677\t training loss: 0.0840\tvalidation loss: 0.1221\t validation accuracy: 0.9644\n",
      "iteration number: 1678\t training loss: 0.0832\tvalidation loss: 0.1231\t validation accuracy: 0.9644\n",
      "iteration number: 1679\t training loss: 0.0832\tvalidation loss: 0.1238\t validation accuracy: 0.9644\n",
      "iteration number: 1680\t training loss: 0.0842\tvalidation loss: 0.1225\t validation accuracy: 0.9667\n",
      "iteration number: 1681\t training loss: 0.0831\tvalidation loss: 0.1232\t validation accuracy: 0.9644\n",
      "iteration number: 1682\t training loss: 0.0826\tvalidation loss: 0.1247\t validation accuracy: 0.9667\n",
      "iteration number: 1683\t training loss: 0.0826\tvalidation loss: 0.1226\t validation accuracy: 0.9667\n",
      "iteration number: 1684\t training loss: 0.0824\tvalidation loss: 0.1226\t validation accuracy: 0.9667\n",
      "iteration number: 1685\t training loss: 0.0826\tvalidation loss: 0.1242\t validation accuracy: 0.9644\n",
      "iteration number: 1686\t training loss: 0.0826\tvalidation loss: 0.1247\t validation accuracy: 0.9644\n",
      "iteration number: 1687\t training loss: 0.0838\tvalidation loss: 0.1243\t validation accuracy: 0.9644\n",
      "iteration number: 1688\t training loss: 0.0839\tvalidation loss: 0.1297\t validation accuracy: 0.9600\n",
      "iteration number: 1689\t training loss: 0.0833\tvalidation loss: 0.1305\t validation accuracy: 0.9600\n",
      "iteration number: 1690\t training loss: 0.0826\tvalidation loss: 0.1274\t validation accuracy: 0.9600\n",
      "iteration number: 1691\t training loss: 0.0823\tvalidation loss: 0.1285\t validation accuracy: 0.9622\n",
      "iteration number: 1692\t training loss: 0.0820\tvalidation loss: 0.1288\t validation accuracy: 0.9622\n",
      "iteration number: 1693\t training loss: 0.0818\tvalidation loss: 0.1268\t validation accuracy: 0.9667\n",
      "iteration number: 1694\t training loss: 0.0822\tvalidation loss: 0.1246\t validation accuracy: 0.9644\n",
      "iteration number: 1695\t training loss: 0.0814\tvalidation loss: 0.1250\t validation accuracy: 0.9667\n",
      "iteration number: 1696\t training loss: 0.0835\tvalidation loss: 0.1348\t validation accuracy: 0.9600\n",
      "iteration number: 1697\t training loss: 0.0818\tvalidation loss: 0.1285\t validation accuracy: 0.9667\n",
      "iteration number: 1698\t training loss: 0.0817\tvalidation loss: 0.1289\t validation accuracy: 0.9667\n",
      "iteration number: 1699\t training loss: 0.0813\tvalidation loss: 0.1261\t validation accuracy: 0.9667\n",
      "iteration number: 1700\t training loss: 0.0813\tvalidation loss: 0.1237\t validation accuracy: 0.9689\n",
      "iteration number: 1701\t training loss: 0.0812\tvalidation loss: 0.1269\t validation accuracy: 0.9667\n",
      "iteration number: 1702\t training loss: 0.0810\tvalidation loss: 0.1258\t validation accuracy: 0.9667\n",
      "iteration number: 1703\t training loss: 0.0812\tvalidation loss: 0.1279\t validation accuracy: 0.9667\n",
      "iteration number: 1704\t training loss: 0.0816\tvalidation loss: 0.1268\t validation accuracy: 0.9644\n",
      "iteration number: 1705\t training loss: 0.0816\tvalidation loss: 0.1258\t validation accuracy: 0.9644\n",
      "iteration number: 1706\t training loss: 0.0813\tvalidation loss: 0.1269\t validation accuracy: 0.9644\n",
      "iteration number: 1707\t training loss: 0.0827\tvalidation loss: 0.1309\t validation accuracy: 0.9622\n",
      "iteration number: 1708\t training loss: 0.0815\tvalidation loss: 0.1263\t validation accuracy: 0.9667\n",
      "iteration number: 1709\t training loss: 0.0809\tvalidation loss: 0.1256\t validation accuracy: 0.9667\n",
      "iteration number: 1710\t training loss: 0.0817\tvalidation loss: 0.1289\t validation accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1711\t training loss: 0.0811\tvalidation loss: 0.1274\t validation accuracy: 0.9644\n",
      "iteration number: 1712\t training loss: 0.0809\tvalidation loss: 0.1259\t validation accuracy: 0.9667\n",
      "iteration number: 1713\t training loss: 0.0813\tvalidation loss: 0.1235\t validation accuracy: 0.9667\n",
      "iteration number: 1714\t training loss: 0.0808\tvalidation loss: 0.1217\t validation accuracy: 0.9711\n",
      "iteration number: 1715\t training loss: 0.0819\tvalidation loss: 0.1214\t validation accuracy: 0.9667\n",
      "iteration number: 1716\t training loss: 0.0808\tvalidation loss: 0.1241\t validation accuracy: 0.9644\n",
      "iteration number: 1717\t training loss: 0.0804\tvalidation loss: 0.1277\t validation accuracy: 0.9644\n",
      "iteration number: 1718\t training loss: 0.0829\tvalidation loss: 0.1257\t validation accuracy: 0.9689\n",
      "iteration number: 1719\t training loss: 0.0843\tvalidation loss: 0.1235\t validation accuracy: 0.9711\n",
      "iteration number: 1720\t training loss: 0.0828\tvalidation loss: 0.1219\t validation accuracy: 0.9711\n",
      "iteration number: 1721\t training loss: 0.0822\tvalidation loss: 0.1213\t validation accuracy: 0.9733\n",
      "iteration number: 1722\t training loss: 0.0811\tvalidation loss: 0.1237\t validation accuracy: 0.9689\n",
      "iteration number: 1723\t training loss: 0.0809\tvalidation loss: 0.1256\t validation accuracy: 0.9600\n",
      "iteration number: 1724\t training loss: 0.0810\tvalidation loss: 0.1232\t validation accuracy: 0.9667\n",
      "iteration number: 1725\t training loss: 0.0807\tvalidation loss: 0.1202\t validation accuracy: 0.9711\n",
      "iteration number: 1726\t training loss: 0.0810\tvalidation loss: 0.1214\t validation accuracy: 0.9689\n",
      "iteration number: 1727\t training loss: 0.0826\tvalidation loss: 0.1202\t validation accuracy: 0.9689\n",
      "iteration number: 1728\t training loss: 0.0823\tvalidation loss: 0.1209\t validation accuracy: 0.9689\n",
      "iteration number: 1729\t training loss: 0.0806\tvalidation loss: 0.1261\t validation accuracy: 0.9667\n",
      "iteration number: 1730\t training loss: 0.0803\tvalidation loss: 0.1256\t validation accuracy: 0.9667\n",
      "iteration number: 1731\t training loss: 0.0802\tvalidation loss: 0.1292\t validation accuracy: 0.9600\n",
      "iteration number: 1732\t training loss: 0.0799\tvalidation loss: 0.1277\t validation accuracy: 0.9644\n",
      "iteration number: 1733\t training loss: 0.0807\tvalidation loss: 0.1313\t validation accuracy: 0.9600\n",
      "iteration number: 1734\t training loss: 0.0805\tvalidation loss: 0.1316\t validation accuracy: 0.9600\n",
      "iteration number: 1735\t training loss: 0.0806\tvalidation loss: 0.1303\t validation accuracy: 0.9600\n",
      "iteration number: 1736\t training loss: 0.0798\tvalidation loss: 0.1301\t validation accuracy: 0.9600\n",
      "iteration number: 1737\t training loss: 0.0796\tvalidation loss: 0.1286\t validation accuracy: 0.9600\n",
      "iteration number: 1738\t training loss: 0.0796\tvalidation loss: 0.1251\t validation accuracy: 0.9667\n",
      "iteration number: 1739\t training loss: 0.0795\tvalidation loss: 0.1264\t validation accuracy: 0.9667\n",
      "iteration number: 1740\t training loss: 0.0800\tvalidation loss: 0.1267\t validation accuracy: 0.9644\n",
      "iteration number: 1741\t training loss: 0.0809\tvalidation loss: 0.1306\t validation accuracy: 0.9600\n",
      "iteration number: 1742\t training loss: 0.0796\tvalidation loss: 0.1293\t validation accuracy: 0.9600\n",
      "iteration number: 1743\t training loss: 0.0799\tvalidation loss: 0.1303\t validation accuracy: 0.9644\n",
      "iteration number: 1744\t training loss: 0.0799\tvalidation loss: 0.1312\t validation accuracy: 0.9600\n",
      "iteration number: 1745\t training loss: 0.0805\tvalidation loss: 0.1342\t validation accuracy: 0.9600\n",
      "iteration number: 1746\t training loss: 0.0795\tvalidation loss: 0.1292\t validation accuracy: 0.9644\n",
      "iteration number: 1747\t training loss: 0.0792\tvalidation loss: 0.1276\t validation accuracy: 0.9644\n",
      "iteration number: 1748\t training loss: 0.0796\tvalidation loss: 0.1273\t validation accuracy: 0.9644\n",
      "iteration number: 1749\t training loss: 0.0798\tvalidation loss: 0.1260\t validation accuracy: 0.9667\n",
      "iteration number: 1750\t training loss: 0.0799\tvalidation loss: 0.1279\t validation accuracy: 0.9644\n",
      "iteration number: 1751\t training loss: 0.0797\tvalidation loss: 0.1296\t validation accuracy: 0.9600\n",
      "iteration number: 1752\t training loss: 0.0797\tvalidation loss: 0.1286\t validation accuracy: 0.9644\n",
      "iteration number: 1753\t training loss: 0.0791\tvalidation loss: 0.1254\t validation accuracy: 0.9644\n",
      "iteration number: 1754\t training loss: 0.0788\tvalidation loss: 0.1254\t validation accuracy: 0.9622\n",
      "iteration number: 1755\t training loss: 0.0809\tvalidation loss: 0.1263\t validation accuracy: 0.9667\n",
      "iteration number: 1756\t training loss: 0.0811\tvalidation loss: 0.1298\t validation accuracy: 0.9600\n",
      "iteration number: 1757\t training loss: 0.0795\tvalidation loss: 0.1258\t validation accuracy: 0.9622\n",
      "iteration number: 1758\t training loss: 0.0795\tvalidation loss: 0.1259\t validation accuracy: 0.9622\n",
      "iteration number: 1759\t training loss: 0.0793\tvalidation loss: 0.1218\t validation accuracy: 0.9644\n",
      "iteration number: 1760\t training loss: 0.0790\tvalidation loss: 0.1229\t validation accuracy: 0.9644\n",
      "iteration number: 1761\t training loss: 0.0794\tvalidation loss: 0.1228\t validation accuracy: 0.9667\n",
      "iteration number: 1762\t training loss: 0.0794\tvalidation loss: 0.1227\t validation accuracy: 0.9644\n",
      "iteration number: 1763\t training loss: 0.0790\tvalidation loss: 0.1247\t validation accuracy: 0.9644\n",
      "iteration number: 1764\t training loss: 0.0790\tvalidation loss: 0.1252\t validation accuracy: 0.9622\n",
      "iteration number: 1765\t training loss: 0.0791\tvalidation loss: 0.1267\t validation accuracy: 0.9600\n",
      "iteration number: 1766\t training loss: 0.0809\tvalidation loss: 0.1318\t validation accuracy: 0.9578\n",
      "iteration number: 1767\t training loss: 0.0796\tvalidation loss: 0.1283\t validation accuracy: 0.9600\n",
      "iteration number: 1768\t training loss: 0.0801\tvalidation loss: 0.1317\t validation accuracy: 0.9578\n",
      "iteration number: 1769\t training loss: 0.0802\tvalidation loss: 0.1315\t validation accuracy: 0.9578\n",
      "iteration number: 1770\t training loss: 0.0805\tvalidation loss: 0.1319\t validation accuracy: 0.9578\n",
      "iteration number: 1771\t training loss: 0.0807\tvalidation loss: 0.1320\t validation accuracy: 0.9578\n",
      "iteration number: 1772\t training loss: 0.0810\tvalidation loss: 0.1321\t validation accuracy: 0.9578\n",
      "iteration number: 1773\t training loss: 0.0798\tvalidation loss: 0.1295\t validation accuracy: 0.9600\n",
      "iteration number: 1774\t training loss: 0.0793\tvalidation loss: 0.1286\t validation accuracy: 0.9600\n",
      "iteration number: 1775\t training loss: 0.0795\tvalidation loss: 0.1271\t validation accuracy: 0.9644\n",
      "iteration number: 1776\t training loss: 0.0797\tvalidation loss: 0.1286\t validation accuracy: 0.9600\n",
      "iteration number: 1777\t training loss: 0.0809\tvalidation loss: 0.1324\t validation accuracy: 0.9600\n",
      "iteration number: 1778\t training loss: 0.0797\tvalidation loss: 0.1306\t validation accuracy: 0.9622\n",
      "iteration number: 1779\t training loss: 0.0803\tvalidation loss: 0.1324\t validation accuracy: 0.9600\n",
      "iteration number: 1780\t training loss: 0.0802\tvalidation loss: 0.1321\t validation accuracy: 0.9600\n",
      "iteration number: 1781\t training loss: 0.0809\tvalidation loss: 0.1337\t validation accuracy: 0.9600\n",
      "iteration number: 1782\t training loss: 0.0805\tvalidation loss: 0.1337\t validation accuracy: 0.9600\n",
      "iteration number: 1783\t training loss: 0.0780\tvalidation loss: 0.1270\t validation accuracy: 0.9600\n",
      "iteration number: 1784\t training loss: 0.0778\tvalidation loss: 0.1265\t validation accuracy: 0.9622\n",
      "iteration number: 1785\t training loss: 0.0784\tvalidation loss: 0.1293\t validation accuracy: 0.9622\n",
      "iteration number: 1786\t training loss: 0.0788\tvalidation loss: 0.1317\t validation accuracy: 0.9578\n",
      "iteration number: 1787\t training loss: 0.0788\tvalidation loss: 0.1307\t validation accuracy: 0.9578\n",
      "iteration number: 1788\t training loss: 0.0792\tvalidation loss: 0.1311\t validation accuracy: 0.9622\n",
      "iteration number: 1789\t training loss: 0.0773\tvalidation loss: 0.1253\t validation accuracy: 0.9667\n",
      "iteration number: 1790\t training loss: 0.0774\tvalidation loss: 0.1199\t validation accuracy: 0.9667\n",
      "iteration number: 1791\t training loss: 0.0793\tvalidation loss: 0.1194\t validation accuracy: 0.9756\n",
      "iteration number: 1792\t training loss: 0.0780\tvalidation loss: 0.1217\t validation accuracy: 0.9689\n",
      "iteration number: 1793\t training loss: 0.0777\tvalidation loss: 0.1229\t validation accuracy: 0.9711\n",
      "iteration number: 1794\t training loss: 0.0768\tvalidation loss: 0.1249\t validation accuracy: 0.9667\n",
      "iteration number: 1795\t training loss: 0.0770\tvalidation loss: 0.1244\t validation accuracy: 0.9644\n",
      "iteration number: 1796\t training loss: 0.0778\tvalidation loss: 0.1230\t validation accuracy: 0.9667\n",
      "iteration number: 1797\t training loss: 0.0776\tvalidation loss: 0.1198\t validation accuracy: 0.9667\n",
      "iteration number: 1798\t training loss: 0.0781\tvalidation loss: 0.1199\t validation accuracy: 0.9644\n",
      "iteration number: 1799\t training loss: 0.0779\tvalidation loss: 0.1223\t validation accuracy: 0.9644\n",
      "iteration number: 1800\t training loss: 0.0769\tvalidation loss: 0.1215\t validation accuracy: 0.9667\n",
      "iteration number: 1801\t training loss: 0.0772\tvalidation loss: 0.1199\t validation accuracy: 0.9667\n",
      "iteration number: 1802\t training loss: 0.0775\tvalidation loss: 0.1203\t validation accuracy: 0.9622\n",
      "iteration number: 1803\t training loss: 0.0797\tvalidation loss: 0.1220\t validation accuracy: 0.9667\n",
      "iteration number: 1804\t training loss: 0.0785\tvalidation loss: 0.1233\t validation accuracy: 0.9622\n",
      "iteration number: 1805\t training loss: 0.0767\tvalidation loss: 0.1249\t validation accuracy: 0.9622\n",
      "iteration number: 1806\t training loss: 0.0762\tvalidation loss: 0.1197\t validation accuracy: 0.9667\n",
      "iteration number: 1807\t training loss: 0.0761\tvalidation loss: 0.1248\t validation accuracy: 0.9667\n",
      "iteration number: 1808\t training loss: 0.0771\tvalidation loss: 0.1284\t validation accuracy: 0.9578\n",
      "iteration number: 1809\t training loss: 0.0781\tvalidation loss: 0.1305\t validation accuracy: 0.9578\n",
      "iteration number: 1810\t training loss: 0.0773\tvalidation loss: 0.1269\t validation accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1811\t training loss: 0.0774\tvalidation loss: 0.1263\t validation accuracy: 0.9622\n",
      "iteration number: 1812\t training loss: 0.0772\tvalidation loss: 0.1275\t validation accuracy: 0.9622\n",
      "iteration number: 1813\t training loss: 0.0781\tvalidation loss: 0.1293\t validation accuracy: 0.9600\n",
      "iteration number: 1814\t training loss: 0.0767\tvalidation loss: 0.1224\t validation accuracy: 0.9644\n",
      "iteration number: 1815\t training loss: 0.0764\tvalidation loss: 0.1214\t validation accuracy: 0.9644\n",
      "iteration number: 1816\t training loss: 0.0762\tvalidation loss: 0.1210\t validation accuracy: 0.9667\n",
      "iteration number: 1817\t training loss: 0.0761\tvalidation loss: 0.1215\t validation accuracy: 0.9644\n",
      "iteration number: 1818\t training loss: 0.0765\tvalidation loss: 0.1210\t validation accuracy: 0.9667\n",
      "iteration number: 1819\t training loss: 0.0768\tvalidation loss: 0.1190\t validation accuracy: 0.9711\n",
      "iteration number: 1820\t training loss: 0.0783\tvalidation loss: 0.1205\t validation accuracy: 0.9689\n",
      "iteration number: 1821\t training loss: 0.0775\tvalidation loss: 0.1209\t validation accuracy: 0.9667\n",
      "iteration number: 1822\t training loss: 0.0766\tvalidation loss: 0.1182\t validation accuracy: 0.9689\n",
      "iteration number: 1823\t training loss: 0.0762\tvalidation loss: 0.1201\t validation accuracy: 0.9689\n",
      "iteration number: 1824\t training loss: 0.0760\tvalidation loss: 0.1198\t validation accuracy: 0.9711\n",
      "iteration number: 1825\t training loss: 0.0769\tvalidation loss: 0.1213\t validation accuracy: 0.9689\n",
      "iteration number: 1826\t training loss: 0.0765\tvalidation loss: 0.1219\t validation accuracy: 0.9689\n",
      "iteration number: 1827\t training loss: 0.0765\tvalidation loss: 0.1221\t validation accuracy: 0.9689\n",
      "iteration number: 1828\t training loss: 0.0774\tvalidation loss: 0.1259\t validation accuracy: 0.9689\n",
      "iteration number: 1829\t training loss: 0.0768\tvalidation loss: 0.1242\t validation accuracy: 0.9689\n",
      "iteration number: 1830\t training loss: 0.0764\tvalidation loss: 0.1240\t validation accuracy: 0.9689\n",
      "iteration number: 1831\t training loss: 0.0771\tvalidation loss: 0.1272\t validation accuracy: 0.9622\n",
      "iteration number: 1832\t training loss: 0.0766\tvalidation loss: 0.1264\t validation accuracy: 0.9600\n",
      "iteration number: 1833\t training loss: 0.0758\tvalidation loss: 0.1252\t validation accuracy: 0.9622\n",
      "iteration number: 1834\t training loss: 0.0756\tvalidation loss: 0.1240\t validation accuracy: 0.9622\n",
      "iteration number: 1835\t training loss: 0.0762\tvalidation loss: 0.1259\t validation accuracy: 0.9622\n",
      "iteration number: 1836\t training loss: 0.0756\tvalidation loss: 0.1290\t validation accuracy: 0.9600\n",
      "iteration number: 1837\t training loss: 0.0755\tvalidation loss: 0.1275\t validation accuracy: 0.9600\n",
      "iteration number: 1838\t training loss: 0.0753\tvalidation loss: 0.1252\t validation accuracy: 0.9622\n",
      "iteration number: 1839\t training loss: 0.0749\tvalidation loss: 0.1247\t validation accuracy: 0.9644\n",
      "iteration number: 1840\t training loss: 0.0747\tvalidation loss: 0.1240\t validation accuracy: 0.9644\n",
      "iteration number: 1841\t training loss: 0.0756\tvalidation loss: 0.1263\t validation accuracy: 0.9622\n",
      "iteration number: 1842\t training loss: 0.0755\tvalidation loss: 0.1273\t validation accuracy: 0.9622\n",
      "iteration number: 1843\t training loss: 0.0750\tvalidation loss: 0.1239\t validation accuracy: 0.9667\n",
      "iteration number: 1844\t training loss: 0.0749\tvalidation loss: 0.1247\t validation accuracy: 0.9622\n",
      "iteration number: 1845\t training loss: 0.0753\tvalidation loss: 0.1263\t validation accuracy: 0.9622\n",
      "iteration number: 1846\t training loss: 0.0779\tvalidation loss: 0.1297\t validation accuracy: 0.9622\n",
      "iteration number: 1847\t training loss: 0.0749\tvalidation loss: 0.1220\t validation accuracy: 0.9689\n",
      "iteration number: 1848\t training loss: 0.0752\tvalidation loss: 0.1200\t validation accuracy: 0.9622\n",
      "iteration number: 1849\t training loss: 0.0753\tvalidation loss: 0.1210\t validation accuracy: 0.9644\n",
      "iteration number: 1850\t training loss: 0.0747\tvalidation loss: 0.1192\t validation accuracy: 0.9644\n",
      "iteration number: 1851\t training loss: 0.0749\tvalidation loss: 0.1207\t validation accuracy: 0.9644\n",
      "iteration number: 1852\t training loss: 0.0744\tvalidation loss: 0.1216\t validation accuracy: 0.9667\n",
      "iteration number: 1853\t training loss: 0.0741\tvalidation loss: 0.1216\t validation accuracy: 0.9667\n",
      "iteration number: 1854\t training loss: 0.0742\tvalidation loss: 0.1203\t validation accuracy: 0.9689\n",
      "iteration number: 1855\t training loss: 0.0757\tvalidation loss: 0.1186\t validation accuracy: 0.9644\n",
      "iteration number: 1856\t training loss: 0.0748\tvalidation loss: 0.1194\t validation accuracy: 0.9644\n",
      "iteration number: 1857\t training loss: 0.0762\tvalidation loss: 0.1181\t validation accuracy: 0.9644\n",
      "iteration number: 1858\t training loss: 0.0755\tvalidation loss: 0.1194\t validation accuracy: 0.9689\n",
      "iteration number: 1859\t training loss: 0.0772\tvalidation loss: 0.1237\t validation accuracy: 0.9689\n",
      "iteration number: 1860\t training loss: 0.0767\tvalidation loss: 0.1241\t validation accuracy: 0.9689\n",
      "iteration number: 1861\t training loss: 0.0746\tvalidation loss: 0.1236\t validation accuracy: 0.9689\n",
      "iteration number: 1862\t training loss: 0.0750\tvalidation loss: 0.1250\t validation accuracy: 0.9689\n",
      "iteration number: 1863\t training loss: 0.0747\tvalidation loss: 0.1272\t validation accuracy: 0.9644\n",
      "iteration number: 1864\t training loss: 0.0741\tvalidation loss: 0.1241\t validation accuracy: 0.9667\n",
      "iteration number: 1865\t training loss: 0.0743\tvalidation loss: 0.1255\t validation accuracy: 0.9667\n",
      "iteration number: 1866\t training loss: 0.0741\tvalidation loss: 0.1278\t validation accuracy: 0.9644\n",
      "iteration number: 1867\t training loss: 0.0741\tvalidation loss: 0.1275\t validation accuracy: 0.9644\n",
      "iteration number: 1868\t training loss: 0.0749\tvalidation loss: 0.1302\t validation accuracy: 0.9600\n",
      "iteration number: 1869\t training loss: 0.0749\tvalidation loss: 0.1277\t validation accuracy: 0.9644\n",
      "iteration number: 1870\t training loss: 0.0746\tvalidation loss: 0.1278\t validation accuracy: 0.9644\n",
      "iteration number: 1871\t training loss: 0.0744\tvalidation loss: 0.1274\t validation accuracy: 0.9644\n",
      "iteration number: 1872\t training loss: 0.0747\tvalidation loss: 0.1281\t validation accuracy: 0.9667\n",
      "iteration number: 1873\t training loss: 0.0764\tvalidation loss: 0.1326\t validation accuracy: 0.9622\n",
      "iteration number: 1874\t training loss: 0.0750\tvalidation loss: 0.1291\t validation accuracy: 0.9644\n",
      "iteration number: 1875\t training loss: 0.0746\tvalidation loss: 0.1270\t validation accuracy: 0.9644\n",
      "iteration number: 1876\t training loss: 0.0740\tvalidation loss: 0.1244\t validation accuracy: 0.9644\n",
      "iteration number: 1877\t training loss: 0.0738\tvalidation loss: 0.1243\t validation accuracy: 0.9667\n",
      "iteration number: 1878\t training loss: 0.0734\tvalidation loss: 0.1226\t validation accuracy: 0.9667\n",
      "iteration number: 1879\t training loss: 0.0737\tvalidation loss: 0.1205\t validation accuracy: 0.9689\n",
      "iteration number: 1880\t training loss: 0.0739\tvalidation loss: 0.1223\t validation accuracy: 0.9667\n",
      "iteration number: 1881\t training loss: 0.0740\tvalidation loss: 0.1212\t validation accuracy: 0.9689\n",
      "iteration number: 1882\t training loss: 0.0738\tvalidation loss: 0.1226\t validation accuracy: 0.9689\n",
      "iteration number: 1883\t training loss: 0.0736\tvalidation loss: 0.1218\t validation accuracy: 0.9667\n",
      "iteration number: 1884\t training loss: 0.0735\tvalidation loss: 0.1213\t validation accuracy: 0.9667\n",
      "iteration number: 1885\t training loss: 0.0733\tvalidation loss: 0.1211\t validation accuracy: 0.9667\n",
      "iteration number: 1886\t training loss: 0.0736\tvalidation loss: 0.1215\t validation accuracy: 0.9667\n",
      "iteration number: 1887\t training loss: 0.0733\tvalidation loss: 0.1219\t validation accuracy: 0.9644\n",
      "iteration number: 1888\t training loss: 0.0735\tvalidation loss: 0.1220\t validation accuracy: 0.9667\n",
      "iteration number: 1889\t training loss: 0.0741\tvalidation loss: 0.1171\t validation accuracy: 0.9667\n",
      "iteration number: 1890\t training loss: 0.0757\tvalidation loss: 0.1184\t validation accuracy: 0.9733\n",
      "iteration number: 1891\t training loss: 0.0742\tvalidation loss: 0.1189\t validation accuracy: 0.9711\n",
      "iteration number: 1892\t training loss: 0.0732\tvalidation loss: 0.1231\t validation accuracy: 0.9667\n",
      "iteration number: 1893\t training loss: 0.0734\tvalidation loss: 0.1257\t validation accuracy: 0.9644\n",
      "iteration number: 1894\t training loss: 0.0729\tvalidation loss: 0.1223\t validation accuracy: 0.9711\n",
      "iteration number: 1895\t training loss: 0.0729\tvalidation loss: 0.1231\t validation accuracy: 0.9667\n",
      "iteration number: 1896\t training loss: 0.0733\tvalidation loss: 0.1243\t validation accuracy: 0.9689\n",
      "iteration number: 1897\t training loss: 0.0730\tvalidation loss: 0.1223\t validation accuracy: 0.9689\n",
      "iteration number: 1898\t training loss: 0.0731\tvalidation loss: 0.1201\t validation accuracy: 0.9667\n",
      "iteration number: 1899\t training loss: 0.0723\tvalidation loss: 0.1227\t validation accuracy: 0.9711\n",
      "iteration number: 1900\t training loss: 0.0724\tvalidation loss: 0.1220\t validation accuracy: 0.9733\n",
      "iteration number: 1901\t training loss: 0.0727\tvalidation loss: 0.1221\t validation accuracy: 0.9711\n",
      "iteration number: 1902\t training loss: 0.0724\tvalidation loss: 0.1225\t validation accuracy: 0.9711\n",
      "iteration number: 1903\t training loss: 0.0727\tvalidation loss: 0.1242\t validation accuracy: 0.9711\n",
      "iteration number: 1904\t training loss: 0.0729\tvalidation loss: 0.1224\t validation accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1905\t training loss: 0.0723\tvalidation loss: 0.1214\t validation accuracy: 0.9689\n",
      "iteration number: 1906\t training loss: 0.0723\tvalidation loss: 0.1255\t validation accuracy: 0.9667\n",
      "iteration number: 1907\t training loss: 0.0719\tvalidation loss: 0.1217\t validation accuracy: 0.9667\n",
      "iteration number: 1908\t training loss: 0.0724\tvalidation loss: 0.1192\t validation accuracy: 0.9644\n",
      "iteration number: 1909\t training loss: 0.0727\tvalidation loss: 0.1192\t validation accuracy: 0.9644\n",
      "iteration number: 1910\t training loss: 0.0738\tvalidation loss: 0.1201\t validation accuracy: 0.9667\n",
      "iteration number: 1911\t training loss: 0.0731\tvalidation loss: 0.1195\t validation accuracy: 0.9689\n",
      "iteration number: 1912\t training loss: 0.0734\tvalidation loss: 0.1214\t validation accuracy: 0.9733\n",
      "iteration number: 1913\t training loss: 0.0729\tvalidation loss: 0.1260\t validation accuracy: 0.9667\n",
      "iteration number: 1914\t training loss: 0.0741\tvalidation loss: 0.1263\t validation accuracy: 0.9644\n",
      "iteration number: 1915\t training loss: 0.0734\tvalidation loss: 0.1269\t validation accuracy: 0.9644\n",
      "iteration number: 1916\t training loss: 0.0734\tvalidation loss: 0.1276\t validation accuracy: 0.9622\n",
      "iteration number: 1917\t training loss: 0.0753\tvalidation loss: 0.1216\t validation accuracy: 0.9667\n",
      "iteration number: 1918\t training loss: 0.0764\tvalidation loss: 0.1214\t validation accuracy: 0.9667\n",
      "iteration number: 1919\t training loss: 0.0744\tvalidation loss: 0.1206\t validation accuracy: 0.9689\n",
      "iteration number: 1920\t training loss: 0.0733\tvalidation loss: 0.1227\t validation accuracy: 0.9622\n",
      "iteration number: 1921\t training loss: 0.0727\tvalidation loss: 0.1205\t validation accuracy: 0.9667\n",
      "iteration number: 1922\t training loss: 0.0715\tvalidation loss: 0.1226\t validation accuracy: 0.9667\n",
      "iteration number: 1923\t training loss: 0.0716\tvalidation loss: 0.1195\t validation accuracy: 0.9667\n",
      "iteration number: 1924\t training loss: 0.0719\tvalidation loss: 0.1225\t validation accuracy: 0.9689\n",
      "iteration number: 1925\t training loss: 0.0716\tvalidation loss: 0.1215\t validation accuracy: 0.9689\n",
      "iteration number: 1926\t training loss: 0.0715\tvalidation loss: 0.1212\t validation accuracy: 0.9644\n",
      "iteration number: 1927\t training loss: 0.0720\tvalidation loss: 0.1254\t validation accuracy: 0.9667\n",
      "iteration number: 1928\t training loss: 0.0718\tvalidation loss: 0.1240\t validation accuracy: 0.9644\n",
      "iteration number: 1929\t training loss: 0.0712\tvalidation loss: 0.1207\t validation accuracy: 0.9667\n",
      "iteration number: 1930\t training loss: 0.0715\tvalidation loss: 0.1216\t validation accuracy: 0.9667\n",
      "iteration number: 1931\t training loss: 0.0727\tvalidation loss: 0.1257\t validation accuracy: 0.9622\n",
      "iteration number: 1932\t training loss: 0.0721\tvalidation loss: 0.1253\t validation accuracy: 0.9622\n",
      "iteration number: 1933\t training loss: 0.0722\tvalidation loss: 0.1247\t validation accuracy: 0.9644\n",
      "iteration number: 1934\t training loss: 0.0716\tvalidation loss: 0.1208\t validation accuracy: 0.9644\n",
      "iteration number: 1935\t training loss: 0.0716\tvalidation loss: 0.1203\t validation accuracy: 0.9622\n",
      "iteration number: 1936\t training loss: 0.0718\tvalidation loss: 0.1209\t validation accuracy: 0.9644\n",
      "iteration number: 1937\t training loss: 0.0714\tvalidation loss: 0.1206\t validation accuracy: 0.9689\n",
      "iteration number: 1938\t training loss: 0.0713\tvalidation loss: 0.1211\t validation accuracy: 0.9689\n",
      "iteration number: 1939\t training loss: 0.0716\tvalidation loss: 0.1211\t validation accuracy: 0.9689\n",
      "iteration number: 1940\t training loss: 0.0711\tvalidation loss: 0.1185\t validation accuracy: 0.9689\n",
      "iteration number: 1941\t training loss: 0.0711\tvalidation loss: 0.1179\t validation accuracy: 0.9711\n",
      "iteration number: 1942\t training loss: 0.0711\tvalidation loss: 0.1198\t validation accuracy: 0.9667\n",
      "iteration number: 1943\t training loss: 0.0711\tvalidation loss: 0.1203\t validation accuracy: 0.9689\n",
      "iteration number: 1944\t training loss: 0.0710\tvalidation loss: 0.1234\t validation accuracy: 0.9667\n",
      "iteration number: 1945\t training loss: 0.0710\tvalidation loss: 0.1224\t validation accuracy: 0.9667\n",
      "iteration number: 1946\t training loss: 0.0709\tvalidation loss: 0.1226\t validation accuracy: 0.9667\n",
      "iteration number: 1947\t training loss: 0.0708\tvalidation loss: 0.1235\t validation accuracy: 0.9667\n",
      "iteration number: 1948\t training loss: 0.0710\tvalidation loss: 0.1228\t validation accuracy: 0.9644\n",
      "iteration number: 1949\t training loss: 0.0719\tvalidation loss: 0.1263\t validation accuracy: 0.9644\n",
      "iteration number: 1950\t training loss: 0.0715\tvalidation loss: 0.1262\t validation accuracy: 0.9644\n",
      "iteration number: 1951\t training loss: 0.0715\tvalidation loss: 0.1269\t validation accuracy: 0.9644\n",
      "iteration number: 1952\t training loss: 0.0715\tvalidation loss: 0.1269\t validation accuracy: 0.9644\n",
      "iteration number: 1953\t training loss: 0.0710\tvalidation loss: 0.1265\t validation accuracy: 0.9622\n",
      "iteration number: 1954\t training loss: 0.0720\tvalidation loss: 0.1282\t validation accuracy: 0.9644\n",
      "iteration number: 1955\t training loss: 0.0725\tvalidation loss: 0.1316\t validation accuracy: 0.9600\n",
      "iteration number: 1956\t training loss: 0.0724\tvalidation loss: 0.1310\t validation accuracy: 0.9600\n",
      "iteration number: 1957\t training loss: 0.0706\tvalidation loss: 0.1253\t validation accuracy: 0.9667\n",
      "iteration number: 1958\t training loss: 0.0714\tvalidation loss: 0.1225\t validation accuracy: 0.9667\n",
      "iteration number: 1959\t training loss: 0.0707\tvalidation loss: 0.1272\t validation accuracy: 0.9644\n",
      "iteration number: 1960\t training loss: 0.0711\tvalidation loss: 0.1279\t validation accuracy: 0.9644\n",
      "iteration number: 1961\t training loss: 0.0711\tvalidation loss: 0.1271\t validation accuracy: 0.9667\n",
      "iteration number: 1962\t training loss: 0.0724\tvalidation loss: 0.1286\t validation accuracy: 0.9644\n",
      "iteration number: 1963\t training loss: 0.0719\tvalidation loss: 0.1268\t validation accuracy: 0.9644\n",
      "iteration number: 1964\t training loss: 0.0714\tvalidation loss: 0.1256\t validation accuracy: 0.9644\n",
      "iteration number: 1965\t training loss: 0.0715\tvalidation loss: 0.1261\t validation accuracy: 0.9644\n",
      "iteration number: 1966\t training loss: 0.0709\tvalidation loss: 0.1245\t validation accuracy: 0.9667\n",
      "iteration number: 1967\t training loss: 0.0706\tvalidation loss: 0.1235\t validation accuracy: 0.9689\n",
      "iteration number: 1968\t training loss: 0.0714\tvalidation loss: 0.1272\t validation accuracy: 0.9667\n",
      "iteration number: 1969\t training loss: 0.0710\tvalidation loss: 0.1224\t validation accuracy: 0.9667\n",
      "iteration number: 1970\t training loss: 0.0724\tvalidation loss: 0.1197\t validation accuracy: 0.9711\n",
      "iteration number: 1971\t training loss: 0.0707\tvalidation loss: 0.1186\t validation accuracy: 0.9689\n",
      "iteration number: 1972\t training loss: 0.0705\tvalidation loss: 0.1220\t validation accuracy: 0.9667\n",
      "iteration number: 1973\t training loss: 0.0702\tvalidation loss: 0.1240\t validation accuracy: 0.9667\n",
      "iteration number: 1974\t training loss: 0.0702\tvalidation loss: 0.1264\t validation accuracy: 0.9644\n",
      "iteration number: 1975\t training loss: 0.0711\tvalidation loss: 0.1267\t validation accuracy: 0.9644\n",
      "iteration number: 1976\t training loss: 0.0712\tvalidation loss: 0.1302\t validation accuracy: 0.9600\n",
      "iteration number: 1977\t training loss: 0.0705\tvalidation loss: 0.1253\t validation accuracy: 0.9644\n",
      "iteration number: 1978\t training loss: 0.0699\tvalidation loss: 0.1206\t validation accuracy: 0.9667\n",
      "iteration number: 1979\t training loss: 0.0702\tvalidation loss: 0.1192\t validation accuracy: 0.9667\n",
      "iteration number: 1980\t training loss: 0.0699\tvalidation loss: 0.1195\t validation accuracy: 0.9667\n",
      "iteration number: 1981\t training loss: 0.0700\tvalidation loss: 0.1220\t validation accuracy: 0.9644\n",
      "iteration number: 1982\t training loss: 0.0701\tvalidation loss: 0.1231\t validation accuracy: 0.9644\n",
      "iteration number: 1983\t training loss: 0.0696\tvalidation loss: 0.1206\t validation accuracy: 0.9667\n",
      "iteration number: 1984\t training loss: 0.0700\tvalidation loss: 0.1194\t validation accuracy: 0.9711\n",
      "iteration number: 1985\t training loss: 0.0700\tvalidation loss: 0.1193\t validation accuracy: 0.9711\n",
      "iteration number: 1986\t training loss: 0.0703\tvalidation loss: 0.1242\t validation accuracy: 0.9711\n",
      "iteration number: 1987\t training loss: 0.0702\tvalidation loss: 0.1257\t validation accuracy: 0.9689\n",
      "iteration number: 1988\t training loss: 0.0699\tvalidation loss: 0.1268\t validation accuracy: 0.9644\n",
      "iteration number: 1989\t training loss: 0.0694\tvalidation loss: 0.1264\t validation accuracy: 0.9667\n",
      "iteration number: 1990\t training loss: 0.0696\tvalidation loss: 0.1235\t validation accuracy: 0.9667\n",
      "iteration number: 1991\t training loss: 0.0698\tvalidation loss: 0.1213\t validation accuracy: 0.9667\n",
      "iteration number: 1992\t training loss: 0.0696\tvalidation loss: 0.1218\t validation accuracy: 0.9667\n",
      "iteration number: 1993\t training loss: 0.0693\tvalidation loss: 0.1230\t validation accuracy: 0.9667\n",
      "iteration number: 1994\t training loss: 0.0694\tvalidation loss: 0.1227\t validation accuracy: 0.9689\n",
      "iteration number: 1995\t training loss: 0.0692\tvalidation loss: 0.1222\t validation accuracy: 0.9689\n",
      "iteration number: 1996\t training loss: 0.0692\tvalidation loss: 0.1225\t validation accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1997\t training loss: 0.0692\tvalidation loss: 0.1244\t validation accuracy: 0.9689\n",
      "iteration number: 1998\t training loss: 0.0701\tvalidation loss: 0.1268\t validation accuracy: 0.9644\n",
      "iteration number: 1999\t training loss: 0.0714\tvalidation loss: 0.1232\t validation accuracy: 0.9711\n",
      "iteration number: 2000\t training loss: 0.0700\tvalidation loss: 0.1223\t validation accuracy: 0.9711\n",
      "iteration number: 2001\t training loss: 0.0705\tvalidation loss: 0.1232\t validation accuracy: 0.9711\n",
      "iteration number: 2002\t training loss: 0.0706\tvalidation loss: 0.1217\t validation accuracy: 0.9711\n",
      "iteration number: 2003\t training loss: 0.0730\tvalidation loss: 0.1193\t validation accuracy: 0.9756\n",
      "iteration number: 2004\t training loss: 0.0733\tvalidation loss: 0.1189\t validation accuracy: 0.9756\n",
      "iteration number: 2005\t training loss: 0.0727\tvalidation loss: 0.1179\t validation accuracy: 0.9756\n",
      "iteration number: 2006\t training loss: 0.0727\tvalidation loss: 0.1178\t validation accuracy: 0.9733\n",
      "iteration number: 2007\t training loss: 0.0709\tvalidation loss: 0.1159\t validation accuracy: 0.9733\n",
      "iteration number: 2008\t training loss: 0.0717\tvalidation loss: 0.1150\t validation accuracy: 0.9756\n",
      "iteration number: 2009\t training loss: 0.0710\tvalidation loss: 0.1152\t validation accuracy: 0.9756\n",
      "iteration number: 2010\t training loss: 0.0693\tvalidation loss: 0.1177\t validation accuracy: 0.9733\n",
      "iteration number: 2011\t training loss: 0.0695\tvalidation loss: 0.1174\t validation accuracy: 0.9733\n",
      "iteration number: 2012\t training loss: 0.0698\tvalidation loss: 0.1148\t validation accuracy: 0.9733\n",
      "iteration number: 2013\t training loss: 0.0690\tvalidation loss: 0.1167\t validation accuracy: 0.9689\n",
      "iteration number: 2014\t training loss: 0.0691\tvalidation loss: 0.1193\t validation accuracy: 0.9644\n",
      "iteration number: 2015\t training loss: 0.0688\tvalidation loss: 0.1217\t validation accuracy: 0.9689\n",
      "iteration number: 2016\t training loss: 0.0683\tvalidation loss: 0.1179\t validation accuracy: 0.9711\n",
      "iteration number: 2017\t training loss: 0.0682\tvalidation loss: 0.1176\t validation accuracy: 0.9711\n",
      "iteration number: 2018\t training loss: 0.0683\tvalidation loss: 0.1188\t validation accuracy: 0.9689\n",
      "iteration number: 2019\t training loss: 0.0682\tvalidation loss: 0.1184\t validation accuracy: 0.9711\n",
      "iteration number: 2020\t training loss: 0.0685\tvalidation loss: 0.1173\t validation accuracy: 0.9733\n",
      "iteration number: 2021\t training loss: 0.0693\tvalidation loss: 0.1194\t validation accuracy: 0.9711\n",
      "iteration number: 2022\t training loss: 0.0702\tvalidation loss: 0.1191\t validation accuracy: 0.9711\n",
      "iteration number: 2023\t training loss: 0.0699\tvalidation loss: 0.1202\t validation accuracy: 0.9689\n",
      "iteration number: 2024\t training loss: 0.0700\tvalidation loss: 0.1263\t validation accuracy: 0.9644\n",
      "iteration number: 2025\t training loss: 0.0690\tvalidation loss: 0.1251\t validation accuracy: 0.9644\n",
      "iteration number: 2026\t training loss: 0.0681\tvalidation loss: 0.1224\t validation accuracy: 0.9644\n",
      "iteration number: 2027\t training loss: 0.0684\tvalidation loss: 0.1245\t validation accuracy: 0.9644\n",
      "iteration number: 2028\t training loss: 0.0680\tvalidation loss: 0.1210\t validation accuracy: 0.9622\n",
      "iteration number: 2029\t training loss: 0.0682\tvalidation loss: 0.1238\t validation accuracy: 0.9644\n",
      "iteration number: 2030\t training loss: 0.0687\tvalidation loss: 0.1230\t validation accuracy: 0.9644\n",
      "iteration number: 2031\t training loss: 0.0683\tvalidation loss: 0.1216\t validation accuracy: 0.9689\n",
      "iteration number: 2032\t training loss: 0.0680\tvalidation loss: 0.1200\t validation accuracy: 0.9711\n",
      "iteration number: 2033\t training loss: 0.0682\tvalidation loss: 0.1202\t validation accuracy: 0.9667\n",
      "iteration number: 2034\t training loss: 0.0692\tvalidation loss: 0.1199\t validation accuracy: 0.9667\n",
      "iteration number: 2035\t training loss: 0.0681\tvalidation loss: 0.1207\t validation accuracy: 0.9667\n",
      "iteration number: 2036\t training loss: 0.0679\tvalidation loss: 0.1226\t validation accuracy: 0.9689\n",
      "iteration number: 2037\t training loss: 0.0682\tvalidation loss: 0.1244\t validation accuracy: 0.9622\n",
      "iteration number: 2038\t training loss: 0.0680\tvalidation loss: 0.1238\t validation accuracy: 0.9667\n",
      "iteration number: 2039\t training loss: 0.0677\tvalidation loss: 0.1223\t validation accuracy: 0.9644\n",
      "iteration number: 2040\t training loss: 0.0681\tvalidation loss: 0.1220\t validation accuracy: 0.9667\n",
      "iteration number: 2041\t training loss: 0.0678\tvalidation loss: 0.1204\t validation accuracy: 0.9644\n",
      "iteration number: 2042\t training loss: 0.0685\tvalidation loss: 0.1204\t validation accuracy: 0.9667\n",
      "iteration number: 2043\t training loss: 0.0684\tvalidation loss: 0.1234\t validation accuracy: 0.9667\n",
      "iteration number: 2044\t training loss: 0.0683\tvalidation loss: 0.1251\t validation accuracy: 0.9644\n",
      "iteration number: 2045\t training loss: 0.0680\tvalidation loss: 0.1212\t validation accuracy: 0.9689\n",
      "iteration number: 2046\t training loss: 0.0684\tvalidation loss: 0.1250\t validation accuracy: 0.9622\n",
      "iteration number: 2047\t training loss: 0.0681\tvalidation loss: 0.1235\t validation accuracy: 0.9644\n",
      "iteration number: 2048\t training loss: 0.0681\tvalidation loss: 0.1235\t validation accuracy: 0.9622\n",
      "iteration number: 2049\t training loss: 0.0681\tvalidation loss: 0.1227\t validation accuracy: 0.9644\n",
      "iteration number: 2050\t training loss: 0.0682\tvalidation loss: 0.1251\t validation accuracy: 0.9600\n",
      "iteration number: 2051\t training loss: 0.0688\tvalidation loss: 0.1248\t validation accuracy: 0.9667\n",
      "iteration number: 2052\t training loss: 0.0682\tvalidation loss: 0.1222\t validation accuracy: 0.9667\n",
      "iteration number: 2053\t training loss: 0.0674\tvalidation loss: 0.1193\t validation accuracy: 0.9689\n",
      "iteration number: 2054\t training loss: 0.0678\tvalidation loss: 0.1202\t validation accuracy: 0.9667\n",
      "iteration number: 2055\t training loss: 0.0671\tvalidation loss: 0.1183\t validation accuracy: 0.9711\n",
      "iteration number: 2056\t training loss: 0.0671\tvalidation loss: 0.1181\t validation accuracy: 0.9689\n",
      "iteration number: 2057\t training loss: 0.0676\tvalidation loss: 0.1222\t validation accuracy: 0.9644\n",
      "iteration number: 2058\t training loss: 0.0681\tvalidation loss: 0.1220\t validation accuracy: 0.9667\n",
      "iteration number: 2059\t training loss: 0.0689\tvalidation loss: 0.1213\t validation accuracy: 0.9667\n",
      "iteration number: 2060\t training loss: 0.0684\tvalidation loss: 0.1199\t validation accuracy: 0.9689\n",
      "iteration number: 2061\t training loss: 0.0690\tvalidation loss: 0.1165\t validation accuracy: 0.9667\n",
      "iteration number: 2062\t training loss: 0.0682\tvalidation loss: 0.1156\t validation accuracy: 0.9689\n",
      "iteration number: 2063\t training loss: 0.0716\tvalidation loss: 0.1168\t validation accuracy: 0.9689\n",
      "iteration number: 2064\t training loss: 0.0706\tvalidation loss: 0.1146\t validation accuracy: 0.9733\n",
      "iteration number: 2065\t training loss: 0.0679\tvalidation loss: 0.1183\t validation accuracy: 0.9689\n",
      "iteration number: 2066\t training loss: 0.0669\tvalidation loss: 0.1214\t validation accuracy: 0.9667\n",
      "iteration number: 2067\t training loss: 0.0674\tvalidation loss: 0.1255\t validation accuracy: 0.9622\n",
      "iteration number: 2068\t training loss: 0.0682\tvalidation loss: 0.1271\t validation accuracy: 0.9667\n",
      "iteration number: 2069\t training loss: 0.0673\tvalidation loss: 0.1238\t validation accuracy: 0.9667\n",
      "iteration number: 2070\t training loss: 0.0666\tvalidation loss: 0.1171\t validation accuracy: 0.9667\n",
      "iteration number: 2071\t training loss: 0.0664\tvalidation loss: 0.1182\t validation accuracy: 0.9667\n",
      "iteration number: 2072\t training loss: 0.0664\tvalidation loss: 0.1227\t validation accuracy: 0.9644\n",
      "iteration number: 2073\t training loss: 0.0666\tvalidation loss: 0.1211\t validation accuracy: 0.9622\n",
      "iteration number: 2074\t training loss: 0.0663\tvalidation loss: 0.1183\t validation accuracy: 0.9689\n",
      "iteration number: 2075\t training loss: 0.0661\tvalidation loss: 0.1175\t validation accuracy: 0.9689\n",
      "iteration number: 2076\t training loss: 0.0660\tvalidation loss: 0.1192\t validation accuracy: 0.9667\n",
      "iteration number: 2077\t training loss: 0.0663\tvalidation loss: 0.1229\t validation accuracy: 0.9667\n",
      "iteration number: 2078\t training loss: 0.0666\tvalidation loss: 0.1246\t validation accuracy: 0.9622\n",
      "iteration number: 2079\t training loss: 0.0670\tvalidation loss: 0.1251\t validation accuracy: 0.9622\n",
      "iteration number: 2080\t training loss: 0.0666\tvalidation loss: 0.1237\t validation accuracy: 0.9644\n",
      "iteration number: 2081\t training loss: 0.0671\tvalidation loss: 0.1267\t validation accuracy: 0.9622\n",
      "iteration number: 2082\t training loss: 0.0671\tvalidation loss: 0.1268\t validation accuracy: 0.9622\n",
      "iteration number: 2083\t training loss: 0.0663\tvalidation loss: 0.1229\t validation accuracy: 0.9644\n",
      "iteration number: 2084\t training loss: 0.0667\tvalidation loss: 0.1227\t validation accuracy: 0.9622\n",
      "iteration number: 2085\t training loss: 0.0670\tvalidation loss: 0.1229\t validation accuracy: 0.9644\n",
      "iteration number: 2086\t training loss: 0.0666\tvalidation loss: 0.1223\t validation accuracy: 0.9644\n",
      "iteration number: 2087\t training loss: 0.0668\tvalidation loss: 0.1211\t validation accuracy: 0.9622\n",
      "iteration number: 2088\t training loss: 0.0674\tvalidation loss: 0.1229\t validation accuracy: 0.9622\n",
      "iteration number: 2089\t training loss: 0.0667\tvalidation loss: 0.1218\t validation accuracy: 0.9622\n",
      "iteration number: 2090\t training loss: 0.0666\tvalidation loss: 0.1224\t validation accuracy: 0.9622\n",
      "iteration number: 2091\t training loss: 0.0671\tvalidation loss: 0.1207\t validation accuracy: 0.9644\n",
      "iteration number: 2092\t training loss: 0.0664\tvalidation loss: 0.1197\t validation accuracy: 0.9644\n",
      "iteration number: 2093\t training loss: 0.0666\tvalidation loss: 0.1195\t validation accuracy: 0.9667\n",
      "iteration number: 2094\t training loss: 0.0668\tvalidation loss: 0.1163\t validation accuracy: 0.9667\n",
      "iteration number: 2095\t training loss: 0.0667\tvalidation loss: 0.1159\t validation accuracy: 0.9711\n",
      "iteration number: 2096\t training loss: 0.0664\tvalidation loss: 0.1172\t validation accuracy: 0.9711\n",
      "iteration number: 2097\t training loss: 0.0663\tvalidation loss: 0.1170\t validation accuracy: 0.9711\n",
      "iteration number: 2098\t training loss: 0.0660\tvalidation loss: 0.1169\t validation accuracy: 0.9711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2099\t training loss: 0.0659\tvalidation loss: 0.1184\t validation accuracy: 0.9689\n",
      "iteration number: 2100\t training loss: 0.0659\tvalidation loss: 0.1198\t validation accuracy: 0.9689\n",
      "iteration number: 2101\t training loss: 0.0664\tvalidation loss: 0.1185\t validation accuracy: 0.9711\n",
      "iteration number: 2102\t training loss: 0.0684\tvalidation loss: 0.1179\t validation accuracy: 0.9689\n",
      "iteration number: 2103\t training loss: 0.0664\tvalidation loss: 0.1170\t validation accuracy: 0.9689\n",
      "iteration number: 2104\t training loss: 0.0657\tvalidation loss: 0.1176\t validation accuracy: 0.9689\n",
      "iteration number: 2105\t training loss: 0.0655\tvalidation loss: 0.1202\t validation accuracy: 0.9711\n",
      "iteration number: 2106\t training loss: 0.0655\tvalidation loss: 0.1194\t validation accuracy: 0.9689\n",
      "iteration number: 2107\t training loss: 0.0654\tvalidation loss: 0.1188\t validation accuracy: 0.9689\n",
      "iteration number: 2108\t training loss: 0.0654\tvalidation loss: 0.1202\t validation accuracy: 0.9711\n",
      "iteration number: 2109\t training loss: 0.0656\tvalidation loss: 0.1204\t validation accuracy: 0.9667\n",
      "iteration number: 2110\t training loss: 0.0660\tvalidation loss: 0.1212\t validation accuracy: 0.9689\n",
      "iteration number: 2111\t training loss: 0.0657\tvalidation loss: 0.1187\t validation accuracy: 0.9667\n",
      "iteration number: 2112\t training loss: 0.0653\tvalidation loss: 0.1193\t validation accuracy: 0.9711\n",
      "iteration number: 2113\t training loss: 0.0658\tvalidation loss: 0.1215\t validation accuracy: 0.9644\n",
      "iteration number: 2114\t training loss: 0.0666\tvalidation loss: 0.1230\t validation accuracy: 0.9644\n",
      "iteration number: 2115\t training loss: 0.0666\tvalidation loss: 0.1220\t validation accuracy: 0.9644\n",
      "iteration number: 2116\t training loss: 0.0673\tvalidation loss: 0.1248\t validation accuracy: 0.9622\n",
      "iteration number: 2117\t training loss: 0.0686\tvalidation loss: 0.1256\t validation accuracy: 0.9622\n",
      "iteration number: 2118\t training loss: 0.0670\tvalidation loss: 0.1224\t validation accuracy: 0.9644\n",
      "iteration number: 2119\t training loss: 0.0674\tvalidation loss: 0.1239\t validation accuracy: 0.9622\n",
      "iteration number: 2120\t training loss: 0.0671\tvalidation loss: 0.1279\t validation accuracy: 0.9600\n",
      "iteration number: 2121\t training loss: 0.0660\tvalidation loss: 0.1241\t validation accuracy: 0.9622\n",
      "iteration number: 2122\t training loss: 0.0658\tvalidation loss: 0.1218\t validation accuracy: 0.9667\n",
      "iteration number: 2123\t training loss: 0.0663\tvalidation loss: 0.1239\t validation accuracy: 0.9622\n",
      "iteration number: 2124\t training loss: 0.0664\tvalidation loss: 0.1244\t validation accuracy: 0.9622\n",
      "iteration number: 2125\t training loss: 0.0663\tvalidation loss: 0.1245\t validation accuracy: 0.9622\n",
      "iteration number: 2126\t training loss: 0.0676\tvalidation loss: 0.1259\t validation accuracy: 0.9622\n",
      "iteration number: 2127\t training loss: 0.0676\tvalidation loss: 0.1242\t validation accuracy: 0.9622\n",
      "iteration number: 2128\t training loss: 0.0680\tvalidation loss: 0.1271\t validation accuracy: 0.9600\n",
      "iteration number: 2129\t training loss: 0.0677\tvalidation loss: 0.1257\t validation accuracy: 0.9622\n",
      "iteration number: 2130\t training loss: 0.0681\tvalidation loss: 0.1287\t validation accuracy: 0.9600\n",
      "iteration number: 2131\t training loss: 0.0654\tvalidation loss: 0.1217\t validation accuracy: 0.9667\n",
      "iteration number: 2132\t training loss: 0.0657\tvalidation loss: 0.1213\t validation accuracy: 0.9667\n",
      "iteration number: 2133\t training loss: 0.0660\tvalidation loss: 0.1253\t validation accuracy: 0.9644\n",
      "iteration number: 2134\t training loss: 0.0659\tvalidation loss: 0.1243\t validation accuracy: 0.9667\n",
      "iteration number: 2135\t training loss: 0.0660\tvalidation loss: 0.1229\t validation accuracy: 0.9667\n",
      "iteration number: 2136\t training loss: 0.0658\tvalidation loss: 0.1243\t validation accuracy: 0.9667\n",
      "iteration number: 2137\t training loss: 0.0656\tvalidation loss: 0.1238\t validation accuracy: 0.9644\n",
      "iteration number: 2138\t training loss: 0.0651\tvalidation loss: 0.1240\t validation accuracy: 0.9667\n",
      "iteration number: 2139\t training loss: 0.0646\tvalidation loss: 0.1216\t validation accuracy: 0.9667\n",
      "iteration number: 2140\t training loss: 0.0653\tvalidation loss: 0.1232\t validation accuracy: 0.9622\n",
      "iteration number: 2141\t training loss: 0.0648\tvalidation loss: 0.1216\t validation accuracy: 0.9667\n",
      "iteration number: 2142\t training loss: 0.0659\tvalidation loss: 0.1236\t validation accuracy: 0.9622\n",
      "iteration number: 2143\t training loss: 0.0645\tvalidation loss: 0.1168\t validation accuracy: 0.9711\n",
      "iteration number: 2144\t training loss: 0.0647\tvalidation loss: 0.1190\t validation accuracy: 0.9689\n",
      "iteration number: 2145\t training loss: 0.0650\tvalidation loss: 0.1172\t validation accuracy: 0.9711\n",
      "iteration number: 2146\t training loss: 0.0644\tvalidation loss: 0.1179\t validation accuracy: 0.9711\n",
      "iteration number: 2147\t training loss: 0.0654\tvalidation loss: 0.1191\t validation accuracy: 0.9644\n",
      "iteration number: 2148\t training loss: 0.0645\tvalidation loss: 0.1185\t validation accuracy: 0.9667\n",
      "iteration number: 2149\t training loss: 0.0651\tvalidation loss: 0.1199\t validation accuracy: 0.9644\n",
      "iteration number: 2150\t training loss: 0.0648\tvalidation loss: 0.1172\t validation accuracy: 0.9667\n",
      "iteration number: 2151\t training loss: 0.0643\tvalidation loss: 0.1157\t validation accuracy: 0.9689\n",
      "iteration number: 2152\t training loss: 0.0638\tvalidation loss: 0.1169\t validation accuracy: 0.9667\n",
      "iteration number: 2153\t training loss: 0.0637\tvalidation loss: 0.1158\t validation accuracy: 0.9667\n",
      "iteration number: 2154\t training loss: 0.0639\tvalidation loss: 0.1166\t validation accuracy: 0.9667\n",
      "iteration number: 2155\t training loss: 0.0643\tvalidation loss: 0.1148\t validation accuracy: 0.9667\n",
      "iteration number: 2156\t training loss: 0.0645\tvalidation loss: 0.1141\t validation accuracy: 0.9667\n",
      "iteration number: 2157\t training loss: 0.0650\tvalidation loss: 0.1144\t validation accuracy: 0.9689\n",
      "iteration number: 2158\t training loss: 0.0645\tvalidation loss: 0.1156\t validation accuracy: 0.9667\n",
      "iteration number: 2159\t training loss: 0.0643\tvalidation loss: 0.1165\t validation accuracy: 0.9644\n",
      "iteration number: 2160\t training loss: 0.0638\tvalidation loss: 0.1188\t validation accuracy: 0.9622\n",
      "iteration number: 2161\t training loss: 0.0637\tvalidation loss: 0.1182\t validation accuracy: 0.9667\n",
      "iteration number: 2162\t training loss: 0.0642\tvalidation loss: 0.1178\t validation accuracy: 0.9622\n",
      "iteration number: 2163\t training loss: 0.0639\tvalidation loss: 0.1198\t validation accuracy: 0.9689\n",
      "iteration number: 2164\t training loss: 0.0638\tvalidation loss: 0.1213\t validation accuracy: 0.9667\n",
      "iteration number: 2165\t training loss: 0.0642\tvalidation loss: 0.1220\t validation accuracy: 0.9644\n",
      "iteration number: 2166\t training loss: 0.0645\tvalidation loss: 0.1210\t validation accuracy: 0.9622\n",
      "iteration number: 2167\t training loss: 0.0643\tvalidation loss: 0.1187\t validation accuracy: 0.9622\n",
      "iteration number: 2168\t training loss: 0.0646\tvalidation loss: 0.1204\t validation accuracy: 0.9622\n",
      "iteration number: 2169\t training loss: 0.0647\tvalidation loss: 0.1203\t validation accuracy: 0.9622\n",
      "iteration number: 2170\t training loss: 0.0645\tvalidation loss: 0.1192\t validation accuracy: 0.9622\n",
      "iteration number: 2171\t training loss: 0.0665\tvalidation loss: 0.1291\t validation accuracy: 0.9622\n",
      "iteration number: 2172\t training loss: 0.0674\tvalidation loss: 0.1316\t validation accuracy: 0.9600\n",
      "iteration number: 2173\t training loss: 0.0676\tvalidation loss: 0.1299\t validation accuracy: 0.9600\n",
      "iteration number: 2174\t training loss: 0.0666\tvalidation loss: 0.1276\t validation accuracy: 0.9622\n",
      "iteration number: 2175\t training loss: 0.0668\tvalidation loss: 0.1288\t validation accuracy: 0.9600\n",
      "iteration number: 2176\t training loss: 0.0669\tvalidation loss: 0.1256\t validation accuracy: 0.9600\n",
      "iteration number: 2177\t training loss: 0.0655\tvalidation loss: 0.1249\t validation accuracy: 0.9622\n",
      "iteration number: 2178\t training loss: 0.0652\tvalidation loss: 0.1191\t validation accuracy: 0.9622\n",
      "iteration number: 2179\t training loss: 0.0647\tvalidation loss: 0.1201\t validation accuracy: 0.9644\n",
      "iteration number: 2180\t training loss: 0.0653\tvalidation loss: 0.1203\t validation accuracy: 0.9622\n",
      "iteration number: 2181\t training loss: 0.0647\tvalidation loss: 0.1193\t validation accuracy: 0.9689\n",
      "iteration number: 2182\t training loss: 0.0645\tvalidation loss: 0.1195\t validation accuracy: 0.9689\n",
      "iteration number: 2183\t training loss: 0.0642\tvalidation loss: 0.1213\t validation accuracy: 0.9689\n",
      "iteration number: 2184\t training loss: 0.0637\tvalidation loss: 0.1176\t validation accuracy: 0.9689\n",
      "iteration number: 2185\t training loss: 0.0636\tvalidation loss: 0.1174\t validation accuracy: 0.9622\n",
      "iteration number: 2186\t training loss: 0.0634\tvalidation loss: 0.1175\t validation accuracy: 0.9622\n",
      "iteration number: 2187\t training loss: 0.0635\tvalidation loss: 0.1141\t validation accuracy: 0.9711\n",
      "iteration number: 2188\t training loss: 0.0635\tvalidation loss: 0.1163\t validation accuracy: 0.9667\n",
      "iteration number: 2189\t training loss: 0.0638\tvalidation loss: 0.1190\t validation accuracy: 0.9667\n",
      "iteration number: 2190\t training loss: 0.0645\tvalidation loss: 0.1211\t validation accuracy: 0.9622\n",
      "iteration number: 2191\t training loss: 0.0649\tvalidation loss: 0.1212\t validation accuracy: 0.9622\n",
      "iteration number: 2192\t training loss: 0.0650\tvalidation loss: 0.1234\t validation accuracy: 0.9622\n",
      "iteration number: 2193\t training loss: 0.0647\tvalidation loss: 0.1236\t validation accuracy: 0.9667\n",
      "iteration number: 2194\t training loss: 0.0643\tvalidation loss: 0.1221\t validation accuracy: 0.9711\n",
      "iteration number: 2195\t training loss: 0.0640\tvalidation loss: 0.1208\t validation accuracy: 0.9689\n",
      "iteration number: 2196\t training loss: 0.0633\tvalidation loss: 0.1174\t validation accuracy: 0.9711\n",
      "iteration number: 2197\t training loss: 0.0636\tvalidation loss: 0.1203\t validation accuracy: 0.9689\n",
      "iteration number: 2198\t training loss: 0.0630\tvalidation loss: 0.1201\t validation accuracy: 0.9667\n",
      "iteration number: 2199\t training loss: 0.0632\tvalidation loss: 0.1206\t validation accuracy: 0.9644\n",
      "iteration number: 2200\t training loss: 0.0637\tvalidation loss: 0.1236\t validation accuracy: 0.9644\n",
      "iteration number: 2201\t training loss: 0.0649\tvalidation loss: 0.1260\t validation accuracy: 0.9667\n",
      "iteration number: 2202\t training loss: 0.0634\tvalidation loss: 0.1213\t validation accuracy: 0.9667\n",
      "iteration number: 2203\t training loss: 0.0636\tvalidation loss: 0.1223\t validation accuracy: 0.9667\n",
      "iteration number: 2204\t training loss: 0.0638\tvalidation loss: 0.1217\t validation accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2205\t training loss: 0.0632\tvalidation loss: 0.1218\t validation accuracy: 0.9644\n",
      "iteration number: 2206\t training loss: 0.0631\tvalidation loss: 0.1206\t validation accuracy: 0.9644\n",
      "iteration number: 2207\t training loss: 0.0628\tvalidation loss: 0.1200\t validation accuracy: 0.9644\n",
      "iteration number: 2208\t training loss: 0.0631\tvalidation loss: 0.1211\t validation accuracy: 0.9667\n",
      "iteration number: 2209\t training loss: 0.0637\tvalidation loss: 0.1184\t validation accuracy: 0.9667\n",
      "iteration number: 2210\t training loss: 0.0638\tvalidation loss: 0.1180\t validation accuracy: 0.9667\n",
      "iteration number: 2211\t training loss: 0.0626\tvalidation loss: 0.1163\t validation accuracy: 0.9667\n",
      "iteration number: 2212\t training loss: 0.0641\tvalidation loss: 0.1161\t validation accuracy: 0.9667\n",
      "iteration number: 2213\t training loss: 0.0629\tvalidation loss: 0.1140\t validation accuracy: 0.9667\n",
      "iteration number: 2214\t training loss: 0.0629\tvalidation loss: 0.1163\t validation accuracy: 0.9667\n",
      "iteration number: 2215\t training loss: 0.0628\tvalidation loss: 0.1164\t validation accuracy: 0.9667\n",
      "iteration number: 2216\t training loss: 0.0625\tvalidation loss: 0.1172\t validation accuracy: 0.9667\n",
      "iteration number: 2217\t training loss: 0.0625\tvalidation loss: 0.1173\t validation accuracy: 0.9689\n",
      "iteration number: 2218\t training loss: 0.0620\tvalidation loss: 0.1195\t validation accuracy: 0.9667\n",
      "iteration number: 2219\t training loss: 0.0619\tvalidation loss: 0.1176\t validation accuracy: 0.9667\n",
      "iteration number: 2220\t training loss: 0.0621\tvalidation loss: 0.1205\t validation accuracy: 0.9667\n",
      "iteration number: 2221\t training loss: 0.0623\tvalidation loss: 0.1190\t validation accuracy: 0.9689\n",
      "iteration number: 2222\t training loss: 0.0621\tvalidation loss: 0.1205\t validation accuracy: 0.9689\n",
      "iteration number: 2223\t training loss: 0.0623\tvalidation loss: 0.1215\t validation accuracy: 0.9689\n",
      "iteration number: 2224\t training loss: 0.0630\tvalidation loss: 0.1265\t validation accuracy: 0.9600\n",
      "iteration number: 2225\t training loss: 0.0627\tvalidation loss: 0.1249\t validation accuracy: 0.9644\n",
      "iteration number: 2226\t training loss: 0.0620\tvalidation loss: 0.1210\t validation accuracy: 0.9711\n",
      "iteration number: 2227\t training loss: 0.0619\tvalidation loss: 0.1203\t validation accuracy: 0.9689\n",
      "iteration number: 2228\t training loss: 0.0619\tvalidation loss: 0.1199\t validation accuracy: 0.9689\n",
      "iteration number: 2229\t training loss: 0.0618\tvalidation loss: 0.1193\t validation accuracy: 0.9689\n",
      "iteration number: 2230\t training loss: 0.0622\tvalidation loss: 0.1168\t validation accuracy: 0.9689\n",
      "iteration number: 2231\t training loss: 0.0619\tvalidation loss: 0.1185\t validation accuracy: 0.9689\n",
      "iteration number: 2232\t training loss: 0.0620\tvalidation loss: 0.1167\t validation accuracy: 0.9711\n",
      "iteration number: 2233\t training loss: 0.0617\tvalidation loss: 0.1173\t validation accuracy: 0.9733\n",
      "iteration number: 2234\t training loss: 0.0624\tvalidation loss: 0.1178\t validation accuracy: 0.9711\n",
      "iteration number: 2235\t training loss: 0.0642\tvalidation loss: 0.1172\t validation accuracy: 0.9733\n",
      "iteration number: 2236\t training loss: 0.0637\tvalidation loss: 0.1182\t validation accuracy: 0.9711\n",
      "iteration number: 2237\t training loss: 0.0650\tvalidation loss: 0.1154\t validation accuracy: 0.9756\n",
      "iteration number: 2238\t training loss: 0.0656\tvalidation loss: 0.1149\t validation accuracy: 0.9733\n",
      "iteration number: 2239\t training loss: 0.0640\tvalidation loss: 0.1177\t validation accuracy: 0.9711\n",
      "iteration number: 2240\t training loss: 0.0629\tvalidation loss: 0.1189\t validation accuracy: 0.9689\n",
      "iteration number: 2241\t training loss: 0.0635\tvalidation loss: 0.1179\t validation accuracy: 0.9667\n",
      "iteration number: 2242\t training loss: 0.0639\tvalidation loss: 0.1177\t validation accuracy: 0.9667\n",
      "iteration number: 2243\t training loss: 0.0635\tvalidation loss: 0.1168\t validation accuracy: 0.9711\n",
      "iteration number: 2244\t training loss: 0.0623\tvalidation loss: 0.1168\t validation accuracy: 0.9667\n",
      "iteration number: 2245\t training loss: 0.0625\tvalidation loss: 0.1151\t validation accuracy: 0.9711\n",
      "iteration number: 2246\t training loss: 0.0620\tvalidation loss: 0.1178\t validation accuracy: 0.9733\n",
      "iteration number: 2247\t training loss: 0.0619\tvalidation loss: 0.1136\t validation accuracy: 0.9689\n",
      "iteration number: 2248\t training loss: 0.0617\tvalidation loss: 0.1142\t validation accuracy: 0.9733\n",
      "iteration number: 2249\t training loss: 0.0623\tvalidation loss: 0.1120\t validation accuracy: 0.9733\n",
      "iteration number: 2250\t training loss: 0.0612\tvalidation loss: 0.1153\t validation accuracy: 0.9689\n",
      "iteration number: 2251\t training loss: 0.0617\tvalidation loss: 0.1142\t validation accuracy: 0.9689\n",
      "iteration number: 2252\t training loss: 0.0609\tvalidation loss: 0.1141\t validation accuracy: 0.9667\n",
      "iteration number: 2253\t training loss: 0.0609\tvalidation loss: 0.1142\t validation accuracy: 0.9689\n",
      "iteration number: 2254\t training loss: 0.0612\tvalidation loss: 0.1124\t validation accuracy: 0.9667\n",
      "iteration number: 2255\t training loss: 0.0617\tvalidation loss: 0.1117\t validation accuracy: 0.9711\n",
      "iteration number: 2256\t training loss: 0.0624\tvalidation loss: 0.1151\t validation accuracy: 0.9689\n",
      "iteration number: 2257\t training loss: 0.0628\tvalidation loss: 0.1158\t validation accuracy: 0.9667\n",
      "iteration number: 2258\t training loss: 0.0623\tvalidation loss: 0.1154\t validation accuracy: 0.9667\n",
      "iteration number: 2259\t training loss: 0.0617\tvalidation loss: 0.1146\t validation accuracy: 0.9689\n",
      "iteration number: 2260\t training loss: 0.0617\tvalidation loss: 0.1159\t validation accuracy: 0.9689\n",
      "iteration number: 2261\t training loss: 0.0621\tvalidation loss: 0.1123\t validation accuracy: 0.9711\n",
      "iteration number: 2262\t training loss: 0.0627\tvalidation loss: 0.1142\t validation accuracy: 0.9667\n",
      "iteration number: 2263\t training loss: 0.0628\tvalidation loss: 0.1161\t validation accuracy: 0.9644\n",
      "iteration number: 2264\t training loss: 0.0625\tvalidation loss: 0.1181\t validation accuracy: 0.9644\n",
      "iteration number: 2265\t training loss: 0.0616\tvalidation loss: 0.1119\t validation accuracy: 0.9689\n",
      "iteration number: 2266\t training loss: 0.0625\tvalidation loss: 0.1122\t validation accuracy: 0.9667\n",
      "iteration number: 2267\t training loss: 0.0623\tvalidation loss: 0.1126\t validation accuracy: 0.9667\n",
      "iteration number: 2268\t training loss: 0.0618\tvalidation loss: 0.1118\t validation accuracy: 0.9711\n",
      "iteration number: 2269\t training loss: 0.0621\tvalidation loss: 0.1115\t validation accuracy: 0.9711\n",
      "iteration number: 2270\t training loss: 0.0619\tvalidation loss: 0.1131\t validation accuracy: 0.9689\n",
      "iteration number: 2271\t training loss: 0.0615\tvalidation loss: 0.1125\t validation accuracy: 0.9689\n",
      "iteration number: 2272\t training loss: 0.0611\tvalidation loss: 0.1121\t validation accuracy: 0.9711\n",
      "iteration number: 2273\t training loss: 0.0631\tvalidation loss: 0.1165\t validation accuracy: 0.9667\n",
      "iteration number: 2274\t training loss: 0.0620\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 2275\t training loss: 0.0627\tvalidation loss: 0.1152\t validation accuracy: 0.9644\n",
      "iteration number: 2276\t training loss: 0.0620\tvalidation loss: 0.1187\t validation accuracy: 0.9600\n",
      "iteration number: 2277\t training loss: 0.0612\tvalidation loss: 0.1199\t validation accuracy: 0.9600\n",
      "iteration number: 2278\t training loss: 0.0622\tvalidation loss: 0.1217\t validation accuracy: 0.9600\n",
      "iteration number: 2279\t training loss: 0.0616\tvalidation loss: 0.1226\t validation accuracy: 0.9600\n",
      "iteration number: 2280\t training loss: 0.0612\tvalidation loss: 0.1225\t validation accuracy: 0.9600\n",
      "iteration number: 2281\t training loss: 0.0606\tvalidation loss: 0.1209\t validation accuracy: 0.9644\n",
      "iteration number: 2282\t training loss: 0.0603\tvalidation loss: 0.1195\t validation accuracy: 0.9622\n",
      "iteration number: 2283\t training loss: 0.0606\tvalidation loss: 0.1212\t validation accuracy: 0.9600\n",
      "iteration number: 2284\t training loss: 0.0600\tvalidation loss: 0.1179\t validation accuracy: 0.9667\n",
      "iteration number: 2285\t training loss: 0.0598\tvalidation loss: 0.1161\t validation accuracy: 0.9644\n",
      "iteration number: 2286\t training loss: 0.0606\tvalidation loss: 0.1196\t validation accuracy: 0.9644\n",
      "iteration number: 2287\t training loss: 0.0611\tvalidation loss: 0.1218\t validation accuracy: 0.9622\n",
      "iteration number: 2288\t training loss: 0.0619\tvalidation loss: 0.1221\t validation accuracy: 0.9600\n",
      "iteration number: 2289\t training loss: 0.0637\tvalidation loss: 0.1220\t validation accuracy: 0.9644\n",
      "iteration number: 2290\t training loss: 0.0615\tvalidation loss: 0.1209\t validation accuracy: 0.9622\n",
      "iteration number: 2291\t training loss: 0.0607\tvalidation loss: 0.1186\t validation accuracy: 0.9622\n",
      "iteration number: 2292\t training loss: 0.0605\tvalidation loss: 0.1157\t validation accuracy: 0.9667\n",
      "iteration number: 2293\t training loss: 0.0597\tvalidation loss: 0.1149\t validation accuracy: 0.9644\n",
      "iteration number: 2294\t training loss: 0.0596\tvalidation loss: 0.1140\t validation accuracy: 0.9667\n",
      "iteration number: 2295\t training loss: 0.0595\tvalidation loss: 0.1144\t validation accuracy: 0.9667\n",
      "iteration number: 2296\t training loss: 0.0598\tvalidation loss: 0.1136\t validation accuracy: 0.9711\n",
      "iteration number: 2297\t training loss: 0.0605\tvalidation loss: 0.1138\t validation accuracy: 0.9711\n",
      "iteration number: 2298\t training loss: 0.0604\tvalidation loss: 0.1164\t validation accuracy: 0.9667\n",
      "iteration number: 2299\t training loss: 0.0601\tvalidation loss: 0.1172\t validation accuracy: 0.9667\n",
      "iteration number: 2300\t training loss: 0.0608\tvalidation loss: 0.1190\t validation accuracy: 0.9622\n",
      "iteration number: 2301\t training loss: 0.0612\tvalidation loss: 0.1195\t validation accuracy: 0.9644\n",
      "iteration number: 2302\t training loss: 0.0609\tvalidation loss: 0.1204\t validation accuracy: 0.9622\n",
      "iteration number: 2303\t training loss: 0.0602\tvalidation loss: 0.1156\t validation accuracy: 0.9644\n",
      "iteration number: 2304\t training loss: 0.0611\tvalidation loss: 0.1167\t validation accuracy: 0.9622\n",
      "iteration number: 2305\t training loss: 0.0616\tvalidation loss: 0.1157\t validation accuracy: 0.9644\n",
      "iteration number: 2306\t training loss: 0.0609\tvalidation loss: 0.1170\t validation accuracy: 0.9667\n",
      "iteration number: 2307\t training loss: 0.0619\tvalidation loss: 0.1168\t validation accuracy: 0.9644\n",
      "iteration number: 2308\t training loss: 0.0617\tvalidation loss: 0.1160\t validation accuracy: 0.9644\n",
      "iteration number: 2309\t training loss: 0.0605\tvalidation loss: 0.1152\t validation accuracy: 0.9689\n",
      "iteration number: 2310\t training loss: 0.0613\tvalidation loss: 0.1145\t validation accuracy: 0.9689\n",
      "iteration number: 2311\t training loss: 0.0631\tvalidation loss: 0.1128\t validation accuracy: 0.9756\n",
      "iteration number: 2312\t training loss: 0.0640\tvalidation loss: 0.1127\t validation accuracy: 0.9733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2313\t training loss: 0.0629\tvalidation loss: 0.1140\t validation accuracy: 0.9711\n",
      "iteration number: 2314\t training loss: 0.0620\tvalidation loss: 0.1161\t validation accuracy: 0.9689\n",
      "iteration number: 2315\t training loss: 0.0601\tvalidation loss: 0.1176\t validation accuracy: 0.9622\n",
      "iteration number: 2316\t training loss: 0.0600\tvalidation loss: 0.1190\t validation accuracy: 0.9622\n",
      "iteration number: 2317\t training loss: 0.0610\tvalidation loss: 0.1168\t validation accuracy: 0.9667\n",
      "iteration number: 2318\t training loss: 0.0605\tvalidation loss: 0.1172\t validation accuracy: 0.9667\n",
      "iteration number: 2319\t training loss: 0.0626\tvalidation loss: 0.1182\t validation accuracy: 0.9644\n",
      "iteration number: 2320\t training loss: 0.0609\tvalidation loss: 0.1147\t validation accuracy: 0.9667\n",
      "iteration number: 2321\t training loss: 0.0599\tvalidation loss: 0.1144\t validation accuracy: 0.9689\n",
      "iteration number: 2322\t training loss: 0.0595\tvalidation loss: 0.1106\t validation accuracy: 0.9711\n",
      "iteration number: 2323\t training loss: 0.0594\tvalidation loss: 0.1117\t validation accuracy: 0.9711\n",
      "iteration number: 2324\t training loss: 0.0586\tvalidation loss: 0.1135\t validation accuracy: 0.9711\n",
      "iteration number: 2325\t training loss: 0.0586\tvalidation loss: 0.1137\t validation accuracy: 0.9689\n",
      "iteration number: 2326\t training loss: 0.0595\tvalidation loss: 0.1181\t validation accuracy: 0.9644\n",
      "iteration number: 2327\t training loss: 0.0601\tvalidation loss: 0.1214\t validation accuracy: 0.9644\n",
      "iteration number: 2328\t training loss: 0.0601\tvalidation loss: 0.1230\t validation accuracy: 0.9622\n",
      "iteration number: 2329\t training loss: 0.0595\tvalidation loss: 0.1201\t validation accuracy: 0.9667\n",
      "iteration number: 2330\t training loss: 0.0590\tvalidation loss: 0.1173\t validation accuracy: 0.9689\n",
      "iteration number: 2331\t training loss: 0.0595\tvalidation loss: 0.1186\t validation accuracy: 0.9644\n",
      "iteration number: 2332\t training loss: 0.0598\tvalidation loss: 0.1200\t validation accuracy: 0.9622\n",
      "iteration number: 2333\t training loss: 0.0613\tvalidation loss: 0.1231\t validation accuracy: 0.9644\n",
      "iteration number: 2334\t training loss: 0.0598\tvalidation loss: 0.1199\t validation accuracy: 0.9622\n",
      "iteration number: 2335\t training loss: 0.0597\tvalidation loss: 0.1195\t validation accuracy: 0.9644\n",
      "iteration number: 2336\t training loss: 0.0590\tvalidation loss: 0.1185\t validation accuracy: 0.9644\n",
      "iteration number: 2337\t training loss: 0.0591\tvalidation loss: 0.1177\t validation accuracy: 0.9622\n",
      "iteration number: 2338\t training loss: 0.0585\tvalidation loss: 0.1163\t validation accuracy: 0.9644\n",
      "iteration number: 2339\t training loss: 0.0585\tvalidation loss: 0.1179\t validation accuracy: 0.9689\n",
      "iteration number: 2340\t training loss: 0.0586\tvalidation loss: 0.1163\t validation accuracy: 0.9711\n",
      "iteration number: 2341\t training loss: 0.0584\tvalidation loss: 0.1187\t validation accuracy: 0.9667\n",
      "iteration number: 2342\t training loss: 0.0585\tvalidation loss: 0.1169\t validation accuracy: 0.9667\n",
      "iteration number: 2343\t training loss: 0.0586\tvalidation loss: 0.1180\t validation accuracy: 0.9644\n",
      "iteration number: 2344\t training loss: 0.0583\tvalidation loss: 0.1183\t validation accuracy: 0.9689\n",
      "iteration number: 2345\t training loss: 0.0582\tvalidation loss: 0.1191\t validation accuracy: 0.9667\n",
      "iteration number: 2346\t training loss: 0.0586\tvalidation loss: 0.1218\t validation accuracy: 0.9644\n",
      "iteration number: 2347\t training loss: 0.0585\tvalidation loss: 0.1220\t validation accuracy: 0.9644\n",
      "iteration number: 2348\t training loss: 0.0592\tvalidation loss: 0.1252\t validation accuracy: 0.9622\n",
      "iteration number: 2349\t training loss: 0.0612\tvalidation loss: 0.1298\t validation accuracy: 0.9578\n",
      "iteration number: 2350\t training loss: 0.0599\tvalidation loss: 0.1237\t validation accuracy: 0.9622\n",
      "iteration number: 2351\t training loss: 0.0593\tvalidation loss: 0.1183\t validation accuracy: 0.9667\n",
      "iteration number: 2352\t training loss: 0.0586\tvalidation loss: 0.1171\t validation accuracy: 0.9667\n",
      "iteration number: 2353\t training loss: 0.0583\tvalidation loss: 0.1167\t validation accuracy: 0.9667\n",
      "iteration number: 2354\t training loss: 0.0583\tvalidation loss: 0.1168\t validation accuracy: 0.9667\n",
      "iteration number: 2355\t training loss: 0.0582\tvalidation loss: 0.1176\t validation accuracy: 0.9644\n",
      "iteration number: 2356\t training loss: 0.0584\tvalidation loss: 0.1181\t validation accuracy: 0.9644\n",
      "iteration number: 2357\t training loss: 0.0591\tvalidation loss: 0.1210\t validation accuracy: 0.9622\n",
      "iteration number: 2358\t training loss: 0.0593\tvalidation loss: 0.1193\t validation accuracy: 0.9622\n",
      "iteration number: 2359\t training loss: 0.0580\tvalidation loss: 0.1175\t validation accuracy: 0.9667\n",
      "iteration number: 2360\t training loss: 0.0578\tvalidation loss: 0.1181\t validation accuracy: 0.9689\n",
      "iteration number: 2361\t training loss: 0.0581\tvalidation loss: 0.1170\t validation accuracy: 0.9689\n",
      "iteration number: 2362\t training loss: 0.0585\tvalidation loss: 0.1133\t validation accuracy: 0.9733\n",
      "iteration number: 2363\t training loss: 0.0584\tvalidation loss: 0.1123\t validation accuracy: 0.9733\n",
      "iteration number: 2364\t training loss: 0.0579\tvalidation loss: 0.1132\t validation accuracy: 0.9689\n",
      "iteration number: 2365\t training loss: 0.0579\tvalidation loss: 0.1133\t validation accuracy: 0.9711\n",
      "iteration number: 2366\t training loss: 0.0580\tvalidation loss: 0.1178\t validation accuracy: 0.9644\n",
      "iteration number: 2367\t training loss: 0.0577\tvalidation loss: 0.1186\t validation accuracy: 0.9667\n",
      "iteration number: 2368\t training loss: 0.0582\tvalidation loss: 0.1191\t validation accuracy: 0.9622\n",
      "iteration number: 2369\t training loss: 0.0592\tvalidation loss: 0.1198\t validation accuracy: 0.9644\n",
      "iteration number: 2370\t training loss: 0.0587\tvalidation loss: 0.1196\t validation accuracy: 0.9644\n",
      "iteration number: 2371\t training loss: 0.0577\tvalidation loss: 0.1161\t validation accuracy: 0.9667\n",
      "iteration number: 2372\t training loss: 0.0574\tvalidation loss: 0.1150\t validation accuracy: 0.9667\n",
      "iteration number: 2373\t training loss: 0.0575\tvalidation loss: 0.1166\t validation accuracy: 0.9667\n",
      "iteration number: 2374\t training loss: 0.0576\tvalidation loss: 0.1143\t validation accuracy: 0.9622\n",
      "iteration number: 2375\t training loss: 0.0575\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 2376\t training loss: 0.0576\tvalidation loss: 0.1175\t validation accuracy: 0.9667\n",
      "iteration number: 2377\t training loss: 0.0580\tvalidation loss: 0.1174\t validation accuracy: 0.9667\n",
      "iteration number: 2378\t training loss: 0.0581\tvalidation loss: 0.1194\t validation accuracy: 0.9644\n",
      "iteration number: 2379\t training loss: 0.0588\tvalidation loss: 0.1187\t validation accuracy: 0.9667\n",
      "iteration number: 2380\t training loss: 0.0592\tvalidation loss: 0.1221\t validation accuracy: 0.9644\n",
      "iteration number: 2381\t training loss: 0.0588\tvalidation loss: 0.1230\t validation accuracy: 0.9622\n",
      "iteration number: 2382\t training loss: 0.0584\tvalidation loss: 0.1205\t validation accuracy: 0.9644\n",
      "iteration number: 2383\t training loss: 0.0576\tvalidation loss: 0.1186\t validation accuracy: 0.9644\n",
      "iteration number: 2384\t training loss: 0.0575\tvalidation loss: 0.1174\t validation accuracy: 0.9667\n",
      "iteration number: 2385\t training loss: 0.0575\tvalidation loss: 0.1175\t validation accuracy: 0.9667\n",
      "iteration number: 2386\t training loss: 0.0574\tvalidation loss: 0.1162\t validation accuracy: 0.9644\n",
      "iteration number: 2387\t training loss: 0.0573\tvalidation loss: 0.1155\t validation accuracy: 0.9644\n",
      "iteration number: 2388\t training loss: 0.0572\tvalidation loss: 0.1158\t validation accuracy: 0.9644\n",
      "iteration number: 2389\t training loss: 0.0581\tvalidation loss: 0.1176\t validation accuracy: 0.9644\n",
      "iteration number: 2390\t training loss: 0.0574\tvalidation loss: 0.1168\t validation accuracy: 0.9711\n",
      "iteration number: 2391\t training loss: 0.0574\tvalidation loss: 0.1170\t validation accuracy: 0.9667\n",
      "iteration number: 2392\t training loss: 0.0570\tvalidation loss: 0.1131\t validation accuracy: 0.9689\n",
      "iteration number: 2393\t training loss: 0.0571\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 2394\t training loss: 0.0569\tvalidation loss: 0.1158\t validation accuracy: 0.9644\n",
      "iteration number: 2395\t training loss: 0.0573\tvalidation loss: 0.1136\t validation accuracy: 0.9689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2396\t training loss: 0.0573\tvalidation loss: 0.1146\t validation accuracy: 0.9689\n",
      "iteration number: 2397\t training loss: 0.0571\tvalidation loss: 0.1155\t validation accuracy: 0.9689\n",
      "iteration number: 2398\t training loss: 0.0569\tvalidation loss: 0.1148\t validation accuracy: 0.9644\n",
      "iteration number: 2399\t training loss: 0.0573\tvalidation loss: 0.1159\t validation accuracy: 0.9667\n",
      "iteration number: 2400\t training loss: 0.0587\tvalidation loss: 0.1169\t validation accuracy: 0.9622\n",
      "iteration number: 2401\t training loss: 0.0575\tvalidation loss: 0.1111\t validation accuracy: 0.9667\n",
      "iteration number: 2402\t training loss: 0.0576\tvalidation loss: 0.1112\t validation accuracy: 0.9689\n",
      "iteration number: 2403\t training loss: 0.0584\tvalidation loss: 0.1103\t validation accuracy: 0.9667\n",
      "iteration number: 2404\t training loss: 0.0589\tvalidation loss: 0.1115\t validation accuracy: 0.9667\n",
      "iteration number: 2405\t training loss: 0.0569\tvalidation loss: 0.1161\t validation accuracy: 0.9667\n",
      "iteration number: 2406\t training loss: 0.0569\tvalidation loss: 0.1144\t validation accuracy: 0.9689\n",
      "iteration number: 2407\t training loss: 0.0570\tvalidation loss: 0.1159\t validation accuracy: 0.9644\n",
      "iteration number: 2408\t training loss: 0.0571\tvalidation loss: 0.1178\t validation accuracy: 0.9644\n",
      "iteration number: 2409\t training loss: 0.0572\tvalidation loss: 0.1179\t validation accuracy: 0.9667\n",
      "iteration number: 2410\t training loss: 0.0576\tvalidation loss: 0.1185\t validation accuracy: 0.9667\n",
      "iteration number: 2411\t training loss: 0.0590\tvalidation loss: 0.1199\t validation accuracy: 0.9622\n",
      "iteration number: 2412\t training loss: 0.0586\tvalidation loss: 0.1223\t validation accuracy: 0.9578\n",
      "iteration number: 2413\t training loss: 0.0584\tvalidation loss: 0.1203\t validation accuracy: 0.9600\n",
      "iteration number: 2414\t training loss: 0.0577\tvalidation loss: 0.1158\t validation accuracy: 0.9622\n",
      "iteration number: 2415\t training loss: 0.0574\tvalidation loss: 0.1125\t validation accuracy: 0.9667\n",
      "iteration number: 2416\t training loss: 0.0573\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 2417\t training loss: 0.0573\tvalidation loss: 0.1101\t validation accuracy: 0.9667\n",
      "iteration number: 2418\t training loss: 0.0568\tvalidation loss: 0.1124\t validation accuracy: 0.9622\n",
      "iteration number: 2419\t training loss: 0.0567\tvalidation loss: 0.1137\t validation accuracy: 0.9667\n",
      "iteration number: 2420\t training loss: 0.0570\tvalidation loss: 0.1139\t validation accuracy: 0.9622\n",
      "iteration number: 2421\t training loss: 0.0568\tvalidation loss: 0.1136\t validation accuracy: 0.9622\n",
      "iteration number: 2422\t training loss: 0.0572\tvalidation loss: 0.1152\t validation accuracy: 0.9667\n",
      "iteration number: 2423\t training loss: 0.0573\tvalidation loss: 0.1155\t validation accuracy: 0.9644\n",
      "iteration number: 2424\t training loss: 0.0568\tvalidation loss: 0.1122\t validation accuracy: 0.9689\n",
      "iteration number: 2425\t training loss: 0.0572\tvalidation loss: 0.1148\t validation accuracy: 0.9689\n",
      "iteration number: 2426\t training loss: 0.0574\tvalidation loss: 0.1140\t validation accuracy: 0.9667\n",
      "iteration number: 2427\t training loss: 0.0578\tvalidation loss: 0.1156\t validation accuracy: 0.9644\n",
      "iteration number: 2428\t training loss: 0.0579\tvalidation loss: 0.1152\t validation accuracy: 0.9667\n",
      "iteration number: 2429\t training loss: 0.0567\tvalidation loss: 0.1123\t validation accuracy: 0.9689\n",
      "iteration number: 2430\t training loss: 0.0566\tvalidation loss: 0.1136\t validation accuracy: 0.9689\n",
      "iteration number: 2431\t training loss: 0.0570\tvalidation loss: 0.1146\t validation accuracy: 0.9667\n",
      "iteration number: 2432\t training loss: 0.0567\tvalidation loss: 0.1125\t validation accuracy: 0.9689\n",
      "iteration number: 2433\t training loss: 0.0568\tvalidation loss: 0.1117\t validation accuracy: 0.9667\n",
      "iteration number: 2434\t training loss: 0.0574\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 2435\t training loss: 0.0571\tvalidation loss: 0.1157\t validation accuracy: 0.9644\n",
      "iteration number: 2436\t training loss: 0.0569\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 2437\t training loss: 0.0568\tvalidation loss: 0.1158\t validation accuracy: 0.9644\n",
      "iteration number: 2438\t training loss: 0.0571\tvalidation loss: 0.1182\t validation accuracy: 0.9622\n",
      "iteration number: 2439\t training loss: 0.0583\tvalidation loss: 0.1233\t validation accuracy: 0.9600\n",
      "iteration number: 2440\t training loss: 0.0571\tvalidation loss: 0.1201\t validation accuracy: 0.9644\n",
      "iteration number: 2441\t training loss: 0.0579\tvalidation loss: 0.1233\t validation accuracy: 0.9600\n",
      "iteration number: 2442\t training loss: 0.0566\tvalidation loss: 0.1176\t validation accuracy: 0.9622\n",
      "iteration number: 2443\t training loss: 0.0565\tvalidation loss: 0.1168\t validation accuracy: 0.9600\n",
      "iteration number: 2444\t training loss: 0.0570\tvalidation loss: 0.1193\t validation accuracy: 0.9578\n",
      "iteration number: 2445\t training loss: 0.0568\tvalidation loss: 0.1171\t validation accuracy: 0.9622\n",
      "iteration number: 2446\t training loss: 0.0568\tvalidation loss: 0.1158\t validation accuracy: 0.9622\n",
      "iteration number: 2447\t training loss: 0.0566\tvalidation loss: 0.1163\t validation accuracy: 0.9622\n",
      "iteration number: 2448\t training loss: 0.0567\tvalidation loss: 0.1171\t validation accuracy: 0.9600\n",
      "iteration number: 2449\t training loss: 0.0561\tvalidation loss: 0.1132\t validation accuracy: 0.9667\n",
      "iteration number: 2450\t training loss: 0.0564\tvalidation loss: 0.1131\t validation accuracy: 0.9667\n",
      "iteration number: 2451\t training loss: 0.0565\tvalidation loss: 0.1130\t validation accuracy: 0.9711\n",
      "iteration number: 2452\t training loss: 0.0570\tvalidation loss: 0.1139\t validation accuracy: 0.9689\n",
      "iteration number: 2453\t training loss: 0.0573\tvalidation loss: 0.1140\t validation accuracy: 0.9667\n",
      "iteration number: 2454\t training loss: 0.0569\tvalidation loss: 0.1123\t validation accuracy: 0.9711\n",
      "iteration number: 2455\t training loss: 0.0573\tvalidation loss: 0.1149\t validation accuracy: 0.9644\n",
      "iteration number: 2456\t training loss: 0.0564\tvalidation loss: 0.1148\t validation accuracy: 0.9689\n",
      "iteration number: 2457\t training loss: 0.0573\tvalidation loss: 0.1181\t validation accuracy: 0.9644\n",
      "iteration number: 2458\t training loss: 0.0564\tvalidation loss: 0.1171\t validation accuracy: 0.9667\n",
      "iteration number: 2459\t training loss: 0.0565\tvalidation loss: 0.1152\t validation accuracy: 0.9644\n",
      "iteration number: 2460\t training loss: 0.0568\tvalidation loss: 0.1185\t validation accuracy: 0.9622\n",
      "iteration number: 2461\t training loss: 0.0568\tvalidation loss: 0.1157\t validation accuracy: 0.9667\n",
      "iteration number: 2462\t training loss: 0.0580\tvalidation loss: 0.1170\t validation accuracy: 0.9667\n",
      "iteration number: 2463\t training loss: 0.0581\tvalidation loss: 0.1183\t validation accuracy: 0.9644\n",
      "iteration number: 2464\t training loss: 0.0574\tvalidation loss: 0.1159\t validation accuracy: 0.9689\n",
      "iteration number: 2465\t training loss: 0.0579\tvalidation loss: 0.1170\t validation accuracy: 0.9667\n",
      "iteration number: 2466\t training loss: 0.0566\tvalidation loss: 0.1165\t validation accuracy: 0.9667\n",
      "iteration number: 2467\t training loss: 0.0566\tvalidation loss: 0.1179\t validation accuracy: 0.9644\n",
      "iteration number: 2468\t training loss: 0.0564\tvalidation loss: 0.1163\t validation accuracy: 0.9667\n",
      "iteration number: 2469\t training loss: 0.0568\tvalidation loss: 0.1211\t validation accuracy: 0.9644\n",
      "iteration number: 2470\t training loss: 0.0565\tvalidation loss: 0.1200\t validation accuracy: 0.9622\n",
      "iteration number: 2471\t training loss: 0.0557\tvalidation loss: 0.1159\t validation accuracy: 0.9600\n",
      "iteration number: 2472\t training loss: 0.0555\tvalidation loss: 0.1142\t validation accuracy: 0.9622\n",
      "iteration number: 2473\t training loss: 0.0555\tvalidation loss: 0.1130\t validation accuracy: 0.9689\n",
      "iteration number: 2474\t training loss: 0.0556\tvalidation loss: 0.1147\t validation accuracy: 0.9689\n",
      "iteration number: 2475\t training loss: 0.0556\tvalidation loss: 0.1156\t validation accuracy: 0.9689\n",
      "iteration number: 2476\t training loss: 0.0559\tvalidation loss: 0.1155\t validation accuracy: 0.9689\n",
      "iteration number: 2477\t training loss: 0.0559\tvalidation loss: 0.1167\t validation accuracy: 0.9644\n",
      "iteration number: 2478\t training loss: 0.0561\tvalidation loss: 0.1166\t validation accuracy: 0.9644\n",
      "iteration number: 2479\t training loss: 0.0559\tvalidation loss: 0.1155\t validation accuracy: 0.9689\n",
      "iteration number: 2480\t training loss: 0.0559\tvalidation loss: 0.1161\t validation accuracy: 0.9667\n",
      "iteration number: 2481\t training loss: 0.0560\tvalidation loss: 0.1160\t validation accuracy: 0.9578\n",
      "iteration number: 2482\t training loss: 0.0560\tvalidation loss: 0.1142\t validation accuracy: 0.9689\n",
      "iteration number: 2483\t training loss: 0.0555\tvalidation loss: 0.1159\t validation accuracy: 0.9667\n",
      "iteration number: 2484\t training loss: 0.0554\tvalidation loss: 0.1160\t validation accuracy: 0.9644\n",
      "iteration number: 2485\t training loss: 0.0566\tvalidation loss: 0.1157\t validation accuracy: 0.9644\n",
      "iteration number: 2486\t training loss: 0.0559\tvalidation loss: 0.1141\t validation accuracy: 0.9711\n",
      "iteration number: 2487\t training loss: 0.0557\tvalidation loss: 0.1138\t validation accuracy: 0.9689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2488\t training loss: 0.0554\tvalidation loss: 0.1164\t validation accuracy: 0.9689\n",
      "iteration number: 2489\t training loss: 0.0556\tvalidation loss: 0.1184\t validation accuracy: 0.9667\n",
      "iteration number: 2490\t training loss: 0.0563\tvalidation loss: 0.1167\t validation accuracy: 0.9689\n",
      "iteration number: 2491\t training loss: 0.0562\tvalidation loss: 0.1179\t validation accuracy: 0.9644\n",
      "iteration number: 2492\t training loss: 0.0562\tvalidation loss: 0.1171\t validation accuracy: 0.9622\n",
      "iteration number: 2493\t training loss: 0.0552\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 2494\t training loss: 0.0549\tvalidation loss: 0.1163\t validation accuracy: 0.9644\n",
      "iteration number: 2495\t training loss: 0.0555\tvalidation loss: 0.1142\t validation accuracy: 0.9600\n",
      "iteration number: 2496\t training loss: 0.0554\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 2497\t training loss: 0.0552\tvalidation loss: 0.1167\t validation accuracy: 0.9644\n",
      "iteration number: 2498\t training loss: 0.0554\tvalidation loss: 0.1180\t validation accuracy: 0.9644\n",
      "iteration number: 2499\t training loss: 0.0553\tvalidation loss: 0.1169\t validation accuracy: 0.9600\n",
      "iteration number: 2500\t training loss: 0.0550\tvalidation loss: 0.1145\t validation accuracy: 0.9622\n",
      "iteration number: 2501\t training loss: 0.0552\tvalidation loss: 0.1146\t validation accuracy: 0.9667\n",
      "iteration number: 2502\t training loss: 0.0553\tvalidation loss: 0.1146\t validation accuracy: 0.9689\n",
      "iteration number: 2503\t training loss: 0.0552\tvalidation loss: 0.1153\t validation accuracy: 0.9689\n",
      "iteration number: 2504\t training loss: 0.0551\tvalidation loss: 0.1157\t validation accuracy: 0.9667\n",
      "iteration number: 2505\t training loss: 0.0553\tvalidation loss: 0.1121\t validation accuracy: 0.9644\n",
      "iteration number: 2506\t training loss: 0.0549\tvalidation loss: 0.1162\t validation accuracy: 0.9644\n",
      "iteration number: 2507\t training loss: 0.0556\tvalidation loss: 0.1208\t validation accuracy: 0.9622\n",
      "iteration number: 2508\t training loss: 0.0560\tvalidation loss: 0.1225\t validation accuracy: 0.9578\n",
      "iteration number: 2509\t training loss: 0.0555\tvalidation loss: 0.1213\t validation accuracy: 0.9622\n",
      "iteration number: 2510\t training loss: 0.0558\tvalidation loss: 0.1208\t validation accuracy: 0.9644\n",
      "iteration number: 2511\t training loss: 0.0556\tvalidation loss: 0.1220\t validation accuracy: 0.9622\n",
      "iteration number: 2512\t training loss: 0.0549\tvalidation loss: 0.1159\t validation accuracy: 0.9644\n",
      "iteration number: 2513\t training loss: 0.0551\tvalidation loss: 0.1160\t validation accuracy: 0.9644\n",
      "iteration number: 2514\t training loss: 0.0549\tvalidation loss: 0.1146\t validation accuracy: 0.9711\n",
      "iteration number: 2515\t training loss: 0.0548\tvalidation loss: 0.1127\t validation accuracy: 0.9644\n",
      "iteration number: 2516\t training loss: 0.0556\tvalidation loss: 0.1124\t validation accuracy: 0.9667\n",
      "iteration number: 2517\t training loss: 0.0568\tvalidation loss: 0.1107\t validation accuracy: 0.9689\n",
      "iteration number: 2518\t training loss: 0.0553\tvalidation loss: 0.1098\t validation accuracy: 0.9711\n",
      "iteration number: 2519\t training loss: 0.0550\tvalidation loss: 0.1110\t validation accuracy: 0.9711\n",
      "iteration number: 2520\t training loss: 0.0549\tvalidation loss: 0.1089\t validation accuracy: 0.9711\n",
      "iteration number: 2521\t training loss: 0.0550\tvalidation loss: 0.1085\t validation accuracy: 0.9711\n",
      "iteration number: 2522\t training loss: 0.0551\tvalidation loss: 0.1080\t validation accuracy: 0.9711\n",
      "iteration number: 2523\t training loss: 0.0544\tvalidation loss: 0.1097\t validation accuracy: 0.9689\n",
      "iteration number: 2524\t training loss: 0.0554\tvalidation loss: 0.1110\t validation accuracy: 0.9711\n",
      "iteration number: 2525\t training loss: 0.0552\tvalidation loss: 0.1122\t validation accuracy: 0.9689\n",
      "iteration number: 2526\t training loss: 0.0551\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 2527\t training loss: 0.0548\tvalidation loss: 0.1140\t validation accuracy: 0.9644\n",
      "iteration number: 2528\t training loss: 0.0555\tvalidation loss: 0.1153\t validation accuracy: 0.9644\n",
      "iteration number: 2529\t training loss: 0.0588\tvalidation loss: 0.1194\t validation accuracy: 0.9622\n",
      "iteration number: 2530\t training loss: 0.0585\tvalidation loss: 0.1188\t validation accuracy: 0.9622\n",
      "iteration number: 2531\t training loss: 0.0561\tvalidation loss: 0.1156\t validation accuracy: 0.9622\n",
      "iteration number: 2532\t training loss: 0.0555\tvalidation loss: 0.1092\t validation accuracy: 0.9711\n",
      "iteration number: 2533\t training loss: 0.0547\tvalidation loss: 0.1110\t validation accuracy: 0.9667\n",
      "iteration number: 2534\t training loss: 0.0552\tvalidation loss: 0.1107\t validation accuracy: 0.9689\n",
      "iteration number: 2535\t training loss: 0.0555\tvalidation loss: 0.1113\t validation accuracy: 0.9689\n",
      "iteration number: 2536\t training loss: 0.0555\tvalidation loss: 0.1095\t validation accuracy: 0.9711\n",
      "iteration number: 2537\t training loss: 0.0560\tvalidation loss: 0.1092\t validation accuracy: 0.9711\n",
      "iteration number: 2538\t training loss: 0.0556\tvalidation loss: 0.1089\t validation accuracy: 0.9711\n",
      "iteration number: 2539\t training loss: 0.0557\tvalidation loss: 0.1122\t validation accuracy: 0.9667\n",
      "iteration number: 2540\t training loss: 0.0549\tvalidation loss: 0.1131\t validation accuracy: 0.9667\n",
      "iteration number: 2541\t training loss: 0.0545\tvalidation loss: 0.1104\t validation accuracy: 0.9689\n",
      "iteration number: 2542\t training loss: 0.0538\tvalidation loss: 0.1096\t validation accuracy: 0.9667\n",
      "iteration number: 2543\t training loss: 0.0538\tvalidation loss: 0.1096\t validation accuracy: 0.9667\n",
      "iteration number: 2544\t training loss: 0.0543\tvalidation loss: 0.1137\t validation accuracy: 0.9644\n",
      "iteration number: 2545\t training loss: 0.0536\tvalidation loss: 0.1142\t validation accuracy: 0.9689\n",
      "iteration number: 2546\t training loss: 0.0537\tvalidation loss: 0.1161\t validation accuracy: 0.9667\n",
      "iteration number: 2547\t training loss: 0.0536\tvalidation loss: 0.1161\t validation accuracy: 0.9667\n",
      "iteration number: 2548\t training loss: 0.0547\tvalidation loss: 0.1180\t validation accuracy: 0.9622\n",
      "iteration number: 2549\t training loss: 0.0551\tvalidation loss: 0.1196\t validation accuracy: 0.9600\n",
      "iteration number: 2550\t training loss: 0.0545\tvalidation loss: 0.1185\t validation accuracy: 0.9600\n",
      "iteration number: 2551\t training loss: 0.0545\tvalidation loss: 0.1176\t validation accuracy: 0.9600\n",
      "iteration number: 2552\t training loss: 0.0550\tvalidation loss: 0.1176\t validation accuracy: 0.9600\n",
      "iteration number: 2553\t training loss: 0.0548\tvalidation loss: 0.1144\t validation accuracy: 0.9556\n",
      "iteration number: 2554\t training loss: 0.0544\tvalidation loss: 0.1111\t validation accuracy: 0.9622\n",
      "iteration number: 2555\t training loss: 0.0544\tvalidation loss: 0.1105\t validation accuracy: 0.9622\n",
      "iteration number: 2556\t training loss: 0.0537\tvalidation loss: 0.1120\t validation accuracy: 0.9622\n",
      "iteration number: 2557\t training loss: 0.0540\tvalidation loss: 0.1138\t validation accuracy: 0.9689\n",
      "iteration number: 2558\t training loss: 0.0545\tvalidation loss: 0.1140\t validation accuracy: 0.9644\n",
      "iteration number: 2559\t training loss: 0.0548\tvalidation loss: 0.1121\t validation accuracy: 0.9622\n",
      "iteration number: 2560\t training loss: 0.0544\tvalidation loss: 0.1115\t validation accuracy: 0.9600\n",
      "iteration number: 2561\t training loss: 0.0540\tvalidation loss: 0.1107\t validation accuracy: 0.9644\n",
      "iteration number: 2562\t training loss: 0.0546\tvalidation loss: 0.1134\t validation accuracy: 0.9600\n",
      "iteration number: 2563\t training loss: 0.0546\tvalidation loss: 0.1120\t validation accuracy: 0.9644\n",
      "iteration number: 2564\t training loss: 0.0544\tvalidation loss: 0.1125\t validation accuracy: 0.9644\n",
      "iteration number: 2565\t training loss: 0.0550\tvalidation loss: 0.1153\t validation accuracy: 0.9600\n",
      "iteration number: 2566\t training loss: 0.0550\tvalidation loss: 0.1150\t validation accuracy: 0.9622\n",
      "iteration number: 2567\t training loss: 0.0552\tvalidation loss: 0.1115\t validation accuracy: 0.9644\n",
      "iteration number: 2568\t training loss: 0.0546\tvalidation loss: 0.1112\t validation accuracy: 0.9644\n",
      "iteration number: 2569\t training loss: 0.0536\tvalidation loss: 0.1117\t validation accuracy: 0.9644\n",
      "iteration number: 2570\t training loss: 0.0534\tvalidation loss: 0.1106\t validation accuracy: 0.9689\n",
      "iteration number: 2571\t training loss: 0.0538\tvalidation loss: 0.1117\t validation accuracy: 0.9689\n",
      "iteration number: 2572\t training loss: 0.0539\tvalidation loss: 0.1101\t validation accuracy: 0.9711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2573\t training loss: 0.0548\tvalidation loss: 0.1125\t validation accuracy: 0.9711\n",
      "iteration number: 2574\t training loss: 0.0541\tvalidation loss: 0.1120\t validation accuracy: 0.9667\n",
      "iteration number: 2575\t training loss: 0.0543\tvalidation loss: 0.1124\t validation accuracy: 0.9689\n",
      "iteration number: 2576\t training loss: 0.0541\tvalidation loss: 0.1145\t validation accuracy: 0.9689\n",
      "iteration number: 2577\t training loss: 0.0540\tvalidation loss: 0.1120\t validation accuracy: 0.9711\n",
      "iteration number: 2578\t training loss: 0.0540\tvalidation loss: 0.1133\t validation accuracy: 0.9667\n",
      "iteration number: 2579\t training loss: 0.0541\tvalidation loss: 0.1153\t validation accuracy: 0.9667\n",
      "iteration number: 2580\t training loss: 0.0531\tvalidation loss: 0.1137\t validation accuracy: 0.9667\n",
      "iteration number: 2581\t training loss: 0.0529\tvalidation loss: 0.1100\t validation accuracy: 0.9711\n",
      "iteration number: 2582\t training loss: 0.0531\tvalidation loss: 0.1112\t validation accuracy: 0.9711\n",
      "iteration number: 2583\t training loss: 0.0529\tvalidation loss: 0.1124\t validation accuracy: 0.9711\n",
      "iteration number: 2584\t training loss: 0.0531\tvalidation loss: 0.1127\t validation accuracy: 0.9689\n",
      "iteration number: 2585\t training loss: 0.0537\tvalidation loss: 0.1156\t validation accuracy: 0.9689\n",
      "iteration number: 2586\t training loss: 0.0538\tvalidation loss: 0.1140\t validation accuracy: 0.9689\n",
      "iteration number: 2587\t training loss: 0.0538\tvalidation loss: 0.1123\t validation accuracy: 0.9667\n",
      "iteration number: 2588\t training loss: 0.0533\tvalidation loss: 0.1089\t validation accuracy: 0.9711\n",
      "iteration number: 2589\t training loss: 0.0534\tvalidation loss: 0.1094\t validation accuracy: 0.9711\n",
      "iteration number: 2590\t training loss: 0.0535\tvalidation loss: 0.1085\t validation accuracy: 0.9667\n",
      "iteration number: 2591\t training loss: 0.0534\tvalidation loss: 0.1099\t validation accuracy: 0.9689\n",
      "iteration number: 2592\t training loss: 0.0539\tvalidation loss: 0.1079\t validation accuracy: 0.9667\n",
      "iteration number: 2593\t training loss: 0.0544\tvalidation loss: 0.1087\t validation accuracy: 0.9644\n",
      "iteration number: 2594\t training loss: 0.0542\tvalidation loss: 0.1101\t validation accuracy: 0.9667\n",
      "iteration number: 2595\t training loss: 0.0544\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 2596\t training loss: 0.0540\tvalidation loss: 0.1154\t validation accuracy: 0.9622\n",
      "iteration number: 2597\t training loss: 0.0535\tvalidation loss: 0.1171\t validation accuracy: 0.9644\n",
      "iteration number: 2598\t training loss: 0.0532\tvalidation loss: 0.1161\t validation accuracy: 0.9667\n",
      "iteration number: 2599\t training loss: 0.0532\tvalidation loss: 0.1139\t validation accuracy: 0.9711\n",
      "iteration number: 2600\t training loss: 0.0530\tvalidation loss: 0.1132\t validation accuracy: 0.9689\n",
      "iteration number: 2601\t training loss: 0.0539\tvalidation loss: 0.1155\t validation accuracy: 0.9689\n",
      "iteration number: 2602\t training loss: 0.0533\tvalidation loss: 0.1130\t validation accuracy: 0.9711\n",
      "iteration number: 2603\t training loss: 0.0539\tvalidation loss: 0.1158\t validation accuracy: 0.9644\n",
      "iteration number: 2604\t training loss: 0.0534\tvalidation loss: 0.1132\t validation accuracy: 0.9667\n",
      "iteration number: 2605\t training loss: 0.0533\tvalidation loss: 0.1143\t validation accuracy: 0.9667\n",
      "iteration number: 2606\t training loss: 0.0524\tvalidation loss: 0.1104\t validation accuracy: 0.9689\n",
      "iteration number: 2607\t training loss: 0.0526\tvalidation loss: 0.1083\t validation accuracy: 0.9667\n",
      "iteration number: 2608\t training loss: 0.0524\tvalidation loss: 0.1118\t validation accuracy: 0.9711\n",
      "iteration number: 2609\t training loss: 0.0526\tvalidation loss: 0.1104\t validation accuracy: 0.9711\n",
      "iteration number: 2610\t training loss: 0.0521\tvalidation loss: 0.1104\t validation accuracy: 0.9689\n",
      "iteration number: 2611\t training loss: 0.0521\tvalidation loss: 0.1101\t validation accuracy: 0.9689\n",
      "iteration number: 2612\t training loss: 0.0525\tvalidation loss: 0.1080\t validation accuracy: 0.9689\n",
      "iteration number: 2613\t training loss: 0.0531\tvalidation loss: 0.1071\t validation accuracy: 0.9711\n",
      "iteration number: 2614\t training loss: 0.0536\tvalidation loss: 0.1075\t validation accuracy: 0.9689\n",
      "iteration number: 2615\t training loss: 0.0528\tvalidation loss: 0.1075\t validation accuracy: 0.9644\n",
      "iteration number: 2616\t training loss: 0.0528\tvalidation loss: 0.1127\t validation accuracy: 0.9689\n",
      "iteration number: 2617\t training loss: 0.0531\tvalidation loss: 0.1140\t validation accuracy: 0.9667\n",
      "iteration number: 2618\t training loss: 0.0523\tvalidation loss: 0.1128\t validation accuracy: 0.9667\n",
      "iteration number: 2619\t training loss: 0.0521\tvalidation loss: 0.1127\t validation accuracy: 0.9644\n",
      "iteration number: 2620\t training loss: 0.0522\tvalidation loss: 0.1133\t validation accuracy: 0.9622\n",
      "iteration number: 2621\t training loss: 0.0523\tvalidation loss: 0.1122\t validation accuracy: 0.9600\n",
      "iteration number: 2622\t training loss: 0.0522\tvalidation loss: 0.1116\t validation accuracy: 0.9622\n",
      "iteration number: 2623\t training loss: 0.0520\tvalidation loss: 0.1142\t validation accuracy: 0.9600\n",
      "iteration number: 2624\t training loss: 0.0523\tvalidation loss: 0.1126\t validation accuracy: 0.9600\n",
      "iteration number: 2625\t training loss: 0.0528\tvalidation loss: 0.1134\t validation accuracy: 0.9578\n",
      "iteration number: 2626\t training loss: 0.0525\tvalidation loss: 0.1081\t validation accuracy: 0.9644\n",
      "iteration number: 2627\t training loss: 0.0525\tvalidation loss: 0.1086\t validation accuracy: 0.9667\n",
      "iteration number: 2628\t training loss: 0.0534\tvalidation loss: 0.1089\t validation accuracy: 0.9644\n",
      "iteration number: 2629\t training loss: 0.0531\tvalidation loss: 0.1071\t validation accuracy: 0.9711\n",
      "iteration number: 2630\t training loss: 0.0526\tvalidation loss: 0.1083\t validation accuracy: 0.9689\n",
      "iteration number: 2631\t training loss: 0.0531\tvalidation loss: 0.1082\t validation accuracy: 0.9689\n",
      "iteration number: 2632\t training loss: 0.0526\tvalidation loss: 0.1077\t validation accuracy: 0.9689\n",
      "iteration number: 2633\t training loss: 0.0530\tvalidation loss: 0.1092\t validation accuracy: 0.9622\n",
      "iteration number: 2634\t training loss: 0.0534\tvalidation loss: 0.1095\t validation accuracy: 0.9644\n",
      "iteration number: 2635\t training loss: 0.0534\tvalidation loss: 0.1070\t validation accuracy: 0.9667\n",
      "iteration number: 2636\t training loss: 0.0539\tvalidation loss: 0.1056\t validation accuracy: 0.9756\n",
      "iteration number: 2637\t training loss: 0.0535\tvalidation loss: 0.1068\t validation accuracy: 0.9756\n",
      "iteration number: 2638\t training loss: 0.0522\tvalidation loss: 0.1077\t validation accuracy: 0.9711\n",
      "iteration number: 2639\t training loss: 0.0524\tvalidation loss: 0.1088\t validation accuracy: 0.9667\n",
      "iteration number: 2640\t training loss: 0.0527\tvalidation loss: 0.1067\t validation accuracy: 0.9733\n",
      "iteration number: 2641\t training loss: 0.0520\tvalidation loss: 0.1108\t validation accuracy: 0.9644\n",
      "iteration number: 2642\t training loss: 0.0521\tvalidation loss: 0.1116\t validation accuracy: 0.9644\n",
      "iteration number: 2643\t training loss: 0.0523\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n",
      "iteration number: 2644\t training loss: 0.0524\tvalidation loss: 0.1159\t validation accuracy: 0.9600\n",
      "iteration number: 2645\t training loss: 0.0526\tvalidation loss: 0.1169\t validation accuracy: 0.9578\n",
      "iteration number: 2646\t training loss: 0.0518\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 2647\t training loss: 0.0517\tvalidation loss: 0.1137\t validation accuracy: 0.9689\n",
      "iteration number: 2648\t training loss: 0.0514\tvalidation loss: 0.1134\t validation accuracy: 0.9644\n",
      "iteration number: 2649\t training loss: 0.0514\tvalidation loss: 0.1126\t validation accuracy: 0.9689\n",
      "iteration number: 2650\t training loss: 0.0512\tvalidation loss: 0.1130\t validation accuracy: 0.9644\n",
      "iteration number: 2651\t training loss: 0.0517\tvalidation loss: 0.1164\t validation accuracy: 0.9644\n",
      "iteration number: 2652\t training loss: 0.0516\tvalidation loss: 0.1172\t validation accuracy: 0.9667\n",
      "iteration number: 2653\t training loss: 0.0522\tvalidation loss: 0.1186\t validation accuracy: 0.9622\n",
      "iteration number: 2654\t training loss: 0.0526\tvalidation loss: 0.1200\t validation accuracy: 0.9622\n",
      "iteration number: 2655\t training loss: 0.0514\tvalidation loss: 0.1153\t validation accuracy: 0.9667\n",
      "iteration number: 2656\t training loss: 0.0511\tvalidation loss: 0.1144\t validation accuracy: 0.9667\n",
      "iteration number: 2657\t training loss: 0.0510\tvalidation loss: 0.1128\t validation accuracy: 0.9667\n",
      "iteration number: 2658\t training loss: 0.0508\tvalidation loss: 0.1125\t validation accuracy: 0.9667\n",
      "iteration number: 2659\t training loss: 0.0509\tvalidation loss: 0.1119\t validation accuracy: 0.9667\n",
      "iteration number: 2660\t training loss: 0.0512\tvalidation loss: 0.1140\t validation accuracy: 0.9644\n",
      "iteration number: 2661\t training loss: 0.0514\tvalidation loss: 0.1175\t validation accuracy: 0.9644\n",
      "iteration number: 2662\t training loss: 0.0512\tvalidation loss: 0.1165\t validation accuracy: 0.9622\n",
      "iteration number: 2663\t training loss: 0.0518\tvalidation loss: 0.1185\t validation accuracy: 0.9600\n",
      "iteration number: 2664\t training loss: 0.0519\tvalidation loss: 0.1180\t validation accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2665\t training loss: 0.0510\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 2666\t training loss: 0.0508\tvalidation loss: 0.1122\t validation accuracy: 0.9667\n",
      "iteration number: 2667\t training loss: 0.0508\tvalidation loss: 0.1134\t validation accuracy: 0.9667\n",
      "iteration number: 2668\t training loss: 0.0508\tvalidation loss: 0.1134\t validation accuracy: 0.9689\n",
      "iteration number: 2669\t training loss: 0.0506\tvalidation loss: 0.1137\t validation accuracy: 0.9689\n",
      "iteration number: 2670\t training loss: 0.0506\tvalidation loss: 0.1127\t validation accuracy: 0.9689\n",
      "iteration number: 2671\t training loss: 0.0507\tvalidation loss: 0.1148\t validation accuracy: 0.9689\n",
      "iteration number: 2672\t training loss: 0.0510\tvalidation loss: 0.1123\t validation accuracy: 0.9711\n",
      "iteration number: 2673\t training loss: 0.0510\tvalidation loss: 0.1114\t validation accuracy: 0.9689\n",
      "iteration number: 2674\t training loss: 0.0516\tvalidation loss: 0.1117\t validation accuracy: 0.9644\n",
      "iteration number: 2675\t training loss: 0.0508\tvalidation loss: 0.1132\t validation accuracy: 0.9689\n",
      "iteration number: 2676\t training loss: 0.0506\tvalidation loss: 0.1143\t validation accuracy: 0.9644\n",
      "iteration number: 2677\t training loss: 0.0510\tvalidation loss: 0.1152\t validation accuracy: 0.9622\n",
      "iteration number: 2678\t training loss: 0.0508\tvalidation loss: 0.1136\t validation accuracy: 0.9644\n",
      "iteration number: 2679\t training loss: 0.0510\tvalidation loss: 0.1112\t validation accuracy: 0.9689\n",
      "iteration number: 2680\t training loss: 0.0510\tvalidation loss: 0.1098\t validation accuracy: 0.9733\n",
      "iteration number: 2681\t training loss: 0.0506\tvalidation loss: 0.1124\t validation accuracy: 0.9689\n",
      "iteration number: 2682\t training loss: 0.0507\tvalidation loss: 0.1136\t validation accuracy: 0.9644\n",
      "iteration number: 2683\t training loss: 0.0509\tvalidation loss: 0.1158\t validation accuracy: 0.9689\n",
      "iteration number: 2684\t training loss: 0.0505\tvalidation loss: 0.1159\t validation accuracy: 0.9689\n",
      "iteration number: 2685\t training loss: 0.0507\tvalidation loss: 0.1150\t validation accuracy: 0.9689\n",
      "iteration number: 2686\t training loss: 0.0506\tvalidation loss: 0.1138\t validation accuracy: 0.9689\n",
      "iteration number: 2687\t training loss: 0.0520\tvalidation loss: 0.1162\t validation accuracy: 0.9644\n",
      "iteration number: 2688\t training loss: 0.0523\tvalidation loss: 0.1181\t validation accuracy: 0.9600\n",
      "iteration number: 2689\t training loss: 0.0524\tvalidation loss: 0.1190\t validation accuracy: 0.9600\n",
      "iteration number: 2690\t training loss: 0.0514\tvalidation loss: 0.1162\t validation accuracy: 0.9622\n",
      "iteration number: 2691\t training loss: 0.0507\tvalidation loss: 0.1161\t validation accuracy: 0.9644\n",
      "iteration number: 2692\t training loss: 0.0505\tvalidation loss: 0.1150\t validation accuracy: 0.9667\n",
      "iteration number: 2693\t training loss: 0.0507\tvalidation loss: 0.1172\t validation accuracy: 0.9644\n",
      "iteration number: 2694\t training loss: 0.0506\tvalidation loss: 0.1182\t validation accuracy: 0.9622\n",
      "iteration number: 2695\t training loss: 0.0516\tvalidation loss: 0.1208\t validation accuracy: 0.9644\n",
      "iteration number: 2696\t training loss: 0.0514\tvalidation loss: 0.1214\t validation accuracy: 0.9644\n",
      "iteration number: 2697\t training loss: 0.0509\tvalidation loss: 0.1184\t validation accuracy: 0.9667\n",
      "iteration number: 2698\t training loss: 0.0504\tvalidation loss: 0.1169\t validation accuracy: 0.9689\n",
      "iteration number: 2699\t training loss: 0.0504\tvalidation loss: 0.1171\t validation accuracy: 0.9689\n",
      "iteration number: 2700\t training loss: 0.0506\tvalidation loss: 0.1179\t validation accuracy: 0.9667\n",
      "iteration number: 2701\t training loss: 0.0508\tvalidation loss: 0.1165\t validation accuracy: 0.9644\n",
      "iteration number: 2702\t training loss: 0.0513\tvalidation loss: 0.1185\t validation accuracy: 0.9644\n",
      "iteration number: 2703\t training loss: 0.0512\tvalidation loss: 0.1168\t validation accuracy: 0.9644\n",
      "iteration number: 2704\t training loss: 0.0508\tvalidation loss: 0.1154\t validation accuracy: 0.9689\n",
      "iteration number: 2705\t training loss: 0.0511\tvalidation loss: 0.1173\t validation accuracy: 0.9644\n",
      "iteration number: 2706\t training loss: 0.0510\tvalidation loss: 0.1172\t validation accuracy: 0.9667\n",
      "iteration number: 2707\t training loss: 0.0512\tvalidation loss: 0.1179\t validation accuracy: 0.9689\n",
      "iteration number: 2708\t training loss: 0.0508\tvalidation loss: 0.1164\t validation accuracy: 0.9689\n",
      "iteration number: 2709\t training loss: 0.0509\tvalidation loss: 0.1165\t validation accuracy: 0.9711\n",
      "iteration number: 2710\t training loss: 0.0508\tvalidation loss: 0.1172\t validation accuracy: 0.9667\n",
      "iteration number: 2711\t training loss: 0.0510\tvalidation loss: 0.1170\t validation accuracy: 0.9689\n",
      "iteration number: 2712\t training loss: 0.0513\tvalidation loss: 0.1184\t validation accuracy: 0.9667\n",
      "iteration number: 2713\t training loss: 0.0511\tvalidation loss: 0.1187\t validation accuracy: 0.9689\n",
      "iteration number: 2714\t training loss: 0.0507\tvalidation loss: 0.1151\t validation accuracy: 0.9667\n",
      "iteration number: 2715\t training loss: 0.0511\tvalidation loss: 0.1187\t validation accuracy: 0.9644\n",
      "iteration number: 2716\t training loss: 0.0510\tvalidation loss: 0.1181\t validation accuracy: 0.9667\n",
      "iteration number: 2717\t training loss: 0.0514\tvalidation loss: 0.1207\t validation accuracy: 0.9622\n",
      "iteration number: 2718\t training loss: 0.0516\tvalidation loss: 0.1197\t validation accuracy: 0.9644\n",
      "iteration number: 2719\t training loss: 0.0515\tvalidation loss: 0.1214\t validation accuracy: 0.9667\n",
      "iteration number: 2720\t training loss: 0.0514\tvalidation loss: 0.1226\t validation accuracy: 0.9622\n",
      "iteration number: 2721\t training loss: 0.0502\tvalidation loss: 0.1177\t validation accuracy: 0.9622\n",
      "iteration number: 2722\t training loss: 0.0504\tvalidation loss: 0.1149\t validation accuracy: 0.9644\n",
      "iteration number: 2723\t training loss: 0.0504\tvalidation loss: 0.1164\t validation accuracy: 0.9622\n",
      "iteration number: 2724\t training loss: 0.0511\tvalidation loss: 0.1184\t validation accuracy: 0.9622\n",
      "iteration number: 2725\t training loss: 0.0517\tvalidation loss: 0.1182\t validation accuracy: 0.9600\n",
      "iteration number: 2726\t training loss: 0.0510\tvalidation loss: 0.1160\t validation accuracy: 0.9622\n",
      "iteration number: 2727\t training loss: 0.0502\tvalidation loss: 0.1157\t validation accuracy: 0.9689\n",
      "iteration number: 2728\t training loss: 0.0498\tvalidation loss: 0.1167\t validation accuracy: 0.9644\n",
      "iteration number: 2729\t training loss: 0.0499\tvalidation loss: 0.1162\t validation accuracy: 0.9689\n",
      "iteration number: 2730\t training loss: 0.0495\tvalidation loss: 0.1163\t validation accuracy: 0.9689\n",
      "iteration number: 2731\t training loss: 0.0496\tvalidation loss: 0.1174\t validation accuracy: 0.9689\n",
      "iteration number: 2732\t training loss: 0.0495\tvalidation loss: 0.1152\t validation accuracy: 0.9689\n",
      "iteration number: 2733\t training loss: 0.0496\tvalidation loss: 0.1118\t validation accuracy: 0.9689\n",
      "iteration number: 2734\t training loss: 0.0494\tvalidation loss: 0.1145\t validation accuracy: 0.9689\n",
      "iteration number: 2735\t training loss: 0.0496\tvalidation loss: 0.1137\t validation accuracy: 0.9689\n",
      "iteration number: 2736\t training loss: 0.0494\tvalidation loss: 0.1142\t validation accuracy: 0.9689\n",
      "iteration number: 2737\t training loss: 0.0497\tvalidation loss: 0.1140\t validation accuracy: 0.9667\n",
      "iteration number: 2738\t training loss: 0.0497\tvalidation loss: 0.1137\t validation accuracy: 0.9689\n",
      "iteration number: 2739\t training loss: 0.0494\tvalidation loss: 0.1130\t validation accuracy: 0.9667\n",
      "iteration number: 2740\t training loss: 0.0507\tvalidation loss: 0.1128\t validation accuracy: 0.9689\n",
      "iteration number: 2741\t training loss: 0.0502\tvalidation loss: 0.1119\t validation accuracy: 0.9689\n",
      "iteration number: 2742\t training loss: 0.0504\tvalidation loss: 0.1142\t validation accuracy: 0.9644\n",
      "iteration number: 2743\t training loss: 0.0505\tvalidation loss: 0.1151\t validation accuracy: 0.9644\n",
      "iteration number: 2744\t training loss: 0.0503\tvalidation loss: 0.1130\t validation accuracy: 0.9667\n",
      "iteration number: 2745\t training loss: 0.0496\tvalidation loss: 0.1128\t validation accuracy: 0.9644\n",
      "iteration number: 2746\t training loss: 0.0498\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 2747\t training loss: 0.0496\tvalidation loss: 0.1165\t validation accuracy: 0.9667\n",
      "iteration number: 2748\t training loss: 0.0493\tvalidation loss: 0.1161\t validation accuracy: 0.9644\n",
      "iteration number: 2749\t training loss: 0.0498\tvalidation loss: 0.1151\t validation accuracy: 0.9622\n",
      "iteration number: 2750\t training loss: 0.0493\tvalidation loss: 0.1149\t validation accuracy: 0.9600\n",
      "iteration number: 2751\t training loss: 0.0500\tvalidation loss: 0.1147\t validation accuracy: 0.9644\n",
      "iteration number: 2752\t training loss: 0.0497\tvalidation loss: 0.1156\t validation accuracy: 0.9600\n",
      "iteration number: 2753\t training loss: 0.0508\tvalidation loss: 0.1127\t validation accuracy: 0.9667\n",
      "iteration number: 2754\t training loss: 0.0493\tvalidation loss: 0.1132\t validation accuracy: 0.9667\n",
      "iteration number: 2755\t training loss: 0.0489\tvalidation loss: 0.1146\t validation accuracy: 0.9667\n",
      "iteration number: 2756\t training loss: 0.0489\tvalidation loss: 0.1111\t validation accuracy: 0.9689\n",
      "iteration number: 2757\t training loss: 0.0489\tvalidation loss: 0.1098\t validation accuracy: 0.9689\n",
      "iteration number: 2758\t training loss: 0.0490\tvalidation loss: 0.1086\t validation accuracy: 0.9667\n",
      "iteration number: 2759\t training loss: 0.0493\tvalidation loss: 0.1091\t validation accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2760\t training loss: 0.0492\tvalidation loss: 0.1101\t validation accuracy: 0.9667\n",
      "iteration number: 2761\t training loss: 0.0494\tvalidation loss: 0.1107\t validation accuracy: 0.9711\n",
      "iteration number: 2762\t training loss: 0.0493\tvalidation loss: 0.1107\t validation accuracy: 0.9689\n",
      "iteration number: 2763\t training loss: 0.0489\tvalidation loss: 0.1118\t validation accuracy: 0.9644\n",
      "iteration number: 2764\t training loss: 0.0493\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 2765\t training loss: 0.0487\tvalidation loss: 0.1122\t validation accuracy: 0.9644\n",
      "iteration number: 2766\t training loss: 0.0489\tvalidation loss: 0.1146\t validation accuracy: 0.9622\n",
      "iteration number: 2767\t training loss: 0.0491\tvalidation loss: 0.1175\t validation accuracy: 0.9622\n",
      "iteration number: 2768\t training loss: 0.0492\tvalidation loss: 0.1171\t validation accuracy: 0.9622\n",
      "iteration number: 2769\t training loss: 0.0491\tvalidation loss: 0.1189\t validation accuracy: 0.9622\n",
      "iteration number: 2770\t training loss: 0.0503\tvalidation loss: 0.1178\t validation accuracy: 0.9622\n",
      "iteration number: 2771\t training loss: 0.0511\tvalidation loss: 0.1186\t validation accuracy: 0.9622\n",
      "iteration number: 2772\t training loss: 0.0503\tvalidation loss: 0.1180\t validation accuracy: 0.9600\n",
      "iteration number: 2773\t training loss: 0.0488\tvalidation loss: 0.1163\t validation accuracy: 0.9644\n",
      "iteration number: 2774\t training loss: 0.0484\tvalidation loss: 0.1142\t validation accuracy: 0.9644\n",
      "iteration number: 2775\t training loss: 0.0495\tvalidation loss: 0.1212\t validation accuracy: 0.9622\n",
      "iteration number: 2776\t training loss: 0.0493\tvalidation loss: 0.1207\t validation accuracy: 0.9622\n",
      "iteration number: 2777\t training loss: 0.0491\tvalidation loss: 0.1184\t validation accuracy: 0.9622\n",
      "iteration number: 2778\t training loss: 0.0489\tvalidation loss: 0.1170\t validation accuracy: 0.9622\n",
      "iteration number: 2779\t training loss: 0.0490\tvalidation loss: 0.1171\t validation accuracy: 0.9622\n",
      "iteration number: 2780\t training loss: 0.0488\tvalidation loss: 0.1176\t validation accuracy: 0.9622\n",
      "iteration number: 2781\t training loss: 0.0486\tvalidation loss: 0.1175\t validation accuracy: 0.9622\n",
      "iteration number: 2782\t training loss: 0.0487\tvalidation loss: 0.1187\t validation accuracy: 0.9622\n",
      "iteration number: 2783\t training loss: 0.0482\tvalidation loss: 0.1150\t validation accuracy: 0.9644\n",
      "iteration number: 2784\t training loss: 0.0481\tvalidation loss: 0.1135\t validation accuracy: 0.9667\n",
      "iteration number: 2785\t training loss: 0.0485\tvalidation loss: 0.1144\t validation accuracy: 0.9622\n",
      "iteration number: 2786\t training loss: 0.0484\tvalidation loss: 0.1163\t validation accuracy: 0.9644\n",
      "iteration number: 2787\t training loss: 0.0487\tvalidation loss: 0.1169\t validation accuracy: 0.9644\n",
      "iteration number: 2788\t training loss: 0.0487\tvalidation loss: 0.1162\t validation accuracy: 0.9644\n",
      "iteration number: 2789\t training loss: 0.0486\tvalidation loss: 0.1151\t validation accuracy: 0.9667\n",
      "iteration number: 2790\t training loss: 0.0483\tvalidation loss: 0.1148\t validation accuracy: 0.9667\n",
      "iteration number: 2791\t training loss: 0.0487\tvalidation loss: 0.1144\t validation accuracy: 0.9644\n",
      "iteration number: 2792\t training loss: 0.0492\tvalidation loss: 0.1204\t validation accuracy: 0.9644\n",
      "iteration number: 2793\t training loss: 0.0489\tvalidation loss: 0.1192\t validation accuracy: 0.9644\n",
      "iteration number: 2794\t training loss: 0.0480\tvalidation loss: 0.1170\t validation accuracy: 0.9667\n",
      "iteration number: 2795\t training loss: 0.0482\tvalidation loss: 0.1171\t validation accuracy: 0.9600\n",
      "iteration number: 2796\t training loss: 0.0484\tvalidation loss: 0.1164\t validation accuracy: 0.9600\n",
      "iteration number: 2797\t training loss: 0.0485\tvalidation loss: 0.1170\t validation accuracy: 0.9622\n",
      "iteration number: 2798\t training loss: 0.0487\tvalidation loss: 0.1142\t validation accuracy: 0.9622\n",
      "iteration number: 2799\t training loss: 0.0492\tvalidation loss: 0.1152\t validation accuracy: 0.9622\n",
      "iteration number: 2800\t training loss: 0.0497\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 2801\t training loss: 0.0498\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 2802\t training loss: 0.0486\tvalidation loss: 0.1126\t validation accuracy: 0.9644\n",
      "iteration number: 2803\t training loss: 0.0487\tvalidation loss: 0.1125\t validation accuracy: 0.9667\n",
      "iteration number: 2804\t training loss: 0.0490\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 2805\t training loss: 0.0497\tvalidation loss: 0.1157\t validation accuracy: 0.9622\n",
      "iteration number: 2806\t training loss: 0.0496\tvalidation loss: 0.1170\t validation accuracy: 0.9600\n",
      "iteration number: 2807\t training loss: 0.0490\tvalidation loss: 0.1159\t validation accuracy: 0.9622\n",
      "iteration number: 2808\t training loss: 0.0491\tvalidation loss: 0.1160\t validation accuracy: 0.9622\n",
      "iteration number: 2809\t training loss: 0.0495\tvalidation loss: 0.1172\t validation accuracy: 0.9600\n",
      "iteration number: 2810\t training loss: 0.0506\tvalidation loss: 0.1137\t validation accuracy: 0.9667\n",
      "iteration number: 2811\t training loss: 0.0500\tvalidation loss: 0.1127\t validation accuracy: 0.9667\n",
      "iteration number: 2812\t training loss: 0.0494\tvalidation loss: 0.1128\t validation accuracy: 0.9667\n",
      "iteration number: 2813\t training loss: 0.0486\tvalidation loss: 0.1128\t validation accuracy: 0.9689\n",
      "iteration number: 2814\t training loss: 0.0482\tvalidation loss: 0.1116\t validation accuracy: 0.9689\n",
      "iteration number: 2815\t training loss: 0.0483\tvalidation loss: 0.1109\t validation accuracy: 0.9711\n",
      "iteration number: 2816\t training loss: 0.0477\tvalidation loss: 0.1135\t validation accuracy: 0.9689\n",
      "iteration number: 2817\t training loss: 0.0479\tvalidation loss: 0.1129\t validation accuracy: 0.9733\n",
      "iteration number: 2818\t training loss: 0.0492\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 2819\t training loss: 0.0485\tvalidation loss: 0.1133\t validation accuracy: 0.9689\n",
      "iteration number: 2820\t training loss: 0.0483\tvalidation loss: 0.1129\t validation accuracy: 0.9711\n",
      "iteration number: 2821\t training loss: 0.0480\tvalidation loss: 0.1120\t validation accuracy: 0.9667\n",
      "iteration number: 2822\t training loss: 0.0479\tvalidation loss: 0.1121\t validation accuracy: 0.9689\n",
      "iteration number: 2823\t training loss: 0.0484\tvalidation loss: 0.1132\t validation accuracy: 0.9622\n",
      "iteration number: 2824\t training loss: 0.0478\tvalidation loss: 0.1136\t validation accuracy: 0.9622\n",
      "iteration number: 2825\t training loss: 0.0481\tvalidation loss: 0.1160\t validation accuracy: 0.9644\n",
      "iteration number: 2826\t training loss: 0.0483\tvalidation loss: 0.1186\t validation accuracy: 0.9622\n",
      "iteration number: 2827\t training loss: 0.0476\tvalidation loss: 0.1150\t validation accuracy: 0.9644\n",
      "iteration number: 2828\t training loss: 0.0482\tvalidation loss: 0.1153\t validation accuracy: 0.9667\n",
      "iteration number: 2829\t training loss: 0.0479\tvalidation loss: 0.1144\t validation accuracy: 0.9644\n",
      "iteration number: 2830\t training loss: 0.0481\tvalidation loss: 0.1125\t validation accuracy: 0.9689\n",
      "iteration number: 2831\t training loss: 0.0479\tvalidation loss: 0.1083\t validation accuracy: 0.9711\n",
      "iteration number: 2832\t training loss: 0.0481\tvalidation loss: 0.1070\t validation accuracy: 0.9756\n",
      "iteration number: 2833\t training loss: 0.0474\tvalidation loss: 0.1097\t validation accuracy: 0.9667\n",
      "iteration number: 2834\t training loss: 0.0474\tvalidation loss: 0.1112\t validation accuracy: 0.9667\n",
      "iteration number: 2835\t training loss: 0.0475\tvalidation loss: 0.1114\t validation accuracy: 0.9667\n",
      "iteration number: 2836\t training loss: 0.0471\tvalidation loss: 0.1121\t validation accuracy: 0.9667\n",
      "iteration number: 2837\t training loss: 0.0475\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 2838\t training loss: 0.0476\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 2839\t training loss: 0.0482\tvalidation loss: 0.1092\t validation accuracy: 0.9667\n",
      "iteration number: 2840\t training loss: 0.0481\tvalidation loss: 0.1093\t validation accuracy: 0.9667\n",
      "iteration number: 2841\t training loss: 0.0474\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 2842\t training loss: 0.0473\tvalidation loss: 0.1109\t validation accuracy: 0.9667\n",
      "iteration number: 2843\t training loss: 0.0471\tvalidation loss: 0.1121\t validation accuracy: 0.9622\n",
      "iteration number: 2844\t training loss: 0.0472\tvalidation loss: 0.1129\t validation accuracy: 0.9644\n",
      "iteration number: 2845\t training loss: 0.0476\tvalidation loss: 0.1176\t validation accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2846\t training loss: 0.0472\tvalidation loss: 0.1140\t validation accuracy: 0.9622\n",
      "iteration number: 2847\t training loss: 0.0474\tvalidation loss: 0.1149\t validation accuracy: 0.9644\n",
      "iteration number: 2848\t training loss: 0.0477\tvalidation loss: 0.1159\t validation accuracy: 0.9622\n",
      "iteration number: 2849\t training loss: 0.0485\tvalidation loss: 0.1182\t validation accuracy: 0.9644\n",
      "iteration number: 2850\t training loss: 0.0480\tvalidation loss: 0.1170\t validation accuracy: 0.9667\n",
      "iteration number: 2851\t training loss: 0.0482\tvalidation loss: 0.1174\t validation accuracy: 0.9644\n",
      "iteration number: 2852\t training loss: 0.0472\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 2853\t training loss: 0.0470\tvalidation loss: 0.1143\t validation accuracy: 0.9644\n",
      "iteration number: 2854\t training loss: 0.0478\tvalidation loss: 0.1167\t validation accuracy: 0.9622\n",
      "iteration number: 2855\t training loss: 0.0482\tvalidation loss: 0.1179\t validation accuracy: 0.9622\n",
      "iteration number: 2856\t training loss: 0.0487\tvalidation loss: 0.1153\t validation accuracy: 0.9667\n",
      "iteration number: 2857\t training loss: 0.0483\tvalidation loss: 0.1153\t validation accuracy: 0.9644\n",
      "iteration number: 2858\t training loss: 0.0480\tvalidation loss: 0.1160\t validation accuracy: 0.9622\n",
      "iteration number: 2859\t training loss: 0.0478\tvalidation loss: 0.1141\t validation accuracy: 0.9689\n",
      "iteration number: 2860\t training loss: 0.0482\tvalidation loss: 0.1144\t validation accuracy: 0.9667\n",
      "iteration number: 2861\t training loss: 0.0482\tvalidation loss: 0.1138\t validation accuracy: 0.9667\n",
      "iteration number: 2862\t training loss: 0.0479\tvalidation loss: 0.1158\t validation accuracy: 0.9622\n",
      "iteration number: 2863\t training loss: 0.0478\tvalidation loss: 0.1181\t validation accuracy: 0.9622\n",
      "iteration number: 2864\t training loss: 0.0484\tvalidation loss: 0.1165\t validation accuracy: 0.9622\n",
      "iteration number: 2865\t training loss: 0.0485\tvalidation loss: 0.1170\t validation accuracy: 0.9644\n",
      "iteration number: 2866\t training loss: 0.0485\tvalidation loss: 0.1176\t validation accuracy: 0.9667\n",
      "iteration number: 2867\t training loss: 0.0484\tvalidation loss: 0.1157\t validation accuracy: 0.9667\n",
      "iteration number: 2868\t training loss: 0.0482\tvalidation loss: 0.1170\t validation accuracy: 0.9644\n",
      "iteration number: 2869\t training loss: 0.0480\tvalidation loss: 0.1158\t validation accuracy: 0.9689\n",
      "iteration number: 2870\t training loss: 0.0479\tvalidation loss: 0.1187\t validation accuracy: 0.9644\n",
      "iteration number: 2871\t training loss: 0.0490\tvalidation loss: 0.1232\t validation accuracy: 0.9622\n",
      "iteration number: 2872\t training loss: 0.0490\tvalidation loss: 0.1259\t validation accuracy: 0.9622\n",
      "iteration number: 2873\t training loss: 0.0479\tvalidation loss: 0.1230\t validation accuracy: 0.9644\n",
      "iteration number: 2874\t training loss: 0.0469\tvalidation loss: 0.1184\t validation accuracy: 0.9622\n",
      "iteration number: 2875\t training loss: 0.0466\tvalidation loss: 0.1160\t validation accuracy: 0.9622\n",
      "iteration number: 2876\t training loss: 0.0465\tvalidation loss: 0.1159\t validation accuracy: 0.9622\n",
      "iteration number: 2877\t training loss: 0.0466\tvalidation loss: 0.1153\t validation accuracy: 0.9622\n",
      "iteration number: 2878\t training loss: 0.0462\tvalidation loss: 0.1126\t validation accuracy: 0.9644\n",
      "iteration number: 2879\t training loss: 0.0465\tvalidation loss: 0.1152\t validation accuracy: 0.9644\n",
      "iteration number: 2880\t training loss: 0.0462\tvalidation loss: 0.1126\t validation accuracy: 0.9644\n",
      "iteration number: 2881\t training loss: 0.0466\tvalidation loss: 0.1126\t validation accuracy: 0.9622\n",
      "iteration number: 2882\t training loss: 0.0464\tvalidation loss: 0.1112\t validation accuracy: 0.9644\n",
      "iteration number: 2883\t training loss: 0.0466\tvalidation loss: 0.1105\t validation accuracy: 0.9667\n",
      "iteration number: 2884\t training loss: 0.0468\tvalidation loss: 0.1120\t validation accuracy: 0.9667\n",
      "iteration number: 2885\t training loss: 0.0469\tvalidation loss: 0.1126\t validation accuracy: 0.9667\n",
      "iteration number: 2886\t training loss: 0.0475\tvalidation loss: 0.1161\t validation accuracy: 0.9622\n",
      "iteration number: 2887\t training loss: 0.0477\tvalidation loss: 0.1164\t validation accuracy: 0.9622\n",
      "iteration number: 2888\t training loss: 0.0475\tvalidation loss: 0.1137\t validation accuracy: 0.9667\n",
      "iteration number: 2889\t training loss: 0.0475\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 2890\t training loss: 0.0469\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 2891\t training loss: 0.0469\tvalidation loss: 0.1167\t validation accuracy: 0.9644\n",
      "iteration number: 2892\t training loss: 0.0465\tvalidation loss: 0.1154\t validation accuracy: 0.9644\n",
      "iteration number: 2893\t training loss: 0.0462\tvalidation loss: 0.1117\t validation accuracy: 0.9667\n",
      "iteration number: 2894\t training loss: 0.0463\tvalidation loss: 0.1107\t validation accuracy: 0.9667\n",
      "iteration number: 2895\t training loss: 0.0461\tvalidation loss: 0.1128\t validation accuracy: 0.9644\n",
      "iteration number: 2896\t training loss: 0.0462\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 2897\t training loss: 0.0466\tvalidation loss: 0.1130\t validation accuracy: 0.9644\n",
      "iteration number: 2898\t training loss: 0.0464\tvalidation loss: 0.1106\t validation accuracy: 0.9644\n",
      "iteration number: 2899\t training loss: 0.0460\tvalidation loss: 0.1120\t validation accuracy: 0.9644\n",
      "iteration number: 2900\t training loss: 0.0463\tvalidation loss: 0.1116\t validation accuracy: 0.9622\n",
      "iteration number: 2901\t training loss: 0.0464\tvalidation loss: 0.1110\t validation accuracy: 0.9622\n",
      "iteration number: 2902\t training loss: 0.0465\tvalidation loss: 0.1102\t validation accuracy: 0.9622\n",
      "iteration number: 2903\t training loss: 0.0467\tvalidation loss: 0.1112\t validation accuracy: 0.9600\n",
      "iteration number: 2904\t training loss: 0.0465\tvalidation loss: 0.1101\t validation accuracy: 0.9600\n",
      "iteration number: 2905\t training loss: 0.0465\tvalidation loss: 0.1097\t validation accuracy: 0.9600\n",
      "iteration number: 2906\t training loss: 0.0463\tvalidation loss: 0.1105\t validation accuracy: 0.9644\n",
      "iteration number: 2907\t training loss: 0.0461\tvalidation loss: 0.1115\t validation accuracy: 0.9622\n",
      "iteration number: 2908\t training loss: 0.0469\tvalidation loss: 0.1091\t validation accuracy: 0.9644\n",
      "iteration number: 2909\t training loss: 0.0462\tvalidation loss: 0.1091\t validation accuracy: 0.9667\n",
      "iteration number: 2910\t training loss: 0.0467\tvalidation loss: 0.1087\t validation accuracy: 0.9667\n",
      "iteration number: 2911\t training loss: 0.0473\tvalidation loss: 0.1085\t validation accuracy: 0.9689\n",
      "iteration number: 2912\t training loss: 0.0464\tvalidation loss: 0.1101\t validation accuracy: 0.9622\n",
      "iteration number: 2913\t training loss: 0.0473\tvalidation loss: 0.1105\t validation accuracy: 0.9644\n",
      "iteration number: 2914\t training loss: 0.0470\tvalidation loss: 0.1111\t validation accuracy: 0.9644\n",
      "iteration number: 2915\t training loss: 0.0471\tvalidation loss: 0.1096\t validation accuracy: 0.9667\n",
      "iteration number: 2916\t training loss: 0.0465\tvalidation loss: 0.1085\t validation accuracy: 0.9644\n",
      "iteration number: 2917\t training loss: 0.0457\tvalidation loss: 0.1113\t validation accuracy: 0.9644\n",
      "iteration number: 2918\t training loss: 0.0458\tvalidation loss: 0.1118\t validation accuracy: 0.9622\n",
      "iteration number: 2919\t training loss: 0.0462\tvalidation loss: 0.1103\t validation accuracy: 0.9644\n",
      "iteration number: 2920\t training loss: 0.0457\tvalidation loss: 0.1099\t validation accuracy: 0.9667\n",
      "iteration number: 2921\t training loss: 0.0458\tvalidation loss: 0.1114\t validation accuracy: 0.9644\n",
      "iteration number: 2922\t training loss: 0.0463\tvalidation loss: 0.1106\t validation accuracy: 0.9689\n",
      "iteration number: 2923\t training loss: 0.0466\tvalidation loss: 0.1079\t validation accuracy: 0.9689\n",
      "iteration number: 2924\t training loss: 0.0471\tvalidation loss: 0.1074\t validation accuracy: 0.9711\n",
      "iteration number: 2925\t training loss: 0.0474\tvalidation loss: 0.1058\t validation accuracy: 0.9711\n",
      "iteration number: 2926\t training loss: 0.0472\tvalidation loss: 0.1066\t validation accuracy: 0.9711\n",
      "iteration number: 2927\t training loss: 0.0469\tvalidation loss: 0.1067\t validation accuracy: 0.9689\n",
      "iteration number: 2928\t training loss: 0.0466\tvalidation loss: 0.1071\t validation accuracy: 0.9667\n",
      "iteration number: 2929\t training loss: 0.0468\tvalidation loss: 0.1076\t validation accuracy: 0.9689\n",
      "iteration number: 2930\t training loss: 0.0465\tvalidation loss: 0.1065\t validation accuracy: 0.9689\n",
      "iteration number: 2931\t training loss: 0.0464\tvalidation loss: 0.1075\t validation accuracy: 0.9711\n",
      "iteration number: 2932\t training loss: 0.0460\tvalidation loss: 0.1100\t validation accuracy: 0.9667\n",
      "iteration number: 2933\t training loss: 0.0460\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 2934\t training loss: 0.0461\tvalidation loss: 0.1136\t validation accuracy: 0.9600\n",
      "iteration number: 2935\t training loss: 0.0469\tvalidation loss: 0.1147\t validation accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2936\t training loss: 0.0467\tvalidation loss: 0.1132\t validation accuracy: 0.9622\n",
      "iteration number: 2937\t training loss: 0.0461\tvalidation loss: 0.1129\t validation accuracy: 0.9622\n",
      "iteration number: 2938\t training loss: 0.0459\tvalidation loss: 0.1100\t validation accuracy: 0.9644\n",
      "iteration number: 2939\t training loss: 0.0468\tvalidation loss: 0.1107\t validation accuracy: 0.9622\n",
      "iteration number: 2940\t training loss: 0.0458\tvalidation loss: 0.1116\t validation accuracy: 0.9622\n",
      "iteration number: 2941\t training loss: 0.0460\tvalidation loss: 0.1122\t validation accuracy: 0.9622\n",
      "iteration number: 2942\t training loss: 0.0455\tvalidation loss: 0.1135\t validation accuracy: 0.9622\n",
      "iteration number: 2943\t training loss: 0.0456\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n",
      "iteration number: 2944\t training loss: 0.0455\tvalidation loss: 0.1128\t validation accuracy: 0.9600\n",
      "iteration number: 2945\t training loss: 0.0455\tvalidation loss: 0.1130\t validation accuracy: 0.9600\n",
      "iteration number: 2946\t training loss: 0.0462\tvalidation loss: 0.1144\t validation accuracy: 0.9622\n",
      "iteration number: 2947\t training loss: 0.0456\tvalidation loss: 0.1121\t validation accuracy: 0.9622\n",
      "iteration number: 2948\t training loss: 0.0452\tvalidation loss: 0.1119\t validation accuracy: 0.9644\n",
      "iteration number: 2949\t training loss: 0.0451\tvalidation loss: 0.1117\t validation accuracy: 0.9644\n",
      "iteration number: 2950\t training loss: 0.0451\tvalidation loss: 0.1108\t validation accuracy: 0.9644\n",
      "iteration number: 2951\t training loss: 0.0452\tvalidation loss: 0.1109\t validation accuracy: 0.9644\n",
      "iteration number: 2952\t training loss: 0.0452\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n",
      "iteration number: 2953\t training loss: 0.0454\tvalidation loss: 0.1143\t validation accuracy: 0.9622\n",
      "iteration number: 2954\t training loss: 0.0461\tvalidation loss: 0.1169\t validation accuracy: 0.9667\n",
      "iteration number: 2955\t training loss: 0.0455\tvalidation loss: 0.1131\t validation accuracy: 0.9622\n",
      "iteration number: 2956\t training loss: 0.0453\tvalidation loss: 0.1109\t validation accuracy: 0.9644\n",
      "iteration number: 2957\t training loss: 0.0451\tvalidation loss: 0.1126\t validation accuracy: 0.9644\n",
      "iteration number: 2958\t training loss: 0.0450\tvalidation loss: 0.1119\t validation accuracy: 0.9644\n",
      "iteration number: 2959\t training loss: 0.0452\tvalidation loss: 0.1104\t validation accuracy: 0.9644\n",
      "iteration number: 2960\t training loss: 0.0454\tvalidation loss: 0.1102\t validation accuracy: 0.9622\n",
      "iteration number: 2961\t training loss: 0.0453\tvalidation loss: 0.1104\t validation accuracy: 0.9667\n",
      "iteration number: 2962\t training loss: 0.0452\tvalidation loss: 0.1093\t validation accuracy: 0.9689\n",
      "iteration number: 2963\t training loss: 0.0450\tvalidation loss: 0.1097\t validation accuracy: 0.9644\n",
      "iteration number: 2964\t training loss: 0.0449\tvalidation loss: 0.1096\t validation accuracy: 0.9644\n",
      "iteration number: 2965\t training loss: 0.0454\tvalidation loss: 0.1095\t validation accuracy: 0.9667\n",
      "iteration number: 2966\t training loss: 0.0452\tvalidation loss: 0.1098\t validation accuracy: 0.9667\n",
      "iteration number: 2967\t training loss: 0.0448\tvalidation loss: 0.1115\t validation accuracy: 0.9644\n",
      "iteration number: 2968\t training loss: 0.0451\tvalidation loss: 0.1118\t validation accuracy: 0.9644\n",
      "iteration number: 2969\t training loss: 0.0453\tvalidation loss: 0.1118\t validation accuracy: 0.9644\n",
      "iteration number: 2970\t training loss: 0.0451\tvalidation loss: 0.1113\t validation accuracy: 0.9644\n",
      "iteration number: 2971\t training loss: 0.0452\tvalidation loss: 0.1098\t validation accuracy: 0.9667\n",
      "iteration number: 2972\t training loss: 0.0451\tvalidation loss: 0.1104\t validation accuracy: 0.9667\n",
      "iteration number: 2973\t training loss: 0.0453\tvalidation loss: 0.1098\t validation accuracy: 0.9644\n",
      "iteration number: 2974\t training loss: 0.0447\tvalidation loss: 0.1105\t validation accuracy: 0.9622\n",
      "iteration number: 2975\t training loss: 0.0447\tvalidation loss: 0.1115\t validation accuracy: 0.9622\n",
      "iteration number: 2976\t training loss: 0.0445\tvalidation loss: 0.1114\t validation accuracy: 0.9622\n",
      "iteration number: 2977\t training loss: 0.0448\tvalidation loss: 0.1124\t validation accuracy: 0.9622\n",
      "iteration number: 2978\t training loss: 0.0447\tvalidation loss: 0.1124\t validation accuracy: 0.9622\n",
      "iteration number: 2979\t training loss: 0.0477\tvalidation loss: 0.1122\t validation accuracy: 0.9622\n",
      "iteration number: 2980\t training loss: 0.0461\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 2981\t training loss: 0.0459\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 2982\t training loss: 0.0460\tvalidation loss: 0.1129\t validation accuracy: 0.9622\n",
      "iteration number: 2983\t training loss: 0.0465\tvalidation loss: 0.1115\t validation accuracy: 0.9667\n",
      "iteration number: 2984\t training loss: 0.0467\tvalidation loss: 0.1104\t validation accuracy: 0.9667\n",
      "iteration number: 2985\t training loss: 0.0467\tvalidation loss: 0.1105\t validation accuracy: 0.9667\n",
      "iteration number: 2986\t training loss: 0.0456\tvalidation loss: 0.1122\t validation accuracy: 0.9644\n",
      "iteration number: 2987\t training loss: 0.0456\tvalidation loss: 0.1158\t validation accuracy: 0.9622\n",
      "iteration number: 2988\t training loss: 0.0448\tvalidation loss: 0.1123\t validation accuracy: 0.9622\n",
      "iteration number: 2989\t training loss: 0.0453\tvalidation loss: 0.1166\t validation accuracy: 0.9622\n",
      "iteration number: 2990\t training loss: 0.0451\tvalidation loss: 0.1144\t validation accuracy: 0.9622\n",
      "iteration number: 2991\t training loss: 0.0450\tvalidation loss: 0.1129\t validation accuracy: 0.9644\n",
      "iteration number: 2992\t training loss: 0.0454\tvalidation loss: 0.1167\t validation accuracy: 0.9622\n",
      "iteration number: 2993\t training loss: 0.0460\tvalidation loss: 0.1154\t validation accuracy: 0.9667\n",
      "iteration number: 2994\t training loss: 0.0460\tvalidation loss: 0.1160\t validation accuracy: 0.9667\n",
      "iteration number: 2995\t training loss: 0.0467\tvalidation loss: 0.1202\t validation accuracy: 0.9622\n",
      "iteration number: 2996\t training loss: 0.0472\tvalidation loss: 0.1224\t validation accuracy: 0.9622\n",
      "iteration number: 2997\t training loss: 0.0469\tvalidation loss: 0.1213\t validation accuracy: 0.9622\n",
      "iteration number: 2998\t training loss: 0.0468\tvalidation loss: 0.1203\t validation accuracy: 0.9622\n",
      "iteration number: 2999\t training loss: 0.0463\tvalidation loss: 0.1153\t validation accuracy: 0.9644\n",
      "iteration number: 3000\t training loss: 0.0465\tvalidation loss: 0.1159\t validation accuracy: 0.9644\n",
      "iteration number: 3001\t training loss: 0.0457\tvalidation loss: 0.1145\t validation accuracy: 0.9622\n",
      "iteration number: 3002\t training loss: 0.0470\tvalidation loss: 0.1159\t validation accuracy: 0.9644\n",
      "iteration number: 3003\t training loss: 0.0448\tvalidation loss: 0.1144\t validation accuracy: 0.9622\n",
      "iteration number: 3004\t training loss: 0.0448\tvalidation loss: 0.1131\t validation accuracy: 0.9622\n",
      "iteration number: 3005\t training loss: 0.0443\tvalidation loss: 0.1090\t validation accuracy: 0.9667\n",
      "iteration number: 3006\t training loss: 0.0455\tvalidation loss: 0.1069\t validation accuracy: 0.9667\n",
      "iteration number: 3007\t training loss: 0.0443\tvalidation loss: 0.1076\t validation accuracy: 0.9644\n",
      "iteration number: 3008\t training loss: 0.0445\tvalidation loss: 0.1067\t validation accuracy: 0.9667\n",
      "iteration number: 3009\t training loss: 0.0447\tvalidation loss: 0.1060\t validation accuracy: 0.9689\n",
      "iteration number: 3010\t training loss: 0.0451\tvalidation loss: 0.1064\t validation accuracy: 0.9733\n",
      "iteration number: 3011\t training loss: 0.0446\tvalidation loss: 0.1071\t validation accuracy: 0.9711\n",
      "iteration number: 3012\t training loss: 0.0446\tvalidation loss: 0.1097\t validation accuracy: 0.9644\n",
      "iteration number: 3013\t training loss: 0.0449\tvalidation loss: 0.1087\t validation accuracy: 0.9622\n",
      "iteration number: 3014\t training loss: 0.0452\tvalidation loss: 0.1087\t validation accuracy: 0.9622\n",
      "iteration number: 3015\t training loss: 0.0452\tvalidation loss: 0.1054\t validation accuracy: 0.9711\n",
      "iteration number: 3016\t training loss: 0.0444\tvalidation loss: 0.1076\t validation accuracy: 0.9667\n",
      "iteration number: 3017\t training loss: 0.0442\tvalidation loss: 0.1087\t validation accuracy: 0.9644\n",
      "iteration number: 3018\t training loss: 0.0442\tvalidation loss: 0.1090\t validation accuracy: 0.9644\n",
      "iteration number: 3019\t training loss: 0.0437\tvalidation loss: 0.1112\t validation accuracy: 0.9644\n",
      "iteration number: 3020\t training loss: 0.0440\tvalidation loss: 0.1104\t validation accuracy: 0.9644\n",
      "iteration number: 3021\t training loss: 0.0443\tvalidation loss: 0.1144\t validation accuracy: 0.9622\n",
      "iteration number: 3022\t training loss: 0.0436\tvalidation loss: 0.1125\t validation accuracy: 0.9644\n",
      "iteration number: 3023\t training loss: 0.0437\tvalidation loss: 0.1124\t validation accuracy: 0.9622\n",
      "iteration number: 3024\t training loss: 0.0439\tvalidation loss: 0.1135\t validation accuracy: 0.9622\n",
      "iteration number: 3025\t training loss: 0.0447\tvalidation loss: 0.1142\t validation accuracy: 0.9644\n",
      "iteration number: 3026\t training loss: 0.0450\tvalidation loss: 0.1153\t validation accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3027\t training loss: 0.0456\tvalidation loss: 0.1177\t validation accuracy: 0.9644\n",
      "iteration number: 3028\t training loss: 0.0456\tvalidation loss: 0.1210\t validation accuracy: 0.9644\n",
      "iteration number: 3029\t training loss: 0.0454\tvalidation loss: 0.1222\t validation accuracy: 0.9622\n",
      "iteration number: 3030\t training loss: 0.0452\tvalidation loss: 0.1192\t validation accuracy: 0.9644\n",
      "iteration number: 3031\t training loss: 0.0437\tvalidation loss: 0.1133\t validation accuracy: 0.9622\n",
      "iteration number: 3032\t training loss: 0.0437\tvalidation loss: 0.1133\t validation accuracy: 0.9667\n",
      "iteration number: 3033\t training loss: 0.0437\tvalidation loss: 0.1140\t validation accuracy: 0.9644\n",
      "iteration number: 3034\t training loss: 0.0445\tvalidation loss: 0.1132\t validation accuracy: 0.9600\n",
      "iteration number: 3035\t training loss: 0.0447\tvalidation loss: 0.1166\t validation accuracy: 0.9578\n",
      "iteration number: 3036\t training loss: 0.0444\tvalidation loss: 0.1177\t validation accuracy: 0.9622\n",
      "iteration number: 3037\t training loss: 0.0440\tvalidation loss: 0.1155\t validation accuracy: 0.9622\n",
      "iteration number: 3038\t training loss: 0.0442\tvalidation loss: 0.1147\t validation accuracy: 0.9622\n",
      "iteration number: 3039\t training loss: 0.0440\tvalidation loss: 0.1150\t validation accuracy: 0.9600\n",
      "iteration number: 3040\t training loss: 0.0437\tvalidation loss: 0.1131\t validation accuracy: 0.9622\n",
      "iteration number: 3041\t training loss: 0.0439\tvalidation loss: 0.1128\t validation accuracy: 0.9644\n",
      "iteration number: 3042\t training loss: 0.0443\tvalidation loss: 0.1114\t validation accuracy: 0.9622\n",
      "iteration number: 3043\t training loss: 0.0439\tvalidation loss: 0.1143\t validation accuracy: 0.9600\n",
      "iteration number: 3044\t training loss: 0.0438\tvalidation loss: 0.1136\t validation accuracy: 0.9644\n",
      "iteration number: 3045\t training loss: 0.0440\tvalidation loss: 0.1145\t validation accuracy: 0.9622\n",
      "iteration number: 3046\t training loss: 0.0437\tvalidation loss: 0.1111\t validation accuracy: 0.9622\n",
      "iteration number: 3047\t training loss: 0.0440\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n",
      "iteration number: 3048\t training loss: 0.0436\tvalidation loss: 0.1126\t validation accuracy: 0.9622\n",
      "iteration number: 3049\t training loss: 0.0443\tvalidation loss: 0.1132\t validation accuracy: 0.9622\n",
      "iteration number: 3050\t training loss: 0.0439\tvalidation loss: 0.1114\t validation accuracy: 0.9622\n",
      "iteration number: 3051\t training loss: 0.0438\tvalidation loss: 0.1145\t validation accuracy: 0.9600\n",
      "iteration number: 3052\t training loss: 0.0439\tvalidation loss: 0.1145\t validation accuracy: 0.9644\n",
      "iteration number: 3053\t training loss: 0.0436\tvalidation loss: 0.1128\t validation accuracy: 0.9667\n",
      "iteration number: 3054\t training loss: 0.0445\tvalidation loss: 0.1132\t validation accuracy: 0.9622\n",
      "iteration number: 3055\t training loss: 0.0444\tvalidation loss: 0.1136\t validation accuracy: 0.9600\n",
      "iteration number: 3056\t training loss: 0.0446\tvalidation loss: 0.1127\t validation accuracy: 0.9622\n",
      "iteration number: 3057\t training loss: 0.0441\tvalidation loss: 0.1131\t validation accuracy: 0.9622\n",
      "iteration number: 3058\t training loss: 0.0432\tvalidation loss: 0.1123\t validation accuracy: 0.9644\n",
      "iteration number: 3059\t training loss: 0.0431\tvalidation loss: 0.1094\t validation accuracy: 0.9667\n",
      "iteration number: 3060\t training loss: 0.0430\tvalidation loss: 0.1101\t validation accuracy: 0.9644\n",
      "iteration number: 3061\t training loss: 0.0437\tvalidation loss: 0.1151\t validation accuracy: 0.9622\n",
      "iteration number: 3062\t training loss: 0.0432\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 3063\t training loss: 0.0437\tvalidation loss: 0.1159\t validation accuracy: 0.9644\n",
      "iteration number: 3064\t training loss: 0.0431\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 3065\t training loss: 0.0432\tvalidation loss: 0.1096\t validation accuracy: 0.9644\n",
      "iteration number: 3066\t training loss: 0.0431\tvalidation loss: 0.1116\t validation accuracy: 0.9622\n",
      "iteration number: 3067\t training loss: 0.0434\tvalidation loss: 0.1125\t validation accuracy: 0.9622\n",
      "iteration number: 3068\t training loss: 0.0445\tvalidation loss: 0.1142\t validation accuracy: 0.9644\n",
      "iteration number: 3069\t training loss: 0.0449\tvalidation loss: 0.1141\t validation accuracy: 0.9622\n",
      "iteration number: 3070\t training loss: 0.0441\tvalidation loss: 0.1100\t validation accuracy: 0.9644\n",
      "iteration number: 3071\t training loss: 0.0437\tvalidation loss: 0.1079\t validation accuracy: 0.9667\n",
      "iteration number: 3072\t training loss: 0.0436\tvalidation loss: 0.1088\t validation accuracy: 0.9644\n",
      "iteration number: 3073\t training loss: 0.0433\tvalidation loss: 0.1089\t validation accuracy: 0.9667\n",
      "iteration number: 3074\t training loss: 0.0432\tvalidation loss: 0.1088\t validation accuracy: 0.9689\n",
      "iteration number: 3075\t training loss: 0.0436\tvalidation loss: 0.1063\t validation accuracy: 0.9711\n",
      "iteration number: 3076\t training loss: 0.0438\tvalidation loss: 0.1050\t validation accuracy: 0.9689\n",
      "iteration number: 3077\t training loss: 0.0433\tvalidation loss: 0.1055\t validation accuracy: 0.9689\n",
      "iteration number: 3078\t training loss: 0.0433\tvalidation loss: 0.1067\t validation accuracy: 0.9689\n",
      "iteration number: 3079\t training loss: 0.0436\tvalidation loss: 0.1057\t validation accuracy: 0.9689\n",
      "iteration number: 3080\t training loss: 0.0439\tvalidation loss: 0.1061\t validation accuracy: 0.9689\n",
      "iteration number: 3081\t training loss: 0.0447\tvalidation loss: 0.1067\t validation accuracy: 0.9689\n",
      "iteration number: 3082\t training loss: 0.0447\tvalidation loss: 0.1042\t validation accuracy: 0.9711\n",
      "iteration number: 3083\t training loss: 0.0440\tvalidation loss: 0.1059\t validation accuracy: 0.9711\n",
      "iteration number: 3084\t training loss: 0.0439\tvalidation loss: 0.1068\t validation accuracy: 0.9689\n",
      "iteration number: 3085\t training loss: 0.0437\tvalidation loss: 0.1091\t validation accuracy: 0.9689\n",
      "iteration number: 3086\t training loss: 0.0438\tvalidation loss: 0.1068\t validation accuracy: 0.9711\n",
      "iteration number: 3087\t training loss: 0.0434\tvalidation loss: 0.1078\t validation accuracy: 0.9689\n",
      "iteration number: 3088\t training loss: 0.0429\tvalidation loss: 0.1072\t validation accuracy: 0.9667\n",
      "iteration number: 3089\t training loss: 0.0425\tvalidation loss: 0.1097\t validation accuracy: 0.9644\n",
      "iteration number: 3090\t training loss: 0.0425\tvalidation loss: 0.1102\t validation accuracy: 0.9644\n",
      "iteration number: 3091\t training loss: 0.0428\tvalidation loss: 0.1116\t validation accuracy: 0.9667\n",
      "iteration number: 3092\t training loss: 0.0430\tvalidation loss: 0.1121\t validation accuracy: 0.9622\n",
      "iteration number: 3093\t training loss: 0.0426\tvalidation loss: 0.1099\t validation accuracy: 0.9689\n",
      "iteration number: 3094\t training loss: 0.0434\tvalidation loss: 0.1084\t validation accuracy: 0.9644\n",
      "iteration number: 3095\t training loss: 0.0425\tvalidation loss: 0.1084\t validation accuracy: 0.9667\n",
      "iteration number: 3096\t training loss: 0.0429\tvalidation loss: 0.1081\t validation accuracy: 0.9667\n",
      "iteration number: 3097\t training loss: 0.0427\tvalidation loss: 0.1097\t validation accuracy: 0.9689\n",
      "iteration number: 3098\t training loss: 0.0429\tvalidation loss: 0.1111\t validation accuracy: 0.9667\n",
      "iteration number: 3099\t training loss: 0.0435\tvalidation loss: 0.1118\t validation accuracy: 0.9689\n",
      "iteration number: 3100\t training loss: 0.0439\tvalidation loss: 0.1112\t validation accuracy: 0.9644\n",
      "iteration number: 3101\t training loss: 0.0451\tvalidation loss: 0.1125\t validation accuracy: 0.9622\n",
      "iteration number: 3102\t training loss: 0.0436\tvalidation loss: 0.1113\t validation accuracy: 0.9622\n",
      "iteration number: 3103\t training loss: 0.0436\tvalidation loss: 0.1120\t validation accuracy: 0.9644\n",
      "iteration number: 3104\t training loss: 0.0433\tvalidation loss: 0.1130\t validation accuracy: 0.9689\n",
      "iteration number: 3105\t training loss: 0.0431\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 3106\t training loss: 0.0427\tvalidation loss: 0.1128\t validation accuracy: 0.9644\n",
      "iteration number: 3107\t training loss: 0.0429\tvalidation loss: 0.1127\t validation accuracy: 0.9622\n",
      "iteration number: 3108\t training loss: 0.0427\tvalidation loss: 0.1112\t validation accuracy: 0.9644\n",
      "iteration number: 3109\t training loss: 0.0429\tvalidation loss: 0.1092\t validation accuracy: 0.9644\n",
      "iteration number: 3110\t training loss: 0.0429\tvalidation loss: 0.1095\t validation accuracy: 0.9622\n",
      "iteration number: 3111\t training loss: 0.0438\tvalidation loss: 0.1100\t validation accuracy: 0.9644\n",
      "iteration number: 3112\t training loss: 0.0427\tvalidation loss: 0.1117\t validation accuracy: 0.9622\n",
      "iteration number: 3113\t training loss: 0.0424\tvalidation loss: 0.1127\t validation accuracy: 0.9622\n",
      "iteration number: 3114\t training loss: 0.0424\tvalidation loss: 0.1125\t validation accuracy: 0.9600\n",
      "iteration number: 3115\t training loss: 0.0422\tvalidation loss: 0.1115\t validation accuracy: 0.9644\n",
      "iteration number: 3116\t training loss: 0.0422\tvalidation loss: 0.1109\t validation accuracy: 0.9644\n",
      "iteration number: 3117\t training loss: 0.0423\tvalidation loss: 0.1113\t validation accuracy: 0.9622\n",
      "iteration number: 3118\t training loss: 0.0427\tvalidation loss: 0.1136\t validation accuracy: 0.9644\n",
      "iteration number: 3119\t training loss: 0.0427\tvalidation loss: 0.1151\t validation accuracy: 0.9644\n",
      "iteration number: 3120\t training loss: 0.0423\tvalidation loss: 0.1129\t validation accuracy: 0.9622\n",
      "iteration number: 3121\t training loss: 0.0425\tvalidation loss: 0.1132\t validation accuracy: 0.9644\n",
      "iteration number: 3122\t training loss: 0.0422\tvalidation loss: 0.1122\t validation accuracy: 0.9622\n",
      "iteration number: 3123\t training loss: 0.0429\tvalidation loss: 0.1133\t validation accuracy: 0.9600\n",
      "iteration number: 3124\t training loss: 0.0426\tvalidation loss: 0.1117\t validation accuracy: 0.9622\n",
      "iteration number: 3125\t training loss: 0.0432\tvalidation loss: 0.1133\t validation accuracy: 0.9622\n",
      "iteration number: 3126\t training loss: 0.0444\tvalidation loss: 0.1108\t validation accuracy: 0.9644\n",
      "iteration number: 3127\t training loss: 0.0435\tvalidation loss: 0.1092\t validation accuracy: 0.9644\n",
      "iteration number: 3128\t training loss: 0.0423\tvalidation loss: 0.1099\t validation accuracy: 0.9644\n",
      "iteration number: 3129\t training loss: 0.0427\tvalidation loss: 0.1102\t validation accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3130\t training loss: 0.0426\tvalidation loss: 0.1102\t validation accuracy: 0.9622\n",
      "iteration number: 3131\t training loss: 0.0425\tvalidation loss: 0.1094\t validation accuracy: 0.9644\n",
      "iteration number: 3132\t training loss: 0.0428\tvalidation loss: 0.1099\t validation accuracy: 0.9622\n",
      "iteration number: 3133\t training loss: 0.0419\tvalidation loss: 0.1110\t validation accuracy: 0.9644\n",
      "iteration number: 3134\t training loss: 0.0417\tvalidation loss: 0.1103\t validation accuracy: 0.9644\n",
      "iteration number: 3135\t training loss: 0.0417\tvalidation loss: 0.1107\t validation accuracy: 0.9644\n",
      "iteration number: 3136\t training loss: 0.0427\tvalidation loss: 0.1128\t validation accuracy: 0.9644\n",
      "iteration number: 3137\t training loss: 0.0421\tvalidation loss: 0.1112\t validation accuracy: 0.9667\n",
      "iteration number: 3138\t training loss: 0.0420\tvalidation loss: 0.1112\t validation accuracy: 0.9644\n",
      "iteration number: 3139\t training loss: 0.0424\tvalidation loss: 0.1159\t validation accuracy: 0.9622\n",
      "iteration number: 3140\t training loss: 0.0435\tvalidation loss: 0.1186\t validation accuracy: 0.9600\n",
      "iteration number: 3141\t training loss: 0.0436\tvalidation loss: 0.1182\t validation accuracy: 0.9622\n",
      "iteration number: 3142\t training loss: 0.0438\tvalidation loss: 0.1182\t validation accuracy: 0.9622\n",
      "iteration number: 3143\t training loss: 0.0424\tvalidation loss: 0.1141\t validation accuracy: 0.9667\n",
      "iteration number: 3144\t training loss: 0.0419\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 3145\t training loss: 0.0421\tvalidation loss: 0.1139\t validation accuracy: 0.9644\n",
      "iteration number: 3146\t training loss: 0.0423\tvalidation loss: 0.1150\t validation accuracy: 0.9644\n",
      "iteration number: 3147\t training loss: 0.0428\tvalidation loss: 0.1169\t validation accuracy: 0.9667\n",
      "iteration number: 3148\t training loss: 0.0418\tvalidation loss: 0.1121\t validation accuracy: 0.9667\n",
      "iteration number: 3149\t training loss: 0.0418\tvalidation loss: 0.1115\t validation accuracy: 0.9667\n",
      "iteration number: 3150\t training loss: 0.0422\tvalidation loss: 0.1135\t validation accuracy: 0.9667\n",
      "iteration number: 3151\t training loss: 0.0416\tvalidation loss: 0.1100\t validation accuracy: 0.9667\n",
      "iteration number: 3152\t training loss: 0.0417\tvalidation loss: 0.1092\t validation accuracy: 0.9667\n",
      "iteration number: 3153\t training loss: 0.0417\tvalidation loss: 0.1085\t validation accuracy: 0.9644\n",
      "iteration number: 3154\t training loss: 0.0421\tvalidation loss: 0.1065\t validation accuracy: 0.9689\n",
      "iteration number: 3155\t training loss: 0.0421\tvalidation loss: 0.1067\t validation accuracy: 0.9733\n",
      "iteration number: 3156\t training loss: 0.0418\tvalidation loss: 0.1092\t validation accuracy: 0.9667\n",
      "iteration number: 3157\t training loss: 0.0422\tvalidation loss: 0.1123\t validation accuracy: 0.9667\n",
      "iteration number: 3158\t training loss: 0.0420\tvalidation loss: 0.1115\t validation accuracy: 0.9689\n",
      "iteration number: 3159\t training loss: 0.0418\tvalidation loss: 0.1111\t validation accuracy: 0.9667\n",
      "iteration number: 3160\t training loss: 0.0417\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 3161\t training loss: 0.0419\tvalidation loss: 0.1114\t validation accuracy: 0.9622\n",
      "iteration number: 3162\t training loss: 0.0416\tvalidation loss: 0.1116\t validation accuracy: 0.9667\n",
      "iteration number: 3163\t training loss: 0.0414\tvalidation loss: 0.1109\t validation accuracy: 0.9667\n",
      "iteration number: 3164\t training loss: 0.0415\tvalidation loss: 0.1108\t validation accuracy: 0.9644\n",
      "iteration number: 3165\t training loss: 0.0415\tvalidation loss: 0.1107\t validation accuracy: 0.9667\n",
      "iteration number: 3166\t training loss: 0.0418\tvalidation loss: 0.1122\t validation accuracy: 0.9644\n",
      "iteration number: 3167\t training loss: 0.0423\tvalidation loss: 0.1106\t validation accuracy: 0.9644\n",
      "iteration number: 3168\t training loss: 0.0418\tvalidation loss: 0.1102\t validation accuracy: 0.9689\n",
      "iteration number: 3169\t training loss: 0.0415\tvalidation loss: 0.1113\t validation accuracy: 0.9689\n",
      "iteration number: 3170\t training loss: 0.0414\tvalidation loss: 0.1125\t validation accuracy: 0.9644\n",
      "iteration number: 3171\t training loss: 0.0416\tvalidation loss: 0.1150\t validation accuracy: 0.9667\n",
      "iteration number: 3172\t training loss: 0.0416\tvalidation loss: 0.1141\t validation accuracy: 0.9689\n",
      "iteration number: 3173\t training loss: 0.0416\tvalidation loss: 0.1111\t validation accuracy: 0.9689\n",
      "iteration number: 3174\t training loss: 0.0420\tvalidation loss: 0.1128\t validation accuracy: 0.9644\n",
      "iteration number: 3175\t training loss: 0.0416\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 3176\t training loss: 0.0418\tvalidation loss: 0.1126\t validation accuracy: 0.9644\n",
      "iteration number: 3177\t training loss: 0.0415\tvalidation loss: 0.1152\t validation accuracy: 0.9644\n",
      "iteration number: 3178\t training loss: 0.0416\tvalidation loss: 0.1154\t validation accuracy: 0.9644\n",
      "iteration number: 3179\t training loss: 0.0415\tvalidation loss: 0.1129\t validation accuracy: 0.9689\n",
      "iteration number: 3180\t training loss: 0.0415\tvalidation loss: 0.1110\t validation accuracy: 0.9667\n",
      "iteration number: 3181\t training loss: 0.0412\tvalidation loss: 0.1124\t validation accuracy: 0.9667\n",
      "iteration number: 3182\t training loss: 0.0413\tvalidation loss: 0.1141\t validation accuracy: 0.9667\n",
      "iteration number: 3183\t training loss: 0.0412\tvalidation loss: 0.1144\t validation accuracy: 0.9667\n",
      "iteration number: 3184\t training loss: 0.0410\tvalidation loss: 0.1131\t validation accuracy: 0.9667\n",
      "iteration number: 3185\t training loss: 0.0412\tvalidation loss: 0.1133\t validation accuracy: 0.9622\n",
      "iteration number: 3186\t training loss: 0.0410\tvalidation loss: 0.1130\t validation accuracy: 0.9667\n",
      "iteration number: 3187\t training loss: 0.0409\tvalidation loss: 0.1114\t validation accuracy: 0.9667\n",
      "iteration number: 3188\t training loss: 0.0421\tvalidation loss: 0.1081\t validation accuracy: 0.9667\n",
      "iteration number: 3189\t training loss: 0.0412\tvalidation loss: 0.1095\t validation accuracy: 0.9689\n",
      "iteration number: 3190\t training loss: 0.0410\tvalidation loss: 0.1113\t validation accuracy: 0.9667\n",
      "iteration number: 3191\t training loss: 0.0411\tvalidation loss: 0.1095\t validation accuracy: 0.9689\n",
      "iteration number: 3192\t training loss: 0.0411\tvalidation loss: 0.1096\t validation accuracy: 0.9689\n",
      "iteration number: 3193\t training loss: 0.0416\tvalidation loss: 0.1100\t validation accuracy: 0.9644\n",
      "iteration number: 3194\t training loss: 0.0416\tvalidation loss: 0.1107\t validation accuracy: 0.9644\n",
      "iteration number: 3195\t training loss: 0.0418\tvalidation loss: 0.1137\t validation accuracy: 0.9667\n",
      "iteration number: 3196\t training loss: 0.0419\tvalidation loss: 0.1147\t validation accuracy: 0.9689\n",
      "iteration number: 3197\t training loss: 0.0410\tvalidation loss: 0.1130\t validation accuracy: 0.9667\n",
      "iteration number: 3198\t training loss: 0.0413\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 3199\t training loss: 0.0412\tvalidation loss: 0.1115\t validation accuracy: 0.9644\n",
      "iteration number: 3200\t training loss: 0.0413\tvalidation loss: 0.1119\t validation accuracy: 0.9644\n",
      "iteration number: 3201\t training loss: 0.0417\tvalidation loss: 0.1123\t validation accuracy: 0.9667\n",
      "iteration number: 3202\t training loss: 0.0415\tvalidation loss: 0.1133\t validation accuracy: 0.9667\n",
      "iteration number: 3203\t training loss: 0.0418\tvalidation loss: 0.1158\t validation accuracy: 0.9667\n",
      "iteration number: 3204\t training loss: 0.0411\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 3205\t training loss: 0.0405\tvalidation loss: 0.1117\t validation accuracy: 0.9622\n",
      "iteration number: 3206\t training loss: 0.0405\tvalidation loss: 0.1109\t validation accuracy: 0.9622\n",
      "iteration number: 3207\t training loss: 0.0408\tvalidation loss: 0.1111\t validation accuracy: 0.9600\n",
      "iteration number: 3208\t training loss: 0.0413\tvalidation loss: 0.1093\t validation accuracy: 0.9600\n",
      "iteration number: 3209\t training loss: 0.0419\tvalidation loss: 0.1104\t validation accuracy: 0.9644\n",
      "iteration number: 3210\t training loss: 0.0413\tvalidation loss: 0.1099\t validation accuracy: 0.9644\n",
      "iteration number: 3211\t training loss: 0.0407\tvalidation loss: 0.1097\t validation accuracy: 0.9667\n",
      "iteration number: 3212\t training loss: 0.0407\tvalidation loss: 0.1096\t validation accuracy: 0.9644\n",
      "iteration number: 3213\t training loss: 0.0407\tvalidation loss: 0.1119\t validation accuracy: 0.9667\n",
      "iteration number: 3214\t training loss: 0.0404\tvalidation loss: 0.1113\t validation accuracy: 0.9644\n",
      "iteration number: 3215\t training loss: 0.0404\tvalidation loss: 0.1123\t validation accuracy: 0.9622\n",
      "iteration number: 3216\t training loss: 0.0404\tvalidation loss: 0.1107\t validation accuracy: 0.9644\n",
      "iteration number: 3217\t training loss: 0.0405\tvalidation loss: 0.1096\t validation accuracy: 0.9689\n",
      "iteration number: 3218\t training loss: 0.0405\tvalidation loss: 0.1105\t validation accuracy: 0.9689\n",
      "iteration number: 3219\t training loss: 0.0409\tvalidation loss: 0.1115\t validation accuracy: 0.9644\n",
      "iteration number: 3220\t training loss: 0.0407\tvalidation loss: 0.1121\t validation accuracy: 0.9644\n",
      "iteration number: 3221\t training loss: 0.0405\tvalidation loss: 0.1105\t validation accuracy: 0.9667\n",
      "iteration number: 3222\t training loss: 0.0404\tvalidation loss: 0.1114\t validation accuracy: 0.9644\n",
      "iteration number: 3223\t training loss: 0.0404\tvalidation loss: 0.1112\t validation accuracy: 0.9622\n",
      "iteration number: 3224\t training loss: 0.0409\tvalidation loss: 0.1123\t validation accuracy: 0.9644\n",
      "iteration number: 3225\t training loss: 0.0409\tvalidation loss: 0.1117\t validation accuracy: 0.9667\n",
      "iteration number: 3226\t training loss: 0.0407\tvalidation loss: 0.1119\t validation accuracy: 0.9622\n",
      "iteration number: 3227\t training loss: 0.0411\tvalidation loss: 0.1114\t validation accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3228\t training loss: 0.0413\tvalidation loss: 0.1121\t validation accuracy: 0.9600\n",
      "iteration number: 3229\t training loss: 0.0408\tvalidation loss: 0.1105\t validation accuracy: 0.9600\n",
      "iteration number: 3230\t training loss: 0.0411\tvalidation loss: 0.1100\t validation accuracy: 0.9622\n",
      "iteration number: 3231\t training loss: 0.0415\tvalidation loss: 0.1093\t validation accuracy: 0.9622\n",
      "iteration number: 3232\t training loss: 0.0421\tvalidation loss: 0.1085\t validation accuracy: 0.9622\n",
      "iteration number: 3233\t training loss: 0.0413\tvalidation loss: 0.1086\t validation accuracy: 0.9622\n",
      "iteration number: 3234\t training loss: 0.0413\tvalidation loss: 0.1081\t validation accuracy: 0.9622\n",
      "iteration number: 3235\t training loss: 0.0414\tvalidation loss: 0.1073\t validation accuracy: 0.9622\n",
      "iteration number: 3236\t training loss: 0.0409\tvalidation loss: 0.1061\t validation accuracy: 0.9667\n",
      "iteration number: 3237\t training loss: 0.0408\tvalidation loss: 0.1066\t validation accuracy: 0.9644\n",
      "iteration number: 3238\t training loss: 0.0407\tvalidation loss: 0.1081\t validation accuracy: 0.9644\n",
      "iteration number: 3239\t training loss: 0.0410\tvalidation loss: 0.1083\t validation accuracy: 0.9622\n",
      "iteration number: 3240\t training loss: 0.0409\tvalidation loss: 0.1075\t validation accuracy: 0.9689\n",
      "iteration number: 3241\t training loss: 0.0408\tvalidation loss: 0.1065\t validation accuracy: 0.9711\n",
      "iteration number: 3242\t training loss: 0.0408\tvalidation loss: 0.1076\t validation accuracy: 0.9689\n",
      "iteration number: 3243\t training loss: 0.0404\tvalidation loss: 0.1079\t validation accuracy: 0.9667\n",
      "iteration number: 3244\t training loss: 0.0412\tvalidation loss: 0.1073\t validation accuracy: 0.9689\n",
      "iteration number: 3245\t training loss: 0.0400\tvalidation loss: 0.1109\t validation accuracy: 0.9689\n",
      "iteration number: 3246\t training loss: 0.0400\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 3247\t training loss: 0.0400\tvalidation loss: 0.1136\t validation accuracy: 0.9644\n",
      "iteration number: 3248\t training loss: 0.0407\tvalidation loss: 0.1127\t validation accuracy: 0.9622\n",
      "iteration number: 3249\t training loss: 0.0406\tvalidation loss: 0.1118\t validation accuracy: 0.9622\n",
      "iteration number: 3250\t training loss: 0.0413\tvalidation loss: 0.1156\t validation accuracy: 0.9622\n",
      "iteration number: 3251\t training loss: 0.0420\tvalidation loss: 0.1180\t validation accuracy: 0.9622\n",
      "iteration number: 3252\t training loss: 0.0406\tvalidation loss: 0.1132\t validation accuracy: 0.9622\n",
      "iteration number: 3253\t training loss: 0.0405\tvalidation loss: 0.1123\t validation accuracy: 0.9622\n",
      "iteration number: 3254\t training loss: 0.0407\tvalidation loss: 0.1144\t validation accuracy: 0.9622\n",
      "iteration number: 3255\t training loss: 0.0399\tvalidation loss: 0.1126\t validation accuracy: 0.9644\n",
      "iteration number: 3256\t training loss: 0.0400\tvalidation loss: 0.1139\t validation accuracy: 0.9622\n",
      "iteration number: 3257\t training loss: 0.0401\tvalidation loss: 0.1159\t validation accuracy: 0.9600\n",
      "iteration number: 3258\t training loss: 0.0400\tvalidation loss: 0.1138\t validation accuracy: 0.9600\n",
      "iteration number: 3259\t training loss: 0.0406\tvalidation loss: 0.1171\t validation accuracy: 0.9600\n",
      "iteration number: 3260\t training loss: 0.0413\tvalidation loss: 0.1209\t validation accuracy: 0.9644\n",
      "iteration number: 3261\t training loss: 0.0409\tvalidation loss: 0.1202\t validation accuracy: 0.9600\n",
      "iteration number: 3262\t training loss: 0.0408\tvalidation loss: 0.1195\t validation accuracy: 0.9600\n",
      "iteration number: 3263\t training loss: 0.0404\tvalidation loss: 0.1174\t validation accuracy: 0.9600\n",
      "iteration number: 3264\t training loss: 0.0406\tvalidation loss: 0.1191\t validation accuracy: 0.9578\n",
      "iteration number: 3265\t training loss: 0.0400\tvalidation loss: 0.1168\t validation accuracy: 0.9600\n",
      "iteration number: 3266\t training loss: 0.0403\tvalidation loss: 0.1165\t validation accuracy: 0.9644\n",
      "iteration number: 3267\t training loss: 0.0405\tvalidation loss: 0.1163\t validation accuracy: 0.9644\n",
      "iteration number: 3268\t training loss: 0.0405\tvalidation loss: 0.1145\t validation accuracy: 0.9644\n",
      "iteration number: 3269\t training loss: 0.0406\tvalidation loss: 0.1151\t validation accuracy: 0.9622\n",
      "iteration number: 3270\t training loss: 0.0402\tvalidation loss: 0.1179\t validation accuracy: 0.9600\n",
      "iteration number: 3271\t training loss: 0.0399\tvalidation loss: 0.1166\t validation accuracy: 0.9600\n",
      "iteration number: 3272\t training loss: 0.0400\tvalidation loss: 0.1157\t validation accuracy: 0.9600\n",
      "iteration number: 3273\t training loss: 0.0401\tvalidation loss: 0.1158\t validation accuracy: 0.9600\n",
      "iteration number: 3274\t training loss: 0.0402\tvalidation loss: 0.1167\t validation accuracy: 0.9622\n",
      "iteration number: 3275\t training loss: 0.0402\tvalidation loss: 0.1150\t validation accuracy: 0.9600\n",
      "iteration number: 3276\t training loss: 0.0404\tvalidation loss: 0.1140\t validation accuracy: 0.9622\n",
      "iteration number: 3277\t training loss: 0.0406\tvalidation loss: 0.1148\t validation accuracy: 0.9644\n",
      "iteration number: 3278\t training loss: 0.0404\tvalidation loss: 0.1144\t validation accuracy: 0.9622\n",
      "iteration number: 3279\t training loss: 0.0403\tvalidation loss: 0.1146\t validation accuracy: 0.9600\n",
      "iteration number: 3280\t training loss: 0.0402\tvalidation loss: 0.1140\t validation accuracy: 0.9622\n",
      "iteration number: 3281\t training loss: 0.0403\tvalidation loss: 0.1150\t validation accuracy: 0.9600\n",
      "iteration number: 3282\t training loss: 0.0407\tvalidation loss: 0.1141\t validation accuracy: 0.9600\n",
      "iteration number: 3283\t training loss: 0.0398\tvalidation loss: 0.1134\t validation accuracy: 0.9578\n",
      "iteration number: 3284\t training loss: 0.0398\tvalidation loss: 0.1131\t validation accuracy: 0.9600\n",
      "iteration number: 3285\t training loss: 0.0398\tvalidation loss: 0.1149\t validation accuracy: 0.9578\n",
      "iteration number: 3286\t training loss: 0.0399\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n",
      "iteration number: 3287\t training loss: 0.0406\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 3288\t training loss: 0.0406\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 3289\t training loss: 0.0405\tvalidation loss: 0.1158\t validation accuracy: 0.9622\n",
      "iteration number: 3290\t training loss: 0.0405\tvalidation loss: 0.1132\t validation accuracy: 0.9644\n",
      "iteration number: 3291\t training loss: 0.0402\tvalidation loss: 0.1125\t validation accuracy: 0.9644\n",
      "iteration number: 3292\t training loss: 0.0399\tvalidation loss: 0.1126\t validation accuracy: 0.9644\n",
      "iteration number: 3293\t training loss: 0.0417\tvalidation loss: 0.1125\t validation accuracy: 0.9622\n",
      "iteration number: 3294\t training loss: 0.0422\tvalidation loss: 0.1156\t validation accuracy: 0.9644\n",
      "iteration number: 3295\t training loss: 0.0404\tvalidation loss: 0.1097\t validation accuracy: 0.9644\n",
      "iteration number: 3296\t training loss: 0.0403\tvalidation loss: 0.1099\t validation accuracy: 0.9644\n",
      "iteration number: 3297\t training loss: 0.0399\tvalidation loss: 0.1101\t validation accuracy: 0.9644\n",
      "iteration number: 3298\t training loss: 0.0403\tvalidation loss: 0.1115\t validation accuracy: 0.9644\n",
      "iteration number: 3299\t training loss: 0.0401\tvalidation loss: 0.1126\t validation accuracy: 0.9644\n",
      "iteration number: 3300\t training loss: 0.0399\tvalidation loss: 0.1128\t validation accuracy: 0.9644\n",
      "iteration number: 3301\t training loss: 0.0404\tvalidation loss: 0.1115\t validation accuracy: 0.9644\n",
      "iteration number: 3302\t training loss: 0.0409\tvalidation loss: 0.1132\t validation accuracy: 0.9644\n",
      "iteration number: 3303\t training loss: 0.0401\tvalidation loss: 0.1142\t validation accuracy: 0.9644\n",
      "iteration number: 3304\t training loss: 0.0398\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 3305\t training loss: 0.0397\tvalidation loss: 0.1139\t validation accuracy: 0.9644\n",
      "iteration number: 3306\t training loss: 0.0394\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 3307\t training loss: 0.0393\tvalidation loss: 0.1106\t validation accuracy: 0.9622\n",
      "iteration number: 3308\t training loss: 0.0394\tvalidation loss: 0.1135\t validation accuracy: 0.9600\n",
      "iteration number: 3309\t training loss: 0.0396\tvalidation loss: 0.1149\t validation accuracy: 0.9622\n",
      "iteration number: 3310\t training loss: 0.0396\tvalidation loss: 0.1114\t validation accuracy: 0.9622\n",
      "iteration number: 3311\t training loss: 0.0398\tvalidation loss: 0.1114\t validation accuracy: 0.9644\n",
      "iteration number: 3312\t training loss: 0.0397\tvalidation loss: 0.1121\t validation accuracy: 0.9644\n",
      "iteration number: 3313\t training loss: 0.0395\tvalidation loss: 0.1116\t validation accuracy: 0.9622\n",
      "iteration number: 3314\t training loss: 0.0393\tvalidation loss: 0.1121\t validation accuracy: 0.9622\n",
      "iteration number: 3315\t training loss: 0.0390\tvalidation loss: 0.1157\t validation accuracy: 0.9644\n",
      "iteration number: 3316\t training loss: 0.0390\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 3317\t training loss: 0.0391\tvalidation loss: 0.1136\t validation accuracy: 0.9600\n",
      "iteration number: 3318\t training loss: 0.0389\tvalidation loss: 0.1134\t validation accuracy: 0.9622\n",
      "iteration number: 3319\t training loss: 0.0392\tvalidation loss: 0.1137\t validation accuracy: 0.9644\n",
      "iteration number: 3320\t training loss: 0.0391\tvalidation loss: 0.1123\t validation accuracy: 0.9644\n",
      "iteration number: 3321\t training loss: 0.0391\tvalidation loss: 0.1115\t validation accuracy: 0.9644\n",
      "iteration number: 3322\t training loss: 0.0394\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3323\t training loss: 0.0390\tvalidation loss: 0.1144\t validation accuracy: 0.9644\n",
      "iteration number: 3324\t training loss: 0.0390\tvalidation loss: 0.1142\t validation accuracy: 0.9644\n",
      "iteration number: 3325\t training loss: 0.0391\tvalidation loss: 0.1147\t validation accuracy: 0.9622\n",
      "iteration number: 3326\t training loss: 0.0393\tvalidation loss: 0.1161\t validation accuracy: 0.9622\n",
      "iteration number: 3327\t training loss: 0.0396\tvalidation loss: 0.1175\t validation accuracy: 0.9622\n",
      "iteration number: 3328\t training loss: 0.0394\tvalidation loss: 0.1165\t validation accuracy: 0.9622\n",
      "iteration number: 3329\t training loss: 0.0395\tvalidation loss: 0.1168\t validation accuracy: 0.9622\n",
      "iteration number: 3330\t training loss: 0.0389\tvalidation loss: 0.1127\t validation accuracy: 0.9667\n",
      "iteration number: 3331\t training loss: 0.0392\tvalidation loss: 0.1134\t validation accuracy: 0.9667\n",
      "iteration number: 3332\t training loss: 0.0388\tvalidation loss: 0.1141\t validation accuracy: 0.9644\n",
      "iteration number: 3333\t training loss: 0.0390\tvalidation loss: 0.1152\t validation accuracy: 0.9667\n",
      "iteration number: 3334\t training loss: 0.0389\tvalidation loss: 0.1143\t validation accuracy: 0.9667\n",
      "iteration number: 3335\t training loss: 0.0391\tvalidation loss: 0.1164\t validation accuracy: 0.9600\n",
      "iteration number: 3336\t training loss: 0.0391\tvalidation loss: 0.1163\t validation accuracy: 0.9622\n",
      "iteration number: 3337\t training loss: 0.0394\tvalidation loss: 0.1180\t validation accuracy: 0.9622\n",
      "iteration number: 3338\t training loss: 0.0393\tvalidation loss: 0.1153\t validation accuracy: 0.9622\n",
      "iteration number: 3339\t training loss: 0.0388\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 3340\t training loss: 0.0393\tvalidation loss: 0.1123\t validation accuracy: 0.9667\n",
      "iteration number: 3341\t training loss: 0.0394\tvalidation loss: 0.1121\t validation accuracy: 0.9644\n",
      "iteration number: 3342\t training loss: 0.0396\tvalidation loss: 0.1128\t validation accuracy: 0.9644\n",
      "iteration number: 3343\t training loss: 0.0387\tvalidation loss: 0.1109\t validation accuracy: 0.9667\n",
      "iteration number: 3344\t training loss: 0.0391\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 3345\t training loss: 0.0402\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 3346\t training loss: 0.0397\tvalidation loss: 0.1127\t validation accuracy: 0.9644\n",
      "iteration number: 3347\t training loss: 0.0392\tvalidation loss: 0.1128\t validation accuracy: 0.9622\n",
      "iteration number: 3348\t training loss: 0.0388\tvalidation loss: 0.1134\t validation accuracy: 0.9622\n",
      "iteration number: 3349\t training loss: 0.0387\tvalidation loss: 0.1140\t validation accuracy: 0.9622\n",
      "iteration number: 3350\t training loss: 0.0386\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 3351\t training loss: 0.0387\tvalidation loss: 0.1114\t validation accuracy: 0.9622\n",
      "iteration number: 3352\t training loss: 0.0396\tvalidation loss: 0.1117\t validation accuracy: 0.9667\n",
      "iteration number: 3353\t training loss: 0.0391\tvalidation loss: 0.1120\t validation accuracy: 0.9667\n",
      "iteration number: 3354\t training loss: 0.0391\tvalidation loss: 0.1125\t validation accuracy: 0.9667\n",
      "iteration number: 3355\t training loss: 0.0389\tvalidation loss: 0.1127\t validation accuracy: 0.9622\n",
      "iteration number: 3356\t training loss: 0.0387\tvalidation loss: 0.1126\t validation accuracy: 0.9622\n",
      "iteration number: 3357\t training loss: 0.0392\tvalidation loss: 0.1131\t validation accuracy: 0.9667\n",
      "iteration number: 3358\t training loss: 0.0390\tvalidation loss: 0.1122\t validation accuracy: 0.9667\n",
      "iteration number: 3359\t training loss: 0.0385\tvalidation loss: 0.1114\t validation accuracy: 0.9622\n",
      "iteration number: 3360\t training loss: 0.0384\tvalidation loss: 0.1122\t validation accuracy: 0.9622\n",
      "iteration number: 3361\t training loss: 0.0386\tvalidation loss: 0.1124\t validation accuracy: 0.9600\n",
      "iteration number: 3362\t training loss: 0.0386\tvalidation loss: 0.1117\t validation accuracy: 0.9644\n",
      "iteration number: 3363\t training loss: 0.0387\tvalidation loss: 0.1112\t validation accuracy: 0.9622\n",
      "iteration number: 3364\t training loss: 0.0392\tvalidation loss: 0.1120\t validation accuracy: 0.9644\n",
      "iteration number: 3365\t training loss: 0.0391\tvalidation loss: 0.1125\t validation accuracy: 0.9622\n",
      "iteration number: 3366\t training loss: 0.0392\tvalidation loss: 0.1125\t validation accuracy: 0.9622\n",
      "iteration number: 3367\t training loss: 0.0392\tvalidation loss: 0.1160\t validation accuracy: 0.9600\n",
      "iteration number: 3368\t training loss: 0.0388\tvalidation loss: 0.1153\t validation accuracy: 0.9600\n",
      "iteration number: 3369\t training loss: 0.0386\tvalidation loss: 0.1131\t validation accuracy: 0.9622\n",
      "iteration number: 3370\t training loss: 0.0386\tvalidation loss: 0.1141\t validation accuracy: 0.9600\n",
      "iteration number: 3371\t training loss: 0.0391\tvalidation loss: 0.1162\t validation accuracy: 0.9600\n",
      "iteration number: 3372\t training loss: 0.0392\tvalidation loss: 0.1181\t validation accuracy: 0.9600\n",
      "iteration number: 3373\t training loss: 0.0391\tvalidation loss: 0.1178\t validation accuracy: 0.9600\n",
      "iteration number: 3374\t training loss: 0.0391\tvalidation loss: 0.1167\t validation accuracy: 0.9622\n",
      "iteration number: 3375\t training loss: 0.0397\tvalidation loss: 0.1185\t validation accuracy: 0.9600\n",
      "iteration number: 3376\t training loss: 0.0398\tvalidation loss: 0.1187\t validation accuracy: 0.9578\n",
      "iteration number: 3377\t training loss: 0.0403\tvalidation loss: 0.1186\t validation accuracy: 0.9622\n",
      "iteration number: 3378\t training loss: 0.0385\tvalidation loss: 0.1129\t validation accuracy: 0.9644\n",
      "iteration number: 3379\t training loss: 0.0395\tvalidation loss: 0.1112\t validation accuracy: 0.9644\n",
      "iteration number: 3380\t training loss: 0.0384\tvalidation loss: 0.1109\t validation accuracy: 0.9689\n",
      "iteration number: 3381\t training loss: 0.0381\tvalidation loss: 0.1096\t validation accuracy: 0.9667\n",
      "iteration number: 3382\t training loss: 0.0387\tvalidation loss: 0.1090\t validation accuracy: 0.9667\n",
      "iteration number: 3383\t training loss: 0.0391\tvalidation loss: 0.1077\t validation accuracy: 0.9689\n",
      "iteration number: 3384\t training loss: 0.0391\tvalidation loss: 0.1065\t validation accuracy: 0.9733\n",
      "iteration number: 3385\t training loss: 0.0401\tvalidation loss: 0.1057\t validation accuracy: 0.9733\n",
      "iteration number: 3386\t training loss: 0.0398\tvalidation loss: 0.1053\t validation accuracy: 0.9733\n",
      "iteration number: 3387\t training loss: 0.0389\tvalidation loss: 0.1075\t validation accuracy: 0.9733\n",
      "iteration number: 3388\t training loss: 0.0391\tvalidation loss: 0.1065\t validation accuracy: 0.9733\n",
      "iteration number: 3389\t training loss: 0.0389\tvalidation loss: 0.1075\t validation accuracy: 0.9733\n",
      "iteration number: 3390\t training loss: 0.0388\tvalidation loss: 0.1088\t validation accuracy: 0.9711\n",
      "iteration number: 3391\t training loss: 0.0388\tvalidation loss: 0.1094\t validation accuracy: 0.9689\n",
      "iteration number: 3392\t training loss: 0.0388\tvalidation loss: 0.1102\t validation accuracy: 0.9689\n",
      "iteration number: 3393\t training loss: 0.0388\tvalidation loss: 0.1120\t validation accuracy: 0.9667\n",
      "iteration number: 3394\t training loss: 0.0384\tvalidation loss: 0.1095\t validation accuracy: 0.9689\n",
      "iteration number: 3395\t training loss: 0.0380\tvalidation loss: 0.1086\t validation accuracy: 0.9667\n",
      "iteration number: 3396\t training loss: 0.0379\tvalidation loss: 0.1092\t validation accuracy: 0.9667\n",
      "iteration number: 3397\t training loss: 0.0381\tvalidation loss: 0.1107\t validation accuracy: 0.9644\n",
      "iteration number: 3398\t training loss: 0.0381\tvalidation loss: 0.1079\t validation accuracy: 0.9667\n",
      "iteration number: 3399\t training loss: 0.0379\tvalidation loss: 0.1082\t validation accuracy: 0.9689\n",
      "iteration number: 3400\t training loss: 0.0378\tvalidation loss: 0.1110\t validation accuracy: 0.9644\n",
      "iteration number: 3401\t training loss: 0.0379\tvalidation loss: 0.1098\t validation accuracy: 0.9667\n",
      "iteration number: 3402\t training loss: 0.0378\tvalidation loss: 0.1084\t validation accuracy: 0.9667\n",
      "iteration number: 3403\t training loss: 0.0378\tvalidation loss: 0.1094\t validation accuracy: 0.9667\n",
      "iteration number: 3404\t training loss: 0.0380\tvalidation loss: 0.1119\t validation accuracy: 0.9622\n",
      "iteration number: 3405\t training loss: 0.0378\tvalidation loss: 0.1122\t validation accuracy: 0.9622\n",
      "iteration number: 3406\t training loss: 0.0377\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n",
      "iteration number: 3407\t training loss: 0.0377\tvalidation loss: 0.1140\t validation accuracy: 0.9622\n",
      "iteration number: 3408\t training loss: 0.0377\tvalidation loss: 0.1137\t validation accuracy: 0.9600\n",
      "iteration number: 3409\t training loss: 0.0379\tvalidation loss: 0.1152\t validation accuracy: 0.9622\n",
      "iteration number: 3410\t training loss: 0.0381\tvalidation loss: 0.1153\t validation accuracy: 0.9622\n",
      "iteration number: 3411\t training loss: 0.0383\tvalidation loss: 0.1154\t validation accuracy: 0.9622\n",
      "iteration number: 3412\t training loss: 0.0383\tvalidation loss: 0.1163\t validation accuracy: 0.9622\n",
      "iteration number: 3413\t training loss: 0.0381\tvalidation loss: 0.1159\t validation accuracy: 0.9622\n",
      "iteration number: 3414\t training loss: 0.0378\tvalidation loss: 0.1154\t validation accuracy: 0.9622\n",
      "iteration number: 3415\t training loss: 0.0377\tvalidation loss: 0.1149\t validation accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3416\t training loss: 0.0379\tvalidation loss: 0.1148\t validation accuracy: 0.9622\n",
      "iteration number: 3417\t training loss: 0.0379\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 3418\t training loss: 0.0379\tvalidation loss: 0.1144\t validation accuracy: 0.9644\n",
      "iteration number: 3419\t training loss: 0.0380\tvalidation loss: 0.1150\t validation accuracy: 0.9622\n",
      "iteration number: 3420\t training loss: 0.0376\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 3421\t training loss: 0.0377\tvalidation loss: 0.1121\t validation accuracy: 0.9644\n",
      "iteration number: 3422\t training loss: 0.0375\tvalidation loss: 0.1110\t validation accuracy: 0.9644\n",
      "iteration number: 3423\t training loss: 0.0374\tvalidation loss: 0.1109\t validation accuracy: 0.9644\n",
      "iteration number: 3424\t training loss: 0.0376\tvalidation loss: 0.1103\t validation accuracy: 0.9644\n",
      "iteration number: 3425\t training loss: 0.0375\tvalidation loss: 0.1109\t validation accuracy: 0.9644\n",
      "iteration number: 3426\t training loss: 0.0376\tvalidation loss: 0.1093\t validation accuracy: 0.9644\n",
      "iteration number: 3427\t training loss: 0.0377\tvalidation loss: 0.1101\t validation accuracy: 0.9667\n",
      "iteration number: 3428\t training loss: 0.0377\tvalidation loss: 0.1109\t validation accuracy: 0.9667\n",
      "iteration number: 3429\t training loss: 0.0376\tvalidation loss: 0.1121\t validation accuracy: 0.9667\n",
      "iteration number: 3430\t training loss: 0.0381\tvalidation loss: 0.1170\t validation accuracy: 0.9600\n",
      "iteration number: 3431\t training loss: 0.0390\tvalidation loss: 0.1169\t validation accuracy: 0.9644\n",
      "iteration number: 3432\t training loss: 0.0389\tvalidation loss: 0.1168\t validation accuracy: 0.9622\n",
      "iteration number: 3433\t training loss: 0.0379\tvalidation loss: 0.1139\t validation accuracy: 0.9622\n",
      "iteration number: 3434\t training loss: 0.0381\tvalidation loss: 0.1135\t validation accuracy: 0.9622\n",
      "iteration number: 3435\t training loss: 0.0379\tvalidation loss: 0.1134\t validation accuracy: 0.9622\n",
      "iteration number: 3436\t training loss: 0.0394\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 3437\t training loss: 0.0388\tvalidation loss: 0.1127\t validation accuracy: 0.9644\n",
      "iteration number: 3438\t training loss: 0.0389\tvalidation loss: 0.1134\t validation accuracy: 0.9644\n",
      "iteration number: 3439\t training loss: 0.0383\tvalidation loss: 0.1141\t validation accuracy: 0.9644\n",
      "iteration number: 3440\t training loss: 0.0379\tvalidation loss: 0.1147\t validation accuracy: 0.9644\n",
      "iteration number: 3441\t training loss: 0.0382\tvalidation loss: 0.1165\t validation accuracy: 0.9644\n",
      "iteration number: 3442\t training loss: 0.0376\tvalidation loss: 0.1140\t validation accuracy: 0.9644\n",
      "iteration number: 3443\t training loss: 0.0375\tvalidation loss: 0.1137\t validation accuracy: 0.9644\n",
      "iteration number: 3444\t training loss: 0.0373\tvalidation loss: 0.1111\t validation accuracy: 0.9644\n",
      "iteration number: 3445\t training loss: 0.0377\tvalidation loss: 0.1118\t validation accuracy: 0.9622\n",
      "iteration number: 3446\t training loss: 0.0374\tvalidation loss: 0.1134\t validation accuracy: 0.9622\n",
      "iteration number: 3447\t training loss: 0.0376\tvalidation loss: 0.1143\t validation accuracy: 0.9622\n",
      "iteration number: 3448\t training loss: 0.0376\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 3449\t training loss: 0.0376\tvalidation loss: 0.1131\t validation accuracy: 0.9578\n",
      "iteration number: 3450\t training loss: 0.0375\tvalidation loss: 0.1140\t validation accuracy: 0.9600\n",
      "iteration number: 3451\t training loss: 0.0377\tvalidation loss: 0.1124\t validation accuracy: 0.9600\n",
      "iteration number: 3452\t training loss: 0.0376\tvalidation loss: 0.1164\t validation accuracy: 0.9556\n",
      "iteration number: 3453\t training loss: 0.0380\tvalidation loss: 0.1184\t validation accuracy: 0.9600\n",
      "iteration number: 3454\t training loss: 0.0378\tvalidation loss: 0.1176\t validation accuracy: 0.9556\n",
      "iteration number: 3455\t training loss: 0.0377\tvalidation loss: 0.1174\t validation accuracy: 0.9556\n",
      "iteration number: 3456\t training loss: 0.0374\tvalidation loss: 0.1155\t validation accuracy: 0.9578\n",
      "iteration number: 3457\t training loss: 0.0377\tvalidation loss: 0.1151\t validation accuracy: 0.9622\n",
      "iteration number: 3458\t training loss: 0.0381\tvalidation loss: 0.1150\t validation accuracy: 0.9600\n",
      "iteration number: 3459\t training loss: 0.0375\tvalidation loss: 0.1132\t validation accuracy: 0.9600\n",
      "iteration number: 3460\t training loss: 0.0374\tvalidation loss: 0.1149\t validation accuracy: 0.9578\n",
      "iteration number: 3461\t training loss: 0.0391\tvalidation loss: 0.1158\t validation accuracy: 0.9600\n",
      "iteration number: 3462\t training loss: 0.0375\tvalidation loss: 0.1146\t validation accuracy: 0.9600\n",
      "iteration number: 3463\t training loss: 0.0383\tvalidation loss: 0.1156\t validation accuracy: 0.9600\n",
      "iteration number: 3464\t training loss: 0.0375\tvalidation loss: 0.1154\t validation accuracy: 0.9578\n",
      "iteration number: 3465\t training loss: 0.0374\tvalidation loss: 0.1147\t validation accuracy: 0.9578\n",
      "iteration number: 3466\t training loss: 0.0385\tvalidation loss: 0.1141\t validation accuracy: 0.9600\n",
      "iteration number: 3467\t training loss: 0.0386\tvalidation loss: 0.1149\t validation accuracy: 0.9622\n",
      "iteration number: 3468\t training loss: 0.0381\tvalidation loss: 0.1137\t validation accuracy: 0.9600\n",
      "iteration number: 3469\t training loss: 0.0374\tvalidation loss: 0.1136\t validation accuracy: 0.9578\n",
      "iteration number: 3470\t training loss: 0.0376\tvalidation loss: 0.1136\t validation accuracy: 0.9600\n",
      "iteration number: 3471\t training loss: 0.0377\tvalidation loss: 0.1135\t validation accuracy: 0.9622\n",
      "iteration number: 3472\t training loss: 0.0374\tvalidation loss: 0.1133\t validation accuracy: 0.9600\n",
      "iteration number: 3473\t training loss: 0.0374\tvalidation loss: 0.1143\t validation accuracy: 0.9600\n",
      "iteration number: 3474\t training loss: 0.0369\tvalidation loss: 0.1124\t validation accuracy: 0.9622\n",
      "iteration number: 3475\t training loss: 0.0372\tvalidation loss: 0.1115\t validation accuracy: 0.9622\n",
      "iteration number: 3476\t training loss: 0.0378\tvalidation loss: 0.1103\t validation accuracy: 0.9644\n",
      "iteration number: 3477\t training loss: 0.0373\tvalidation loss: 0.1133\t validation accuracy: 0.9600\n",
      "iteration number: 3478\t training loss: 0.0373\tvalidation loss: 0.1148\t validation accuracy: 0.9600\n",
      "iteration number: 3479\t training loss: 0.0371\tvalidation loss: 0.1136\t validation accuracy: 0.9622\n",
      "iteration number: 3480\t training loss: 0.0368\tvalidation loss: 0.1138\t validation accuracy: 0.9600\n",
      "iteration number: 3481\t training loss: 0.0371\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 3482\t training loss: 0.0378\tvalidation loss: 0.1168\t validation accuracy: 0.9644\n",
      "iteration number: 3483\t training loss: 0.0383\tvalidation loss: 0.1185\t validation accuracy: 0.9622\n",
      "iteration number: 3484\t training loss: 0.0368\tvalidation loss: 0.1135\t validation accuracy: 0.9622\n",
      "iteration number: 3485\t training loss: 0.0371\tvalidation loss: 0.1144\t validation accuracy: 0.9600\n",
      "iteration number: 3486\t training loss: 0.0371\tvalidation loss: 0.1164\t validation accuracy: 0.9622\n",
      "iteration number: 3487\t training loss: 0.0369\tvalidation loss: 0.1115\t validation accuracy: 0.9622\n",
      "iteration number: 3488\t training loss: 0.0370\tvalidation loss: 0.1122\t validation accuracy: 0.9600\n",
      "iteration number: 3489\t training loss: 0.0368\tvalidation loss: 0.1133\t validation accuracy: 0.9600\n",
      "iteration number: 3490\t training loss: 0.0370\tvalidation loss: 0.1149\t validation accuracy: 0.9600\n",
      "iteration number: 3491\t training loss: 0.0370\tvalidation loss: 0.1146\t validation accuracy: 0.9600\n",
      "iteration number: 3492\t training loss: 0.0370\tvalidation loss: 0.1140\t validation accuracy: 0.9622\n",
      "iteration number: 3493\t training loss: 0.0369\tvalidation loss: 0.1141\t validation accuracy: 0.9600\n",
      "iteration number: 3494\t training loss: 0.0368\tvalidation loss: 0.1149\t validation accuracy: 0.9600\n",
      "iteration number: 3495\t training loss: 0.0369\tvalidation loss: 0.1156\t validation accuracy: 0.9600\n",
      "iteration number: 3496\t training loss: 0.0368\tvalidation loss: 0.1144\t validation accuracy: 0.9600\n",
      "iteration number: 3497\t training loss: 0.0368\tvalidation loss: 0.1141\t validation accuracy: 0.9622\n",
      "iteration number: 3498\t training loss: 0.0365\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 3499\t training loss: 0.0367\tvalidation loss: 0.1142\t validation accuracy: 0.9622\n",
      "iteration number: 3500\t training loss: 0.0372\tvalidation loss: 0.1131\t validation accuracy: 0.9622\n",
      "iteration number: 3501\t training loss: 0.0369\tvalidation loss: 0.1134\t validation accuracy: 0.9644\n",
      "iteration number: 3502\t training loss: 0.0364\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 3503\t training loss: 0.0365\tvalidation loss: 0.1117\t validation accuracy: 0.9644\n",
      "iteration number: 3504\t training loss: 0.0370\tvalidation loss: 0.1126\t validation accuracy: 0.9667\n",
      "iteration number: 3505\t training loss: 0.0371\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 3506\t training loss: 0.0368\tvalidation loss: 0.1141\t validation accuracy: 0.9622\n",
      "iteration number: 3507\t training loss: 0.0370\tvalidation loss: 0.1143\t validation accuracy: 0.9622\n",
      "iteration number: 3508\t training loss: 0.0371\tvalidation loss: 0.1142\t validation accuracy: 0.9622\n",
      "iteration number: 3509\t training loss: 0.0369\tvalidation loss: 0.1136\t validation accuracy: 0.9644\n",
      "iteration number: 3510\t training loss: 0.0368\tvalidation loss: 0.1143\t validation accuracy: 0.9622\n",
      "iteration number: 3511\t training loss: 0.0369\tvalidation loss: 0.1135\t validation accuracy: 0.9622\n",
      "iteration number: 3512\t training loss: 0.0371\tvalidation loss: 0.1149\t validation accuracy: 0.9622\n",
      "iteration number: 3513\t training loss: 0.0370\tvalidation loss: 0.1131\t validation accuracy: 0.9667\n",
      "iteration number: 3514\t training loss: 0.0372\tvalidation loss: 0.1107\t validation accuracy: 0.9667\n",
      "iteration number: 3515\t training loss: 0.0369\tvalidation loss: 0.1104\t validation accuracy: 0.9689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3516\t training loss: 0.0365\tvalidation loss: 0.1119\t validation accuracy: 0.9622\n",
      "iteration number: 3517\t training loss: 0.0368\tvalidation loss: 0.1134\t validation accuracy: 0.9622\n",
      "iteration number: 3518\t training loss: 0.0363\tvalidation loss: 0.1132\t validation accuracy: 0.9622\n",
      "iteration number: 3519\t training loss: 0.0370\tvalidation loss: 0.1139\t validation accuracy: 0.9644\n",
      "iteration number: 3520\t training loss: 0.0366\tvalidation loss: 0.1137\t validation accuracy: 0.9622\n",
      "iteration number: 3521\t training loss: 0.0367\tvalidation loss: 0.1159\t validation accuracy: 0.9578\n",
      "iteration number: 3522\t training loss: 0.0372\tvalidation loss: 0.1163\t validation accuracy: 0.9600\n",
      "iteration number: 3523\t training loss: 0.0372\tvalidation loss: 0.1143\t validation accuracy: 0.9622\n",
      "iteration number: 3524\t training loss: 0.0371\tvalidation loss: 0.1151\t validation accuracy: 0.9622\n",
      "iteration number: 3525\t training loss: 0.0371\tvalidation loss: 0.1158\t validation accuracy: 0.9578\n",
      "iteration number: 3526\t training loss: 0.0372\tvalidation loss: 0.1167\t validation accuracy: 0.9600\n",
      "iteration number: 3527\t training loss: 0.0381\tvalidation loss: 0.1196\t validation accuracy: 0.9622\n",
      "iteration number: 3528\t training loss: 0.0372\tvalidation loss: 0.1178\t validation accuracy: 0.9600\n",
      "iteration number: 3529\t training loss: 0.0371\tvalidation loss: 0.1180\t validation accuracy: 0.9600\n",
      "iteration number: 3530\t training loss: 0.0367\tvalidation loss: 0.1165\t validation accuracy: 0.9578\n",
      "iteration number: 3531\t training loss: 0.0367\tvalidation loss: 0.1162\t validation accuracy: 0.9578\n",
      "iteration number: 3532\t training loss: 0.0371\tvalidation loss: 0.1183\t validation accuracy: 0.9622\n",
      "iteration number: 3533\t training loss: 0.0370\tvalidation loss: 0.1172\t validation accuracy: 0.9600\n",
      "iteration number: 3534\t training loss: 0.0370\tvalidation loss: 0.1173\t validation accuracy: 0.9600\n",
      "iteration number: 3535\t training loss: 0.0375\tvalidation loss: 0.1194\t validation accuracy: 0.9578\n",
      "iteration number: 3536\t training loss: 0.0383\tvalidation loss: 0.1217\t validation accuracy: 0.9600\n",
      "iteration number: 3537\t training loss: 0.0383\tvalidation loss: 0.1215\t validation accuracy: 0.9622\n",
      "iteration number: 3538\t training loss: 0.0364\tvalidation loss: 0.1149\t validation accuracy: 0.9600\n",
      "iteration number: 3539\t training loss: 0.0364\tvalidation loss: 0.1148\t validation accuracy: 0.9600\n",
      "iteration number: 3540\t training loss: 0.0371\tvalidation loss: 0.1151\t validation accuracy: 0.9644\n",
      "iteration number: 3541\t training loss: 0.0371\tvalidation loss: 0.1168\t validation accuracy: 0.9622\n",
      "iteration number: 3542\t training loss: 0.0376\tvalidation loss: 0.1188\t validation accuracy: 0.9600\n",
      "iteration number: 3543\t training loss: 0.0369\tvalidation loss: 0.1176\t validation accuracy: 0.9622\n",
      "iteration number: 3544\t training loss: 0.0366\tvalidation loss: 0.1168\t validation accuracy: 0.9622\n",
      "iteration number: 3545\t training loss: 0.0364\tvalidation loss: 0.1150\t validation accuracy: 0.9622\n",
      "iteration number: 3546\t training loss: 0.0367\tvalidation loss: 0.1162\t validation accuracy: 0.9622\n",
      "iteration number: 3547\t training loss: 0.0370\tvalidation loss: 0.1169\t validation accuracy: 0.9600\n",
      "iteration number: 3548\t training loss: 0.0367\tvalidation loss: 0.1142\t validation accuracy: 0.9622\n",
      "iteration number: 3549\t training loss: 0.0364\tvalidation loss: 0.1139\t validation accuracy: 0.9622\n",
      "iteration number: 3550\t training loss: 0.0363\tvalidation loss: 0.1123\t validation accuracy: 0.9644\n",
      "iteration number: 3551\t training loss: 0.0363\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 3552\t training loss: 0.0364\tvalidation loss: 0.1148\t validation accuracy: 0.9644\n",
      "iteration number: 3553\t training loss: 0.0368\tvalidation loss: 0.1126\t validation accuracy: 0.9644\n",
      "iteration number: 3554\t training loss: 0.0368\tvalidation loss: 0.1136\t validation accuracy: 0.9644\n",
      "iteration number: 3555\t training loss: 0.0367\tvalidation loss: 0.1134\t validation accuracy: 0.9644\n",
      "iteration number: 3556\t training loss: 0.0369\tvalidation loss: 0.1141\t validation accuracy: 0.9622\n",
      "iteration number: 3557\t training loss: 0.0372\tvalidation loss: 0.1150\t validation accuracy: 0.9622\n",
      "iteration number: 3558\t training loss: 0.0361\tvalidation loss: 0.1098\t validation accuracy: 0.9644\n",
      "iteration number: 3559\t training loss: 0.0360\tvalidation loss: 0.1102\t validation accuracy: 0.9644\n",
      "iteration number: 3560\t training loss: 0.0360\tvalidation loss: 0.1110\t validation accuracy: 0.9667\n",
      "iteration number: 3561\t training loss: 0.0359\tvalidation loss: 0.1109\t validation accuracy: 0.9622\n",
      "iteration number: 3562\t training loss: 0.0361\tvalidation loss: 0.1110\t validation accuracy: 0.9622\n",
      "iteration number: 3563\t training loss: 0.0361\tvalidation loss: 0.1119\t validation accuracy: 0.9622\n",
      "iteration number: 3564\t training loss: 0.0359\tvalidation loss: 0.1126\t validation accuracy: 0.9622\n",
      "iteration number: 3565\t training loss: 0.0361\tvalidation loss: 0.1154\t validation accuracy: 0.9622\n",
      "iteration number: 3566\t training loss: 0.0362\tvalidation loss: 0.1167\t validation accuracy: 0.9622\n",
      "iteration number: 3567\t training loss: 0.0360\tvalidation loss: 0.1150\t validation accuracy: 0.9578\n",
      "iteration number: 3568\t training loss: 0.0360\tvalidation loss: 0.1125\t validation accuracy: 0.9644\n",
      "iteration number: 3569\t training loss: 0.0362\tvalidation loss: 0.1119\t validation accuracy: 0.9622\n",
      "iteration number: 3570\t training loss: 0.0376\tvalidation loss: 0.1122\t validation accuracy: 0.9644\n",
      "iteration number: 3571\t training loss: 0.0375\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 3572\t training loss: 0.0368\tvalidation loss: 0.1149\t validation accuracy: 0.9644\n",
      "iteration number: 3573\t training loss: 0.0361\tvalidation loss: 0.1162\t validation accuracy: 0.9622\n",
      "iteration number: 3574\t training loss: 0.0362\tvalidation loss: 0.1163\t validation accuracy: 0.9622\n",
      "iteration number: 3575\t training loss: 0.0367\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 3576\t training loss: 0.0366\tvalidation loss: 0.1150\t validation accuracy: 0.9644\n",
      "iteration number: 3577\t training loss: 0.0369\tvalidation loss: 0.1121\t validation accuracy: 0.9622\n",
      "iteration number: 3578\t training loss: 0.0361\tvalidation loss: 0.1109\t validation accuracy: 0.9644\n",
      "iteration number: 3579\t training loss: 0.0376\tvalidation loss: 0.1114\t validation accuracy: 0.9667\n",
      "iteration number: 3580\t training loss: 0.0373\tvalidation loss: 0.1119\t validation accuracy: 0.9644\n",
      "iteration number: 3581\t training loss: 0.0370\tvalidation loss: 0.1152\t validation accuracy: 0.9622\n",
      "iteration number: 3582\t training loss: 0.0365\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 3583\t training loss: 0.0363\tvalidation loss: 0.1136\t validation accuracy: 0.9622\n",
      "iteration number: 3584\t training loss: 0.0356\tvalidation loss: 0.1126\t validation accuracy: 0.9644\n",
      "iteration number: 3585\t training loss: 0.0359\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 3586\t training loss: 0.0354\tvalidation loss: 0.1114\t validation accuracy: 0.9622\n",
      "iteration number: 3587\t training loss: 0.0356\tvalidation loss: 0.1109\t validation accuracy: 0.9622\n",
      "iteration number: 3588\t training loss: 0.0354\tvalidation loss: 0.1097\t validation accuracy: 0.9644\n",
      "iteration number: 3589\t training loss: 0.0354\tvalidation loss: 0.1119\t validation accuracy: 0.9644\n",
      "iteration number: 3590\t training loss: 0.0353\tvalidation loss: 0.1112\t validation accuracy: 0.9644\n",
      "iteration number: 3591\t training loss: 0.0359\tvalidation loss: 0.1085\t validation accuracy: 0.9644\n",
      "iteration number: 3592\t training loss: 0.0359\tvalidation loss: 0.1090\t validation accuracy: 0.9689\n",
      "iteration number: 3593\t training loss: 0.0357\tvalidation loss: 0.1105\t validation accuracy: 0.9622\n",
      "iteration number: 3594\t training loss: 0.0358\tvalidation loss: 0.1124\t validation accuracy: 0.9622\n",
      "iteration number: 3595\t training loss: 0.0354\tvalidation loss: 0.1108\t validation accuracy: 0.9644\n",
      "iteration number: 3596\t training loss: 0.0355\tvalidation loss: 0.1095\t validation accuracy: 0.9644\n",
      "iteration number: 3597\t training loss: 0.0357\tvalidation loss: 0.1078\t validation accuracy: 0.9667\n",
      "iteration number: 3598\t training loss: 0.0354\tvalidation loss: 0.1090\t validation accuracy: 0.9667\n",
      "iteration number: 3599\t training loss: 0.0360\tvalidation loss: 0.1073\t validation accuracy: 0.9644\n",
      "iteration number: 3600\t training loss: 0.0363\tvalidation loss: 0.1082\t validation accuracy: 0.9622\n",
      "iteration number: 3601\t training loss: 0.0369\tvalidation loss: 0.1083\t validation accuracy: 0.9622\n",
      "iteration number: 3602\t training loss: 0.0361\tvalidation loss: 0.1082\t validation accuracy: 0.9622\n",
      "iteration number: 3603\t training loss: 0.0360\tvalidation loss: 0.1079\t validation accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3604\t training loss: 0.0356\tvalidation loss: 0.1088\t validation accuracy: 0.9644\n",
      "iteration number: 3605\t training loss: 0.0361\tvalidation loss: 0.1079\t validation accuracy: 0.9622\n",
      "iteration number: 3606\t training loss: 0.0364\tvalidation loss: 0.1073\t validation accuracy: 0.9644\n",
      "iteration number: 3607\t training loss: 0.0368\tvalidation loss: 0.1077\t validation accuracy: 0.9622\n",
      "iteration number: 3608\t training loss: 0.0359\tvalidation loss: 0.1094\t validation accuracy: 0.9622\n",
      "iteration number: 3609\t training loss: 0.0358\tvalidation loss: 0.1078\t validation accuracy: 0.9667\n",
      "iteration number: 3610\t training loss: 0.0356\tvalidation loss: 0.1093\t validation accuracy: 0.9644\n",
      "iteration number: 3611\t training loss: 0.0355\tvalidation loss: 0.1085\t validation accuracy: 0.9644\n",
      "iteration number: 3612\t training loss: 0.0354\tvalidation loss: 0.1082\t validation accuracy: 0.9667\n",
      "iteration number: 3613\t training loss: 0.0357\tvalidation loss: 0.1076\t validation accuracy: 0.9667\n",
      "iteration number: 3614\t training loss: 0.0360\tvalidation loss: 0.1068\t validation accuracy: 0.9689\n",
      "iteration number: 3615\t training loss: 0.0370\tvalidation loss: 0.1111\t validation accuracy: 0.9644\n",
      "iteration number: 3616\t training loss: 0.0368\tvalidation loss: 0.1093\t validation accuracy: 0.9689\n",
      "iteration number: 3617\t training loss: 0.0355\tvalidation loss: 0.1084\t validation accuracy: 0.9644\n",
      "iteration number: 3618\t training loss: 0.0352\tvalidation loss: 0.1092\t validation accuracy: 0.9644\n",
      "iteration number: 3619\t training loss: 0.0351\tvalidation loss: 0.1101\t validation accuracy: 0.9644\n",
      "iteration number: 3620\t training loss: 0.0351\tvalidation loss: 0.1095\t validation accuracy: 0.9644\n",
      "iteration number: 3621\t training loss: 0.0352\tvalidation loss: 0.1108\t validation accuracy: 0.9644\n",
      "iteration number: 3622\t training loss: 0.0351\tvalidation loss: 0.1109\t validation accuracy: 0.9644\n",
      "iteration number: 3623\t training loss: 0.0352\tvalidation loss: 0.1119\t validation accuracy: 0.9644\n",
      "iteration number: 3624\t training loss: 0.0351\tvalidation loss: 0.1122\t validation accuracy: 0.9622\n",
      "iteration number: 3625\t training loss: 0.0352\tvalidation loss: 0.1120\t validation accuracy: 0.9622\n",
      "iteration number: 3626\t training loss: 0.0350\tvalidation loss: 0.1117\t validation accuracy: 0.9622\n",
      "iteration number: 3627\t training loss: 0.0352\tvalidation loss: 0.1104\t validation accuracy: 0.9644\n",
      "iteration number: 3628\t training loss: 0.0360\tvalidation loss: 0.1079\t validation accuracy: 0.9667\n",
      "iteration number: 3629\t training loss: 0.0354\tvalidation loss: 0.1087\t validation accuracy: 0.9667\n",
      "iteration number: 3630\t training loss: 0.0360\tvalidation loss: 0.1089\t validation accuracy: 0.9667\n",
      "iteration number: 3631\t training loss: 0.0357\tvalidation loss: 0.1092\t validation accuracy: 0.9667\n",
      "iteration number: 3632\t training loss: 0.0359\tvalidation loss: 0.1086\t validation accuracy: 0.9667\n",
      "iteration number: 3633\t training loss: 0.0360\tvalidation loss: 0.1083\t validation accuracy: 0.9711\n",
      "iteration number: 3634\t training loss: 0.0361\tvalidation loss: 0.1091\t validation accuracy: 0.9689\n",
      "iteration number: 3635\t training loss: 0.0358\tvalidation loss: 0.1103\t validation accuracy: 0.9689\n",
      "iteration number: 3636\t training loss: 0.0356\tvalidation loss: 0.1118\t validation accuracy: 0.9689\n",
      "iteration number: 3637\t training loss: 0.0355\tvalidation loss: 0.1119\t validation accuracy: 0.9667\n",
      "iteration number: 3638\t training loss: 0.0355\tvalidation loss: 0.1113\t validation accuracy: 0.9667\n",
      "iteration number: 3639\t training loss: 0.0351\tvalidation loss: 0.1129\t validation accuracy: 0.9622\n",
      "iteration number: 3640\t training loss: 0.0352\tvalidation loss: 0.1126\t validation accuracy: 0.9622\n",
      "iteration number: 3641\t training loss: 0.0356\tvalidation loss: 0.1130\t validation accuracy: 0.9600\n",
      "iteration number: 3642\t training loss: 0.0356\tvalidation loss: 0.1136\t validation accuracy: 0.9600\n",
      "iteration number: 3643\t training loss: 0.0359\tvalidation loss: 0.1149\t validation accuracy: 0.9600\n",
      "iteration number: 3644\t training loss: 0.0360\tvalidation loss: 0.1164\t validation accuracy: 0.9600\n",
      "iteration number: 3645\t training loss: 0.0360\tvalidation loss: 0.1148\t validation accuracy: 0.9644\n",
      "iteration number: 3646\t training loss: 0.0356\tvalidation loss: 0.1146\t validation accuracy: 0.9622\n",
      "iteration number: 3647\t training loss: 0.0357\tvalidation loss: 0.1109\t validation accuracy: 0.9644\n",
      "iteration number: 3648\t training loss: 0.0353\tvalidation loss: 0.1116\t validation accuracy: 0.9644\n",
      "iteration number: 3649\t training loss: 0.0355\tvalidation loss: 0.1108\t validation accuracy: 0.9689\n",
      "iteration number: 3650\t training loss: 0.0351\tvalidation loss: 0.1106\t validation accuracy: 0.9644\n",
      "iteration number: 3651\t training loss: 0.0357\tvalidation loss: 0.1089\t validation accuracy: 0.9667\n",
      "iteration number: 3652\t training loss: 0.0353\tvalidation loss: 0.1105\t validation accuracy: 0.9644\n",
      "iteration number: 3653\t training loss: 0.0352\tvalidation loss: 0.1104\t validation accuracy: 0.9644\n",
      "iteration number: 3654\t training loss: 0.0351\tvalidation loss: 0.1112\t validation accuracy: 0.9644\n",
      "iteration number: 3655\t training loss: 0.0358\tvalidation loss: 0.1093\t validation accuracy: 0.9689\n",
      "iteration number: 3656\t training loss: 0.0365\tvalidation loss: 0.1102\t validation accuracy: 0.9622\n",
      "iteration number: 3657\t training loss: 0.0369\tvalidation loss: 0.1104\t validation accuracy: 0.9622\n",
      "iteration number: 3658\t training loss: 0.0363\tvalidation loss: 0.1122\t validation accuracy: 0.9600\n",
      "iteration number: 3659\t training loss: 0.0367\tvalidation loss: 0.1114\t validation accuracy: 0.9600\n",
      "iteration number: 3660\t training loss: 0.0364\tvalidation loss: 0.1117\t validation accuracy: 0.9600\n",
      "iteration number: 3661\t training loss: 0.0348\tvalidation loss: 0.1123\t validation accuracy: 0.9667\n",
      "iteration number: 3662\t training loss: 0.0347\tvalidation loss: 0.1134\t validation accuracy: 0.9644\n",
      "iteration number: 3663\t training loss: 0.0349\tvalidation loss: 0.1116\t validation accuracy: 0.9644\n",
      "iteration number: 3664\t training loss: 0.0352\tvalidation loss: 0.1134\t validation accuracy: 0.9644\n",
      "iteration number: 3665\t training loss: 0.0349\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 3666\t training loss: 0.0350\tvalidation loss: 0.1174\t validation accuracy: 0.9622\n",
      "iteration number: 3667\t training loss: 0.0351\tvalidation loss: 0.1166\t validation accuracy: 0.9622\n",
      "iteration number: 3668\t training loss: 0.0355\tvalidation loss: 0.1147\t validation accuracy: 0.9644\n",
      "iteration number: 3669\t training loss: 0.0363\tvalidation loss: 0.1130\t validation accuracy: 0.9667\n",
      "iteration number: 3670\t training loss: 0.0360\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 3671\t training loss: 0.0368\tvalidation loss: 0.1105\t validation accuracy: 0.9667\n",
      "iteration number: 3672\t training loss: 0.0371\tvalidation loss: 0.1096\t validation accuracy: 0.9667\n",
      "iteration number: 3673\t training loss: 0.0363\tvalidation loss: 0.1103\t validation accuracy: 0.9667\n",
      "iteration number: 3674\t training loss: 0.0350\tvalidation loss: 0.1120\t validation accuracy: 0.9644\n",
      "iteration number: 3675\t training loss: 0.0346\tvalidation loss: 0.1137\t validation accuracy: 0.9644\n",
      "iteration number: 3676\t training loss: 0.0348\tvalidation loss: 0.1154\t validation accuracy: 0.9622\n",
      "iteration number: 3677\t training loss: 0.0348\tvalidation loss: 0.1155\t validation accuracy: 0.9622\n",
      "iteration number: 3678\t training loss: 0.0358\tvalidation loss: 0.1125\t validation accuracy: 0.9667\n",
      "iteration number: 3679\t training loss: 0.0356\tvalidation loss: 0.1123\t validation accuracy: 0.9644\n",
      "iteration number: 3680\t training loss: 0.0355\tvalidation loss: 0.1128\t validation accuracy: 0.9667\n",
      "iteration number: 3681\t training loss: 0.0361\tvalidation loss: 0.1129\t validation accuracy: 0.9667\n",
      "iteration number: 3682\t training loss: 0.0358\tvalidation loss: 0.1150\t validation accuracy: 0.9644\n",
      "iteration number: 3683\t training loss: 0.0363\tvalidation loss: 0.1156\t validation accuracy: 0.9622\n",
      "iteration number: 3684\t training loss: 0.0366\tvalidation loss: 0.1158\t validation accuracy: 0.9622\n",
      "iteration number: 3685\t training loss: 0.0362\tvalidation loss: 0.1141\t validation accuracy: 0.9622\n",
      "iteration number: 3686\t training loss: 0.0364\tvalidation loss: 0.1145\t validation accuracy: 0.9644\n",
      "iteration number: 3687\t training loss: 0.0359\tvalidation loss: 0.1148\t validation accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3688\t training loss: 0.0355\tvalidation loss: 0.1167\t validation accuracy: 0.9622\n",
      "iteration number: 3689\t training loss: 0.0368\tvalidation loss: 0.1169\t validation accuracy: 0.9622\n",
      "iteration number: 3690\t training loss: 0.0360\tvalidation loss: 0.1175\t validation accuracy: 0.9622\n",
      "iteration number: 3691\t training loss: 0.0357\tvalidation loss: 0.1180\t validation accuracy: 0.9600\n",
      "iteration number: 3692\t training loss: 0.0359\tvalidation loss: 0.1202\t validation accuracy: 0.9578\n",
      "iteration number: 3693\t training loss: 0.0348\tvalidation loss: 0.1163\t validation accuracy: 0.9600\n",
      "iteration number: 3694\t training loss: 0.0347\tvalidation loss: 0.1161\t validation accuracy: 0.9622\n",
      "iteration number: 3695\t training loss: 0.0346\tvalidation loss: 0.1134\t validation accuracy: 0.9600\n",
      "iteration number: 3696\t training loss: 0.0347\tvalidation loss: 0.1139\t validation accuracy: 0.9600\n",
      "iteration number: 3697\t training loss: 0.0354\tvalidation loss: 0.1141\t validation accuracy: 0.9600\n",
      "iteration number: 3698\t training loss: 0.0368\tvalidation loss: 0.1136\t validation accuracy: 0.9600\n",
      "iteration number: 3699\t training loss: 0.0357\tvalidation loss: 0.1135\t validation accuracy: 0.9600\n",
      "iteration number: 3700\t training loss: 0.0358\tvalidation loss: 0.1133\t validation accuracy: 0.9600\n",
      "iteration number: 3701\t training loss: 0.0348\tvalidation loss: 0.1128\t validation accuracy: 0.9622\n",
      "iteration number: 3702\t training loss: 0.0344\tvalidation loss: 0.1143\t validation accuracy: 0.9600\n",
      "iteration number: 3703\t training loss: 0.0342\tvalidation loss: 0.1143\t validation accuracy: 0.9600\n",
      "iteration number: 3704\t training loss: 0.0343\tvalidation loss: 0.1144\t validation accuracy: 0.9622\n",
      "iteration number: 3705\t training loss: 0.0346\tvalidation loss: 0.1163\t validation accuracy: 0.9622\n",
      "iteration number: 3706\t training loss: 0.0344\tvalidation loss: 0.1149\t validation accuracy: 0.9622\n",
      "iteration number: 3707\t training loss: 0.0344\tvalidation loss: 0.1141\t validation accuracy: 0.9644\n",
      "iteration number: 3708\t training loss: 0.0340\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 3709\t training loss: 0.0347\tvalidation loss: 0.1145\t validation accuracy: 0.9622\n",
      "iteration number: 3710\t training loss: 0.0347\tvalidation loss: 0.1146\t validation accuracy: 0.9622\n",
      "iteration number: 3711\t training loss: 0.0348\tvalidation loss: 0.1116\t validation accuracy: 0.9644\n",
      "iteration number: 3712\t training loss: 0.0345\tvalidation loss: 0.1114\t validation accuracy: 0.9622\n",
      "iteration number: 3713\t training loss: 0.0342\tvalidation loss: 0.1128\t validation accuracy: 0.9600\n",
      "iteration number: 3714\t training loss: 0.0340\tvalidation loss: 0.1105\t validation accuracy: 0.9667\n",
      "iteration number: 3715\t training loss: 0.0342\tvalidation loss: 0.1103\t validation accuracy: 0.9667\n",
      "iteration number: 3716\t training loss: 0.0345\tvalidation loss: 0.1103\t validation accuracy: 0.9667\n",
      "iteration number: 3717\t training loss: 0.0344\tvalidation loss: 0.1097\t validation accuracy: 0.9667\n",
      "iteration number: 3718\t training loss: 0.0345\tvalidation loss: 0.1084\t validation accuracy: 0.9667\n",
      "iteration number: 3719\t training loss: 0.0349\tvalidation loss: 0.1102\t validation accuracy: 0.9622\n",
      "iteration number: 3720\t training loss: 0.0344\tvalidation loss: 0.1108\t validation accuracy: 0.9622\n",
      "iteration number: 3721\t training loss: 0.0343\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n",
      "iteration number: 3722\t training loss: 0.0340\tvalidation loss: 0.1109\t validation accuracy: 0.9644\n",
      "iteration number: 3723\t training loss: 0.0338\tvalidation loss: 0.1102\t validation accuracy: 0.9667\n",
      "iteration number: 3724\t training loss: 0.0340\tvalidation loss: 0.1104\t validation accuracy: 0.9667\n",
      "iteration number: 3725\t training loss: 0.0338\tvalidation loss: 0.1104\t validation accuracy: 0.9644\n",
      "iteration number: 3726\t training loss: 0.0339\tvalidation loss: 0.1126\t validation accuracy: 0.9667\n",
      "iteration number: 3727\t training loss: 0.0342\tvalidation loss: 0.1099\t validation accuracy: 0.9667\n",
      "iteration number: 3728\t training loss: 0.0342\tvalidation loss: 0.1120\t validation accuracy: 0.9667\n",
      "iteration number: 3729\t training loss: 0.0335\tvalidation loss: 0.1093\t validation accuracy: 0.9667\n",
      "iteration number: 3730\t training loss: 0.0335\tvalidation loss: 0.1093\t validation accuracy: 0.9667\n",
      "iteration number: 3731\t training loss: 0.0336\tvalidation loss: 0.1114\t validation accuracy: 0.9667\n",
      "iteration number: 3732\t training loss: 0.0341\tvalidation loss: 0.1132\t validation accuracy: 0.9667\n",
      "iteration number: 3733\t training loss: 0.0340\tvalidation loss: 0.1129\t validation accuracy: 0.9667\n",
      "iteration number: 3734\t training loss: 0.0344\tvalidation loss: 0.1166\t validation accuracy: 0.9644\n",
      "iteration number: 3735\t training loss: 0.0349\tvalidation loss: 0.1183\t validation accuracy: 0.9644\n",
      "iteration number: 3736\t training loss: 0.0349\tvalidation loss: 0.1182\t validation accuracy: 0.9644\n",
      "iteration number: 3737\t training loss: 0.0356\tvalidation loss: 0.1215\t validation accuracy: 0.9644\n",
      "iteration number: 3738\t training loss: 0.0356\tvalidation loss: 0.1221\t validation accuracy: 0.9644\n",
      "iteration number: 3739\t training loss: 0.0356\tvalidation loss: 0.1226\t validation accuracy: 0.9644\n",
      "iteration number: 3740\t training loss: 0.0341\tvalidation loss: 0.1173\t validation accuracy: 0.9644\n",
      "iteration number: 3741\t training loss: 0.0345\tvalidation loss: 0.1169\t validation accuracy: 0.9667\n",
      "iteration number: 3742\t training loss: 0.0343\tvalidation loss: 0.1167\t validation accuracy: 0.9667\n",
      "iteration number: 3743\t training loss: 0.0338\tvalidation loss: 0.1157\t validation accuracy: 0.9667\n",
      "iteration number: 3744\t training loss: 0.0339\tvalidation loss: 0.1155\t validation accuracy: 0.9667\n",
      "iteration number: 3745\t training loss: 0.0348\tvalidation loss: 0.1136\t validation accuracy: 0.9644\n",
      "iteration number: 3746\t training loss: 0.0343\tvalidation loss: 0.1137\t validation accuracy: 0.9644\n",
      "iteration number: 3747\t training loss: 0.0350\tvalidation loss: 0.1141\t validation accuracy: 0.9644\n",
      "iteration number: 3748\t training loss: 0.0343\tvalidation loss: 0.1120\t validation accuracy: 0.9667\n",
      "iteration number: 3749\t training loss: 0.0346\tvalidation loss: 0.1154\t validation accuracy: 0.9644\n",
      "iteration number: 3750\t training loss: 0.0340\tvalidation loss: 0.1164\t validation accuracy: 0.9667\n",
      "iteration number: 3751\t training loss: 0.0342\tvalidation loss: 0.1181\t validation accuracy: 0.9622\n",
      "iteration number: 3752\t training loss: 0.0345\tvalidation loss: 0.1183\t validation accuracy: 0.9667\n",
      "iteration number: 3753\t training loss: 0.0343\tvalidation loss: 0.1144\t validation accuracy: 0.9667\n",
      "iteration number: 3754\t training loss: 0.0338\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 3755\t training loss: 0.0338\tvalidation loss: 0.1088\t validation accuracy: 0.9667\n",
      "iteration number: 3756\t training loss: 0.0347\tvalidation loss: 0.1072\t validation accuracy: 0.9644\n",
      "iteration number: 3757\t training loss: 0.0348\tvalidation loss: 0.1068\t validation accuracy: 0.9667\n",
      "iteration number: 3758\t training loss: 0.0349\tvalidation loss: 0.1058\t validation accuracy: 0.9667\n",
      "iteration number: 3759\t training loss: 0.0350\tvalidation loss: 0.1090\t validation accuracy: 0.9667\n",
      "iteration number: 3760\t training loss: 0.0350\tvalidation loss: 0.1092\t validation accuracy: 0.9667\n",
      "iteration number: 3761\t training loss: 0.0351\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 3762\t training loss: 0.0344\tvalidation loss: 0.1137\t validation accuracy: 0.9667\n",
      "iteration number: 3763\t training loss: 0.0335\tvalidation loss: 0.1101\t validation accuracy: 0.9667\n",
      "iteration number: 3764\t training loss: 0.0334\tvalidation loss: 0.1103\t validation accuracy: 0.9667\n",
      "iteration number: 3765\t training loss: 0.0335\tvalidation loss: 0.1102\t validation accuracy: 0.9667\n",
      "iteration number: 3766\t training loss: 0.0340\tvalidation loss: 0.1171\t validation accuracy: 0.9644\n",
      "iteration number: 3767\t training loss: 0.0340\tvalidation loss: 0.1176\t validation accuracy: 0.9644\n",
      "iteration number: 3768\t training loss: 0.0346\tvalidation loss: 0.1187\t validation accuracy: 0.9644\n",
      "iteration number: 3769\t training loss: 0.0348\tvalidation loss: 0.1193\t validation accuracy: 0.9644\n",
      "iteration number: 3770\t training loss: 0.0353\tvalidation loss: 0.1211\t validation accuracy: 0.9644\n",
      "iteration number: 3771\t training loss: 0.0348\tvalidation loss: 0.1198\t validation accuracy: 0.9644\n",
      "iteration number: 3772\t training loss: 0.0347\tvalidation loss: 0.1183\t validation accuracy: 0.9644\n",
      "iteration number: 3773\t training loss: 0.0349\tvalidation loss: 0.1178\t validation accuracy: 0.9644\n",
      "iteration number: 3774\t training loss: 0.0340\tvalidation loss: 0.1129\t validation accuracy: 0.9644\n",
      "iteration number: 3775\t training loss: 0.0339\tvalidation loss: 0.1139\t validation accuracy: 0.9644\n",
      "iteration number: 3776\t training loss: 0.0347\tvalidation loss: 0.1139\t validation accuracy: 0.9622\n",
      "iteration number: 3777\t training loss: 0.0338\tvalidation loss: 0.1121\t validation accuracy: 0.9644\n",
      "iteration number: 3778\t training loss: 0.0340\tvalidation loss: 0.1134\t validation accuracy: 0.9644\n",
      "iteration number: 3779\t training loss: 0.0342\tvalidation loss: 0.1125\t validation accuracy: 0.9644\n",
      "iteration number: 3780\t training loss: 0.0340\tvalidation loss: 0.1136\t validation accuracy: 0.9644\n",
      "iteration number: 3781\t training loss: 0.0337\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 3782\t training loss: 0.0332\tvalidation loss: 0.1104\t validation accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3783\t training loss: 0.0330\tvalidation loss: 0.1122\t validation accuracy: 0.9644\n",
      "iteration number: 3784\t training loss: 0.0335\tvalidation loss: 0.1154\t validation accuracy: 0.9667\n",
      "iteration number: 3785\t training loss: 0.0337\tvalidation loss: 0.1167\t validation accuracy: 0.9667\n",
      "iteration number: 3786\t training loss: 0.0336\tvalidation loss: 0.1157\t validation accuracy: 0.9667\n",
      "iteration number: 3787\t training loss: 0.0336\tvalidation loss: 0.1148\t validation accuracy: 0.9644\n",
      "iteration number: 3788\t training loss: 0.0330\tvalidation loss: 0.1106\t validation accuracy: 0.9667\n",
      "iteration number: 3789\t training loss: 0.0331\tvalidation loss: 0.1110\t validation accuracy: 0.9667\n",
      "iteration number: 3790\t training loss: 0.0333\tvalidation loss: 0.1118\t validation accuracy: 0.9644\n",
      "iteration number: 3791\t training loss: 0.0333\tvalidation loss: 0.1113\t validation accuracy: 0.9667\n",
      "iteration number: 3792\t training loss: 0.0331\tvalidation loss: 0.1092\t validation accuracy: 0.9667\n",
      "iteration number: 3793\t training loss: 0.0331\tvalidation loss: 0.1093\t validation accuracy: 0.9667\n",
      "iteration number: 3794\t training loss: 0.0331\tvalidation loss: 0.1080\t validation accuracy: 0.9689\n",
      "iteration number: 3795\t training loss: 0.0336\tvalidation loss: 0.1085\t validation accuracy: 0.9667\n",
      "iteration number: 3796\t training loss: 0.0332\tvalidation loss: 0.1100\t validation accuracy: 0.9667\n",
      "iteration number: 3797\t training loss: 0.0329\tvalidation loss: 0.1097\t validation accuracy: 0.9667\n",
      "iteration number: 3798\t training loss: 0.0329\tvalidation loss: 0.1095\t validation accuracy: 0.9667\n",
      "iteration number: 3799\t training loss: 0.0336\tvalidation loss: 0.1096\t validation accuracy: 0.9622\n",
      "iteration number: 3800\t training loss: 0.0328\tvalidation loss: 0.1105\t validation accuracy: 0.9644\n",
      "iteration number: 3801\t training loss: 0.0332\tvalidation loss: 0.1079\t validation accuracy: 0.9644\n",
      "iteration number: 3802\t training loss: 0.0331\tvalidation loss: 0.1092\t validation accuracy: 0.9644\n",
      "iteration number: 3803\t training loss: 0.0327\tvalidation loss: 0.1101\t validation accuracy: 0.9689\n",
      "iteration number: 3804\t training loss: 0.0327\tvalidation loss: 0.1108\t validation accuracy: 0.9689\n",
      "iteration number: 3805\t training loss: 0.0327\tvalidation loss: 0.1105\t validation accuracy: 0.9667\n",
      "iteration number: 3806\t training loss: 0.0330\tvalidation loss: 0.1126\t validation accuracy: 0.9667\n",
      "iteration number: 3807\t training loss: 0.0327\tvalidation loss: 0.1113\t validation accuracy: 0.9667\n",
      "iteration number: 3808\t training loss: 0.0329\tvalidation loss: 0.1104\t validation accuracy: 0.9667\n",
      "iteration number: 3809\t training loss: 0.0330\tvalidation loss: 0.1107\t validation accuracy: 0.9689\n",
      "iteration number: 3810\t training loss: 0.0328\tvalidation loss: 0.1125\t validation accuracy: 0.9667\n",
      "iteration number: 3811\t training loss: 0.0329\tvalidation loss: 0.1142\t validation accuracy: 0.9667\n",
      "iteration number: 3812\t training loss: 0.0329\tvalidation loss: 0.1142\t validation accuracy: 0.9667\n",
      "iteration number: 3813\t training loss: 0.0329\tvalidation loss: 0.1117\t validation accuracy: 0.9667\n",
      "iteration number: 3814\t training loss: 0.0328\tvalidation loss: 0.1090\t validation accuracy: 0.9644\n",
      "iteration number: 3815\t training loss: 0.0333\tvalidation loss: 0.1080\t validation accuracy: 0.9644\n",
      "iteration number: 3816\t training loss: 0.0332\tvalidation loss: 0.1085\t validation accuracy: 0.9644\n",
      "iteration number: 3817\t training loss: 0.0335\tvalidation loss: 0.1075\t validation accuracy: 0.9667\n",
      "iteration number: 3818\t training loss: 0.0343\tvalidation loss: 0.1067\t validation accuracy: 0.9644\n",
      "iteration number: 3819\t training loss: 0.0337\tvalidation loss: 0.1084\t validation accuracy: 0.9622\n",
      "iteration number: 3820\t training loss: 0.0336\tvalidation loss: 0.1095\t validation accuracy: 0.9622\n",
      "iteration number: 3821\t training loss: 0.0331\tvalidation loss: 0.1087\t validation accuracy: 0.9644\n",
      "iteration number: 3822\t training loss: 0.0327\tvalidation loss: 0.1103\t validation accuracy: 0.9622\n",
      "iteration number: 3823\t training loss: 0.0327\tvalidation loss: 0.1121\t validation accuracy: 0.9644\n",
      "iteration number: 3824\t training loss: 0.0325\tvalidation loss: 0.1110\t validation accuracy: 0.9644\n",
      "iteration number: 3825\t training loss: 0.0326\tvalidation loss: 0.1109\t validation accuracy: 0.9622\n",
      "iteration number: 3826\t training loss: 0.0326\tvalidation loss: 0.1130\t validation accuracy: 0.9667\n",
      "iteration number: 3827\t training loss: 0.0328\tvalidation loss: 0.1130\t validation accuracy: 0.9644\n",
      "iteration number: 3828\t training loss: 0.0327\tvalidation loss: 0.1139\t validation accuracy: 0.9622\n",
      "iteration number: 3829\t training loss: 0.0328\tvalidation loss: 0.1159\t validation accuracy: 0.9600\n",
      "iteration number: 3830\t training loss: 0.0328\tvalidation loss: 0.1165\t validation accuracy: 0.9600\n",
      "iteration number: 3831\t training loss: 0.0329\tvalidation loss: 0.1166\t validation accuracy: 0.9644\n",
      "iteration number: 3832\t training loss: 0.0328\tvalidation loss: 0.1161\t validation accuracy: 0.9644\n",
      "iteration number: 3833\t training loss: 0.0329\tvalidation loss: 0.1165\t validation accuracy: 0.9600\n",
      "iteration number: 3834\t training loss: 0.0335\tvalidation loss: 0.1188\t validation accuracy: 0.9622\n",
      "iteration number: 3835\t training loss: 0.0338\tvalidation loss: 0.1192\t validation accuracy: 0.9600\n",
      "iteration number: 3836\t training loss: 0.0337\tvalidation loss: 0.1190\t validation accuracy: 0.9622\n",
      "iteration number: 3837\t training loss: 0.0337\tvalidation loss: 0.1193\t validation accuracy: 0.9622\n",
      "iteration number: 3838\t training loss: 0.0326\tvalidation loss: 0.1158\t validation accuracy: 0.9644\n",
      "iteration number: 3839\t training loss: 0.0326\tvalidation loss: 0.1141\t validation accuracy: 0.9622\n",
      "iteration number: 3840\t training loss: 0.0326\tvalidation loss: 0.1159\t validation accuracy: 0.9622\n",
      "iteration number: 3841\t training loss: 0.0329\tvalidation loss: 0.1157\t validation accuracy: 0.9600\n",
      "iteration number: 3842\t training loss: 0.0345\tvalidation loss: 0.1164\t validation accuracy: 0.9600\n",
      "iteration number: 3843\t training loss: 0.0334\tvalidation loss: 0.1169\t validation accuracy: 0.9600\n",
      "iteration number: 3844\t training loss: 0.0334\tvalidation loss: 0.1170\t validation accuracy: 0.9600\n",
      "iteration number: 3845\t training loss: 0.0332\tvalidation loss: 0.1178\t validation accuracy: 0.9622\n",
      "iteration number: 3846\t training loss: 0.0331\tvalidation loss: 0.1186\t validation accuracy: 0.9578\n",
      "iteration number: 3847\t training loss: 0.0328\tvalidation loss: 0.1186\t validation accuracy: 0.9600\n",
      "iteration number: 3848\t training loss: 0.0330\tvalidation loss: 0.1200\t validation accuracy: 0.9578\n",
      "iteration number: 3849\t training loss: 0.0326\tvalidation loss: 0.1178\t validation accuracy: 0.9600\n",
      "iteration number: 3850\t training loss: 0.0327\tvalidation loss: 0.1173\t validation accuracy: 0.9600\n",
      "iteration number: 3851\t training loss: 0.0326\tvalidation loss: 0.1172\t validation accuracy: 0.9600\n",
      "iteration number: 3852\t training loss: 0.0326\tvalidation loss: 0.1177\t validation accuracy: 0.9622\n",
      "iteration number: 3853\t training loss: 0.0329\tvalidation loss: 0.1139\t validation accuracy: 0.9622\n",
      "iteration number: 3854\t training loss: 0.0328\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 3855\t training loss: 0.0324\tvalidation loss: 0.1144\t validation accuracy: 0.9622\n",
      "iteration number: 3856\t training loss: 0.0325\tvalidation loss: 0.1142\t validation accuracy: 0.9622\n",
      "iteration number: 3857\t training loss: 0.0326\tvalidation loss: 0.1140\t validation accuracy: 0.9600\n",
      "iteration number: 3858\t training loss: 0.0327\tvalidation loss: 0.1145\t validation accuracy: 0.9644\n",
      "iteration number: 3859\t training loss: 0.0327\tvalidation loss: 0.1137\t validation accuracy: 0.9622\n",
      "iteration number: 3860\t training loss: 0.0329\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 3861\t training loss: 0.0331\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 3862\t training loss: 0.0331\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 3863\t training loss: 0.0332\tvalidation loss: 0.1148\t validation accuracy: 0.9622\n",
      "iteration number: 3864\t training loss: 0.0327\tvalidation loss: 0.1122\t validation accuracy: 0.9622\n",
      "iteration number: 3865\t training loss: 0.0323\tvalidation loss: 0.1104\t validation accuracy: 0.9644\n",
      "iteration number: 3866\t training loss: 0.0324\tvalidation loss: 0.1108\t validation accuracy: 0.9644\n",
      "iteration number: 3867\t training loss: 0.0323\tvalidation loss: 0.1119\t validation accuracy: 0.9644\n",
      "iteration number: 3868\t training loss: 0.0321\tvalidation loss: 0.1122\t validation accuracy: 0.9644\n",
      "iteration number: 3869\t training loss: 0.0322\tvalidation loss: 0.1132\t validation accuracy: 0.9622\n",
      "iteration number: 3870\t training loss: 0.0325\tvalidation loss: 0.1112\t validation accuracy: 0.9644\n",
      "iteration number: 3871\t training loss: 0.0325\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 3872\t training loss: 0.0322\tvalidation loss: 0.1105\t validation accuracy: 0.9644\n",
      "iteration number: 3873\t training loss: 0.0321\tvalidation loss: 0.1103\t validation accuracy: 0.9644\n",
      "iteration number: 3874\t training loss: 0.0325\tvalidation loss: 0.1113\t validation accuracy: 0.9667\n",
      "iteration number: 3875\t training loss: 0.0324\tvalidation loss: 0.1096\t validation accuracy: 0.9644\n",
      "iteration number: 3876\t training loss: 0.0329\tvalidation loss: 0.1088\t validation accuracy: 0.9644\n",
      "iteration number: 3877\t training loss: 0.0336\tvalidation loss: 0.1099\t validation accuracy: 0.9644\n",
      "iteration number: 3878\t training loss: 0.0348\tvalidation loss: 0.1111\t validation accuracy: 0.9622\n",
      "iteration number: 3879\t training loss: 0.0354\tvalidation loss: 0.1113\t validation accuracy: 0.9644\n",
      "iteration number: 3880\t training loss: 0.0369\tvalidation loss: 0.1128\t validation accuracy: 0.9622\n",
      "iteration number: 3881\t training loss: 0.0352\tvalidation loss: 0.1108\t validation accuracy: 0.9600\n",
      "iteration number: 3882\t training loss: 0.0336\tvalidation loss: 0.1106\t validation accuracy: 0.9644\n",
      "iteration number: 3883\t training loss: 0.0326\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3884\t training loss: 0.0323\tvalidation loss: 0.1112\t validation accuracy: 0.9644\n",
      "iteration number: 3885\t training loss: 0.0327\tvalidation loss: 0.1137\t validation accuracy: 0.9622\n",
      "iteration number: 3886\t training loss: 0.0331\tvalidation loss: 0.1135\t validation accuracy: 0.9667\n",
      "iteration number: 3887\t training loss: 0.0323\tvalidation loss: 0.1139\t validation accuracy: 0.9644\n",
      "iteration number: 3888\t training loss: 0.0321\tvalidation loss: 0.1136\t validation accuracy: 0.9622\n",
      "iteration number: 3889\t training loss: 0.0320\tvalidation loss: 0.1143\t validation accuracy: 0.9622\n",
      "iteration number: 3890\t training loss: 0.0317\tvalidation loss: 0.1122\t validation accuracy: 0.9644\n",
      "iteration number: 3891\t training loss: 0.0320\tvalidation loss: 0.1110\t validation accuracy: 0.9644\n",
      "iteration number: 3892\t training loss: 0.0324\tvalidation loss: 0.1136\t validation accuracy: 0.9667\n",
      "iteration number: 3893\t training loss: 0.0320\tvalidation loss: 0.1123\t validation accuracy: 0.9667\n",
      "iteration number: 3894\t training loss: 0.0323\tvalidation loss: 0.1141\t validation accuracy: 0.9667\n",
      "iteration number: 3895\t training loss: 0.0319\tvalidation loss: 0.1122\t validation accuracy: 0.9667\n",
      "iteration number: 3896\t training loss: 0.0320\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 3897\t training loss: 0.0323\tvalidation loss: 0.1126\t validation accuracy: 0.9644\n",
      "iteration number: 3898\t training loss: 0.0331\tvalidation loss: 0.1163\t validation accuracy: 0.9622\n",
      "iteration number: 3899\t training loss: 0.0320\tvalidation loss: 0.1131\t validation accuracy: 0.9622\n",
      "iteration number: 3900\t training loss: 0.0320\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 3901\t training loss: 0.0322\tvalidation loss: 0.1154\t validation accuracy: 0.9667\n",
      "iteration number: 3902\t training loss: 0.0317\tvalidation loss: 0.1136\t validation accuracy: 0.9622\n",
      "iteration number: 3903\t training loss: 0.0320\tvalidation loss: 0.1147\t validation accuracy: 0.9600\n",
      "iteration number: 3904\t training loss: 0.0326\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 3905\t training loss: 0.0328\tvalidation loss: 0.1140\t validation accuracy: 0.9622\n",
      "iteration number: 3906\t training loss: 0.0329\tvalidation loss: 0.1137\t validation accuracy: 0.9622\n",
      "iteration number: 3907\t training loss: 0.0326\tvalidation loss: 0.1144\t validation accuracy: 0.9622\n",
      "iteration number: 3908\t training loss: 0.0323\tvalidation loss: 0.1150\t validation accuracy: 0.9622\n",
      "iteration number: 3909\t training loss: 0.0320\tvalidation loss: 0.1152\t validation accuracy: 0.9644\n",
      "iteration number: 3910\t training loss: 0.0322\tvalidation loss: 0.1167\t validation accuracy: 0.9622\n",
      "iteration number: 3911\t training loss: 0.0320\tvalidation loss: 0.1164\t validation accuracy: 0.9644\n",
      "iteration number: 3912\t training loss: 0.0319\tvalidation loss: 0.1159\t validation accuracy: 0.9644\n",
      "iteration number: 3913\t training loss: 0.0325\tvalidation loss: 0.1188\t validation accuracy: 0.9622\n",
      "iteration number: 3914\t training loss: 0.0321\tvalidation loss: 0.1177\t validation accuracy: 0.9644\n",
      "iteration number: 3915\t training loss: 0.0321\tvalidation loss: 0.1175\t validation accuracy: 0.9644\n",
      "iteration number: 3916\t training loss: 0.0324\tvalidation loss: 0.1189\t validation accuracy: 0.9622\n",
      "iteration number: 3917\t training loss: 0.0325\tvalidation loss: 0.1189\t validation accuracy: 0.9644\n",
      "iteration number: 3918\t training loss: 0.0324\tvalidation loss: 0.1194\t validation accuracy: 0.9644\n",
      "iteration number: 3919\t training loss: 0.0321\tvalidation loss: 0.1175\t validation accuracy: 0.9644\n",
      "iteration number: 3920\t training loss: 0.0318\tvalidation loss: 0.1158\t validation accuracy: 0.9644\n",
      "iteration number: 3921\t training loss: 0.0320\tvalidation loss: 0.1179\t validation accuracy: 0.9644\n",
      "iteration number: 3922\t training loss: 0.0316\tvalidation loss: 0.1149\t validation accuracy: 0.9622\n",
      "iteration number: 3923\t training loss: 0.0318\tvalidation loss: 0.1165\t validation accuracy: 0.9622\n",
      "iteration number: 3924\t training loss: 0.0321\tvalidation loss: 0.1183\t validation accuracy: 0.9622\n",
      "iteration number: 3925\t training loss: 0.0322\tvalidation loss: 0.1184\t validation accuracy: 0.9644\n",
      "iteration number: 3926\t training loss: 0.0317\tvalidation loss: 0.1161\t validation accuracy: 0.9622\n",
      "iteration number: 3927\t training loss: 0.0321\tvalidation loss: 0.1184\t validation accuracy: 0.9644\n",
      "iteration number: 3928\t training loss: 0.0320\tvalidation loss: 0.1153\t validation accuracy: 0.9600\n",
      "iteration number: 3929\t training loss: 0.0320\tvalidation loss: 0.1150\t validation accuracy: 0.9600\n",
      "iteration number: 3930\t training loss: 0.0320\tvalidation loss: 0.1181\t validation accuracy: 0.9644\n",
      "iteration number: 3931\t training loss: 0.0318\tvalidation loss: 0.1167\t validation accuracy: 0.9644\n",
      "iteration number: 3932\t training loss: 0.0315\tvalidation loss: 0.1147\t validation accuracy: 0.9644\n",
      "iteration number: 3933\t training loss: 0.0317\tvalidation loss: 0.1103\t validation accuracy: 0.9622\n",
      "iteration number: 3934\t training loss: 0.0318\tvalidation loss: 0.1093\t validation accuracy: 0.9622\n",
      "iteration number: 3935\t training loss: 0.0316\tvalidation loss: 0.1087\t validation accuracy: 0.9644\n",
      "iteration number: 3936\t training loss: 0.0313\tvalidation loss: 0.1094\t validation accuracy: 0.9667\n",
      "iteration number: 3937\t training loss: 0.0315\tvalidation loss: 0.1096\t validation accuracy: 0.9644\n",
      "iteration number: 3938\t training loss: 0.0315\tvalidation loss: 0.1099\t validation accuracy: 0.9644\n",
      "iteration number: 3939\t training loss: 0.0314\tvalidation loss: 0.1097\t validation accuracy: 0.9644\n",
      "iteration number: 3940\t training loss: 0.0314\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 3941\t training loss: 0.0315\tvalidation loss: 0.1124\t validation accuracy: 0.9667\n",
      "iteration number: 3942\t training loss: 0.0313\tvalidation loss: 0.1113\t validation accuracy: 0.9667\n",
      "iteration number: 3943\t training loss: 0.0324\tvalidation loss: 0.1170\t validation accuracy: 0.9667\n",
      "iteration number: 3944\t training loss: 0.0313\tvalidation loss: 0.1115\t validation accuracy: 0.9667\n",
      "iteration number: 3945\t training loss: 0.0312\tvalidation loss: 0.1105\t validation accuracy: 0.9644\n",
      "iteration number: 3946\t training loss: 0.0313\tvalidation loss: 0.1104\t validation accuracy: 0.9644\n",
      "iteration number: 3947\t training loss: 0.0312\tvalidation loss: 0.1136\t validation accuracy: 0.9644\n",
      "iteration number: 3948\t training loss: 0.0314\tvalidation loss: 0.1137\t validation accuracy: 0.9644\n",
      "iteration number: 3949\t training loss: 0.0316\tvalidation loss: 0.1162\t validation accuracy: 0.9644\n",
      "iteration number: 3950\t training loss: 0.0314\tvalidation loss: 0.1145\t validation accuracy: 0.9622\n",
      "iteration number: 3951\t training loss: 0.0312\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n",
      "iteration number: 3952\t training loss: 0.0312\tvalidation loss: 0.1116\t validation accuracy: 0.9667\n",
      "iteration number: 3953\t training loss: 0.0316\tvalidation loss: 0.1147\t validation accuracy: 0.9667\n",
      "iteration number: 3954\t training loss: 0.0318\tvalidation loss: 0.1142\t validation accuracy: 0.9667\n",
      "iteration number: 3955\t training loss: 0.0317\tvalidation loss: 0.1132\t validation accuracy: 0.9667\n",
      "iteration number: 3956\t training loss: 0.0317\tvalidation loss: 0.1135\t validation accuracy: 0.9667\n",
      "iteration number: 3957\t training loss: 0.0318\tvalidation loss: 0.1145\t validation accuracy: 0.9644\n",
      "iteration number: 3958\t training loss: 0.0323\tvalidation loss: 0.1175\t validation accuracy: 0.9667\n",
      "iteration number: 3959\t training loss: 0.0319\tvalidation loss: 0.1166\t validation accuracy: 0.9667\n",
      "iteration number: 3960\t training loss: 0.0312\tvalidation loss: 0.1137\t validation accuracy: 0.9667\n",
      "iteration number: 3961\t training loss: 0.0314\tvalidation loss: 0.1126\t validation accuracy: 0.9667\n",
      "iteration number: 3962\t training loss: 0.0314\tvalidation loss: 0.1121\t validation accuracy: 0.9667\n",
      "iteration number: 3963\t training loss: 0.0315\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 3964\t training loss: 0.0313\tvalidation loss: 0.1109\t validation accuracy: 0.9667\n",
      "iteration number: 3965\t training loss: 0.0311\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 3966\t training loss: 0.0311\tvalidation loss: 0.1106\t validation accuracy: 0.9667\n",
      "iteration number: 3967\t training loss: 0.0311\tvalidation loss: 0.1105\t validation accuracy: 0.9667\n",
      "iteration number: 3968\t training loss: 0.0312\tvalidation loss: 0.1101\t validation accuracy: 0.9644\n",
      "iteration number: 3969\t training loss: 0.0313\tvalidation loss: 0.1108\t validation accuracy: 0.9644\n",
      "iteration number: 3970\t training loss: 0.0312\tvalidation loss: 0.1125\t validation accuracy: 0.9667\n",
      "iteration number: 3971\t training loss: 0.0315\tvalidation loss: 0.1134\t validation accuracy: 0.9667\n",
      "iteration number: 3972\t training loss: 0.0313\tvalidation loss: 0.1132\t validation accuracy: 0.9667\n",
      "iteration number: 3973\t training loss: 0.0311\tvalidation loss: 0.1128\t validation accuracy: 0.9644\n",
      "iteration number: 3974\t training loss: 0.0310\tvalidation loss: 0.1113\t validation accuracy: 0.9667\n",
      "iteration number: 3975\t training loss: 0.0313\tvalidation loss: 0.1101\t validation accuracy: 0.9644\n",
      "iteration number: 3976\t training loss: 0.0311\tvalidation loss: 0.1114\t validation accuracy: 0.9644\n",
      "iteration number: 3977\t training loss: 0.0312\tvalidation loss: 0.1121\t validation accuracy: 0.9622\n",
      "iteration number: 3978\t training loss: 0.0314\tvalidation loss: 0.1110\t validation accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 3979\t training loss: 0.0312\tvalidation loss: 0.1117\t validation accuracy: 0.9622\n",
      "iteration number: 3980\t training loss: 0.0317\tvalidation loss: 0.1132\t validation accuracy: 0.9622\n",
      "iteration number: 3981\t training loss: 0.0322\tvalidation loss: 0.1123\t validation accuracy: 0.9644\n",
      "iteration number: 3982\t training loss: 0.0318\tvalidation loss: 0.1120\t validation accuracy: 0.9622\n",
      "iteration number: 3983\t training loss: 0.0315\tvalidation loss: 0.1135\t validation accuracy: 0.9622\n",
      "iteration number: 3984\t training loss: 0.0317\tvalidation loss: 0.1134\t validation accuracy: 0.9622\n",
      "iteration number: 3985\t training loss: 0.0317\tvalidation loss: 0.1140\t validation accuracy: 0.9622\n",
      "iteration number: 3986\t training loss: 0.0318\tvalidation loss: 0.1147\t validation accuracy: 0.9622\n",
      "iteration number: 3987\t training loss: 0.0320\tvalidation loss: 0.1155\t validation accuracy: 0.9622\n",
      "iteration number: 3988\t training loss: 0.0319\tvalidation loss: 0.1151\t validation accuracy: 0.9622\n",
      "iteration number: 3989\t training loss: 0.0322\tvalidation loss: 0.1144\t validation accuracy: 0.9622\n",
      "iteration number: 3990\t training loss: 0.0317\tvalidation loss: 0.1136\t validation accuracy: 0.9622\n",
      "iteration number: 3991\t training loss: 0.0314\tvalidation loss: 0.1121\t validation accuracy: 0.9622\n",
      "iteration number: 3992\t training loss: 0.0313\tvalidation loss: 0.1113\t validation accuracy: 0.9622\n",
      "iteration number: 3993\t training loss: 0.0312\tvalidation loss: 0.1122\t validation accuracy: 0.9600\n",
      "iteration number: 3994\t training loss: 0.0318\tvalidation loss: 0.1115\t validation accuracy: 0.9622\n",
      "iteration number: 3995\t training loss: 0.0322\tvalidation loss: 0.1114\t validation accuracy: 0.9622\n",
      "iteration number: 3996\t training loss: 0.0327\tvalidation loss: 0.1105\t validation accuracy: 0.9622\n",
      "iteration number: 3997\t training loss: 0.0313\tvalidation loss: 0.1118\t validation accuracy: 0.9622\n",
      "iteration number: 3998\t training loss: 0.0314\tvalidation loss: 0.1134\t validation accuracy: 0.9600\n",
      "iteration number: 3999\t training loss: 0.0311\tvalidation loss: 0.1126\t validation accuracy: 0.9644\n",
      "iteration number: 4000\t training loss: 0.0316\tvalidation loss: 0.1105\t validation accuracy: 0.9622\n",
      "iteration number: 4001\t training loss: 0.0315\tvalidation loss: 0.1118\t validation accuracy: 0.9622\n",
      "iteration number: 4002\t training loss: 0.0313\tvalidation loss: 0.1112\t validation accuracy: 0.9622\n",
      "iteration number: 4003\t training loss: 0.0311\tvalidation loss: 0.1093\t validation accuracy: 0.9667\n",
      "iteration number: 4004\t training loss: 0.0310\tvalidation loss: 0.1106\t validation accuracy: 0.9667\n",
      "iteration number: 4005\t training loss: 0.0309\tvalidation loss: 0.1094\t validation accuracy: 0.9689\n",
      "iteration number: 4006\t training loss: 0.0307\tvalidation loss: 0.1119\t validation accuracy: 0.9667\n",
      "iteration number: 4007\t training loss: 0.0307\tvalidation loss: 0.1123\t validation accuracy: 0.9667\n",
      "iteration number: 4008\t training loss: 0.0308\tvalidation loss: 0.1129\t validation accuracy: 0.9667\n",
      "iteration number: 4009\t training loss: 0.0311\tvalidation loss: 0.1127\t validation accuracy: 0.9667\n",
      "iteration number: 4010\t training loss: 0.0311\tvalidation loss: 0.1130\t validation accuracy: 0.9667\n",
      "iteration number: 4011\t training loss: 0.0311\tvalidation loss: 0.1157\t validation accuracy: 0.9667\n",
      "iteration number: 4012\t training loss: 0.0310\tvalidation loss: 0.1149\t validation accuracy: 0.9644\n",
      "iteration number: 4013\t training loss: 0.0316\tvalidation loss: 0.1182\t validation accuracy: 0.9622\n",
      "iteration number: 4014\t training loss: 0.0320\tvalidation loss: 0.1200\t validation accuracy: 0.9622\n",
      "iteration number: 4015\t training loss: 0.0318\tvalidation loss: 0.1187\t validation accuracy: 0.9622\n",
      "iteration number: 4016\t training loss: 0.0315\tvalidation loss: 0.1170\t validation accuracy: 0.9667\n",
      "iteration number: 4017\t training loss: 0.0320\tvalidation loss: 0.1182\t validation accuracy: 0.9644\n",
      "iteration number: 4018\t training loss: 0.0325\tvalidation loss: 0.1194\t validation accuracy: 0.9644\n",
      "iteration number: 4019\t training loss: 0.0315\tvalidation loss: 0.1166\t validation accuracy: 0.9667\n",
      "iteration number: 4020\t training loss: 0.0327\tvalidation loss: 0.1206\t validation accuracy: 0.9622\n",
      "iteration number: 4021\t training loss: 0.0315\tvalidation loss: 0.1159\t validation accuracy: 0.9667\n",
      "iteration number: 4022\t training loss: 0.0308\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 4023\t training loss: 0.0310\tvalidation loss: 0.1147\t validation accuracy: 0.9667\n",
      "iteration number: 4024\t training loss: 0.0310\tvalidation loss: 0.1149\t validation accuracy: 0.9667\n",
      "iteration number: 4025\t training loss: 0.0312\tvalidation loss: 0.1153\t validation accuracy: 0.9667\n",
      "iteration number: 4026\t training loss: 0.0318\tvalidation loss: 0.1165\t validation accuracy: 0.9667\n",
      "iteration number: 4027\t training loss: 0.0310\tvalidation loss: 0.1158\t validation accuracy: 0.9667\n",
      "iteration number: 4028\t training loss: 0.0314\tvalidation loss: 0.1139\t validation accuracy: 0.9600\n",
      "iteration number: 4029\t training loss: 0.0318\tvalidation loss: 0.1196\t validation accuracy: 0.9644\n",
      "iteration number: 4030\t training loss: 0.0318\tvalidation loss: 0.1206\t validation accuracy: 0.9622\n",
      "iteration number: 4031\t training loss: 0.0311\tvalidation loss: 0.1180\t validation accuracy: 0.9622\n",
      "iteration number: 4032\t training loss: 0.0312\tvalidation loss: 0.1184\t validation accuracy: 0.9622\n",
      "iteration number: 4033\t training loss: 0.0307\tvalidation loss: 0.1172\t validation accuracy: 0.9622\n",
      "iteration number: 4034\t training loss: 0.0309\tvalidation loss: 0.1182\t validation accuracy: 0.9600\n",
      "iteration number: 4035\t training loss: 0.0306\tvalidation loss: 0.1165\t validation accuracy: 0.9622\n",
      "iteration number: 4036\t training loss: 0.0306\tvalidation loss: 0.1158\t validation accuracy: 0.9622\n",
      "iteration number: 4037\t training loss: 0.0311\tvalidation loss: 0.1186\t validation accuracy: 0.9622\n",
      "iteration number: 4038\t training loss: 0.0304\tvalidation loss: 0.1149\t validation accuracy: 0.9644\n",
      "iteration number: 4039\t training loss: 0.0305\tvalidation loss: 0.1140\t validation accuracy: 0.9622\n",
      "iteration number: 4040\t training loss: 0.0304\tvalidation loss: 0.1126\t validation accuracy: 0.9622\n",
      "iteration number: 4041\t training loss: 0.0304\tvalidation loss: 0.1121\t validation accuracy: 0.9622\n",
      "iteration number: 4042\t training loss: 0.0306\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 4043\t training loss: 0.0309\tvalidation loss: 0.1106\t validation accuracy: 0.9667\n",
      "iteration number: 4044\t training loss: 0.0305\tvalidation loss: 0.1110\t validation accuracy: 0.9667\n",
      "iteration number: 4045\t training loss: 0.0304\tvalidation loss: 0.1112\t validation accuracy: 0.9644\n",
      "iteration number: 4046\t training loss: 0.0305\tvalidation loss: 0.1120\t validation accuracy: 0.9622\n",
      "iteration number: 4047\t training loss: 0.0304\tvalidation loss: 0.1111\t validation accuracy: 0.9644\n",
      "iteration number: 4048\t training loss: 0.0303\tvalidation loss: 0.1111\t validation accuracy: 0.9622\n",
      "iteration number: 4049\t training loss: 0.0303\tvalidation loss: 0.1113\t validation accuracy: 0.9644\n",
      "iteration number: 4050\t training loss: 0.0301\tvalidation loss: 0.1128\t validation accuracy: 0.9622\n",
      "iteration number: 4051\t training loss: 0.0303\tvalidation loss: 0.1127\t validation accuracy: 0.9622\n",
      "iteration number: 4052\t training loss: 0.0303\tvalidation loss: 0.1147\t validation accuracy: 0.9622\n",
      "iteration number: 4053\t training loss: 0.0303\tvalidation loss: 0.1146\t validation accuracy: 0.9622\n",
      "iteration number: 4054\t training loss: 0.0303\tvalidation loss: 0.1135\t validation accuracy: 0.9622\n",
      "iteration number: 4055\t training loss: 0.0302\tvalidation loss: 0.1107\t validation accuracy: 0.9644\n",
      "iteration number: 4056\t training loss: 0.0306\tvalidation loss: 0.1100\t validation accuracy: 0.9667\n",
      "iteration number: 4057\t training loss: 0.0301\tvalidation loss: 0.1114\t validation accuracy: 0.9644\n",
      "iteration number: 4058\t training loss: 0.0301\tvalidation loss: 0.1132\t validation accuracy: 0.9644\n",
      "iteration number: 4059\t training loss: 0.0302\tvalidation loss: 0.1127\t validation accuracy: 0.9644\n",
      "iteration number: 4060\t training loss: 0.0302\tvalidation loss: 0.1122\t validation accuracy: 0.9644\n",
      "iteration number: 4061\t training loss: 0.0305\tvalidation loss: 0.1122\t validation accuracy: 0.9644\n",
      "iteration number: 4062\t training loss: 0.0306\tvalidation loss: 0.1116\t validation accuracy: 0.9644\n",
      "iteration number: 4063\t training loss: 0.0306\tvalidation loss: 0.1137\t validation accuracy: 0.9578\n",
      "iteration number: 4064\t training loss: 0.0306\tvalidation loss: 0.1124\t validation accuracy: 0.9622\n",
      "iteration number: 4065\t training loss: 0.0307\tvalidation loss: 0.1122\t validation accuracy: 0.9600\n",
      "iteration number: 4066\t training loss: 0.0304\tvalidation loss: 0.1119\t validation accuracy: 0.9600\n",
      "iteration number: 4067\t training loss: 0.0307\tvalidation loss: 0.1140\t validation accuracy: 0.9622\n",
      "iteration number: 4068\t training loss: 0.0307\tvalidation loss: 0.1132\t validation accuracy: 0.9622\n",
      "iteration number: 4069\t training loss: 0.0305\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n",
      "iteration number: 4070\t training loss: 0.0306\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 4071\t training loss: 0.0305\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 4072\t training loss: 0.0305\tvalidation loss: 0.1147\t validation accuracy: 0.9644\n",
      "iteration number: 4073\t training loss: 0.0305\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 4074\t training loss: 0.0303\tvalidation loss: 0.1113\t validation accuracy: 0.9622\n",
      "iteration number: 4075\t training loss: 0.0303\tvalidation loss: 0.1122\t validation accuracy: 0.9622\n",
      "iteration number: 4076\t training loss: 0.0308\tvalidation loss: 0.1118\t validation accuracy: 0.9622\n",
      "iteration number: 4077\t training loss: 0.0306\tvalidation loss: 0.1120\t validation accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4078\t training loss: 0.0307\tvalidation loss: 0.1108\t validation accuracy: 0.9622\n",
      "iteration number: 4079\t training loss: 0.0307\tvalidation loss: 0.1103\t validation accuracy: 0.9622\n",
      "iteration number: 4080\t training loss: 0.0311\tvalidation loss: 0.1103\t validation accuracy: 0.9644\n",
      "iteration number: 4081\t training loss: 0.0305\tvalidation loss: 0.1099\t validation accuracy: 0.9644\n",
      "iteration number: 4082\t training loss: 0.0304\tvalidation loss: 0.1086\t validation accuracy: 0.9644\n",
      "iteration number: 4083\t training loss: 0.0302\tvalidation loss: 0.1091\t validation accuracy: 0.9622\n",
      "iteration number: 4084\t training loss: 0.0302\tvalidation loss: 0.1101\t validation accuracy: 0.9622\n",
      "iteration number: 4085\t training loss: 0.0307\tvalidation loss: 0.1055\t validation accuracy: 0.9667\n",
      "iteration number: 4086\t training loss: 0.0303\tvalidation loss: 0.1064\t validation accuracy: 0.9622\n",
      "iteration number: 4087\t training loss: 0.0305\tvalidation loss: 0.1087\t validation accuracy: 0.9667\n",
      "iteration number: 4088\t training loss: 0.0303\tvalidation loss: 0.1088\t validation accuracy: 0.9622\n",
      "iteration number: 4089\t training loss: 0.0303\tvalidation loss: 0.1078\t validation accuracy: 0.9622\n",
      "iteration number: 4090\t training loss: 0.0303\tvalidation loss: 0.1083\t validation accuracy: 0.9622\n",
      "iteration number: 4091\t training loss: 0.0302\tvalidation loss: 0.1096\t validation accuracy: 0.9667\n",
      "iteration number: 4092\t training loss: 0.0305\tvalidation loss: 0.1103\t validation accuracy: 0.9667\n",
      "iteration number: 4093\t training loss: 0.0305\tvalidation loss: 0.1099\t validation accuracy: 0.9644\n",
      "iteration number: 4094\t training loss: 0.0304\tvalidation loss: 0.1114\t validation accuracy: 0.9644\n",
      "iteration number: 4095\t training loss: 0.0306\tvalidation loss: 0.1105\t validation accuracy: 0.9667\n",
      "iteration number: 4096\t training loss: 0.0309\tvalidation loss: 0.1091\t validation accuracy: 0.9667\n",
      "iteration number: 4097\t training loss: 0.0308\tvalidation loss: 0.1098\t validation accuracy: 0.9667\n",
      "iteration number: 4098\t training loss: 0.0304\tvalidation loss: 0.1096\t validation accuracy: 0.9667\n",
      "iteration number: 4099\t training loss: 0.0308\tvalidation loss: 0.1113\t validation accuracy: 0.9644\n",
      "iteration number: 4100\t training loss: 0.0304\tvalidation loss: 0.1097\t validation accuracy: 0.9667\n",
      "iteration number: 4101\t training loss: 0.0302\tvalidation loss: 0.1096\t validation accuracy: 0.9644\n",
      "iteration number: 4102\t training loss: 0.0301\tvalidation loss: 0.1073\t validation accuracy: 0.9644\n",
      "iteration number: 4103\t training loss: 0.0301\tvalidation loss: 0.1073\t validation accuracy: 0.9667\n",
      "iteration number: 4104\t training loss: 0.0302\tvalidation loss: 0.1075\t validation accuracy: 0.9689\n",
      "iteration number: 4105\t training loss: 0.0301\tvalidation loss: 0.1083\t validation accuracy: 0.9689\n",
      "iteration number: 4106\t training loss: 0.0298\tvalidation loss: 0.1099\t validation accuracy: 0.9644\n",
      "iteration number: 4107\t training loss: 0.0299\tvalidation loss: 0.1095\t validation accuracy: 0.9644\n",
      "iteration number: 4108\t training loss: 0.0297\tvalidation loss: 0.1103\t validation accuracy: 0.9644\n",
      "iteration number: 4109\t training loss: 0.0300\tvalidation loss: 0.1092\t validation accuracy: 0.9667\n",
      "iteration number: 4110\t training loss: 0.0297\tvalidation loss: 0.1111\t validation accuracy: 0.9667\n",
      "iteration number: 4111\t training loss: 0.0297\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 4112\t training loss: 0.0296\tvalidation loss: 0.1122\t validation accuracy: 0.9644\n",
      "iteration number: 4113\t training loss: 0.0296\tvalidation loss: 0.1123\t validation accuracy: 0.9644\n",
      "iteration number: 4114\t training loss: 0.0300\tvalidation loss: 0.1120\t validation accuracy: 0.9622\n",
      "iteration number: 4115\t training loss: 0.0300\tvalidation loss: 0.1121\t validation accuracy: 0.9622\n",
      "iteration number: 4116\t training loss: 0.0304\tvalidation loss: 0.1113\t validation accuracy: 0.9622\n",
      "iteration number: 4117\t training loss: 0.0303\tvalidation loss: 0.1111\t validation accuracy: 0.9622\n",
      "iteration number: 4118\t training loss: 0.0299\tvalidation loss: 0.1125\t validation accuracy: 0.9644\n",
      "iteration number: 4119\t training loss: 0.0297\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 4120\t training loss: 0.0299\tvalidation loss: 0.1142\t validation accuracy: 0.9667\n",
      "iteration number: 4121\t training loss: 0.0299\tvalidation loss: 0.1139\t validation accuracy: 0.9667\n",
      "iteration number: 4122\t training loss: 0.0298\tvalidation loss: 0.1118\t validation accuracy: 0.9644\n",
      "iteration number: 4123\t training loss: 0.0304\tvalidation loss: 0.1111\t validation accuracy: 0.9622\n",
      "iteration number: 4124\t training loss: 0.0305\tvalidation loss: 0.1103\t validation accuracy: 0.9622\n",
      "iteration number: 4125\t training loss: 0.0297\tvalidation loss: 0.1113\t validation accuracy: 0.9622\n",
      "iteration number: 4126\t training loss: 0.0295\tvalidation loss: 0.1122\t validation accuracy: 0.9600\n",
      "iteration number: 4127\t training loss: 0.0294\tvalidation loss: 0.1121\t validation accuracy: 0.9644\n",
      "iteration number: 4128\t training loss: 0.0296\tvalidation loss: 0.1107\t validation accuracy: 0.9644\n",
      "iteration number: 4129\t training loss: 0.0297\tvalidation loss: 0.1096\t validation accuracy: 0.9644\n",
      "iteration number: 4130\t training loss: 0.0297\tvalidation loss: 0.1111\t validation accuracy: 0.9622\n",
      "iteration number: 4131\t training loss: 0.0295\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 4132\t training loss: 0.0296\tvalidation loss: 0.1111\t validation accuracy: 0.9644\n",
      "iteration number: 4133\t training loss: 0.0299\tvalidation loss: 0.1077\t validation accuracy: 0.9711\n",
      "iteration number: 4134\t training loss: 0.0298\tvalidation loss: 0.1083\t validation accuracy: 0.9711\n",
      "iteration number: 4135\t training loss: 0.0301\tvalidation loss: 0.1073\t validation accuracy: 0.9667\n",
      "iteration number: 4136\t training loss: 0.0298\tvalidation loss: 0.1091\t validation accuracy: 0.9667\n",
      "iteration number: 4137\t training loss: 0.0297\tvalidation loss: 0.1094\t validation accuracy: 0.9644\n",
      "iteration number: 4138\t training loss: 0.0297\tvalidation loss: 0.1098\t validation accuracy: 0.9644\n",
      "iteration number: 4139\t training loss: 0.0296\tvalidation loss: 0.1102\t validation accuracy: 0.9644\n",
      "iteration number: 4140\t training loss: 0.0296\tvalidation loss: 0.1126\t validation accuracy: 0.9622\n",
      "iteration number: 4141\t training loss: 0.0296\tvalidation loss: 0.1132\t validation accuracy: 0.9622\n",
      "iteration number: 4142\t training loss: 0.0296\tvalidation loss: 0.1142\t validation accuracy: 0.9622\n",
      "iteration number: 4143\t training loss: 0.0297\tvalidation loss: 0.1151\t validation accuracy: 0.9622\n",
      "iteration number: 4144\t training loss: 0.0297\tvalidation loss: 0.1148\t validation accuracy: 0.9622\n",
      "iteration number: 4145\t training loss: 0.0300\tvalidation loss: 0.1139\t validation accuracy: 0.9667\n",
      "iteration number: 4146\t training loss: 0.0301\tvalidation loss: 0.1150\t validation accuracy: 0.9667\n",
      "iteration number: 4147\t training loss: 0.0302\tvalidation loss: 0.1156\t validation accuracy: 0.9667\n",
      "iteration number: 4148\t training loss: 0.0303\tvalidation loss: 0.1197\t validation accuracy: 0.9644\n",
      "iteration number: 4149\t training loss: 0.0304\tvalidation loss: 0.1205\t validation accuracy: 0.9667\n",
      "iteration number: 4150\t training loss: 0.0305\tvalidation loss: 0.1215\t validation accuracy: 0.9644\n",
      "iteration number: 4151\t training loss: 0.0302\tvalidation loss: 0.1203\t validation accuracy: 0.9622\n",
      "iteration number: 4152\t training loss: 0.0305\tvalidation loss: 0.1208\t validation accuracy: 0.9600\n",
      "iteration number: 4153\t training loss: 0.0303\tvalidation loss: 0.1196\t validation accuracy: 0.9644\n",
      "iteration number: 4154\t training loss: 0.0300\tvalidation loss: 0.1190\t validation accuracy: 0.9622\n",
      "iteration number: 4155\t training loss: 0.0305\tvalidation loss: 0.1214\t validation accuracy: 0.9667\n",
      "iteration number: 4156\t training loss: 0.0305\tvalidation loss: 0.1213\t validation accuracy: 0.9667\n",
      "iteration number: 4157\t training loss: 0.0303\tvalidation loss: 0.1205\t validation accuracy: 0.9667\n",
      "iteration number: 4158\t training loss: 0.0302\tvalidation loss: 0.1203\t validation accuracy: 0.9667\n",
      "iteration number: 4159\t training loss: 0.0301\tvalidation loss: 0.1198\t validation accuracy: 0.9644\n",
      "iteration number: 4160\t training loss: 0.0297\tvalidation loss: 0.1181\t validation accuracy: 0.9600\n",
      "iteration number: 4161\t training loss: 0.0295\tvalidation loss: 0.1158\t validation accuracy: 0.9644\n",
      "iteration number: 4162\t training loss: 0.0299\tvalidation loss: 0.1149\t validation accuracy: 0.9622\n",
      "iteration number: 4163\t training loss: 0.0297\tvalidation loss: 0.1158\t validation accuracy: 0.9622\n",
      "iteration number: 4164\t training loss: 0.0296\tvalidation loss: 0.1160\t validation accuracy: 0.9622\n",
      "iteration number: 4165\t training loss: 0.0296\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 4166\t training loss: 0.0297\tvalidation loss: 0.1140\t validation accuracy: 0.9644\n",
      "iteration number: 4167\t training loss: 0.0296\tvalidation loss: 0.1147\t validation accuracy: 0.9644\n",
      "iteration number: 4168\t training loss: 0.0296\tvalidation loss: 0.1161\t validation accuracy: 0.9622\n",
      "iteration number: 4169\t training loss: 0.0297\tvalidation loss: 0.1146\t validation accuracy: 0.9622\n",
      "iteration number: 4170\t training loss: 0.0292\tvalidation loss: 0.1159\t validation accuracy: 0.9622\n",
      "iteration number: 4171\t training loss: 0.0294\tvalidation loss: 0.1154\t validation accuracy: 0.9622\n",
      "iteration number: 4172\t training loss: 0.0297\tvalidation loss: 0.1163\t validation accuracy: 0.9622\n",
      "iteration number: 4173\t training loss: 0.0294\tvalidation loss: 0.1177\t validation accuracy: 0.9622\n",
      "iteration number: 4174\t training loss: 0.0294\tvalidation loss: 0.1172\t validation accuracy: 0.9622\n",
      "iteration number: 4175\t training loss: 0.0293\tvalidation loss: 0.1161\t validation accuracy: 0.9644\n",
      "iteration number: 4176\t training loss: 0.0294\tvalidation loss: 0.1170\t validation accuracy: 0.9644\n",
      "iteration number: 4177\t training loss: 0.0301\tvalidation loss: 0.1198\t validation accuracy: 0.9600\n",
      "iteration number: 4178\t training loss: 0.0299\tvalidation loss: 0.1171\t validation accuracy: 0.9600\n",
      "iteration number: 4179\t training loss: 0.0296\tvalidation loss: 0.1167\t validation accuracy: 0.9622\n",
      "iteration number: 4180\t training loss: 0.0297\tvalidation loss: 0.1170\t validation accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4181\t training loss: 0.0298\tvalidation loss: 0.1158\t validation accuracy: 0.9600\n",
      "iteration number: 4182\t training loss: 0.0295\tvalidation loss: 0.1154\t validation accuracy: 0.9600\n",
      "iteration number: 4183\t training loss: 0.0294\tvalidation loss: 0.1134\t validation accuracy: 0.9622\n",
      "iteration number: 4184\t training loss: 0.0295\tvalidation loss: 0.1137\t validation accuracy: 0.9622\n",
      "iteration number: 4185\t training loss: 0.0295\tvalidation loss: 0.1129\t validation accuracy: 0.9622\n",
      "iteration number: 4186\t training loss: 0.0293\tvalidation loss: 0.1127\t validation accuracy: 0.9622\n",
      "iteration number: 4187\t training loss: 0.0295\tvalidation loss: 0.1124\t validation accuracy: 0.9622\n",
      "iteration number: 4188\t training loss: 0.0294\tvalidation loss: 0.1121\t validation accuracy: 0.9622\n",
      "iteration number: 4189\t training loss: 0.0295\tvalidation loss: 0.1122\t validation accuracy: 0.9622\n",
      "iteration number: 4190\t training loss: 0.0296\tvalidation loss: 0.1113\t validation accuracy: 0.9622\n",
      "iteration number: 4191\t training loss: 0.0294\tvalidation loss: 0.1114\t validation accuracy: 0.9644\n",
      "iteration number: 4192\t training loss: 0.0298\tvalidation loss: 0.1107\t validation accuracy: 0.9600\n",
      "iteration number: 4193\t training loss: 0.0295\tvalidation loss: 0.1122\t validation accuracy: 0.9622\n",
      "iteration number: 4194\t training loss: 0.0291\tvalidation loss: 0.1129\t validation accuracy: 0.9622\n",
      "iteration number: 4195\t training loss: 0.0292\tvalidation loss: 0.1119\t validation accuracy: 0.9622\n",
      "iteration number: 4196\t training loss: 0.0291\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 4197\t training loss: 0.0291\tvalidation loss: 0.1128\t validation accuracy: 0.9644\n",
      "iteration number: 4198\t training loss: 0.0290\tvalidation loss: 0.1122\t validation accuracy: 0.9667\n",
      "iteration number: 4199\t training loss: 0.0294\tvalidation loss: 0.1152\t validation accuracy: 0.9667\n",
      "iteration number: 4200\t training loss: 0.0291\tvalidation loss: 0.1136\t validation accuracy: 0.9644\n",
      "iteration number: 4201\t training loss: 0.0290\tvalidation loss: 0.1143\t validation accuracy: 0.9622\n",
      "iteration number: 4202\t training loss: 0.0294\tvalidation loss: 0.1150\t validation accuracy: 0.9644\n",
      "iteration number: 4203\t training loss: 0.0295\tvalidation loss: 0.1155\t validation accuracy: 0.9644\n",
      "iteration number: 4204\t training loss: 0.0294\tvalidation loss: 0.1158\t validation accuracy: 0.9622\n",
      "iteration number: 4205\t training loss: 0.0293\tvalidation loss: 0.1143\t validation accuracy: 0.9600\n",
      "iteration number: 4206\t training loss: 0.0292\tvalidation loss: 0.1123\t validation accuracy: 0.9667\n",
      "iteration number: 4207\t training loss: 0.0293\tvalidation loss: 0.1129\t validation accuracy: 0.9667\n",
      "iteration number: 4208\t training loss: 0.0291\tvalidation loss: 0.1137\t validation accuracy: 0.9622\n",
      "iteration number: 4209\t training loss: 0.0291\tvalidation loss: 0.1128\t validation accuracy: 0.9644\n",
      "iteration number: 4210\t training loss: 0.0289\tvalidation loss: 0.1134\t validation accuracy: 0.9644\n",
      "iteration number: 4211\t training loss: 0.0292\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 4212\t training loss: 0.0288\tvalidation loss: 0.1114\t validation accuracy: 0.9644\n",
      "iteration number: 4213\t training loss: 0.0288\tvalidation loss: 0.1113\t validation accuracy: 0.9667\n",
      "iteration number: 4214\t training loss: 0.0289\tvalidation loss: 0.1117\t validation accuracy: 0.9667\n",
      "iteration number: 4215\t training loss: 0.0293\tvalidation loss: 0.1127\t validation accuracy: 0.9644\n",
      "iteration number: 4216\t training loss: 0.0291\tvalidation loss: 0.1116\t validation accuracy: 0.9644\n",
      "iteration number: 4217\t training loss: 0.0289\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 4218\t training loss: 0.0290\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 4219\t training loss: 0.0288\tvalidation loss: 0.1148\t validation accuracy: 0.9644\n",
      "iteration number: 4220\t training loss: 0.0287\tvalidation loss: 0.1129\t validation accuracy: 0.9644\n",
      "iteration number: 4221\t training loss: 0.0287\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n",
      "iteration number: 4222\t training loss: 0.0290\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 4223\t training loss: 0.0289\tvalidation loss: 0.1137\t validation accuracy: 0.9644\n",
      "iteration number: 4224\t training loss: 0.0289\tvalidation loss: 0.1133\t validation accuracy: 0.9600\n",
      "iteration number: 4225\t training loss: 0.0299\tvalidation loss: 0.1106\t validation accuracy: 0.9622\n",
      "iteration number: 4226\t training loss: 0.0293\tvalidation loss: 0.1109\t validation accuracy: 0.9622\n",
      "iteration number: 4227\t training loss: 0.0289\tvalidation loss: 0.1123\t validation accuracy: 0.9644\n",
      "iteration number: 4228\t training loss: 0.0288\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 4229\t training loss: 0.0287\tvalidation loss: 0.1129\t validation accuracy: 0.9644\n",
      "iteration number: 4230\t training loss: 0.0287\tvalidation loss: 0.1113\t validation accuracy: 0.9644\n",
      "iteration number: 4231\t training loss: 0.0289\tvalidation loss: 0.1133\t validation accuracy: 0.9667\n",
      "iteration number: 4232\t training loss: 0.0288\tvalidation loss: 0.1125\t validation accuracy: 0.9667\n",
      "iteration number: 4233\t training loss: 0.0289\tvalidation loss: 0.1127\t validation accuracy: 0.9667\n",
      "iteration number: 4234\t training loss: 0.0289\tvalidation loss: 0.1121\t validation accuracy: 0.9689\n",
      "iteration number: 4235\t training loss: 0.0289\tvalidation loss: 0.1123\t validation accuracy: 0.9689\n",
      "iteration number: 4236\t training loss: 0.0291\tvalidation loss: 0.1125\t validation accuracy: 0.9667\n",
      "iteration number: 4237\t training loss: 0.0291\tvalidation loss: 0.1125\t validation accuracy: 0.9667\n",
      "iteration number: 4238\t training loss: 0.0291\tvalidation loss: 0.1122\t validation accuracy: 0.9667\n",
      "iteration number: 4239\t training loss: 0.0289\tvalidation loss: 0.1111\t validation accuracy: 0.9667\n",
      "iteration number: 4240\t training loss: 0.0289\tvalidation loss: 0.1097\t validation accuracy: 0.9667\n",
      "iteration number: 4241\t training loss: 0.0287\tvalidation loss: 0.1085\t validation accuracy: 0.9667\n",
      "iteration number: 4242\t training loss: 0.0288\tvalidation loss: 0.1077\t validation accuracy: 0.9689\n",
      "iteration number: 4243\t training loss: 0.0287\tvalidation loss: 0.1081\t validation accuracy: 0.9689\n",
      "iteration number: 4244\t training loss: 0.0285\tvalidation loss: 0.1086\t validation accuracy: 0.9689\n",
      "iteration number: 4245\t training loss: 0.0285\tvalidation loss: 0.1093\t validation accuracy: 0.9667\n",
      "iteration number: 4246\t training loss: 0.0285\tvalidation loss: 0.1115\t validation accuracy: 0.9667\n",
      "iteration number: 4247\t training loss: 0.0291\tvalidation loss: 0.1120\t validation accuracy: 0.9667\n",
      "iteration number: 4248\t training loss: 0.0290\tvalidation loss: 0.1117\t validation accuracy: 0.9689\n",
      "iteration number: 4249\t training loss: 0.0287\tvalidation loss: 0.1133\t validation accuracy: 0.9667\n",
      "iteration number: 4250\t training loss: 0.0289\tvalidation loss: 0.1136\t validation accuracy: 0.9644\n",
      "iteration number: 4251\t training loss: 0.0288\tvalidation loss: 0.1128\t validation accuracy: 0.9644\n",
      "iteration number: 4252\t training loss: 0.0285\tvalidation loss: 0.1119\t validation accuracy: 0.9667\n",
      "iteration number: 4253\t training loss: 0.0287\tvalidation loss: 0.1121\t validation accuracy: 0.9667\n",
      "iteration number: 4254\t training loss: 0.0286\tvalidation loss: 0.1134\t validation accuracy: 0.9667\n",
      "iteration number: 4255\t training loss: 0.0287\tvalidation loss: 0.1111\t validation accuracy: 0.9644\n",
      "iteration number: 4256\t training loss: 0.0292\tvalidation loss: 0.1097\t validation accuracy: 0.9689\n",
      "iteration number: 4257\t training loss: 0.0294\tvalidation loss: 0.1098\t validation accuracy: 0.9667\n",
      "iteration number: 4258\t training loss: 0.0293\tvalidation loss: 0.1094\t validation accuracy: 0.9667\n",
      "iteration number: 4259\t training loss: 0.0293\tvalidation loss: 0.1093\t validation accuracy: 0.9667\n",
      "iteration number: 4260\t training loss: 0.0291\tvalidation loss: 0.1097\t validation accuracy: 0.9667\n",
      "iteration number: 4261\t training loss: 0.0290\tvalidation loss: 0.1108\t validation accuracy: 0.9644\n",
      "iteration number: 4262\t training loss: 0.0289\tvalidation loss: 0.1105\t validation accuracy: 0.9667\n",
      "iteration number: 4263\t training loss: 0.0289\tvalidation loss: 0.1103\t validation accuracy: 0.9667\n",
      "iteration number: 4264\t training loss: 0.0286\tvalidation loss: 0.1114\t validation accuracy: 0.9667\n",
      "iteration number: 4265\t training loss: 0.0288\tvalidation loss: 0.1106\t validation accuracy: 0.9644\n",
      "iteration number: 4266\t training loss: 0.0289\tvalidation loss: 0.1102\t validation accuracy: 0.9667\n",
      "iteration number: 4267\t training loss: 0.0291\tvalidation loss: 0.1112\t validation accuracy: 0.9600\n",
      "iteration number: 4268\t training loss: 0.0289\tvalidation loss: 0.1115\t validation accuracy: 0.9622\n",
      "iteration number: 4269\t training loss: 0.0289\tvalidation loss: 0.1113\t validation accuracy: 0.9600\n",
      "iteration number: 4270\t training loss: 0.0291\tvalidation loss: 0.1128\t validation accuracy: 0.9600\n",
      "iteration number: 4271\t training loss: 0.0289\tvalidation loss: 0.1135\t validation accuracy: 0.9622\n",
      "iteration number: 4272\t training loss: 0.0287\tvalidation loss: 0.1146\t validation accuracy: 0.9600\n",
      "iteration number: 4273\t training loss: 0.0284\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 4274\t training loss: 0.0285\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n",
      "iteration number: 4275\t training loss: 0.0285\tvalidation loss: 0.1134\t validation accuracy: 0.9600\n",
      "iteration number: 4276\t training loss: 0.0286\tvalidation loss: 0.1136\t validation accuracy: 0.9622\n",
      "iteration number: 4277\t training loss: 0.0286\tvalidation loss: 0.1162\t validation accuracy: 0.9600\n",
      "iteration number: 4278\t training loss: 0.0292\tvalidation loss: 0.1175\t validation accuracy: 0.9622\n",
      "iteration number: 4279\t training loss: 0.0290\tvalidation loss: 0.1167\t validation accuracy: 0.9622\n",
      "iteration number: 4280\t training loss: 0.0285\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 4281\t training loss: 0.0286\tvalidation loss: 0.1154\t validation accuracy: 0.9622\n",
      "iteration number: 4282\t training loss: 0.0283\tvalidation loss: 0.1102\t validation accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4283\t training loss: 0.0283\tvalidation loss: 0.1087\t validation accuracy: 0.9644\n",
      "iteration number: 4284\t training loss: 0.0281\tvalidation loss: 0.1119\t validation accuracy: 0.9644\n",
      "iteration number: 4285\t training loss: 0.0282\tvalidation loss: 0.1107\t validation accuracy: 0.9667\n",
      "iteration number: 4286\t training loss: 0.0282\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 4287\t training loss: 0.0287\tvalidation loss: 0.1121\t validation accuracy: 0.9644\n",
      "iteration number: 4288\t training loss: 0.0283\tvalidation loss: 0.1100\t validation accuracy: 0.9667\n",
      "iteration number: 4289\t training loss: 0.0283\tvalidation loss: 0.1092\t validation accuracy: 0.9667\n",
      "iteration number: 4290\t training loss: 0.0283\tvalidation loss: 0.1103\t validation accuracy: 0.9667\n",
      "iteration number: 4291\t training loss: 0.0283\tvalidation loss: 0.1109\t validation accuracy: 0.9644\n",
      "iteration number: 4292\t training loss: 0.0284\tvalidation loss: 0.1094\t validation accuracy: 0.9644\n",
      "iteration number: 4293\t training loss: 0.0286\tvalidation loss: 0.1079\t validation accuracy: 0.9644\n",
      "iteration number: 4294\t training loss: 0.0285\tvalidation loss: 0.1087\t validation accuracy: 0.9644\n",
      "iteration number: 4295\t training loss: 0.0281\tvalidation loss: 0.1092\t validation accuracy: 0.9644\n",
      "iteration number: 4296\t training loss: 0.0283\tvalidation loss: 0.1114\t validation accuracy: 0.9622\n",
      "iteration number: 4297\t training loss: 0.0286\tvalidation loss: 0.1139\t validation accuracy: 0.9644\n",
      "iteration number: 4298\t training loss: 0.0295\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 4299\t training loss: 0.0291\tvalidation loss: 0.1100\t validation accuracy: 0.9667\n",
      "iteration number: 4300\t training loss: 0.0288\tvalidation loss: 0.1113\t validation accuracy: 0.9667\n",
      "iteration number: 4301\t training loss: 0.0287\tvalidation loss: 0.1107\t validation accuracy: 0.9667\n",
      "iteration number: 4302\t training loss: 0.0282\tvalidation loss: 0.1099\t validation accuracy: 0.9667\n",
      "iteration number: 4303\t training loss: 0.0282\tvalidation loss: 0.1090\t validation accuracy: 0.9689\n",
      "iteration number: 4304\t training loss: 0.0280\tvalidation loss: 0.1095\t validation accuracy: 0.9667\n",
      "iteration number: 4305\t training loss: 0.0282\tvalidation loss: 0.1084\t validation accuracy: 0.9689\n",
      "iteration number: 4306\t training loss: 0.0279\tvalidation loss: 0.1106\t validation accuracy: 0.9667\n",
      "iteration number: 4307\t training loss: 0.0279\tvalidation loss: 0.1114\t validation accuracy: 0.9667\n",
      "iteration number: 4308\t training loss: 0.0279\tvalidation loss: 0.1125\t validation accuracy: 0.9644\n",
      "iteration number: 4309\t training loss: 0.0280\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 4310\t training loss: 0.0279\tvalidation loss: 0.1116\t validation accuracy: 0.9667\n",
      "iteration number: 4311\t training loss: 0.0280\tvalidation loss: 0.1109\t validation accuracy: 0.9667\n",
      "iteration number: 4312\t training loss: 0.0279\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 4313\t training loss: 0.0280\tvalidation loss: 0.1142\t validation accuracy: 0.9622\n",
      "iteration number: 4314\t training loss: 0.0281\tvalidation loss: 0.1160\t validation accuracy: 0.9622\n",
      "iteration number: 4315\t training loss: 0.0280\tvalidation loss: 0.1159\t validation accuracy: 0.9622\n",
      "iteration number: 4316\t training loss: 0.0279\tvalidation loss: 0.1139\t validation accuracy: 0.9667\n",
      "iteration number: 4317\t training loss: 0.0279\tvalidation loss: 0.1140\t validation accuracy: 0.9622\n",
      "iteration number: 4318\t training loss: 0.0279\tvalidation loss: 0.1129\t validation accuracy: 0.9644\n",
      "iteration number: 4319\t training loss: 0.0282\tvalidation loss: 0.1120\t validation accuracy: 0.9644\n",
      "iteration number: 4320\t training loss: 0.0277\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 4321\t training loss: 0.0278\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 4322\t training loss: 0.0277\tvalidation loss: 0.1139\t validation accuracy: 0.9644\n",
      "iteration number: 4323\t training loss: 0.0277\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 4324\t training loss: 0.0278\tvalidation loss: 0.1122\t validation accuracy: 0.9667\n",
      "iteration number: 4325\t training loss: 0.0280\tvalidation loss: 0.1127\t validation accuracy: 0.9667\n",
      "iteration number: 4326\t training loss: 0.0283\tvalidation loss: 0.1140\t validation accuracy: 0.9667\n",
      "iteration number: 4327\t training loss: 0.0289\tvalidation loss: 0.1163\t validation accuracy: 0.9622\n",
      "iteration number: 4328\t training loss: 0.0288\tvalidation loss: 0.1144\t validation accuracy: 0.9644\n",
      "iteration number: 4329\t training loss: 0.0300\tvalidation loss: 0.1154\t validation accuracy: 0.9622\n",
      "iteration number: 4330\t training loss: 0.0292\tvalidation loss: 0.1144\t validation accuracy: 0.9622\n",
      "iteration number: 4331\t training loss: 0.0284\tvalidation loss: 0.1107\t validation accuracy: 0.9689\n",
      "iteration number: 4332\t training loss: 0.0280\tvalidation loss: 0.1120\t validation accuracy: 0.9667\n",
      "iteration number: 4333\t training loss: 0.0287\tvalidation loss: 0.1134\t validation accuracy: 0.9689\n",
      "iteration number: 4334\t training loss: 0.0287\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 4335\t training loss: 0.0285\tvalidation loss: 0.1136\t validation accuracy: 0.9644\n",
      "iteration number: 4336\t training loss: 0.0279\tvalidation loss: 0.1133\t validation accuracy: 0.9667\n",
      "iteration number: 4337\t training loss: 0.0278\tvalidation loss: 0.1120\t validation accuracy: 0.9667\n",
      "iteration number: 4338\t training loss: 0.0280\tvalidation loss: 0.1117\t validation accuracy: 0.9644\n",
      "iteration number: 4339\t training loss: 0.0283\tvalidation loss: 0.1123\t validation accuracy: 0.9667\n",
      "iteration number: 4340\t training loss: 0.0283\tvalidation loss: 0.1114\t validation accuracy: 0.9667\n",
      "iteration number: 4341\t training loss: 0.0280\tvalidation loss: 0.1107\t validation accuracy: 0.9667\n",
      "iteration number: 4342\t training loss: 0.0282\tvalidation loss: 0.1110\t validation accuracy: 0.9644\n",
      "iteration number: 4343\t training loss: 0.0279\tvalidation loss: 0.1115\t validation accuracy: 0.9667\n",
      "iteration number: 4344\t training loss: 0.0278\tvalidation loss: 0.1112\t validation accuracy: 0.9667\n",
      "iteration number: 4345\t training loss: 0.0279\tvalidation loss: 0.1115\t validation accuracy: 0.9644\n",
      "iteration number: 4346\t training loss: 0.0278\tvalidation loss: 0.1113\t validation accuracy: 0.9644\n",
      "iteration number: 4347\t training loss: 0.0280\tvalidation loss: 0.1121\t validation accuracy: 0.9667\n",
      "iteration number: 4348\t training loss: 0.0279\tvalidation loss: 0.1102\t validation accuracy: 0.9689\n",
      "iteration number: 4349\t training loss: 0.0275\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 4350\t training loss: 0.0276\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 4351\t training loss: 0.0276\tvalidation loss: 0.1134\t validation accuracy: 0.9644\n",
      "iteration number: 4352\t training loss: 0.0282\tvalidation loss: 0.1131\t validation accuracy: 0.9622\n",
      "iteration number: 4353\t training loss: 0.0277\tvalidation loss: 0.1142\t validation accuracy: 0.9622\n",
      "iteration number: 4354\t training loss: 0.0278\tvalidation loss: 0.1147\t validation accuracy: 0.9622\n",
      "iteration number: 4355\t training loss: 0.0282\tvalidation loss: 0.1157\t validation accuracy: 0.9644\n",
      "iteration number: 4356\t training loss: 0.0283\tvalidation loss: 0.1162\t validation accuracy: 0.9667\n",
      "iteration number: 4357\t training loss: 0.0282\tvalidation loss: 0.1158\t validation accuracy: 0.9667\n",
      "iteration number: 4358\t training loss: 0.0284\tvalidation loss: 0.1163\t validation accuracy: 0.9689\n",
      "iteration number: 4359\t training loss: 0.0279\tvalidation loss: 0.1141\t validation accuracy: 0.9644\n",
      "iteration number: 4360\t training loss: 0.0278\tvalidation loss: 0.1120\t validation accuracy: 0.9667\n",
      "iteration number: 4361\t training loss: 0.0277\tvalidation loss: 0.1116\t validation accuracy: 0.9667\n",
      "iteration number: 4362\t training loss: 0.0277\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 4363\t training loss: 0.0276\tvalidation loss: 0.1098\t validation accuracy: 0.9667\n",
      "iteration number: 4364\t training loss: 0.0276\tvalidation loss: 0.1102\t validation accuracy: 0.9689\n",
      "iteration number: 4365\t training loss: 0.0280\tvalidation loss: 0.1090\t validation accuracy: 0.9667\n",
      "iteration number: 4366\t training loss: 0.0280\tvalidation loss: 0.1086\t validation accuracy: 0.9667\n",
      "iteration number: 4367\t training loss: 0.0283\tvalidation loss: 0.1103\t validation accuracy: 0.9689\n",
      "iteration number: 4368\t training loss: 0.0279\tvalidation loss: 0.1112\t validation accuracy: 0.9711\n",
      "iteration number: 4369\t training loss: 0.0277\tvalidation loss: 0.1093\t validation accuracy: 0.9711\n",
      "iteration number: 4370\t training loss: 0.0278\tvalidation loss: 0.1101\t validation accuracy: 0.9711\n",
      "iteration number: 4371\t training loss: 0.0278\tvalidation loss: 0.1103\t validation accuracy: 0.9711\n",
      "iteration number: 4372\t training loss: 0.0279\tvalidation loss: 0.1108\t validation accuracy: 0.9711\n",
      "iteration number: 4373\t training loss: 0.0279\tvalidation loss: 0.1092\t validation accuracy: 0.9689\n",
      "iteration number: 4374\t training loss: 0.0276\tvalidation loss: 0.1099\t validation accuracy: 0.9689\n",
      "iteration number: 4375\t training loss: 0.0282\tvalidation loss: 0.1089\t validation accuracy: 0.9644\n",
      "iteration number: 4376\t training loss: 0.0287\tvalidation loss: 0.1093\t validation accuracy: 0.9689\n",
      "iteration number: 4377\t training loss: 0.0284\tvalidation loss: 0.1092\t validation accuracy: 0.9689\n",
      "iteration number: 4378\t training loss: 0.0287\tvalidation loss: 0.1117\t validation accuracy: 0.9689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4379\t training loss: 0.0285\tvalidation loss: 0.1111\t validation accuracy: 0.9667\n",
      "iteration number: 4380\t training loss: 0.0282\tvalidation loss: 0.1114\t validation accuracy: 0.9667\n",
      "iteration number: 4381\t training loss: 0.0283\tvalidation loss: 0.1111\t validation accuracy: 0.9667\n",
      "iteration number: 4382\t training loss: 0.0278\tvalidation loss: 0.1129\t validation accuracy: 0.9667\n",
      "iteration number: 4383\t training loss: 0.0282\tvalidation loss: 0.1139\t validation accuracy: 0.9689\n",
      "iteration number: 4384\t training loss: 0.0281\tvalidation loss: 0.1130\t validation accuracy: 0.9689\n",
      "iteration number: 4385\t training loss: 0.0279\tvalidation loss: 0.1137\t validation accuracy: 0.9667\n",
      "iteration number: 4386\t training loss: 0.0279\tvalidation loss: 0.1130\t validation accuracy: 0.9667\n",
      "iteration number: 4387\t training loss: 0.0277\tvalidation loss: 0.1116\t validation accuracy: 0.9667\n",
      "iteration number: 4388\t training loss: 0.0277\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 4389\t training loss: 0.0281\tvalidation loss: 0.1126\t validation accuracy: 0.9689\n",
      "iteration number: 4390\t training loss: 0.0283\tvalidation loss: 0.1138\t validation accuracy: 0.9667\n",
      "iteration number: 4391\t training loss: 0.0284\tvalidation loss: 0.1120\t validation accuracy: 0.9667\n",
      "iteration number: 4392\t training loss: 0.0286\tvalidation loss: 0.1124\t validation accuracy: 0.9689\n",
      "iteration number: 4393\t training loss: 0.0285\tvalidation loss: 0.1135\t validation accuracy: 0.9689\n",
      "iteration number: 4394\t training loss: 0.0282\tvalidation loss: 0.1134\t validation accuracy: 0.9689\n",
      "iteration number: 4395\t training loss: 0.0281\tvalidation loss: 0.1136\t validation accuracy: 0.9689\n",
      "iteration number: 4396\t training loss: 0.0289\tvalidation loss: 0.1152\t validation accuracy: 0.9689\n",
      "iteration number: 4397\t training loss: 0.0287\tvalidation loss: 0.1148\t validation accuracy: 0.9689\n",
      "iteration number: 4398\t training loss: 0.0285\tvalidation loss: 0.1144\t validation accuracy: 0.9689\n",
      "iteration number: 4399\t training loss: 0.0282\tvalidation loss: 0.1139\t validation accuracy: 0.9711\n",
      "iteration number: 4400\t training loss: 0.0280\tvalidation loss: 0.1148\t validation accuracy: 0.9711\n",
      "iteration number: 4401\t training loss: 0.0272\tvalidation loss: 0.1146\t validation accuracy: 0.9689\n",
      "iteration number: 4402\t training loss: 0.0272\tvalidation loss: 0.1144\t validation accuracy: 0.9689\n",
      "iteration number: 4403\t training loss: 0.0271\tvalidation loss: 0.1144\t validation accuracy: 0.9689\n",
      "iteration number: 4404\t training loss: 0.0273\tvalidation loss: 0.1164\t validation accuracy: 0.9667\n",
      "iteration number: 4405\t training loss: 0.0272\tvalidation loss: 0.1162\t validation accuracy: 0.9644\n",
      "iteration number: 4406\t training loss: 0.0272\tvalidation loss: 0.1161\t validation accuracy: 0.9644\n",
      "iteration number: 4407\t training loss: 0.0275\tvalidation loss: 0.1170\t validation accuracy: 0.9667\n",
      "iteration number: 4408\t training loss: 0.0271\tvalidation loss: 0.1146\t validation accuracy: 0.9667\n",
      "iteration number: 4409\t training loss: 0.0270\tvalidation loss: 0.1137\t validation accuracy: 0.9667\n",
      "iteration number: 4410\t training loss: 0.0272\tvalidation loss: 0.1150\t validation accuracy: 0.9644\n",
      "iteration number: 4411\t training loss: 0.0270\tvalidation loss: 0.1141\t validation accuracy: 0.9667\n",
      "iteration number: 4412\t training loss: 0.0271\tvalidation loss: 0.1158\t validation accuracy: 0.9667\n",
      "iteration number: 4413\t training loss: 0.0272\tvalidation loss: 0.1167\t validation accuracy: 0.9644\n",
      "iteration number: 4414\t training loss: 0.0271\tvalidation loss: 0.1160\t validation accuracy: 0.9644\n",
      "iteration number: 4415\t training loss: 0.0270\tvalidation loss: 0.1148\t validation accuracy: 0.9667\n",
      "iteration number: 4416\t training loss: 0.0271\tvalidation loss: 0.1150\t validation accuracy: 0.9644\n",
      "iteration number: 4417\t training loss: 0.0271\tvalidation loss: 0.1162\t validation accuracy: 0.9644\n",
      "iteration number: 4418\t training loss: 0.0270\tvalidation loss: 0.1154\t validation accuracy: 0.9667\n",
      "iteration number: 4419\t training loss: 0.0271\tvalidation loss: 0.1153\t validation accuracy: 0.9667\n",
      "iteration number: 4420\t training loss: 0.0271\tvalidation loss: 0.1168\t validation accuracy: 0.9667\n",
      "iteration number: 4421\t training loss: 0.0272\tvalidation loss: 0.1167\t validation accuracy: 0.9667\n",
      "iteration number: 4422\t training loss: 0.0270\tvalidation loss: 0.1147\t validation accuracy: 0.9667\n",
      "iteration number: 4423\t training loss: 0.0271\tvalidation loss: 0.1165\t validation accuracy: 0.9667\n",
      "iteration number: 4424\t training loss: 0.0272\tvalidation loss: 0.1182\t validation accuracy: 0.9667\n",
      "iteration number: 4425\t training loss: 0.0276\tvalidation loss: 0.1200\t validation accuracy: 0.9644\n",
      "iteration number: 4426\t training loss: 0.0277\tvalidation loss: 0.1212\t validation accuracy: 0.9622\n",
      "iteration number: 4427\t training loss: 0.0277\tvalidation loss: 0.1212\t validation accuracy: 0.9622\n",
      "iteration number: 4428\t training loss: 0.0270\tvalidation loss: 0.1156\t validation accuracy: 0.9644\n",
      "iteration number: 4429\t training loss: 0.0269\tvalidation loss: 0.1143\t validation accuracy: 0.9667\n",
      "iteration number: 4430\t training loss: 0.0270\tvalidation loss: 0.1163\t validation accuracy: 0.9644\n",
      "iteration number: 4431\t training loss: 0.0272\tvalidation loss: 0.1175\t validation accuracy: 0.9622\n",
      "iteration number: 4432\t training loss: 0.0269\tvalidation loss: 0.1160\t validation accuracy: 0.9667\n",
      "iteration number: 4433\t training loss: 0.0271\tvalidation loss: 0.1149\t validation accuracy: 0.9644\n",
      "iteration number: 4434\t training loss: 0.0269\tvalidation loss: 0.1151\t validation accuracy: 0.9644\n",
      "iteration number: 4435\t training loss: 0.0268\tvalidation loss: 0.1139\t validation accuracy: 0.9667\n",
      "iteration number: 4436\t training loss: 0.0275\tvalidation loss: 0.1123\t validation accuracy: 0.9644\n",
      "iteration number: 4437\t training loss: 0.0271\tvalidation loss: 0.1120\t validation accuracy: 0.9644\n",
      "iteration number: 4438\t training loss: 0.0276\tvalidation loss: 0.1118\t validation accuracy: 0.9689\n",
      "iteration number: 4439\t training loss: 0.0275\tvalidation loss: 0.1098\t validation accuracy: 0.9689\n",
      "iteration number: 4440\t training loss: 0.0272\tvalidation loss: 0.1095\t validation accuracy: 0.9689\n",
      "iteration number: 4441\t training loss: 0.0272\tvalidation loss: 0.1093\t validation accuracy: 0.9689\n",
      "iteration number: 4442\t training loss: 0.0275\tvalidation loss: 0.1090\t validation accuracy: 0.9689\n",
      "iteration number: 4443\t training loss: 0.0279\tvalidation loss: 0.1111\t validation accuracy: 0.9689\n",
      "iteration number: 4444\t training loss: 0.0283\tvalidation loss: 0.1116\t validation accuracy: 0.9711\n",
      "iteration number: 4445\t training loss: 0.0277\tvalidation loss: 0.1106\t validation accuracy: 0.9711\n",
      "iteration number: 4446\t training loss: 0.0272\tvalidation loss: 0.1095\t validation accuracy: 0.9711\n",
      "iteration number: 4447\t training loss: 0.0273\tvalidation loss: 0.1096\t validation accuracy: 0.9711\n",
      "iteration number: 4448\t training loss: 0.0272\tvalidation loss: 0.1105\t validation accuracy: 0.9667\n",
      "iteration number: 4449\t training loss: 0.0283\tvalidation loss: 0.1105\t validation accuracy: 0.9644\n",
      "iteration number: 4450\t training loss: 0.0278\tvalidation loss: 0.1122\t validation accuracy: 0.9600\n",
      "iteration number: 4451\t training loss: 0.0279\tvalidation loss: 0.1116\t validation accuracy: 0.9600\n",
      "iteration number: 4452\t training loss: 0.0270\tvalidation loss: 0.1101\t validation accuracy: 0.9667\n",
      "iteration number: 4453\t training loss: 0.0276\tvalidation loss: 0.1091\t validation accuracy: 0.9622\n",
      "iteration number: 4454\t training loss: 0.0271\tvalidation loss: 0.1101\t validation accuracy: 0.9622\n",
      "iteration number: 4455\t training loss: 0.0273\tvalidation loss: 0.1099\t validation accuracy: 0.9667\n",
      "iteration number: 4456\t training loss: 0.0280\tvalidation loss: 0.1103\t validation accuracy: 0.9622\n",
      "iteration number: 4457\t training loss: 0.0276\tvalidation loss: 0.1093\t validation accuracy: 0.9622\n",
      "iteration number: 4458\t training loss: 0.0278\tvalidation loss: 0.1100\t validation accuracy: 0.9622\n",
      "iteration number: 4459\t training loss: 0.0273\tvalidation loss: 0.1096\t validation accuracy: 0.9622\n",
      "iteration number: 4460\t training loss: 0.0269\tvalidation loss: 0.1081\t validation accuracy: 0.9667\n",
      "iteration number: 4461\t training loss: 0.0268\tvalidation loss: 0.1106\t validation accuracy: 0.9667\n",
      "iteration number: 4462\t training loss: 0.0281\tvalidation loss: 0.1111\t validation accuracy: 0.9667\n",
      "iteration number: 4463\t training loss: 0.0276\tvalidation loss: 0.1107\t validation accuracy: 0.9667\n",
      "iteration number: 4464\t training loss: 0.0269\tvalidation loss: 0.1107\t validation accuracy: 0.9667\n",
      "iteration number: 4465\t training loss: 0.0267\tvalidation loss: 0.1102\t validation accuracy: 0.9667\n",
      "iteration number: 4466\t training loss: 0.0269\tvalidation loss: 0.1094\t validation accuracy: 0.9667\n",
      "iteration number: 4467\t training loss: 0.0270\tvalidation loss: 0.1101\t validation accuracy: 0.9667\n",
      "iteration number: 4468\t training loss: 0.0268\tvalidation loss: 0.1098\t validation accuracy: 0.9667\n",
      "iteration number: 4469\t training loss: 0.0275\tvalidation loss: 0.1081\t validation accuracy: 0.9667\n",
      "iteration number: 4470\t training loss: 0.0271\tvalidation loss: 0.1085\t validation accuracy: 0.9667\n",
      "iteration number: 4471\t training loss: 0.0269\tvalidation loss: 0.1093\t validation accuracy: 0.9667\n",
      "iteration number: 4472\t training loss: 0.0270\tvalidation loss: 0.1095\t validation accuracy: 0.9667\n",
      "iteration number: 4473\t training loss: 0.0269\tvalidation loss: 0.1089\t validation accuracy: 0.9667\n",
      "iteration number: 4474\t training loss: 0.0266\tvalidation loss: 0.1096\t validation accuracy: 0.9689\n",
      "iteration number: 4475\t training loss: 0.0265\tvalidation loss: 0.1114\t validation accuracy: 0.9689\n",
      "iteration number: 4476\t training loss: 0.0264\tvalidation loss: 0.1125\t validation accuracy: 0.9689\n",
      "iteration number: 4477\t training loss: 0.0264\tvalidation loss: 0.1123\t validation accuracy: 0.9667\n",
      "iteration number: 4478\t training loss: 0.0265\tvalidation loss: 0.1144\t validation accuracy: 0.9667\n",
      "iteration number: 4479\t training loss: 0.0267\tvalidation loss: 0.1149\t validation accuracy: 0.9644\n",
      "iteration number: 4480\t training loss: 0.0271\tvalidation loss: 0.1166\t validation accuracy: 0.9667\n",
      "iteration number: 4481\t training loss: 0.0270\tvalidation loss: 0.1145\t validation accuracy: 0.9667\n",
      "iteration number: 4482\t training loss: 0.0270\tvalidation loss: 0.1150\t validation accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4483\t training loss: 0.0272\tvalidation loss: 0.1149\t validation accuracy: 0.9667\n",
      "iteration number: 4484\t training loss: 0.0273\tvalidation loss: 0.1137\t validation accuracy: 0.9667\n",
      "iteration number: 4485\t training loss: 0.0273\tvalidation loss: 0.1164\t validation accuracy: 0.9644\n",
      "iteration number: 4486\t training loss: 0.0265\tvalidation loss: 0.1105\t validation accuracy: 0.9689\n",
      "iteration number: 4487\t training loss: 0.0266\tvalidation loss: 0.1101\t validation accuracy: 0.9689\n",
      "iteration number: 4488\t training loss: 0.0266\tvalidation loss: 0.1107\t validation accuracy: 0.9689\n",
      "iteration number: 4489\t training loss: 0.0272\tvalidation loss: 0.1101\t validation accuracy: 0.9711\n",
      "iteration number: 4490\t training loss: 0.0271\tvalidation loss: 0.1138\t validation accuracy: 0.9667\n",
      "iteration number: 4491\t training loss: 0.0269\tvalidation loss: 0.1135\t validation accuracy: 0.9667\n",
      "iteration number: 4492\t training loss: 0.0270\tvalidation loss: 0.1154\t validation accuracy: 0.9644\n",
      "iteration number: 4493\t training loss: 0.0263\tvalidation loss: 0.1105\t validation accuracy: 0.9667\n",
      "iteration number: 4494\t training loss: 0.0263\tvalidation loss: 0.1113\t validation accuracy: 0.9667\n",
      "iteration number: 4495\t training loss: 0.0262\tvalidation loss: 0.1132\t validation accuracy: 0.9689\n",
      "iteration number: 4496\t training loss: 0.0263\tvalidation loss: 0.1130\t validation accuracy: 0.9667\n",
      "iteration number: 4497\t training loss: 0.0265\tvalidation loss: 0.1137\t validation accuracy: 0.9644\n",
      "iteration number: 4498\t training loss: 0.0262\tvalidation loss: 0.1123\t validation accuracy: 0.9667\n",
      "iteration number: 4499\t training loss: 0.0262\tvalidation loss: 0.1140\t validation accuracy: 0.9689\n",
      "iteration number: 4500\t training loss: 0.0262\tvalidation loss: 0.1152\t validation accuracy: 0.9644\n",
      "iteration number: 4501\t training loss: 0.0264\tvalidation loss: 0.1174\t validation accuracy: 0.9622\n",
      "iteration number: 4502\t training loss: 0.0265\tvalidation loss: 0.1183\t validation accuracy: 0.9644\n",
      "iteration number: 4503\t training loss: 0.0267\tvalidation loss: 0.1183\t validation accuracy: 0.9644\n",
      "iteration number: 4504\t training loss: 0.0269\tvalidation loss: 0.1189\t validation accuracy: 0.9644\n",
      "iteration number: 4505\t training loss: 0.0273\tvalidation loss: 0.1178\t validation accuracy: 0.9600\n",
      "iteration number: 4506\t training loss: 0.0269\tvalidation loss: 0.1166\t validation accuracy: 0.9600\n",
      "iteration number: 4507\t training loss: 0.0266\tvalidation loss: 0.1168\t validation accuracy: 0.9600\n",
      "iteration number: 4508\t training loss: 0.0267\tvalidation loss: 0.1168\t validation accuracy: 0.9600\n",
      "iteration number: 4509\t training loss: 0.0261\tvalidation loss: 0.1145\t validation accuracy: 0.9644\n",
      "iteration number: 4510\t training loss: 0.0261\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 4511\t training loss: 0.0260\tvalidation loss: 0.1134\t validation accuracy: 0.9644\n",
      "iteration number: 4512\t training loss: 0.0260\tvalidation loss: 0.1140\t validation accuracy: 0.9644\n",
      "iteration number: 4513\t training loss: 0.0261\tvalidation loss: 0.1140\t validation accuracy: 0.9644\n",
      "iteration number: 4514\t training loss: 0.0261\tvalidation loss: 0.1145\t validation accuracy: 0.9644\n",
      "iteration number: 4515\t training loss: 0.0261\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 4516\t training loss: 0.0262\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 4517\t training loss: 0.0263\tvalidation loss: 0.1121\t validation accuracy: 0.9644\n",
      "iteration number: 4518\t training loss: 0.0261\tvalidation loss: 0.1140\t validation accuracy: 0.9644\n",
      "iteration number: 4519\t training loss: 0.0260\tvalidation loss: 0.1131\t validation accuracy: 0.9667\n",
      "iteration number: 4520\t training loss: 0.0260\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 4521\t training loss: 0.0260\tvalidation loss: 0.1138\t validation accuracy: 0.9667\n",
      "iteration number: 4522\t training loss: 0.0260\tvalidation loss: 0.1143\t validation accuracy: 0.9667\n",
      "iteration number: 4523\t training loss: 0.0260\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 4524\t training loss: 0.0260\tvalidation loss: 0.1139\t validation accuracy: 0.9644\n",
      "iteration number: 4525\t training loss: 0.0265\tvalidation loss: 0.1126\t validation accuracy: 0.9644\n",
      "iteration number: 4526\t training loss: 0.0261\tvalidation loss: 0.1143\t validation accuracy: 0.9644\n",
      "iteration number: 4527\t training loss: 0.0260\tvalidation loss: 0.1152\t validation accuracy: 0.9644\n",
      "iteration number: 4528\t training loss: 0.0260\tvalidation loss: 0.1158\t validation accuracy: 0.9644\n",
      "iteration number: 4529\t training loss: 0.0266\tvalidation loss: 0.1203\t validation accuracy: 0.9600\n",
      "iteration number: 4530\t training loss: 0.0270\tvalidation loss: 0.1210\t validation accuracy: 0.9600\n",
      "iteration number: 4531\t training loss: 0.0268\tvalidation loss: 0.1201\t validation accuracy: 0.9600\n",
      "iteration number: 4532\t training loss: 0.0266\tvalidation loss: 0.1198\t validation accuracy: 0.9622\n",
      "iteration number: 4533\t training loss: 0.0262\tvalidation loss: 0.1167\t validation accuracy: 0.9644\n",
      "iteration number: 4534\t training loss: 0.0260\tvalidation loss: 0.1150\t validation accuracy: 0.9644\n",
      "iteration number: 4535\t training loss: 0.0258\tvalidation loss: 0.1138\t validation accuracy: 0.9667\n",
      "iteration number: 4536\t training loss: 0.0260\tvalidation loss: 0.1150\t validation accuracy: 0.9644\n",
      "iteration number: 4537\t training loss: 0.0260\tvalidation loss: 0.1150\t validation accuracy: 0.9667\n",
      "iteration number: 4538\t training loss: 0.0264\tvalidation loss: 0.1184\t validation accuracy: 0.9600\n",
      "iteration number: 4539\t training loss: 0.0264\tvalidation loss: 0.1182\t validation accuracy: 0.9600\n",
      "iteration number: 4540\t training loss: 0.0269\tvalidation loss: 0.1205\t validation accuracy: 0.9644\n",
      "iteration number: 4541\t training loss: 0.0263\tvalidation loss: 0.1167\t validation accuracy: 0.9622\n",
      "iteration number: 4542\t training loss: 0.0265\tvalidation loss: 0.1179\t validation accuracy: 0.9622\n",
      "iteration number: 4543\t training loss: 0.0260\tvalidation loss: 0.1145\t validation accuracy: 0.9644\n",
      "iteration number: 4544\t training loss: 0.0261\tvalidation loss: 0.1156\t validation accuracy: 0.9644\n",
      "iteration number: 4545\t training loss: 0.0264\tvalidation loss: 0.1174\t validation accuracy: 0.9644\n",
      "iteration number: 4546\t training loss: 0.0266\tvalidation loss: 0.1180\t validation accuracy: 0.9622\n",
      "iteration number: 4547\t training loss: 0.0260\tvalidation loss: 0.1149\t validation accuracy: 0.9667\n",
      "iteration number: 4548\t training loss: 0.0259\tvalidation loss: 0.1143\t validation accuracy: 0.9689\n",
      "iteration number: 4549\t training loss: 0.0260\tvalidation loss: 0.1149\t validation accuracy: 0.9644\n",
      "iteration number: 4550\t training loss: 0.0260\tvalidation loss: 0.1153\t validation accuracy: 0.9644\n",
      "iteration number: 4551\t training loss: 0.0261\tvalidation loss: 0.1158\t validation accuracy: 0.9622\n",
      "iteration number: 4552\t training loss: 0.0264\tvalidation loss: 0.1177\t validation accuracy: 0.9644\n",
      "iteration number: 4553\t training loss: 0.0263\tvalidation loss: 0.1179\t validation accuracy: 0.9600\n",
      "iteration number: 4554\t training loss: 0.0265\tvalidation loss: 0.1182\t validation accuracy: 0.9600\n",
      "iteration number: 4555\t training loss: 0.0272\tvalidation loss: 0.1200\t validation accuracy: 0.9622\n",
      "iteration number: 4556\t training loss: 0.0265\tvalidation loss: 0.1175\t validation accuracy: 0.9600\n",
      "iteration number: 4557\t training loss: 0.0260\tvalidation loss: 0.1157\t validation accuracy: 0.9600\n",
      "iteration number: 4558\t training loss: 0.0261\tvalidation loss: 0.1149\t validation accuracy: 0.9600\n",
      "iteration number: 4559\t training loss: 0.0267\tvalidation loss: 0.1145\t validation accuracy: 0.9600\n",
      "iteration number: 4560\t training loss: 0.0263\tvalidation loss: 0.1142\t validation accuracy: 0.9600\n",
      "iteration number: 4561\t training loss: 0.0267\tvalidation loss: 0.1137\t validation accuracy: 0.9600\n",
      "iteration number: 4562\t training loss: 0.0269\tvalidation loss: 0.1136\t validation accuracy: 0.9600\n",
      "iteration number: 4563\t training loss: 0.0268\tvalidation loss: 0.1134\t validation accuracy: 0.9600\n",
      "iteration number: 4564\t training loss: 0.0276\tvalidation loss: 0.1126\t validation accuracy: 0.9600\n",
      "iteration number: 4565\t training loss: 0.0269\tvalidation loss: 0.1126\t validation accuracy: 0.9600\n",
      "iteration number: 4566\t training loss: 0.0268\tvalidation loss: 0.1133\t validation accuracy: 0.9600\n",
      "iteration number: 4567\t training loss: 0.0263\tvalidation loss: 0.1139\t validation accuracy: 0.9600\n",
      "iteration number: 4568\t training loss: 0.0264\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 4569\t training loss: 0.0263\tvalidation loss: 0.1144\t validation accuracy: 0.9644\n",
      "iteration number: 4570\t training loss: 0.0264\tvalidation loss: 0.1165\t validation accuracy: 0.9667\n",
      "iteration number: 4571\t training loss: 0.0267\tvalidation loss: 0.1159\t validation accuracy: 0.9622\n",
      "iteration number: 4572\t training loss: 0.0264\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 4573\t training loss: 0.0261\tvalidation loss: 0.1142\t validation accuracy: 0.9689\n",
      "iteration number: 4574\t training loss: 0.0261\tvalidation loss: 0.1140\t validation accuracy: 0.9689\n",
      "iteration number: 4575\t training loss: 0.0259\tvalidation loss: 0.1153\t validation accuracy: 0.9667\n",
      "iteration number: 4576\t training loss: 0.0262\tvalidation loss: 0.1149\t validation accuracy: 0.9689\n",
      "iteration number: 4577\t training loss: 0.0263\tvalidation loss: 0.1162\t validation accuracy: 0.9667\n",
      "iteration number: 4578\t training loss: 0.0266\tvalidation loss: 0.1164\t validation accuracy: 0.9689\n",
      "iteration number: 4579\t training loss: 0.0264\tvalidation loss: 0.1152\t validation accuracy: 0.9689\n",
      "iteration number: 4580\t training loss: 0.0262\tvalidation loss: 0.1152\t validation accuracy: 0.9689\n",
      "iteration number: 4581\t training loss: 0.0262\tvalidation loss: 0.1125\t validation accuracy: 0.9667\n",
      "iteration number: 4582\t training loss: 0.0261\tvalidation loss: 0.1130\t validation accuracy: 0.9667\n",
      "iteration number: 4583\t training loss: 0.0261\tvalidation loss: 0.1124\t validation accuracy: 0.9689\n",
      "iteration number: 4584\t training loss: 0.0261\tvalidation loss: 0.1127\t validation accuracy: 0.9689\n",
      "iteration number: 4585\t training loss: 0.0261\tvalidation loss: 0.1126\t validation accuracy: 0.9667\n",
      "iteration number: 4586\t training loss: 0.0263\tvalidation loss: 0.1123\t validation accuracy: 0.9689\n",
      "iteration number: 4587\t training loss: 0.0261\tvalidation loss: 0.1124\t validation accuracy: 0.9689\n",
      "iteration number: 4588\t training loss: 0.0267\tvalidation loss: 0.1153\t validation accuracy: 0.9689\n",
      "iteration number: 4589\t training loss: 0.0270\tvalidation loss: 0.1157\t validation accuracy: 0.9689\n",
      "iteration number: 4590\t training loss: 0.0271\tvalidation loss: 0.1150\t validation accuracy: 0.9711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4591\t training loss: 0.0268\tvalidation loss: 0.1148\t validation accuracy: 0.9644\n",
      "iteration number: 4592\t training loss: 0.0261\tvalidation loss: 0.1135\t validation accuracy: 0.9689\n",
      "iteration number: 4593\t training loss: 0.0260\tvalidation loss: 0.1143\t validation accuracy: 0.9689\n",
      "iteration number: 4594\t training loss: 0.0260\tvalidation loss: 0.1129\t validation accuracy: 0.9711\n",
      "iteration number: 4595\t training loss: 0.0258\tvalidation loss: 0.1138\t validation accuracy: 0.9689\n",
      "iteration number: 4596\t training loss: 0.0258\tvalidation loss: 0.1151\t validation accuracy: 0.9689\n",
      "iteration number: 4597\t training loss: 0.0261\tvalidation loss: 0.1182\t validation accuracy: 0.9622\n",
      "iteration number: 4598\t training loss: 0.0258\tvalidation loss: 0.1159\t validation accuracy: 0.9689\n",
      "iteration number: 4599\t training loss: 0.0258\tvalidation loss: 0.1151\t validation accuracy: 0.9689\n",
      "iteration number: 4600\t training loss: 0.0258\tvalidation loss: 0.1167\t validation accuracy: 0.9644\n",
      "iteration number: 4601\t training loss: 0.0258\tvalidation loss: 0.1165\t validation accuracy: 0.9667\n",
      "iteration number: 4602\t training loss: 0.0258\tvalidation loss: 0.1184\t validation accuracy: 0.9689\n",
      "iteration number: 4603\t training loss: 0.0257\tvalidation loss: 0.1186\t validation accuracy: 0.9622\n",
      "iteration number: 4604\t training loss: 0.0257\tvalidation loss: 0.1164\t validation accuracy: 0.9622\n",
      "iteration number: 4605\t training loss: 0.0255\tvalidation loss: 0.1149\t validation accuracy: 0.9644\n",
      "iteration number: 4606\t training loss: 0.0256\tvalidation loss: 0.1145\t validation accuracy: 0.9644\n",
      "iteration number: 4607\t training loss: 0.0255\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 4608\t training loss: 0.0255\tvalidation loss: 0.1138\t validation accuracy: 0.9667\n",
      "iteration number: 4609\t training loss: 0.0254\tvalidation loss: 0.1145\t validation accuracy: 0.9667\n",
      "iteration number: 4610\t training loss: 0.0261\tvalidation loss: 0.1141\t validation accuracy: 0.9622\n",
      "iteration number: 4611\t training loss: 0.0260\tvalidation loss: 0.1140\t validation accuracy: 0.9600\n",
      "iteration number: 4612\t training loss: 0.0259\tvalidation loss: 0.1150\t validation accuracy: 0.9644\n",
      "iteration number: 4613\t training loss: 0.0262\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 4614\t training loss: 0.0259\tvalidation loss: 0.1150\t validation accuracy: 0.9622\n",
      "iteration number: 4615\t training loss: 0.0259\tvalidation loss: 0.1144\t validation accuracy: 0.9622\n",
      "iteration number: 4616\t training loss: 0.0257\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 4617\t training loss: 0.0255\tvalidation loss: 0.1145\t validation accuracy: 0.9667\n",
      "iteration number: 4618\t training loss: 0.0259\tvalidation loss: 0.1138\t validation accuracy: 0.9689\n",
      "iteration number: 4619\t training loss: 0.0265\tvalidation loss: 0.1117\t validation accuracy: 0.9689\n",
      "iteration number: 4620\t training loss: 0.0254\tvalidation loss: 0.1130\t validation accuracy: 0.9689\n",
      "iteration number: 4621\t training loss: 0.0256\tvalidation loss: 0.1121\t validation accuracy: 0.9689\n",
      "iteration number: 4622\t training loss: 0.0261\tvalidation loss: 0.1114\t validation accuracy: 0.9689\n",
      "iteration number: 4623\t training loss: 0.0258\tvalidation loss: 0.1127\t validation accuracy: 0.9689\n",
      "iteration number: 4624\t training loss: 0.0258\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 4625\t training loss: 0.0256\tvalidation loss: 0.1134\t validation accuracy: 0.9667\n",
      "iteration number: 4626\t training loss: 0.0259\tvalidation loss: 0.1144\t validation accuracy: 0.9644\n",
      "iteration number: 4627\t training loss: 0.0254\tvalidation loss: 0.1130\t validation accuracy: 0.9644\n",
      "iteration number: 4628\t training loss: 0.0252\tvalidation loss: 0.1141\t validation accuracy: 0.9644\n",
      "iteration number: 4629\t training loss: 0.0252\tvalidation loss: 0.1140\t validation accuracy: 0.9644\n",
      "iteration number: 4630\t training loss: 0.0252\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 4631\t training loss: 0.0254\tvalidation loss: 0.1162\t validation accuracy: 0.9622\n",
      "iteration number: 4632\t training loss: 0.0257\tvalidation loss: 0.1192\t validation accuracy: 0.9622\n",
      "iteration number: 4633\t training loss: 0.0256\tvalidation loss: 0.1183\t validation accuracy: 0.9622\n",
      "iteration number: 4634\t training loss: 0.0254\tvalidation loss: 0.1164\t validation accuracy: 0.9622\n",
      "iteration number: 4635\t training loss: 0.0254\tvalidation loss: 0.1164\t validation accuracy: 0.9622\n",
      "iteration number: 4636\t training loss: 0.0254\tvalidation loss: 0.1166\t validation accuracy: 0.9622\n",
      "iteration number: 4637\t training loss: 0.0253\tvalidation loss: 0.1165\t validation accuracy: 0.9622\n",
      "iteration number: 4638\t training loss: 0.0254\tvalidation loss: 0.1159\t validation accuracy: 0.9622\n",
      "iteration number: 4639\t training loss: 0.0254\tvalidation loss: 0.1150\t validation accuracy: 0.9622\n",
      "iteration number: 4640\t training loss: 0.0255\tvalidation loss: 0.1149\t validation accuracy: 0.9622\n",
      "iteration number: 4641\t training loss: 0.0254\tvalidation loss: 0.1128\t validation accuracy: 0.9689\n",
      "iteration number: 4642\t training loss: 0.0258\tvalidation loss: 0.1121\t validation accuracy: 0.9667\n",
      "iteration number: 4643\t training loss: 0.0254\tvalidation loss: 0.1125\t validation accuracy: 0.9689\n",
      "iteration number: 4644\t training loss: 0.0254\tvalidation loss: 0.1126\t validation accuracy: 0.9689\n",
      "iteration number: 4645\t training loss: 0.0252\tvalidation loss: 0.1158\t validation accuracy: 0.9644\n",
      "iteration number: 4646\t training loss: 0.0254\tvalidation loss: 0.1145\t validation accuracy: 0.9644\n",
      "iteration number: 4647\t training loss: 0.0254\tvalidation loss: 0.1121\t validation accuracy: 0.9644\n",
      "iteration number: 4648\t training loss: 0.0257\tvalidation loss: 0.1116\t validation accuracy: 0.9644\n",
      "iteration number: 4649\t training loss: 0.0253\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 4650\t training loss: 0.0257\tvalidation loss: 0.1143\t validation accuracy: 0.9644\n",
      "iteration number: 4651\t training loss: 0.0257\tvalidation loss: 0.1148\t validation accuracy: 0.9644\n",
      "iteration number: 4652\t training loss: 0.0253\tvalidation loss: 0.1148\t validation accuracy: 0.9644\n",
      "iteration number: 4653\t training loss: 0.0252\tvalidation loss: 0.1157\t validation accuracy: 0.9622\n",
      "iteration number: 4654\t training loss: 0.0252\tvalidation loss: 0.1168\t validation accuracy: 0.9622\n",
      "iteration number: 4655\t training loss: 0.0252\tvalidation loss: 0.1172\t validation accuracy: 0.9622\n",
      "iteration number: 4656\t training loss: 0.0252\tvalidation loss: 0.1143\t validation accuracy: 0.9644\n",
      "iteration number: 4657\t training loss: 0.0253\tvalidation loss: 0.1139\t validation accuracy: 0.9622\n",
      "iteration number: 4658\t training loss: 0.0251\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 4659\t training loss: 0.0252\tvalidation loss: 0.1140\t validation accuracy: 0.9644\n",
      "iteration number: 4660\t training loss: 0.0251\tvalidation loss: 0.1140\t validation accuracy: 0.9667\n",
      "iteration number: 4661\t training loss: 0.0251\tvalidation loss: 0.1137\t validation accuracy: 0.9667\n",
      "iteration number: 4662\t training loss: 0.0250\tvalidation loss: 0.1119\t validation accuracy: 0.9667\n",
      "iteration number: 4663\t training loss: 0.0250\tvalidation loss: 0.1127\t validation accuracy: 0.9667\n",
      "iteration number: 4664\t training loss: 0.0250\tvalidation loss: 0.1131\t validation accuracy: 0.9667\n",
      "iteration number: 4665\t training loss: 0.0251\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 4666\t training loss: 0.0251\tvalidation loss: 0.1137\t validation accuracy: 0.9667\n",
      "iteration number: 4667\t training loss: 0.0251\tvalidation loss: 0.1139\t validation accuracy: 0.9667\n",
      "iteration number: 4668\t training loss: 0.0251\tvalidation loss: 0.1138\t validation accuracy: 0.9667\n",
      "iteration number: 4669\t training loss: 0.0251\tvalidation loss: 0.1152\t validation accuracy: 0.9644\n",
      "iteration number: 4670\t training loss: 0.0251\tvalidation loss: 0.1151\t validation accuracy: 0.9644\n",
      "iteration number: 4671\t training loss: 0.0250\tvalidation loss: 0.1134\t validation accuracy: 0.9667\n",
      "iteration number: 4672\t training loss: 0.0253\tvalidation loss: 0.1128\t validation accuracy: 0.9667\n",
      "iteration number: 4673\t training loss: 0.0251\tvalidation loss: 0.1127\t validation accuracy: 0.9667\n",
      "iteration number: 4674\t training loss: 0.0252\tvalidation loss: 0.1140\t validation accuracy: 0.9689\n",
      "iteration number: 4675\t training loss: 0.0251\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 4676\t training loss: 0.0252\tvalidation loss: 0.1121\t validation accuracy: 0.9667\n",
      "iteration number: 4677\t training loss: 0.0252\tvalidation loss: 0.1130\t validation accuracy: 0.9689\n",
      "iteration number: 4678\t training loss: 0.0253\tvalidation loss: 0.1136\t validation accuracy: 0.9667\n",
      "iteration number: 4679\t training loss: 0.0253\tvalidation loss: 0.1140\t validation accuracy: 0.9667\n",
      "iteration number: 4680\t training loss: 0.0249\tvalidation loss: 0.1146\t validation accuracy: 0.9667\n",
      "iteration number: 4681\t training loss: 0.0249\tvalidation loss: 0.1132\t validation accuracy: 0.9667\n",
      "iteration number: 4682\t training loss: 0.0248\tvalidation loss: 0.1134\t validation accuracy: 0.9667\n",
      "iteration number: 4683\t training loss: 0.0249\tvalidation loss: 0.1137\t validation accuracy: 0.9644\n",
      "iteration number: 4684\t training loss: 0.0251\tvalidation loss: 0.1150\t validation accuracy: 0.9644\n",
      "iteration number: 4685\t training loss: 0.0251\tvalidation loss: 0.1152\t validation accuracy: 0.9644\n",
      "iteration number: 4686\t training loss: 0.0248\tvalidation loss: 0.1150\t validation accuracy: 0.9622\n",
      "iteration number: 4687\t training loss: 0.0248\tvalidation loss: 0.1139\t validation accuracy: 0.9667\n",
      "iteration number: 4688\t training loss: 0.0257\tvalidation loss: 0.1119\t validation accuracy: 0.9644\n",
      "iteration number: 4689\t training loss: 0.0257\tvalidation loss: 0.1128\t validation accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4690\t training loss: 0.0251\tvalidation loss: 0.1145\t validation accuracy: 0.9644\n",
      "iteration number: 4691\t training loss: 0.0250\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 4692\t training loss: 0.0251\tvalidation loss: 0.1132\t validation accuracy: 0.9644\n",
      "iteration number: 4693\t training loss: 0.0251\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 4694\t training loss: 0.0252\tvalidation loss: 0.1144\t validation accuracy: 0.9644\n",
      "iteration number: 4695\t training loss: 0.0251\tvalidation loss: 0.1124\t validation accuracy: 0.9689\n",
      "iteration number: 4696\t training loss: 0.0252\tvalidation loss: 0.1131\t validation accuracy: 0.9667\n",
      "iteration number: 4697\t training loss: 0.0256\tvalidation loss: 0.1147\t validation accuracy: 0.9622\n",
      "iteration number: 4698\t training loss: 0.0256\tvalidation loss: 0.1146\t validation accuracy: 0.9622\n",
      "iteration number: 4699\t training loss: 0.0247\tvalidation loss: 0.1158\t validation accuracy: 0.9667\n",
      "iteration number: 4700\t training loss: 0.0249\tvalidation loss: 0.1154\t validation accuracy: 0.9644\n",
      "iteration number: 4701\t training loss: 0.0248\tvalidation loss: 0.1159\t validation accuracy: 0.9644\n",
      "iteration number: 4702\t training loss: 0.0248\tvalidation loss: 0.1152\t validation accuracy: 0.9644\n",
      "iteration number: 4703\t training loss: 0.0247\tvalidation loss: 0.1153\t validation accuracy: 0.9644\n",
      "iteration number: 4704\t training loss: 0.0247\tvalidation loss: 0.1156\t validation accuracy: 0.9644\n",
      "iteration number: 4705\t training loss: 0.0249\tvalidation loss: 0.1142\t validation accuracy: 0.9644\n",
      "iteration number: 4706\t training loss: 0.0250\tvalidation loss: 0.1143\t validation accuracy: 0.9644\n",
      "iteration number: 4707\t training loss: 0.0251\tvalidation loss: 0.1141\t validation accuracy: 0.9644\n",
      "iteration number: 4708\t training loss: 0.0251\tvalidation loss: 0.1145\t validation accuracy: 0.9644\n",
      "iteration number: 4709\t training loss: 0.0251\tvalidation loss: 0.1142\t validation accuracy: 0.9644\n",
      "iteration number: 4710\t training loss: 0.0249\tvalidation loss: 0.1137\t validation accuracy: 0.9644\n",
      "iteration number: 4711\t training loss: 0.0252\tvalidation loss: 0.1153\t validation accuracy: 0.9622\n",
      "iteration number: 4712\t training loss: 0.0250\tvalidation loss: 0.1139\t validation accuracy: 0.9644\n",
      "iteration number: 4713\t training loss: 0.0249\tvalidation loss: 0.1136\t validation accuracy: 0.9644\n",
      "iteration number: 4714\t training loss: 0.0246\tvalidation loss: 0.1124\t validation accuracy: 0.9667\n",
      "iteration number: 4715\t training loss: 0.0252\tvalidation loss: 0.1125\t validation accuracy: 0.9667\n",
      "iteration number: 4716\t training loss: 0.0249\tvalidation loss: 0.1099\t validation accuracy: 0.9711\n",
      "iteration number: 4717\t training loss: 0.0250\tvalidation loss: 0.1096\t validation accuracy: 0.9711\n",
      "iteration number: 4718\t training loss: 0.0248\tvalidation loss: 0.1106\t validation accuracy: 0.9667\n",
      "iteration number: 4719\t training loss: 0.0247\tvalidation loss: 0.1113\t validation accuracy: 0.9667\n",
      "iteration number: 4720\t training loss: 0.0246\tvalidation loss: 0.1120\t validation accuracy: 0.9667\n",
      "iteration number: 4721\t training loss: 0.0247\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 4722\t training loss: 0.0249\tvalidation loss: 0.1131\t validation accuracy: 0.9667\n",
      "iteration number: 4723\t training loss: 0.0249\tvalidation loss: 0.1145\t validation accuracy: 0.9622\n",
      "iteration number: 4724\t training loss: 0.0250\tvalidation loss: 0.1152\t validation accuracy: 0.9622\n",
      "iteration number: 4725\t training loss: 0.0248\tvalidation loss: 0.1142\t validation accuracy: 0.9644\n",
      "iteration number: 4726\t training loss: 0.0248\tvalidation loss: 0.1133\t validation accuracy: 0.9667\n",
      "iteration number: 4727\t training loss: 0.0247\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 4728\t training loss: 0.0248\tvalidation loss: 0.1157\t validation accuracy: 0.9644\n",
      "iteration number: 4729\t training loss: 0.0248\tvalidation loss: 0.1156\t validation accuracy: 0.9622\n",
      "iteration number: 4730\t training loss: 0.0247\tvalidation loss: 0.1153\t validation accuracy: 0.9622\n",
      "iteration number: 4731\t training loss: 0.0249\tvalidation loss: 0.1157\t validation accuracy: 0.9644\n",
      "iteration number: 4732\t training loss: 0.0249\tvalidation loss: 0.1163\t validation accuracy: 0.9600\n",
      "iteration number: 4733\t training loss: 0.0249\tvalidation loss: 0.1173\t validation accuracy: 0.9600\n",
      "iteration number: 4734\t training loss: 0.0247\tvalidation loss: 0.1158\t validation accuracy: 0.9622\n",
      "iteration number: 4735\t training loss: 0.0249\tvalidation loss: 0.1168\t validation accuracy: 0.9600\n",
      "iteration number: 4736\t training loss: 0.0246\tvalidation loss: 0.1147\t validation accuracy: 0.9622\n",
      "iteration number: 4737\t training loss: 0.0245\tvalidation loss: 0.1143\t validation accuracy: 0.9622\n",
      "iteration number: 4738\t training loss: 0.0245\tvalidation loss: 0.1139\t validation accuracy: 0.9622\n",
      "iteration number: 4739\t training loss: 0.0246\tvalidation loss: 0.1150\t validation accuracy: 0.9622\n",
      "iteration number: 4740\t training loss: 0.0248\tvalidation loss: 0.1163\t validation accuracy: 0.9644\n",
      "iteration number: 4741\t training loss: 0.0248\tvalidation loss: 0.1157\t validation accuracy: 0.9644\n",
      "iteration number: 4742\t training loss: 0.0246\tvalidation loss: 0.1140\t validation accuracy: 0.9644\n",
      "iteration number: 4743\t training loss: 0.0247\tvalidation loss: 0.1146\t validation accuracy: 0.9644\n",
      "iteration number: 4744\t training loss: 0.0246\tvalidation loss: 0.1131\t validation accuracy: 0.9667\n",
      "iteration number: 4745\t training loss: 0.0250\tvalidation loss: 0.1129\t validation accuracy: 0.9622\n",
      "iteration number: 4746\t training loss: 0.0246\tvalidation loss: 0.1130\t validation accuracy: 0.9667\n",
      "iteration number: 4747\t training loss: 0.0246\tvalidation loss: 0.1153\t validation accuracy: 0.9622\n",
      "iteration number: 4748\t training loss: 0.0246\tvalidation loss: 0.1156\t validation accuracy: 0.9644\n",
      "iteration number: 4749\t training loss: 0.0246\tvalidation loss: 0.1172\t validation accuracy: 0.9622\n",
      "iteration number: 4750\t training loss: 0.0246\tvalidation loss: 0.1160\t validation accuracy: 0.9667\n",
      "iteration number: 4751\t training loss: 0.0248\tvalidation loss: 0.1156\t validation accuracy: 0.9644\n",
      "iteration number: 4752\t training loss: 0.0246\tvalidation loss: 0.1162\t validation accuracy: 0.9644\n",
      "iteration number: 4753\t training loss: 0.0247\tvalidation loss: 0.1177\t validation accuracy: 0.9622\n",
      "iteration number: 4754\t training loss: 0.0246\tvalidation loss: 0.1171\t validation accuracy: 0.9622\n",
      "iteration number: 4755\t training loss: 0.0245\tvalidation loss: 0.1151\t validation accuracy: 0.9667\n",
      "iteration number: 4756\t training loss: 0.0244\tvalidation loss: 0.1140\t validation accuracy: 0.9667\n",
      "iteration number: 4757\t training loss: 0.0244\tvalidation loss: 0.1140\t validation accuracy: 0.9622\n",
      "iteration number: 4758\t training loss: 0.0244\tvalidation loss: 0.1147\t validation accuracy: 0.9622\n",
      "iteration number: 4759\t training loss: 0.0245\tvalidation loss: 0.1143\t validation accuracy: 0.9622\n",
      "iteration number: 4760\t training loss: 0.0245\tvalidation loss: 0.1137\t validation accuracy: 0.9622\n",
      "iteration number: 4761\t training loss: 0.0245\tvalidation loss: 0.1137\t validation accuracy: 0.9622\n",
      "iteration number: 4762\t training loss: 0.0244\tvalidation loss: 0.1141\t validation accuracy: 0.9622\n",
      "iteration number: 4763\t training loss: 0.0244\tvalidation loss: 0.1143\t validation accuracy: 0.9622\n",
      "iteration number: 4764\t training loss: 0.0246\tvalidation loss: 0.1128\t validation accuracy: 0.9622\n",
      "iteration number: 4765\t training loss: 0.0248\tvalidation loss: 0.1134\t validation accuracy: 0.9622\n",
      "iteration number: 4766\t training loss: 0.0249\tvalidation loss: 0.1127\t validation accuracy: 0.9600\n",
      "iteration number: 4767\t training loss: 0.0250\tvalidation loss: 0.1120\t validation accuracy: 0.9622\n",
      "iteration number: 4768\t training loss: 0.0250\tvalidation loss: 0.1110\t validation accuracy: 0.9600\n",
      "iteration number: 4769\t training loss: 0.0257\tvalidation loss: 0.1101\t validation accuracy: 0.9600\n",
      "iteration number: 4770\t training loss: 0.0254\tvalidation loss: 0.1107\t validation accuracy: 0.9600\n",
      "iteration number: 4771\t training loss: 0.0247\tvalidation loss: 0.1109\t validation accuracy: 0.9622\n",
      "iteration number: 4772\t training loss: 0.0245\tvalidation loss: 0.1101\t validation accuracy: 0.9644\n",
      "iteration number: 4773\t training loss: 0.0246\tvalidation loss: 0.1093\t validation accuracy: 0.9644\n",
      "iteration number: 4774\t training loss: 0.0253\tvalidation loss: 0.1102\t validation accuracy: 0.9644\n",
      "iteration number: 4775\t training loss: 0.0254\tvalidation loss: 0.1120\t validation accuracy: 0.9600\n",
      "iteration number: 4776\t training loss: 0.0250\tvalidation loss: 0.1098\t validation accuracy: 0.9644\n",
      "iteration number: 4777\t training loss: 0.0248\tvalidation loss: 0.1104\t validation accuracy: 0.9622\n",
      "iteration number: 4778\t training loss: 0.0248\tvalidation loss: 0.1097\t validation accuracy: 0.9622\n",
      "iteration number: 4779\t training loss: 0.0248\tvalidation loss: 0.1083\t validation accuracy: 0.9622\n",
      "iteration number: 4780\t training loss: 0.0249\tvalidation loss: 0.1098\t validation accuracy: 0.9600\n",
      "iteration number: 4781\t training loss: 0.0250\tvalidation loss: 0.1098\t validation accuracy: 0.9600\n",
      "iteration number: 4782\t training loss: 0.0249\tvalidation loss: 0.1087\t validation accuracy: 0.9622\n",
      "iteration number: 4783\t training loss: 0.0248\tvalidation loss: 0.1088\t validation accuracy: 0.9600\n",
      "iteration number: 4784\t training loss: 0.0247\tvalidation loss: 0.1075\t validation accuracy: 0.9622\n",
      "iteration number: 4785\t training loss: 0.0247\tvalidation loss: 0.1072\t validation accuracy: 0.9644\n",
      "iteration number: 4786\t training loss: 0.0251\tvalidation loss: 0.1063\t validation accuracy: 0.9667\n",
      "iteration number: 4787\t training loss: 0.0250\tvalidation loss: 0.1068\t validation accuracy: 0.9667\n",
      "iteration number: 4788\t training loss: 0.0244\tvalidation loss: 0.1087\t validation accuracy: 0.9644\n",
      "iteration number: 4789\t training loss: 0.0244\tvalidation loss: 0.1093\t validation accuracy: 0.9644\n",
      "iteration number: 4790\t training loss: 0.0244\tvalidation loss: 0.1093\t validation accuracy: 0.9644\n",
      "iteration number: 4791\t training loss: 0.0245\tvalidation loss: 0.1089\t validation accuracy: 0.9644\n",
      "iteration number: 4792\t training loss: 0.0244\tvalidation loss: 0.1082\t validation accuracy: 0.9644\n",
      "iteration number: 4793\t training loss: 0.0244\tvalidation loss: 0.1084\t validation accuracy: 0.9644\n",
      "iteration number: 4794\t training loss: 0.0243\tvalidation loss: 0.1092\t validation accuracy: 0.9622\n",
      "iteration number: 4795\t training loss: 0.0245\tvalidation loss: 0.1130\t validation accuracy: 0.9600\n",
      "iteration number: 4796\t training loss: 0.0244\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n",
      "iteration number: 4797\t training loss: 0.0244\tvalidation loss: 0.1139\t validation accuracy: 0.9600\n",
      "iteration number: 4798\t training loss: 0.0247\tvalidation loss: 0.1139\t validation accuracy: 0.9622\n",
      "iteration number: 4799\t training loss: 0.0244\tvalidation loss: 0.1117\t validation accuracy: 0.9644\n",
      "iteration number: 4800\t training loss: 0.0247\tvalidation loss: 0.1105\t validation accuracy: 0.9689\n",
      "iteration number: 4801\t training loss: 0.0246\tvalidation loss: 0.1097\t validation accuracy: 0.9689\n",
      "iteration number: 4802\t training loss: 0.0243\tvalidation loss: 0.1105\t validation accuracy: 0.9667\n",
      "iteration number: 4803\t training loss: 0.0245\tvalidation loss: 0.1104\t validation accuracy: 0.9711\n",
      "iteration number: 4804\t training loss: 0.0251\tvalidation loss: 0.1091\t validation accuracy: 0.9711\n",
      "iteration number: 4805\t training loss: 0.0250\tvalidation loss: 0.1095\t validation accuracy: 0.9756\n",
      "iteration number: 4806\t training loss: 0.0248\tvalidation loss: 0.1100\t validation accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4807\t training loss: 0.0247\tvalidation loss: 0.1107\t validation accuracy: 0.9733\n",
      "iteration number: 4808\t training loss: 0.0245\tvalidation loss: 0.1123\t validation accuracy: 0.9689\n",
      "iteration number: 4809\t training loss: 0.0245\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 4810\t training loss: 0.0246\tvalidation loss: 0.1126\t validation accuracy: 0.9667\n",
      "iteration number: 4811\t training loss: 0.0244\tvalidation loss: 0.1119\t validation accuracy: 0.9667\n",
      "iteration number: 4812\t training loss: 0.0248\tvalidation loss: 0.1134\t validation accuracy: 0.9667\n",
      "iteration number: 4813\t training loss: 0.0246\tvalidation loss: 0.1143\t validation accuracy: 0.9667\n",
      "iteration number: 4814\t training loss: 0.0243\tvalidation loss: 0.1116\t validation accuracy: 0.9667\n",
      "iteration number: 4815\t training loss: 0.0242\tvalidation loss: 0.1107\t validation accuracy: 0.9667\n",
      "iteration number: 4816\t training loss: 0.0242\tvalidation loss: 0.1099\t validation accuracy: 0.9667\n",
      "iteration number: 4817\t training loss: 0.0242\tvalidation loss: 0.1092\t validation accuracy: 0.9689\n",
      "iteration number: 4818\t training loss: 0.0251\tvalidation loss: 0.1100\t validation accuracy: 0.9667\n",
      "iteration number: 4819\t training loss: 0.0251\tvalidation loss: 0.1094\t validation accuracy: 0.9644\n",
      "iteration number: 4820\t training loss: 0.0249\tvalidation loss: 0.1079\t validation accuracy: 0.9667\n",
      "iteration number: 4821\t training loss: 0.0243\tvalidation loss: 0.1092\t validation accuracy: 0.9733\n",
      "iteration number: 4822\t training loss: 0.0245\tvalidation loss: 0.1087\t validation accuracy: 0.9689\n",
      "iteration number: 4823\t training loss: 0.0243\tvalidation loss: 0.1086\t validation accuracy: 0.9689\n",
      "iteration number: 4824\t training loss: 0.0241\tvalidation loss: 0.1093\t validation accuracy: 0.9667\n",
      "iteration number: 4825\t training loss: 0.0240\tvalidation loss: 0.1092\t validation accuracy: 0.9667\n",
      "iteration number: 4826\t training loss: 0.0240\tvalidation loss: 0.1102\t validation accuracy: 0.9667\n",
      "iteration number: 4827\t training loss: 0.0240\tvalidation loss: 0.1095\t validation accuracy: 0.9667\n",
      "iteration number: 4828\t training loss: 0.0242\tvalidation loss: 0.1088\t validation accuracy: 0.9667\n",
      "iteration number: 4829\t training loss: 0.0244\tvalidation loss: 0.1087\t validation accuracy: 0.9644\n",
      "iteration number: 4830\t training loss: 0.0243\tvalidation loss: 0.1085\t validation accuracy: 0.9667\n",
      "iteration number: 4831\t training loss: 0.0243\tvalidation loss: 0.1082\t validation accuracy: 0.9689\n",
      "iteration number: 4832\t training loss: 0.0242\tvalidation loss: 0.1088\t validation accuracy: 0.9667\n",
      "iteration number: 4833\t training loss: 0.0246\tvalidation loss: 0.1102\t validation accuracy: 0.9689\n",
      "iteration number: 4834\t training loss: 0.0245\tvalidation loss: 0.1099\t validation accuracy: 0.9667\n",
      "iteration number: 4835\t training loss: 0.0244\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 4836\t training loss: 0.0242\tvalidation loss: 0.1093\t validation accuracy: 0.9689\n",
      "iteration number: 4837\t training loss: 0.0244\tvalidation loss: 0.1091\t validation accuracy: 0.9689\n",
      "iteration number: 4838\t training loss: 0.0244\tvalidation loss: 0.1084\t validation accuracy: 0.9711\n",
      "iteration number: 4839\t training loss: 0.0243\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 4840\t training loss: 0.0240\tvalidation loss: 0.1114\t validation accuracy: 0.9667\n",
      "iteration number: 4841\t training loss: 0.0239\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 4842\t training loss: 0.0240\tvalidation loss: 0.1139\t validation accuracy: 0.9644\n",
      "iteration number: 4843\t training loss: 0.0239\tvalidation loss: 0.1137\t validation accuracy: 0.9644\n",
      "iteration number: 4844\t training loss: 0.0239\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 4845\t training loss: 0.0239\tvalidation loss: 0.1139\t validation accuracy: 0.9644\n",
      "iteration number: 4846\t training loss: 0.0243\tvalidation loss: 0.1137\t validation accuracy: 0.9622\n",
      "iteration number: 4847\t training loss: 0.0245\tvalidation loss: 0.1143\t validation accuracy: 0.9622\n",
      "iteration number: 4848\t training loss: 0.0245\tvalidation loss: 0.1139\t validation accuracy: 0.9622\n",
      "iteration number: 4849\t training loss: 0.0243\tvalidation loss: 0.1142\t validation accuracy: 0.9622\n",
      "iteration number: 4850\t training loss: 0.0241\tvalidation loss: 0.1158\t validation accuracy: 0.9644\n",
      "iteration number: 4851\t training loss: 0.0241\tvalidation loss: 0.1177\t validation accuracy: 0.9600\n",
      "iteration number: 4852\t training loss: 0.0242\tvalidation loss: 0.1177\t validation accuracy: 0.9622\n",
      "iteration number: 4853\t training loss: 0.0237\tvalidation loss: 0.1150\t validation accuracy: 0.9622\n",
      "iteration number: 4854\t training loss: 0.0236\tvalidation loss: 0.1138\t validation accuracy: 0.9667\n",
      "iteration number: 4855\t training loss: 0.0237\tvalidation loss: 0.1126\t validation accuracy: 0.9667\n",
      "iteration number: 4856\t training loss: 0.0237\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 4857\t training loss: 0.0235\tvalidation loss: 0.1140\t validation accuracy: 0.9644\n",
      "iteration number: 4858\t training loss: 0.0236\tvalidation loss: 0.1145\t validation accuracy: 0.9644\n",
      "iteration number: 4859\t training loss: 0.0235\tvalidation loss: 0.1145\t validation accuracy: 0.9644\n",
      "iteration number: 4860\t training loss: 0.0239\tvalidation loss: 0.1139\t validation accuracy: 0.9622\n",
      "iteration number: 4861\t training loss: 0.0237\tvalidation loss: 0.1132\t validation accuracy: 0.9622\n",
      "iteration number: 4862\t training loss: 0.0238\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 4863\t training loss: 0.0236\tvalidation loss: 0.1139\t validation accuracy: 0.9622\n",
      "iteration number: 4864\t training loss: 0.0237\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 4865\t training loss: 0.0237\tvalidation loss: 0.1140\t validation accuracy: 0.9622\n",
      "iteration number: 4866\t training loss: 0.0239\tvalidation loss: 0.1154\t validation accuracy: 0.9622\n",
      "iteration number: 4867\t training loss: 0.0238\tvalidation loss: 0.1149\t validation accuracy: 0.9622\n",
      "iteration number: 4868\t training loss: 0.0238\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 4869\t training loss: 0.0238\tvalidation loss: 0.1129\t validation accuracy: 0.9644\n",
      "iteration number: 4870\t training loss: 0.0237\tvalidation loss: 0.1125\t validation accuracy: 0.9644\n",
      "iteration number: 4871\t training loss: 0.0239\tvalidation loss: 0.1137\t validation accuracy: 0.9644\n",
      "iteration number: 4872\t training loss: 0.0241\tvalidation loss: 0.1154\t validation accuracy: 0.9667\n",
      "iteration number: 4873\t training loss: 0.0243\tvalidation loss: 0.1138\t validation accuracy: 0.9667\n",
      "iteration number: 4874\t training loss: 0.0240\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 4875\t training loss: 0.0238\tvalidation loss: 0.1143\t validation accuracy: 0.9622\n",
      "iteration number: 4876\t training loss: 0.0238\tvalidation loss: 0.1137\t validation accuracy: 0.9644\n",
      "iteration number: 4877\t training loss: 0.0235\tvalidation loss: 0.1127\t validation accuracy: 0.9622\n",
      "iteration number: 4878\t training loss: 0.0240\tvalidation loss: 0.1114\t validation accuracy: 0.9622\n",
      "iteration number: 4879\t training loss: 0.0238\tvalidation loss: 0.1114\t validation accuracy: 0.9622\n",
      "iteration number: 4880\t training loss: 0.0237\tvalidation loss: 0.1118\t validation accuracy: 0.9622\n",
      "iteration number: 4881\t training loss: 0.0240\tvalidation loss: 0.1110\t validation accuracy: 0.9622\n",
      "iteration number: 4882\t training loss: 0.0241\tvalidation loss: 0.1117\t validation accuracy: 0.9622\n",
      "iteration number: 4883\t training loss: 0.0237\tvalidation loss: 0.1141\t validation accuracy: 0.9622\n",
      "iteration number: 4884\t training loss: 0.0244\tvalidation loss: 0.1124\t validation accuracy: 0.9622\n",
      "iteration number: 4885\t training loss: 0.0241\tvalidation loss: 0.1093\t validation accuracy: 0.9667\n",
      "iteration number: 4886\t training loss: 0.0237\tvalidation loss: 0.1104\t validation accuracy: 0.9689\n",
      "iteration number: 4887\t training loss: 0.0234\tvalidation loss: 0.1116\t validation accuracy: 0.9667\n",
      "iteration number: 4888\t training loss: 0.0234\tvalidation loss: 0.1120\t validation accuracy: 0.9667\n",
      "iteration number: 4889\t training loss: 0.0235\tvalidation loss: 0.1125\t validation accuracy: 0.9644\n",
      "iteration number: 4890\t training loss: 0.0234\tvalidation loss: 0.1110\t validation accuracy: 0.9644\n",
      "iteration number: 4891\t training loss: 0.0234\tvalidation loss: 0.1104\t validation accuracy: 0.9667\n",
      "iteration number: 4892\t training loss: 0.0235\tvalidation loss: 0.1103\t validation accuracy: 0.9667\n",
      "iteration number: 4893\t training loss: 0.0236\tvalidation loss: 0.1105\t validation accuracy: 0.9667\n",
      "iteration number: 4894\t training loss: 0.0235\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 4895\t training loss: 0.0235\tvalidation loss: 0.1111\t validation accuracy: 0.9689\n",
      "iteration number: 4896\t training loss: 0.0237\tvalidation loss: 0.1104\t validation accuracy: 0.9667\n",
      "iteration number: 4897\t training loss: 0.0236\tvalidation loss: 0.1089\t validation accuracy: 0.9689\n",
      "iteration number: 4898\t training loss: 0.0236\tvalidation loss: 0.1096\t validation accuracy: 0.9689\n",
      "iteration number: 4899\t training loss: 0.0234\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 4900\t training loss: 0.0234\tvalidation loss: 0.1131\t validation accuracy: 0.9644\n",
      "iteration number: 4901\t training loss: 0.0233\tvalidation loss: 0.1135\t validation accuracy: 0.9644\n",
      "iteration number: 4902\t training loss: 0.0233\tvalidation loss: 0.1143\t validation accuracy: 0.9644\n",
      "iteration number: 4903\t training loss: 0.0234\tvalidation loss: 0.1135\t validation accuracy: 0.9667\n",
      "iteration number: 4904\t training loss: 0.0235\tvalidation loss: 0.1107\t validation accuracy: 0.9667\n",
      "iteration number: 4905\t training loss: 0.0233\tvalidation loss: 0.1116\t validation accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 4906\t training loss: 0.0236\tvalidation loss: 0.1114\t validation accuracy: 0.9644\n",
      "iteration number: 4907\t training loss: 0.0234\tvalidation loss: 0.1114\t validation accuracy: 0.9644\n",
      "iteration number: 4908\t training loss: 0.0233\tvalidation loss: 0.1123\t validation accuracy: 0.9644\n",
      "iteration number: 4909\t training loss: 0.0233\tvalidation loss: 0.1130\t validation accuracy: 0.9644\n",
      "iteration number: 4910\t training loss: 0.0233\tvalidation loss: 0.1116\t validation accuracy: 0.9644\n",
      "iteration number: 4911\t training loss: 0.0234\tvalidation loss: 0.1116\t validation accuracy: 0.9667\n",
      "iteration number: 4912\t training loss: 0.0236\tvalidation loss: 0.1128\t validation accuracy: 0.9689\n",
      "iteration number: 4913\t training loss: 0.0234\tvalidation loss: 0.1119\t validation accuracy: 0.9689\n",
      "iteration number: 4914\t training loss: 0.0235\tvalidation loss: 0.1116\t validation accuracy: 0.9644\n",
      "iteration number: 4915\t training loss: 0.0235\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 4916\t training loss: 0.0234\tvalidation loss: 0.1120\t validation accuracy: 0.9689\n",
      "iteration number: 4917\t training loss: 0.0235\tvalidation loss: 0.1122\t validation accuracy: 0.9689\n",
      "iteration number: 4918\t training loss: 0.0234\tvalidation loss: 0.1115\t validation accuracy: 0.9689\n",
      "iteration number: 4919\t training loss: 0.0235\tvalidation loss: 0.1097\t validation accuracy: 0.9667\n",
      "iteration number: 4920\t training loss: 0.0237\tvalidation loss: 0.1085\t validation accuracy: 0.9689\n",
      "iteration number: 4921\t training loss: 0.0237\tvalidation loss: 0.1088\t validation accuracy: 0.9733\n",
      "iteration number: 4922\t training loss: 0.0237\tvalidation loss: 0.1080\t validation accuracy: 0.9689\n",
      "iteration number: 4923\t training loss: 0.0238\tvalidation loss: 0.1082\t validation accuracy: 0.9711\n",
      "iteration number: 4924\t training loss: 0.0235\tvalidation loss: 0.1088\t validation accuracy: 0.9689\n",
      "iteration number: 4925\t training loss: 0.0235\tvalidation loss: 0.1095\t validation accuracy: 0.9689\n",
      "iteration number: 4926\t training loss: 0.0234\tvalidation loss: 0.1087\t validation accuracy: 0.9689\n",
      "iteration number: 4927\t training loss: 0.0233\tvalidation loss: 0.1098\t validation accuracy: 0.9689\n",
      "iteration number: 4928\t training loss: 0.0234\tvalidation loss: 0.1097\t validation accuracy: 0.9689\n",
      "iteration number: 4929\t training loss: 0.0232\tvalidation loss: 0.1095\t validation accuracy: 0.9689\n",
      "iteration number: 4930\t training loss: 0.0231\tvalidation loss: 0.1101\t validation accuracy: 0.9667\n",
      "iteration number: 4931\t training loss: 0.0230\tvalidation loss: 0.1114\t validation accuracy: 0.9667\n",
      "iteration number: 4932\t training loss: 0.0230\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 4933\t training loss: 0.0230\tvalidation loss: 0.1110\t validation accuracy: 0.9667\n",
      "iteration number: 4934\t training loss: 0.0231\tvalidation loss: 0.1106\t validation accuracy: 0.9667\n",
      "iteration number: 4935\t training loss: 0.0230\tvalidation loss: 0.1113\t validation accuracy: 0.9667\n",
      "iteration number: 4936\t training loss: 0.0230\tvalidation loss: 0.1123\t validation accuracy: 0.9667\n",
      "iteration number: 4937\t training loss: 0.0230\tvalidation loss: 0.1132\t validation accuracy: 0.9667\n",
      "iteration number: 4938\t training loss: 0.0231\tvalidation loss: 0.1127\t validation accuracy: 0.9689\n",
      "iteration number: 4939\t training loss: 0.0231\tvalidation loss: 0.1134\t validation accuracy: 0.9689\n",
      "iteration number: 4940\t training loss: 0.0230\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 4941\t training loss: 0.0230\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n",
      "iteration number: 4942\t training loss: 0.0231\tvalidation loss: 0.1144\t validation accuracy: 0.9644\n",
      "iteration number: 4943\t training loss: 0.0230\tvalidation loss: 0.1133\t validation accuracy: 0.9644\n",
      "iteration number: 4944\t training loss: 0.0230\tvalidation loss: 0.1138\t validation accuracy: 0.9644\n",
      "iteration number: 4945\t training loss: 0.0230\tvalidation loss: 0.1156\t validation accuracy: 0.9622\n",
      "iteration number: 4946\t training loss: 0.0230\tvalidation loss: 0.1167\t validation accuracy: 0.9644\n",
      "iteration number: 4947\t training loss: 0.0232\tvalidation loss: 0.1149\t validation accuracy: 0.9644\n",
      "iteration number: 4948\t training loss: 0.0231\tvalidation loss: 0.1132\t validation accuracy: 0.9644\n",
      "iteration number: 4949\t training loss: 0.0229\tvalidation loss: 0.1127\t validation accuracy: 0.9667\n",
      "iteration number: 4950\t training loss: 0.0229\tvalidation loss: 0.1114\t validation accuracy: 0.9667\n",
      "iteration number: 4951\t training loss: 0.0230\tvalidation loss: 0.1119\t validation accuracy: 0.9644\n",
      "iteration number: 4952\t training loss: 0.0229\tvalidation loss: 0.1102\t validation accuracy: 0.9667\n",
      "iteration number: 4953\t training loss: 0.0229\tvalidation loss: 0.1101\t validation accuracy: 0.9667\n",
      "iteration number: 4954\t training loss: 0.0228\tvalidation loss: 0.1109\t validation accuracy: 0.9667\n",
      "iteration number: 4955\t training loss: 0.0228\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 4956\t training loss: 0.0230\tvalidation loss: 0.1104\t validation accuracy: 0.9644\n",
      "iteration number: 4957\t training loss: 0.0231\tvalidation loss: 0.1124\t validation accuracy: 0.9644\n",
      "iteration number: 4958\t training loss: 0.0232\tvalidation loss: 0.1140\t validation accuracy: 0.9622\n",
      "iteration number: 4959\t training loss: 0.0230\tvalidation loss: 0.1142\t validation accuracy: 0.9644\n",
      "iteration number: 4960\t training loss: 0.0237\tvalidation loss: 0.1183\t validation accuracy: 0.9622\n",
      "iteration number: 4961\t training loss: 0.0236\tvalidation loss: 0.1179\t validation accuracy: 0.9622\n",
      "iteration number: 4962\t training loss: 0.0238\tvalidation loss: 0.1190\t validation accuracy: 0.9622\n",
      "iteration number: 4963\t training loss: 0.0236\tvalidation loss: 0.1171\t validation accuracy: 0.9600\n",
      "iteration number: 4964\t training loss: 0.0235\tvalidation loss: 0.1168\t validation accuracy: 0.9600\n",
      "iteration number: 4965\t training loss: 0.0236\tvalidation loss: 0.1169\t validation accuracy: 0.9644\n",
      "iteration number: 4966\t training loss: 0.0235\tvalidation loss: 0.1163\t validation accuracy: 0.9600\n",
      "iteration number: 4967\t training loss: 0.0235\tvalidation loss: 0.1166\t validation accuracy: 0.9600\n",
      "iteration number: 4968\t training loss: 0.0233\tvalidation loss: 0.1159\t validation accuracy: 0.9622\n",
      "iteration number: 4969\t training loss: 0.0234\tvalidation loss: 0.1170\t validation accuracy: 0.9600\n",
      "iteration number: 4970\t training loss: 0.0233\tvalidation loss: 0.1170\t validation accuracy: 0.9600\n",
      "iteration number: 4971\t training loss: 0.0233\tvalidation loss: 0.1173\t validation accuracy: 0.9622\n",
      "iteration number: 4972\t training loss: 0.0239\tvalidation loss: 0.1197\t validation accuracy: 0.9600\n",
      "iteration number: 4973\t training loss: 0.0232\tvalidation loss: 0.1170\t validation accuracy: 0.9622\n",
      "iteration number: 4974\t training loss: 0.0230\tvalidation loss: 0.1154\t validation accuracy: 0.9622\n",
      "iteration number: 4975\t training loss: 0.0231\tvalidation loss: 0.1149\t validation accuracy: 0.9622\n",
      "iteration number: 4976\t training loss: 0.0235\tvalidation loss: 0.1153\t validation accuracy: 0.9644\n",
      "iteration number: 4977\t training loss: 0.0231\tvalidation loss: 0.1111\t validation accuracy: 0.9689\n",
      "iteration number: 4978\t training loss: 0.0230\tvalidation loss: 0.1093\t validation accuracy: 0.9667\n",
      "iteration number: 4979\t training loss: 0.0230\tvalidation loss: 0.1105\t validation accuracy: 0.9689\n",
      "iteration number: 4980\t training loss: 0.0228\tvalidation loss: 0.1100\t validation accuracy: 0.9689\n",
      "iteration number: 4981\t training loss: 0.0232\tvalidation loss: 0.1092\t validation accuracy: 0.9711\n",
      "iteration number: 4982\t training loss: 0.0227\tvalidation loss: 0.1091\t validation accuracy: 0.9667\n",
      "iteration number: 4983\t training loss: 0.0228\tvalidation loss: 0.1103\t validation accuracy: 0.9644\n",
      "iteration number: 4984\t training loss: 0.0229\tvalidation loss: 0.1105\t validation accuracy: 0.9644\n",
      "iteration number: 4985\t training loss: 0.0231\tvalidation loss: 0.1104\t validation accuracy: 0.9644\n",
      "iteration number: 4986\t training loss: 0.0229\tvalidation loss: 0.1109\t validation accuracy: 0.9622\n",
      "iteration number: 4987\t training loss: 0.0230\tvalidation loss: 0.1122\t validation accuracy: 0.9622\n",
      "iteration number: 4988\t training loss: 0.0229\tvalidation loss: 0.1117\t validation accuracy: 0.9622\n",
      "iteration number: 4989\t training loss: 0.0227\tvalidation loss: 0.1122\t validation accuracy: 0.9644\n",
      "iteration number: 4990\t training loss: 0.0228\tvalidation loss: 0.1131\t validation accuracy: 0.9622\n",
      "iteration number: 4991\t training loss: 0.0229\tvalidation loss: 0.1138\t validation accuracy: 0.9622\n",
      "iteration number: 4992\t training loss: 0.0227\tvalidation loss: 0.1128\t validation accuracy: 0.9644\n",
      "iteration number: 4993\t training loss: 0.0227\tvalidation loss: 0.1130\t validation accuracy: 0.9622\n",
      "iteration number: 4994\t training loss: 0.0226\tvalidation loss: 0.1118\t validation accuracy: 0.9667\n",
      "iteration number: 4995\t training loss: 0.0228\tvalidation loss: 0.1123\t validation accuracy: 0.9644\n",
      "iteration number: 4996\t training loss: 0.0228\tvalidation loss: 0.1125\t validation accuracy: 0.9644\n",
      "iteration number: 4997\t training loss: 0.0228\tvalidation loss: 0.1127\t validation accuracy: 0.9689\n",
      "iteration number: 4998\t training loss: 0.0228\tvalidation loss: 0.1128\t validation accuracy: 0.9689\n",
      "iteration number: 4999\t training loss: 0.0229\tvalidation loss: 0.1140\t validation accuracy: 0.9667\n",
      "iteration number: 5000\t training loss: 0.0226\tvalidation loss: 0.1132\t validation accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "mlp = MultiLayerPerceptron(X, Y, hidden_size=50, activation='relu')\n",
    "mlp.train(vectorized=True, early_stopping=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "#### - Did you succeed to train the MLP and get a high validation accuracy? <br> Display available metrics (training and validation accuracies, training and validation losses)\n",
    "#### - Plot the prediction for a given validation sample. Is it accurate?\n",
    "#### - Compare the full gradient descent with the SGD.\n",
    "#### - Play with the hyper parameters you have: the hidden size, the activation function, the initial step and the batch size. <br> Comment. Don't hesitate to visualize results.\n",
    "#### - Once properly implemented, compare the training using early stopping, dropout, or both of them. <br> Why are these methods useful here?\n",
    "<span style=\"color:green\">\n",
    "Early stopping is useful to make training time shorter and returning a model that performs well on validation set. In case no improvement on validation set has been noticed for many iterations, a reasonable assumption is that any more iteration will only worsen the model on validation set. Thus traning is stopped and model returned as in its last state.\n",
    "</span>\n",
    "#### - Once properly implemented, compare the training using momentum.\n",
    "<span style=\"color:green\">\n",
    "The optimizer converges much faster, reaching $50\\%$ in validation accuracy after $\\approx42$ iteration only while it took $\\approx 185$ iterations without it.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â MLP with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1\t training loss: 2.3024\tvalidation loss: 2.3023\t validation accuracy: 0.1156\n",
      "iteration number: 2\t training loss: 2.3024\tvalidation loss: 2.3024\t validation accuracy: 0.1022\n",
      "iteration number: 3\t training loss: 2.3023\tvalidation loss: 2.3026\t validation accuracy: 0.1022\n",
      "iteration number: 4\t training loss: 2.3023\tvalidation loss: 2.3026\t validation accuracy: 0.0844\n",
      "iteration number: 5\t training loss: 2.3023\tvalidation loss: 2.3025\t validation accuracy: 0.1022\n",
      "iteration number: 6\t training loss: 2.3022\tvalidation loss: 2.3026\t validation accuracy: 0.1067\n",
      "iteration number: 7\t training loss: 2.3022\tvalidation loss: 2.3026\t validation accuracy: 0.0911\n",
      "iteration number: 8\t training loss: 2.3022\tvalidation loss: 2.3026\t validation accuracy: 0.0911\n",
      "iteration number: 9\t training loss: 2.3022\tvalidation loss: 2.3025\t validation accuracy: 0.0911\n",
      "iteration number: 10\t training loss: 2.3022\tvalidation loss: 2.3024\t validation accuracy: 0.0911\n",
      "iteration number: 11\t training loss: 2.3021\tvalidation loss: 2.3025\t validation accuracy: 0.1311\n",
      "iteration number: 12\t training loss: 2.3021\tvalidation loss: 2.3024\t validation accuracy: 0.0911\n",
      "iteration number: 13\t training loss: 2.3021\tvalidation loss: 2.3024\t validation accuracy: 0.0911\n",
      "iteration number: 14\t training loss: 2.3020\tvalidation loss: 2.3022\t validation accuracy: 0.0911\n",
      "iteration number: 15\t training loss: 2.3019\tvalidation loss: 2.3023\t validation accuracy: 0.0911\n",
      "iteration number: 16\t training loss: 2.3019\tvalidation loss: 2.3021\t validation accuracy: 0.1556\n",
      "iteration number: 17\t training loss: 2.3019\tvalidation loss: 2.3018\t validation accuracy: 0.0911\n",
      "iteration number: 18\t training loss: 2.3019\tvalidation loss: 2.3019\t validation accuracy: 0.0911\n",
      "iteration number: 19\t training loss: 2.3019\tvalidation loss: 2.3020\t validation accuracy: 0.1511\n",
      "iteration number: 20\t training loss: 2.3018\tvalidation loss: 2.3018\t validation accuracy: 0.1067\n",
      "iteration number: 21\t training loss: 2.3017\tvalidation loss: 2.3022\t validation accuracy: 0.0911\n",
      "iteration number: 22\t training loss: 2.3017\tvalidation loss: 2.3020\t validation accuracy: 0.0911\n",
      "iteration number: 23\t training loss: 2.3016\tvalidation loss: 2.3021\t validation accuracy: 0.0911\n",
      "iteration number: 24\t training loss: 2.3015\tvalidation loss: 2.3020\t validation accuracy: 0.0911\n",
      "iteration number: 25\t training loss: 2.3015\tvalidation loss: 2.3020\t validation accuracy: 0.0911\n",
      "iteration number: 26\t training loss: 2.3014\tvalidation loss: 2.3021\t validation accuracy: 0.0911\n",
      "iteration number: 27\t training loss: 2.3013\tvalidation loss: 2.3023\t validation accuracy: 0.0911\n",
      "iteration number: 28\t training loss: 2.3012\tvalidation loss: 2.3023\t validation accuracy: 0.0911\n",
      "iteration number: 29\t training loss: 2.3012\tvalidation loss: 2.3021\t validation accuracy: 0.0911\n",
      "iteration number: 30\t training loss: 2.3011\tvalidation loss: 2.3020\t validation accuracy: 0.0911\n",
      "iteration number: 31\t training loss: 2.3010\tvalidation loss: 2.3020\t validation accuracy: 0.0911\n",
      "iteration number: 32\t training loss: 2.3010\tvalidation loss: 2.3018\t validation accuracy: 0.0911\n",
      "iteration number: 33\t training loss: 2.3009\tvalidation loss: 2.3017\t validation accuracy: 0.0911\n",
      "iteration number: 34\t training loss: 2.3009\tvalidation loss: 2.3016\t validation accuracy: 0.1622\n",
      "iteration number: 35\t training loss: 2.3008\tvalidation loss: 2.3016\t validation accuracy: 0.1356\n",
      "iteration number: 36\t training loss: 2.3007\tvalidation loss: 2.3018\t validation accuracy: 0.0911\n",
      "iteration number: 37\t training loss: 2.3006\tvalidation loss: 2.3019\t validation accuracy: 0.0911\n",
      "iteration number: 38\t training loss: 2.3006\tvalidation loss: 2.3019\t validation accuracy: 0.0911\n",
      "iteration number: 39\t training loss: 2.3005\tvalidation loss: 2.3019\t validation accuracy: 0.0911\n",
      "iteration number: 40\t training loss: 2.3004\tvalidation loss: 2.3019\t validation accuracy: 0.0911\n",
      "iteration number: 41\t training loss: 2.3002\tvalidation loss: 2.3020\t validation accuracy: 0.0911\n",
      "iteration number: 42\t training loss: 2.3001\tvalidation loss: 2.3020\t validation accuracy: 0.0911\n",
      "iteration number: 43\t training loss: 2.2999\tvalidation loss: 2.3015\t validation accuracy: 0.0911\n",
      "iteration number: 44\t training loss: 2.2999\tvalidation loss: 2.3012\t validation accuracy: 0.0911\n",
      "iteration number: 45\t training loss: 2.2998\tvalidation loss: 2.3012\t validation accuracy: 0.0911\n",
      "iteration number: 46\t training loss: 2.2997\tvalidation loss: 2.3010\t validation accuracy: 0.0911\n",
      "iteration number: 47\t training loss: 2.2995\tvalidation loss: 2.3010\t validation accuracy: 0.0911\n",
      "iteration number: 48\t training loss: 2.2994\tvalidation loss: 2.3008\t validation accuracy: 0.0911\n",
      "iteration number: 49\t training loss: 2.2992\tvalidation loss: 2.3008\t validation accuracy: 0.0911\n",
      "iteration number: 50\t training loss: 2.2990\tvalidation loss: 2.3006\t validation accuracy: 0.0911\n",
      "iteration number: 51\t training loss: 2.2989\tvalidation loss: 2.3006\t validation accuracy: 0.0911\n",
      "iteration number: 52\t training loss: 2.2988\tvalidation loss: 2.3005\t validation accuracy: 0.0911\n",
      "iteration number: 53\t training loss: 2.2988\tvalidation loss: 2.3004\t validation accuracy: 0.0911\n",
      "iteration number: 54\t training loss: 2.2985\tvalidation loss: 2.3003\t validation accuracy: 0.0911\n",
      "iteration number: 55\t training loss: 2.2984\tvalidation loss: 2.3003\t validation accuracy: 0.0911\n",
      "iteration number: 56\t training loss: 2.2983\tvalidation loss: 2.3002\t validation accuracy: 0.0911\n",
      "iteration number: 57\t training loss: 2.2982\tvalidation loss: 2.2998\t validation accuracy: 0.0911\n",
      "iteration number: 58\t training loss: 2.2980\tvalidation loss: 2.2995\t validation accuracy: 0.0911\n",
      "iteration number: 59\t training loss: 2.2978\tvalidation loss: 2.2994\t validation accuracy: 0.0911\n",
      "iteration number: 60\t training loss: 2.2976\tvalidation loss: 2.2992\t validation accuracy: 0.0911\n",
      "iteration number: 61\t training loss: 2.2974\tvalidation loss: 2.2992\t validation accuracy: 0.0911\n",
      "iteration number: 62\t training loss: 2.2972\tvalidation loss: 2.2989\t validation accuracy: 0.0911\n",
      "iteration number: 63\t training loss: 2.2970\tvalidation loss: 2.2987\t validation accuracy: 0.1822\n",
      "iteration number: 64\t training loss: 2.2968\tvalidation loss: 2.2984\t validation accuracy: 0.2000\n",
      "iteration number: 65\t training loss: 2.2965\tvalidation loss: 2.2981\t validation accuracy: 0.1956\n",
      "iteration number: 66\t training loss: 2.2963\tvalidation loss: 2.2979\t validation accuracy: 0.2111\n",
      "iteration number: 67\t training loss: 2.2960\tvalidation loss: 2.2978\t validation accuracy: 0.1933\n",
      "iteration number: 68\t training loss: 2.2958\tvalidation loss: 2.2977\t validation accuracy: 0.1978\n",
      "iteration number: 69\t training loss: 2.2956\tvalidation loss: 2.2974\t validation accuracy: 0.1822\n",
      "iteration number: 70\t training loss: 2.2954\tvalidation loss: 2.2970\t validation accuracy: 0.1844\n",
      "iteration number: 71\t training loss: 2.2951\tvalidation loss: 2.2966\t validation accuracy: 0.1933\n",
      "iteration number: 72\t training loss: 2.2948\tvalidation loss: 2.2964\t validation accuracy: 0.1822\n",
      "iteration number: 73\t training loss: 2.2945\tvalidation loss: 2.2961\t validation accuracy: 0.1356\n",
      "iteration number: 74\t training loss: 2.2942\tvalidation loss: 2.2958\t validation accuracy: 0.1622\n",
      "iteration number: 75\t training loss: 2.2937\tvalidation loss: 2.2958\t validation accuracy: 0.0911\n",
      "iteration number: 76\t training loss: 2.2934\tvalidation loss: 2.2951\t validation accuracy: 0.0911\n",
      "iteration number: 77\t training loss: 2.2931\tvalidation loss: 2.2948\t validation accuracy: 0.0911\n",
      "iteration number: 78\t training loss: 2.2927\tvalidation loss: 2.2946\t validation accuracy: 0.0911\n",
      "iteration number: 79\t training loss: 2.2923\tvalidation loss: 2.2940\t validation accuracy: 0.1822\n",
      "iteration number: 80\t training loss: 2.2919\tvalidation loss: 2.2934\t validation accuracy: 0.1933\n",
      "iteration number: 81\t training loss: 2.2916\tvalidation loss: 2.2931\t validation accuracy: 0.1956\n",
      "iteration number: 82\t training loss: 2.2911\tvalidation loss: 2.2929\t validation accuracy: 0.2689\n",
      "iteration number: 83\t training loss: 2.2907\tvalidation loss: 2.2926\t validation accuracy: 0.2111\n",
      "iteration number: 84\t training loss: 2.2901\tvalidation loss: 2.2920\t validation accuracy: 0.3444\n",
      "iteration number: 85\t training loss: 2.2896\tvalidation loss: 2.2915\t validation accuracy: 0.3533\n",
      "iteration number: 86\t training loss: 2.2890\tvalidation loss: 2.2911\t validation accuracy: 0.3600\n",
      "iteration number: 87\t training loss: 2.2884\tvalidation loss: 2.2907\t validation accuracy: 0.3444\n",
      "iteration number: 88\t training loss: 2.2878\tvalidation loss: 2.2901\t validation accuracy: 0.3244\n",
      "iteration number: 89\t training loss: 2.2874\tvalidation loss: 2.2895\t validation accuracy: 0.3511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 90\t training loss: 2.2867\tvalidation loss: 2.2889\t validation accuracy: 0.2533\n",
      "iteration number: 91\t training loss: 2.2861\tvalidation loss: 2.2884\t validation accuracy: 0.3000\n",
      "iteration number: 92\t training loss: 2.2854\tvalidation loss: 2.2873\t validation accuracy: 0.3067\n",
      "iteration number: 93\t training loss: 2.2847\tvalidation loss: 2.2868\t validation accuracy: 0.1956\n",
      "iteration number: 94\t training loss: 2.2839\tvalidation loss: 2.2858\t validation accuracy: 0.2133\n",
      "iteration number: 95\t training loss: 2.2832\tvalidation loss: 2.2850\t validation accuracy: 0.2178\n",
      "iteration number: 96\t training loss: 2.2826\tvalidation loss: 2.2842\t validation accuracy: 0.2089\n",
      "iteration number: 97\t training loss: 2.2820\tvalidation loss: 2.2836\t validation accuracy: 0.2533\n",
      "iteration number: 98\t training loss: 2.2812\tvalidation loss: 2.2826\t validation accuracy: 0.2689\n",
      "iteration number: 99\t training loss: 2.2803\tvalidation loss: 2.2820\t validation accuracy: 0.2400\n",
      "iteration number: 100\t training loss: 2.2794\tvalidation loss: 2.2811\t validation accuracy: 0.2022\n",
      "iteration number: 101\t training loss: 2.2785\tvalidation loss: 2.2803\t validation accuracy: 0.2578\n",
      "iteration number: 102\t training loss: 2.2776\tvalidation loss: 2.2792\t validation accuracy: 0.2889\n",
      "iteration number: 103\t training loss: 2.2768\tvalidation loss: 2.2782\t validation accuracy: 0.2889\n",
      "iteration number: 104\t training loss: 2.2757\tvalidation loss: 2.2776\t validation accuracy: 0.2267\n",
      "iteration number: 105\t training loss: 2.2748\tvalidation loss: 2.2766\t validation accuracy: 0.1844\n",
      "iteration number: 106\t training loss: 2.2735\tvalidation loss: 2.2753\t validation accuracy: 0.1822\n",
      "iteration number: 107\t training loss: 2.2723\tvalidation loss: 2.2743\t validation accuracy: 0.1800\n",
      "iteration number: 108\t training loss: 2.2712\tvalidation loss: 2.2732\t validation accuracy: 0.1800\n",
      "iteration number: 109\t training loss: 2.2700\tvalidation loss: 2.2722\t validation accuracy: 0.1733\n",
      "iteration number: 110\t training loss: 2.2687\tvalidation loss: 2.2714\t validation accuracy: 0.2289\n",
      "iteration number: 111\t training loss: 2.2672\tvalidation loss: 2.2700\t validation accuracy: 0.2444\n",
      "iteration number: 112\t training loss: 2.2657\tvalidation loss: 2.2684\t validation accuracy: 0.2622\n",
      "iteration number: 113\t training loss: 2.2644\tvalidation loss: 2.2672\t validation accuracy: 0.3022\n",
      "iteration number: 114\t training loss: 2.2630\tvalidation loss: 2.2656\t validation accuracy: 0.3467\n",
      "iteration number: 115\t training loss: 2.2613\tvalidation loss: 2.2643\t validation accuracy: 0.3289\n",
      "iteration number: 116\t training loss: 2.2598\tvalidation loss: 2.2629\t validation accuracy: 0.3311\n",
      "iteration number: 117\t training loss: 2.2580\tvalidation loss: 2.2609\t validation accuracy: 0.3622\n",
      "iteration number: 118\t training loss: 2.2562\tvalidation loss: 2.2593\t validation accuracy: 0.3622\n",
      "iteration number: 119\t training loss: 2.2546\tvalidation loss: 2.2580\t validation accuracy: 0.3667\n",
      "iteration number: 120\t training loss: 2.2528\tvalidation loss: 2.2559\t validation accuracy: 0.3756\n",
      "iteration number: 121\t training loss: 2.2508\tvalidation loss: 2.2540\t validation accuracy: 0.3444\n",
      "iteration number: 122\t training loss: 2.2486\tvalidation loss: 2.2521\t validation accuracy: 0.2911\n",
      "iteration number: 123\t training loss: 2.2464\tvalidation loss: 2.2502\t validation accuracy: 0.2311\n",
      "iteration number: 124\t training loss: 2.2442\tvalidation loss: 2.2480\t validation accuracy: 0.2422\n",
      "iteration number: 125\t training loss: 2.2421\tvalidation loss: 2.2458\t validation accuracy: 0.2644\n",
      "iteration number: 126\t training loss: 2.2399\tvalidation loss: 2.2438\t validation accuracy: 0.2733\n",
      "iteration number: 127\t training loss: 2.2380\tvalidation loss: 2.2423\t validation accuracy: 0.2289\n",
      "iteration number: 128\t training loss: 2.2352\tvalidation loss: 2.2393\t validation accuracy: 0.2311\n",
      "iteration number: 129\t training loss: 2.2328\tvalidation loss: 2.2370\t validation accuracy: 0.2111\n",
      "iteration number: 130\t training loss: 2.2301\tvalidation loss: 2.2343\t validation accuracy: 0.2422\n",
      "iteration number: 131\t training loss: 2.2272\tvalidation loss: 2.2313\t validation accuracy: 0.2400\n",
      "iteration number: 132\t training loss: 2.2248\tvalidation loss: 2.2293\t validation accuracy: 0.2644\n",
      "iteration number: 133\t training loss: 2.2221\tvalidation loss: 2.2259\t validation accuracy: 0.2889\n",
      "iteration number: 134\t training loss: 2.2188\tvalidation loss: 2.2224\t validation accuracy: 0.3400\n",
      "iteration number: 135\t training loss: 2.2160\tvalidation loss: 2.2200\t validation accuracy: 0.3200\n",
      "iteration number: 136\t training loss: 2.2129\tvalidation loss: 2.2171\t validation accuracy: 0.2933\n",
      "iteration number: 137\t training loss: 2.2101\tvalidation loss: 2.2137\t validation accuracy: 0.3067\n",
      "iteration number: 138\t training loss: 2.2066\tvalidation loss: 2.2101\t validation accuracy: 0.3200\n",
      "iteration number: 139\t training loss: 2.2031\tvalidation loss: 2.2070\t validation accuracy: 0.3089\n",
      "iteration number: 140\t training loss: 2.1997\tvalidation loss: 2.2035\t validation accuracy: 0.3422\n",
      "iteration number: 141\t training loss: 2.1950\tvalidation loss: 2.1998\t validation accuracy: 0.2978\n",
      "iteration number: 142\t training loss: 2.1925\tvalidation loss: 2.1972\t validation accuracy: 0.2867\n",
      "iteration number: 143\t training loss: 2.1889\tvalidation loss: 2.1940\t validation accuracy: 0.2644\n",
      "iteration number: 144\t training loss: 2.1844\tvalidation loss: 2.1891\t validation accuracy: 0.2756\n",
      "iteration number: 145\t training loss: 2.1810\tvalidation loss: 2.1863\t validation accuracy: 0.2867\n",
      "iteration number: 146\t training loss: 2.1763\tvalidation loss: 2.1811\t validation accuracy: 0.3089\n",
      "iteration number: 147\t training loss: 2.1714\tvalidation loss: 2.1758\t validation accuracy: 0.3267\n",
      "iteration number: 148\t training loss: 2.1666\tvalidation loss: 2.1708\t validation accuracy: 0.3578\n",
      "iteration number: 149\t training loss: 2.1625\tvalidation loss: 2.1659\t validation accuracy: 0.4000\n",
      "iteration number: 150\t training loss: 2.1568\tvalidation loss: 2.1612\t validation accuracy: 0.3933\n",
      "iteration number: 151\t training loss: 2.1522\tvalidation loss: 2.1582\t validation accuracy: 0.3489\n",
      "iteration number: 152\t training loss: 2.1479\tvalidation loss: 2.1545\t validation accuracy: 0.3422\n",
      "iteration number: 153\t training loss: 2.1419\tvalidation loss: 2.1478\t validation accuracy: 0.3667\n",
      "iteration number: 154\t training loss: 2.1360\tvalidation loss: 2.1415\t validation accuracy: 0.4089\n",
      "iteration number: 155\t training loss: 2.1299\tvalidation loss: 2.1358\t validation accuracy: 0.4244\n",
      "iteration number: 156\t training loss: 2.1244\tvalidation loss: 2.1303\t validation accuracy: 0.4067\n",
      "iteration number: 157\t training loss: 2.1186\tvalidation loss: 2.1245\t validation accuracy: 0.4156\n",
      "iteration number: 158\t training loss: 2.1125\tvalidation loss: 2.1187\t validation accuracy: 0.4311\n",
      "iteration number: 159\t training loss: 2.1064\tvalidation loss: 2.1141\t validation accuracy: 0.3933\n",
      "iteration number: 160\t training loss: 2.0993\tvalidation loss: 2.1060\t validation accuracy: 0.4222\n",
      "iteration number: 161\t training loss: 2.0922\tvalidation loss: 2.0988\t validation accuracy: 0.3867\n",
      "iteration number: 162\t training loss: 2.0858\tvalidation loss: 2.0932\t validation accuracy: 0.3911\n",
      "iteration number: 163\t training loss: 2.0792\tvalidation loss: 2.0870\t validation accuracy: 0.3667\n",
      "iteration number: 164\t training loss: 2.0734\tvalidation loss: 2.0827\t validation accuracy: 0.3689\n",
      "iteration number: 165\t training loss: 2.0656\tvalidation loss: 2.0728\t validation accuracy: 0.4133\n",
      "iteration number: 166\t training loss: 2.0578\tvalidation loss: 2.0651\t validation accuracy: 0.4244\n",
      "iteration number: 167\t training loss: 2.0502\tvalidation loss: 2.0575\t validation accuracy: 0.4244\n",
      "iteration number: 168\t training loss: 2.0437\tvalidation loss: 2.0515\t validation accuracy: 0.4000\n",
      "iteration number: 169\t training loss: 2.0357\tvalidation loss: 2.0433\t validation accuracy: 0.3911\n",
      "iteration number: 170\t training loss: 2.0281\tvalidation loss: 2.0364\t validation accuracy: 0.3844\n",
      "iteration number: 171\t training loss: 2.0215\tvalidation loss: 2.0284\t validation accuracy: 0.4133\n",
      "iteration number: 172\t training loss: 2.0132\tvalidation loss: 2.0217\t validation accuracy: 0.4022\n",
      "iteration number: 173\t training loss: 2.0054\tvalidation loss: 2.0138\t validation accuracy: 0.4067\n",
      "iteration number: 174\t training loss: 1.9974\tvalidation loss: 2.0072\t validation accuracy: 0.3844\n",
      "iteration number: 175\t training loss: 1.9891\tvalidation loss: 1.9991\t validation accuracy: 0.4200\n",
      "iteration number: 176\t training loss: 1.9813\tvalidation loss: 1.9916\t validation accuracy: 0.4289\n",
      "iteration number: 177\t training loss: 1.9727\tvalidation loss: 1.9821\t validation accuracy: 0.4244\n",
      "iteration number: 178\t training loss: 1.9637\tvalidation loss: 1.9725\t validation accuracy: 0.4267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 179\t training loss: 1.9546\tvalidation loss: 1.9629\t validation accuracy: 0.4200\n",
      "iteration number: 180\t training loss: 1.9457\tvalidation loss: 1.9535\t validation accuracy: 0.4489\n",
      "iteration number: 181\t training loss: 1.9378\tvalidation loss: 1.9462\t validation accuracy: 0.4689\n",
      "iteration number: 182\t training loss: 1.9290\tvalidation loss: 1.9373\t validation accuracy: 0.4911\n",
      "iteration number: 183\t training loss: 1.9194\tvalidation loss: 1.9276\t validation accuracy: 0.4622\n",
      "iteration number: 184\t training loss: 1.9099\tvalidation loss: 1.9176\t validation accuracy: 0.4778\n",
      "iteration number: 185\t training loss: 1.8999\tvalidation loss: 1.9058\t validation accuracy: 0.5022\n",
      "iteration number: 186\t training loss: 1.8916\tvalidation loss: 1.8996\t validation accuracy: 0.5133\n",
      "iteration number: 187\t training loss: 1.8807\tvalidation loss: 1.8886\t validation accuracy: 0.5244\n",
      "iteration number: 188\t training loss: 1.8701\tvalidation loss: 1.8777\t validation accuracy: 0.5333\n",
      "iteration number: 189\t training loss: 1.8619\tvalidation loss: 1.8682\t validation accuracy: 0.5333\n",
      "iteration number: 190\t training loss: 1.8525\tvalidation loss: 1.8575\t validation accuracy: 0.5422\n",
      "iteration number: 191\t training loss: 1.8405\tvalidation loss: 1.8464\t validation accuracy: 0.5422\n",
      "iteration number: 192\t training loss: 1.8313\tvalidation loss: 1.8365\t validation accuracy: 0.5600\n",
      "iteration number: 193\t training loss: 1.8198\tvalidation loss: 1.8238\t validation accuracy: 0.5244\n",
      "iteration number: 194\t training loss: 1.8061\tvalidation loss: 1.8112\t validation accuracy: 0.5133\n",
      "iteration number: 195\t training loss: 1.7955\tvalidation loss: 1.8002\t validation accuracy: 0.5756\n",
      "iteration number: 196\t training loss: 1.7839\tvalidation loss: 1.7889\t validation accuracy: 0.5733\n",
      "iteration number: 197\t training loss: 1.7724\tvalidation loss: 1.7792\t validation accuracy: 0.6000\n",
      "iteration number: 198\t training loss: 1.7625\tvalidation loss: 1.7714\t validation accuracy: 0.5844\n",
      "iteration number: 199\t training loss: 1.7505\tvalidation loss: 1.7573\t validation accuracy: 0.6044\n",
      "iteration number: 200\t training loss: 1.7385\tvalidation loss: 1.7446\t validation accuracy: 0.6156\n",
      "iteration number: 201\t training loss: 1.7272\tvalidation loss: 1.7360\t validation accuracy: 0.6244\n",
      "iteration number: 202\t training loss: 1.7164\tvalidation loss: 1.7251\t validation accuracy: 0.6044\n",
      "iteration number: 203\t training loss: 1.7050\tvalidation loss: 1.7111\t validation accuracy: 0.6133\n",
      "iteration number: 204\t training loss: 1.6939\tvalidation loss: 1.7001\t validation accuracy: 0.6400\n",
      "iteration number: 205\t training loss: 1.6830\tvalidation loss: 1.6890\t validation accuracy: 0.6444\n",
      "iteration number: 206\t training loss: 1.6726\tvalidation loss: 1.6786\t validation accuracy: 0.6689\n",
      "iteration number: 207\t training loss: 1.6611\tvalidation loss: 1.6694\t validation accuracy: 0.6822\n",
      "iteration number: 208\t training loss: 1.6484\tvalidation loss: 1.6553\t validation accuracy: 0.6800\n",
      "iteration number: 209\t training loss: 1.6366\tvalidation loss: 1.6421\t validation accuracy: 0.6778\n",
      "iteration number: 210\t training loss: 1.6265\tvalidation loss: 1.6332\t validation accuracy: 0.6800\n",
      "iteration number: 211\t training loss: 1.6139\tvalidation loss: 1.6177\t validation accuracy: 0.6689\n",
      "iteration number: 212\t training loss: 1.6023\tvalidation loss: 1.6059\t validation accuracy: 0.6711\n",
      "iteration number: 213\t training loss: 1.5931\tvalidation loss: 1.5973\t validation accuracy: 0.7000\n",
      "iteration number: 214\t training loss: 1.5808\tvalidation loss: 1.5819\t validation accuracy: 0.7000\n",
      "iteration number: 215\t training loss: 1.5677\tvalidation loss: 1.5655\t validation accuracy: 0.7000\n",
      "iteration number: 216\t training loss: 1.5564\tvalidation loss: 1.5541\t validation accuracy: 0.7244\n",
      "iteration number: 217\t training loss: 1.5435\tvalidation loss: 1.5404\t validation accuracy: 0.7222\n",
      "iteration number: 218\t training loss: 1.5333\tvalidation loss: 1.5288\t validation accuracy: 0.7133\n",
      "iteration number: 219\t training loss: 1.5216\tvalidation loss: 1.5209\t validation accuracy: 0.7089\n",
      "iteration number: 220\t training loss: 1.5079\tvalidation loss: 1.5071\t validation accuracy: 0.6956\n",
      "iteration number: 221\t training loss: 1.4954\tvalidation loss: 1.4944\t validation accuracy: 0.7222\n",
      "iteration number: 222\t training loss: 1.4842\tvalidation loss: 1.4826\t validation accuracy: 0.7200\n",
      "iteration number: 223\t training loss: 1.4715\tvalidation loss: 1.4680\t validation accuracy: 0.7356\n",
      "iteration number: 224\t training loss: 1.4607\tvalidation loss: 1.4567\t validation accuracy: 0.7333\n",
      "iteration number: 225\t training loss: 1.4490\tvalidation loss: 1.4434\t validation accuracy: 0.7378\n",
      "iteration number: 226\t training loss: 1.4413\tvalidation loss: 1.4393\t validation accuracy: 0.7622\n",
      "iteration number: 227\t training loss: 1.4301\tvalidation loss: 1.4306\t validation accuracy: 0.7711\n",
      "iteration number: 228\t training loss: 1.4179\tvalidation loss: 1.4161\t validation accuracy: 0.7822\n",
      "iteration number: 229\t training loss: 1.4067\tvalidation loss: 1.4045\t validation accuracy: 0.8067\n",
      "iteration number: 230\t training loss: 1.3963\tvalidation loss: 1.3948\t validation accuracy: 0.7867\n",
      "iteration number: 231\t training loss: 1.3851\tvalidation loss: 1.3833\t validation accuracy: 0.7822\n",
      "iteration number: 232\t training loss: 1.3768\tvalidation loss: 1.3756\t validation accuracy: 0.7889\n",
      "iteration number: 233\t training loss: 1.3665\tvalidation loss: 1.3650\t validation accuracy: 0.7956\n",
      "iteration number: 234\t training loss: 1.3516\tvalidation loss: 1.3477\t validation accuracy: 0.8000\n",
      "iteration number: 235\t training loss: 1.3437\tvalidation loss: 1.3418\t validation accuracy: 0.8089\n",
      "iteration number: 236\t training loss: 1.3271\tvalidation loss: 1.3223\t validation accuracy: 0.8244\n",
      "iteration number: 237\t training loss: 1.3168\tvalidation loss: 1.3090\t validation accuracy: 0.7978\n",
      "iteration number: 238\t training loss: 1.3072\tvalidation loss: 1.3015\t validation accuracy: 0.8178\n",
      "iteration number: 239\t training loss: 1.2956\tvalidation loss: 1.2897\t validation accuracy: 0.8178\n",
      "iteration number: 240\t training loss: 1.2838\tvalidation loss: 1.2783\t validation accuracy: 0.8000\n",
      "iteration number: 241\t training loss: 1.2748\tvalidation loss: 1.2707\t validation accuracy: 0.8022\n",
      "iteration number: 242\t training loss: 1.2695\tvalidation loss: 1.2692\t validation accuracy: 0.7867\n",
      "iteration number: 243\t training loss: 1.2585\tvalidation loss: 1.2561\t validation accuracy: 0.7756\n",
      "iteration number: 244\t training loss: 1.2473\tvalidation loss: 1.2423\t validation accuracy: 0.7867\n",
      "iteration number: 245\t training loss: 1.2354\tvalidation loss: 1.2264\t validation accuracy: 0.7600\n",
      "iteration number: 246\t training loss: 1.2256\tvalidation loss: 1.2147\t validation accuracy: 0.7733\n",
      "iteration number: 247\t training loss: 1.2180\tvalidation loss: 1.2085\t validation accuracy: 0.7667\n",
      "iteration number: 248\t training loss: 1.2082\tvalidation loss: 1.1999\t validation accuracy: 0.7622\n",
      "iteration number: 249\t training loss: 1.1965\tvalidation loss: 1.1880\t validation accuracy: 0.7644\n",
      "iteration number: 250\t training loss: 1.1875\tvalidation loss: 1.1775\t validation accuracy: 0.7578\n",
      "iteration number: 251\t training loss: 1.1813\tvalidation loss: 1.1711\t validation accuracy: 0.7444\n",
      "iteration number: 252\t training loss: 1.1657\tvalidation loss: 1.1524\t validation accuracy: 0.7622\n",
      "iteration number: 253\t training loss: 1.1576\tvalidation loss: 1.1416\t validation accuracy: 0.7689\n",
      "iteration number: 254\t training loss: 1.1444\tvalidation loss: 1.1305\t validation accuracy: 0.7578\n",
      "iteration number: 255\t training loss: 1.1379\tvalidation loss: 1.1224\t validation accuracy: 0.7689\n",
      "iteration number: 256\t training loss: 1.1269\tvalidation loss: 1.1128\t validation accuracy: 0.7756\n",
      "iteration number: 257\t training loss: 1.1178\tvalidation loss: 1.1012\t validation accuracy: 0.7578\n",
      "iteration number: 258\t training loss: 1.1100\tvalidation loss: 1.0916\t validation accuracy: 0.7556\n",
      "iteration number: 259\t training loss: 1.0978\tvalidation loss: 1.0807\t validation accuracy: 0.7689\n",
      "iteration number: 260\t training loss: 1.0877\tvalidation loss: 1.0696\t validation accuracy: 0.7822\n",
      "iteration number: 261\t training loss: 1.0779\tvalidation loss: 1.0608\t validation accuracy: 0.7911\n",
      "iteration number: 262\t training loss: 1.0698\tvalidation loss: 1.0517\t validation accuracy: 0.7956\n",
      "iteration number: 263\t training loss: 1.0590\tvalidation loss: 1.0412\t validation accuracy: 0.8111\n",
      "iteration number: 264\t training loss: 1.0535\tvalidation loss: 1.0361\t validation accuracy: 0.7822\n",
      "iteration number: 265\t training loss: 1.0458\tvalidation loss: 1.0265\t validation accuracy: 0.7756\n",
      "iteration number: 266\t training loss: 1.0374\tvalidation loss: 1.0188\t validation accuracy: 0.7889\n",
      "iteration number: 267\t training loss: 1.0359\tvalidation loss: 1.0172\t validation accuracy: 0.7889\n",
      "iteration number: 268\t training loss: 1.0297\tvalidation loss: 1.0101\t validation accuracy: 0.7711\n",
      "iteration number: 269\t training loss: 1.0168\tvalidation loss: 0.9985\t validation accuracy: 0.7889\n",
      "iteration number: 270\t training loss: 1.0055\tvalidation loss: 0.9881\t validation accuracy: 0.7911\n",
      "iteration number: 271\t training loss: 0.9988\tvalidation loss: 0.9817\t validation accuracy: 0.7956\n",
      "iteration number: 272\t training loss: 0.9933\tvalidation loss: 0.9740\t validation accuracy: 0.7822\n",
      "iteration number: 273\t training loss: 0.9902\tvalidation loss: 0.9704\t validation accuracy: 0.7689\n",
      "iteration number: 274\t training loss: 0.9753\tvalidation loss: 0.9519\t validation accuracy: 0.7844\n",
      "iteration number: 275\t training loss: 0.9688\tvalidation loss: 0.9475\t validation accuracy: 0.8022\n",
      "iteration number: 276\t training loss: 0.9622\tvalidation loss: 0.9425\t validation accuracy: 0.8044\n",
      "iteration number: 277\t training loss: 0.9564\tvalidation loss: 0.9347\t validation accuracy: 0.8022\n",
      "iteration number: 278\t training loss: 0.9543\tvalidation loss: 0.9353\t validation accuracy: 0.8178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 279\t training loss: 0.9412\tvalidation loss: 0.9209\t validation accuracy: 0.8156\n",
      "iteration number: 280\t training loss: 0.9361\tvalidation loss: 0.9169\t validation accuracy: 0.8000\n",
      "iteration number: 281\t training loss: 0.9245\tvalidation loss: 0.9043\t validation accuracy: 0.8200\n",
      "iteration number: 282\t training loss: 0.9195\tvalidation loss: 0.9022\t validation accuracy: 0.8244\n",
      "iteration number: 283\t training loss: 0.9105\tvalidation loss: 0.8915\t validation accuracy: 0.8200\n",
      "iteration number: 284\t training loss: 0.9037\tvalidation loss: 0.8833\t validation accuracy: 0.8400\n",
      "iteration number: 285\t training loss: 0.8962\tvalidation loss: 0.8738\t validation accuracy: 0.8311\n",
      "iteration number: 286\t training loss: 0.8924\tvalidation loss: 0.8716\t validation accuracy: 0.8244\n",
      "iteration number: 287\t training loss: 0.8856\tvalidation loss: 0.8652\t validation accuracy: 0.8178\n",
      "iteration number: 288\t training loss: 0.8771\tvalidation loss: 0.8552\t validation accuracy: 0.8244\n",
      "iteration number: 289\t training loss: 0.8695\tvalidation loss: 0.8496\t validation accuracy: 0.8356\n",
      "iteration number: 290\t training loss: 0.8673\tvalidation loss: 0.8451\t validation accuracy: 0.8111\n",
      "iteration number: 291\t training loss: 0.8639\tvalidation loss: 0.8457\t validation accuracy: 0.8267\n",
      "iteration number: 292\t training loss: 0.8625\tvalidation loss: 0.8406\t validation accuracy: 0.8178\n",
      "iteration number: 293\t training loss: 0.8509\tvalidation loss: 0.8282\t validation accuracy: 0.8178\n",
      "iteration number: 294\t training loss: 0.8410\tvalidation loss: 0.8158\t validation accuracy: 0.8200\n",
      "iteration number: 295\t training loss: 0.8337\tvalidation loss: 0.8082\t validation accuracy: 0.8244\n",
      "iteration number: 296\t training loss: 0.8281\tvalidation loss: 0.8045\t validation accuracy: 0.8178\n",
      "iteration number: 297\t training loss: 0.8245\tvalidation loss: 0.8000\t validation accuracy: 0.8133\n",
      "iteration number: 298\t training loss: 0.8168\tvalidation loss: 0.7933\t validation accuracy: 0.8200\n",
      "iteration number: 299\t training loss: 0.8131\tvalidation loss: 0.7888\t validation accuracy: 0.8111\n",
      "iteration number: 300\t training loss: 0.8067\tvalidation loss: 0.7825\t validation accuracy: 0.8133\n",
      "iteration number: 301\t training loss: 0.8058\tvalidation loss: 0.7793\t validation accuracy: 0.8178\n",
      "iteration number: 302\t training loss: 0.7979\tvalidation loss: 0.7706\t validation accuracy: 0.8178\n",
      "iteration number: 303\t training loss: 0.7891\tvalidation loss: 0.7646\t validation accuracy: 0.8222\n",
      "iteration number: 304\t training loss: 0.7815\tvalidation loss: 0.7567\t validation accuracy: 0.8311\n",
      "iteration number: 305\t training loss: 0.7801\tvalidation loss: 0.7530\t validation accuracy: 0.8356\n",
      "iteration number: 306\t training loss: 0.7749\tvalidation loss: 0.7495\t validation accuracy: 0.8422\n",
      "iteration number: 307\t training loss: 0.7676\tvalidation loss: 0.7420\t validation accuracy: 0.8311\n",
      "iteration number: 308\t training loss: 0.7603\tvalidation loss: 0.7367\t validation accuracy: 0.8511\n",
      "iteration number: 309\t training loss: 0.7556\tvalidation loss: 0.7340\t validation accuracy: 0.8711\n",
      "iteration number: 310\t training loss: 0.7532\tvalidation loss: 0.7305\t validation accuracy: 0.8667\n",
      "iteration number: 311\t training loss: 0.7462\tvalidation loss: 0.7254\t validation accuracy: 0.8689\n",
      "iteration number: 312\t training loss: 0.7393\tvalidation loss: 0.7179\t validation accuracy: 0.8578\n",
      "iteration number: 313\t training loss: 0.7371\tvalidation loss: 0.7169\t validation accuracy: 0.8667\n",
      "iteration number: 314\t training loss: 0.7322\tvalidation loss: 0.7107\t validation accuracy: 0.8622\n",
      "iteration number: 315\t training loss: 0.7292\tvalidation loss: 0.7062\t validation accuracy: 0.8511\n",
      "iteration number: 316\t training loss: 0.7224\tvalidation loss: 0.7015\t validation accuracy: 0.8511\n",
      "iteration number: 317\t training loss: 0.7185\tvalidation loss: 0.6985\t validation accuracy: 0.8556\n",
      "iteration number: 318\t training loss: 0.7138\tvalidation loss: 0.6915\t validation accuracy: 0.8711\n",
      "iteration number: 319\t training loss: 0.7097\tvalidation loss: 0.6893\t validation accuracy: 0.8578\n",
      "iteration number: 320\t training loss: 0.7071\tvalidation loss: 0.6873\t validation accuracy: 0.8533\n",
      "iteration number: 321\t training loss: 0.7062\tvalidation loss: 0.6879\t validation accuracy: 0.8511\n",
      "iteration number: 322\t training loss: 0.7014\tvalidation loss: 0.6844\t validation accuracy: 0.8711\n",
      "iteration number: 323\t training loss: 0.6959\tvalidation loss: 0.6793\t validation accuracy: 0.8689\n",
      "iteration number: 324\t training loss: 0.6878\tvalidation loss: 0.6685\t validation accuracy: 0.8600\n",
      "iteration number: 325\t training loss: 0.6870\tvalidation loss: 0.6696\t validation accuracy: 0.8600\n",
      "iteration number: 326\t training loss: 0.6812\tvalidation loss: 0.6645\t validation accuracy: 0.8644\n",
      "iteration number: 327\t training loss: 0.6813\tvalidation loss: 0.6646\t validation accuracy: 0.8511\n",
      "iteration number: 328\t training loss: 0.6779\tvalidation loss: 0.6596\t validation accuracy: 0.8622\n",
      "iteration number: 329\t training loss: 0.6664\tvalidation loss: 0.6473\t validation accuracy: 0.8511\n",
      "iteration number: 330\t training loss: 0.6656\tvalidation loss: 0.6463\t validation accuracy: 0.8667\n",
      "iteration number: 331\t training loss: 0.6599\tvalidation loss: 0.6417\t validation accuracy: 0.8778\n",
      "iteration number: 332\t training loss: 0.6555\tvalidation loss: 0.6365\t validation accuracy: 0.8667\n",
      "iteration number: 333\t training loss: 0.6510\tvalidation loss: 0.6296\t validation accuracy: 0.8644\n",
      "iteration number: 334\t training loss: 0.6500\tvalidation loss: 0.6273\t validation accuracy: 0.8533\n",
      "iteration number: 335\t training loss: 0.6475\tvalidation loss: 0.6239\t validation accuracy: 0.8422\n",
      "iteration number: 336\t training loss: 0.6407\tvalidation loss: 0.6169\t validation accuracy: 0.8689\n",
      "iteration number: 337\t training loss: 0.6362\tvalidation loss: 0.6132\t validation accuracy: 0.8622\n",
      "iteration number: 338\t training loss: 0.6321\tvalidation loss: 0.6107\t validation accuracy: 0.8711\n",
      "iteration number: 339\t training loss: 0.6300\tvalidation loss: 0.6088\t validation accuracy: 0.8667\n",
      "iteration number: 340\t training loss: 0.6280\tvalidation loss: 0.6068\t validation accuracy: 0.8578\n",
      "iteration number: 341\t training loss: 0.6221\tvalidation loss: 0.6032\t validation accuracy: 0.8667\n",
      "iteration number: 342\t training loss: 0.6213\tvalidation loss: 0.6032\t validation accuracy: 0.8689\n",
      "iteration number: 343\t training loss: 0.6183\tvalidation loss: 0.6016\t validation accuracy: 0.8644\n",
      "iteration number: 344\t training loss: 0.6172\tvalidation loss: 0.6010\t validation accuracy: 0.8556\n",
      "iteration number: 345\t training loss: 0.6090\tvalidation loss: 0.5915\t validation accuracy: 0.8644\n",
      "iteration number: 346\t training loss: 0.6041\tvalidation loss: 0.5851\t validation accuracy: 0.8600\n",
      "iteration number: 347\t training loss: 0.6016\tvalidation loss: 0.5827\t validation accuracy: 0.8711\n",
      "iteration number: 348\t training loss: 0.6012\tvalidation loss: 0.5843\t validation accuracy: 0.8711\n",
      "iteration number: 349\t training loss: 0.5968\tvalidation loss: 0.5778\t validation accuracy: 0.8756\n",
      "iteration number: 350\t training loss: 0.5969\tvalidation loss: 0.5759\t validation accuracy: 0.8689\n",
      "iteration number: 351\t training loss: 0.5955\tvalidation loss: 0.5766\t validation accuracy: 0.8756\n",
      "iteration number: 352\t training loss: 0.5927\tvalidation loss: 0.5746\t validation accuracy: 0.8800\n",
      "iteration number: 353\t training loss: 0.5883\tvalidation loss: 0.5695\t validation accuracy: 0.8800\n",
      "iteration number: 354\t training loss: 0.5804\tvalidation loss: 0.5606\t validation accuracy: 0.8800\n",
      "iteration number: 355\t training loss: 0.5763\tvalidation loss: 0.5557\t validation accuracy: 0.8911\n",
      "iteration number: 356\t training loss: 0.5744\tvalidation loss: 0.5562\t validation accuracy: 0.8822\n",
      "iteration number: 357\t training loss: 0.5753\tvalidation loss: 0.5575\t validation accuracy: 0.8911\n",
      "iteration number: 358\t training loss: 0.5741\tvalidation loss: 0.5547\t validation accuracy: 0.8844\n",
      "iteration number: 359\t training loss: 0.5655\tvalidation loss: 0.5468\t validation accuracy: 0.8844\n",
      "iteration number: 360\t training loss: 0.5658\tvalidation loss: 0.5482\t validation accuracy: 0.8911\n",
      "iteration number: 361\t training loss: 0.5629\tvalidation loss: 0.5454\t validation accuracy: 0.8844\n",
      "iteration number: 362\t training loss: 0.5583\tvalidation loss: 0.5401\t validation accuracy: 0.8733\n",
      "iteration number: 363\t training loss: 0.5558\tvalidation loss: 0.5370\t validation accuracy: 0.9000\n",
      "iteration number: 364\t training loss: 0.5609\tvalidation loss: 0.5473\t validation accuracy: 0.8844\n",
      "iteration number: 365\t training loss: 0.5495\tvalidation loss: 0.5350\t validation accuracy: 0.8844\n",
      "iteration number: 366\t training loss: 0.5457\tvalidation loss: 0.5289\t validation accuracy: 0.8956\n",
      "iteration number: 367\t training loss: 0.5431\tvalidation loss: 0.5255\t validation accuracy: 0.8867\n",
      "iteration number: 368\t training loss: 0.5474\tvalidation loss: 0.5284\t validation accuracy: 0.8822\n",
      "iteration number: 369\t training loss: 0.5411\tvalidation loss: 0.5239\t validation accuracy: 0.8867\n",
      "iteration number: 370\t training loss: 0.5366\tvalidation loss: 0.5181\t validation accuracy: 0.8889\n",
      "iteration number: 371\t training loss: 0.5337\tvalidation loss: 0.5178\t validation accuracy: 0.8800\n",
      "iteration number: 372\t training loss: 0.5333\tvalidation loss: 0.5190\t validation accuracy: 0.8778\n",
      "iteration number: 373\t training loss: 0.5312\tvalidation loss: 0.5159\t validation accuracy: 0.8800\n",
      "iteration number: 374\t training loss: 0.5280\tvalidation loss: 0.5142\t validation accuracy: 0.8844\n",
      "iteration number: 375\t training loss: 0.5275\tvalidation loss: 0.5134\t validation accuracy: 0.8867\n",
      "iteration number: 376\t training loss: 0.5308\tvalidation loss: 0.5153\t validation accuracy: 0.8844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 377\t training loss: 0.5291\tvalidation loss: 0.5137\t validation accuracy: 0.8911\n",
      "iteration number: 378\t training loss: 0.5198\tvalidation loss: 0.5039\t validation accuracy: 0.8978\n",
      "iteration number: 379\t training loss: 0.5147\tvalidation loss: 0.4990\t validation accuracy: 0.8844\n",
      "iteration number: 380\t training loss: 0.5124\tvalidation loss: 0.4966\t validation accuracy: 0.8978\n",
      "iteration number: 381\t training loss: 0.5111\tvalidation loss: 0.4939\t validation accuracy: 0.9044\n",
      "iteration number: 382\t training loss: 0.5123\tvalidation loss: 0.4972\t validation accuracy: 0.8889\n",
      "iteration number: 383\t training loss: 0.5083\tvalidation loss: 0.4934\t validation accuracy: 0.8889\n",
      "iteration number: 384\t training loss: 0.5056\tvalidation loss: 0.4883\t validation accuracy: 0.8867\n",
      "iteration number: 385\t training loss: 0.5014\tvalidation loss: 0.4876\t validation accuracy: 0.8844\n",
      "iteration number: 386\t training loss: 0.5016\tvalidation loss: 0.4893\t validation accuracy: 0.8844\n",
      "iteration number: 387\t training loss: 0.4966\tvalidation loss: 0.4834\t validation accuracy: 0.8867\n",
      "iteration number: 388\t training loss: 0.4956\tvalidation loss: 0.4810\t validation accuracy: 0.8933\n",
      "iteration number: 389\t training loss: 0.4919\tvalidation loss: 0.4761\t validation accuracy: 0.8889\n",
      "iteration number: 390\t training loss: 0.4935\tvalidation loss: 0.4787\t validation accuracy: 0.8822\n",
      "iteration number: 391\t training loss: 0.4898\tvalidation loss: 0.4760\t validation accuracy: 0.8889\n",
      "iteration number: 392\t training loss: 0.4870\tvalidation loss: 0.4712\t validation accuracy: 0.8911\n",
      "iteration number: 393\t training loss: 0.4903\tvalidation loss: 0.4739\t validation accuracy: 0.8822\n",
      "iteration number: 394\t training loss: 0.4842\tvalidation loss: 0.4678\t validation accuracy: 0.8933\n",
      "iteration number: 395\t training loss: 0.4830\tvalidation loss: 0.4679\t validation accuracy: 0.8911\n",
      "iteration number: 396\t training loss: 0.4814\tvalidation loss: 0.4671\t validation accuracy: 0.8911\n",
      "iteration number: 397\t training loss: 0.4804\tvalidation loss: 0.4668\t validation accuracy: 0.8867\n",
      "iteration number: 398\t training loss: 0.4771\tvalidation loss: 0.4609\t validation accuracy: 0.8911\n",
      "iteration number: 399\t training loss: 0.4721\tvalidation loss: 0.4560\t validation accuracy: 0.8956\n",
      "iteration number: 400\t training loss: 0.4709\tvalidation loss: 0.4556\t validation accuracy: 0.8956\n",
      "iteration number: 401\t training loss: 0.4679\tvalidation loss: 0.4520\t validation accuracy: 0.9067\n",
      "iteration number: 402\t training loss: 0.4653\tvalidation loss: 0.4488\t validation accuracy: 0.9044\n",
      "iteration number: 403\t training loss: 0.4651\tvalidation loss: 0.4517\t validation accuracy: 0.8978\n",
      "iteration number: 404\t training loss: 0.4733\tvalidation loss: 0.4596\t validation accuracy: 0.8889\n",
      "iteration number: 405\t training loss: 0.4733\tvalidation loss: 0.4588\t validation accuracy: 0.8844\n",
      "iteration number: 406\t training loss: 0.4677\tvalidation loss: 0.4567\t validation accuracy: 0.8867\n",
      "iteration number: 407\t training loss: 0.4722\tvalidation loss: 0.4596\t validation accuracy: 0.8822\n",
      "iteration number: 408\t training loss: 0.4637\tvalidation loss: 0.4527\t validation accuracy: 0.8867\n",
      "iteration number: 409\t training loss: 0.4562\tvalidation loss: 0.4435\t validation accuracy: 0.8978\n",
      "iteration number: 410\t training loss: 0.4533\tvalidation loss: 0.4427\t validation accuracy: 0.8911\n",
      "iteration number: 411\t training loss: 0.4513\tvalidation loss: 0.4402\t validation accuracy: 0.8933\n",
      "iteration number: 412\t training loss: 0.4478\tvalidation loss: 0.4353\t validation accuracy: 0.9044\n",
      "iteration number: 413\t training loss: 0.4459\tvalidation loss: 0.4340\t validation accuracy: 0.9022\n",
      "iteration number: 414\t training loss: 0.4437\tvalidation loss: 0.4317\t validation accuracy: 0.9044\n",
      "iteration number: 415\t training loss: 0.4430\tvalidation loss: 0.4316\t validation accuracy: 0.9089\n",
      "iteration number: 416\t training loss: 0.4442\tvalidation loss: 0.4302\t validation accuracy: 0.9044\n",
      "iteration number: 417\t training loss: 0.4439\tvalidation loss: 0.4307\t validation accuracy: 0.9133\n",
      "iteration number: 418\t training loss: 0.4372\tvalidation loss: 0.4248\t validation accuracy: 0.9111\n",
      "iteration number: 419\t training loss: 0.4373\tvalidation loss: 0.4245\t validation accuracy: 0.9044\n",
      "iteration number: 420\t training loss: 0.4359\tvalidation loss: 0.4241\t validation accuracy: 0.9156\n",
      "iteration number: 421\t training loss: 0.4338\tvalidation loss: 0.4230\t validation accuracy: 0.9156\n",
      "iteration number: 422\t training loss: 0.4321\tvalidation loss: 0.4210\t validation accuracy: 0.9089\n",
      "iteration number: 423\t training loss: 0.4324\tvalidation loss: 0.4217\t validation accuracy: 0.9044\n",
      "iteration number: 424\t training loss: 0.4312\tvalidation loss: 0.4216\t validation accuracy: 0.9178\n",
      "iteration number: 425\t training loss: 0.4276\tvalidation loss: 0.4194\t validation accuracy: 0.9067\n",
      "iteration number: 426\t training loss: 0.4246\tvalidation loss: 0.4156\t validation accuracy: 0.9111\n",
      "iteration number: 427\t training loss: 0.4229\tvalidation loss: 0.4150\t validation accuracy: 0.9067\n",
      "iteration number: 428\t training loss: 0.4213\tvalidation loss: 0.4130\t validation accuracy: 0.9044\n",
      "iteration number: 429\t training loss: 0.4228\tvalidation loss: 0.4129\t validation accuracy: 0.9067\n",
      "iteration number: 430\t training loss: 0.4248\tvalidation loss: 0.4137\t validation accuracy: 0.9000\n",
      "iteration number: 431\t training loss: 0.4222\tvalidation loss: 0.4114\t validation accuracy: 0.9000\n",
      "iteration number: 432\t training loss: 0.4239\tvalidation loss: 0.4121\t validation accuracy: 0.8933\n",
      "iteration number: 433\t training loss: 0.4187\tvalidation loss: 0.4091\t validation accuracy: 0.8956\n",
      "iteration number: 434\t training loss: 0.4185\tvalidation loss: 0.4081\t validation accuracy: 0.9022\n",
      "iteration number: 435\t training loss: 0.4129\tvalidation loss: 0.4039\t validation accuracy: 0.9044\n",
      "iteration number: 436\t training loss: 0.4106\tvalidation loss: 0.4013\t validation accuracy: 0.9067\n",
      "iteration number: 437\t training loss: 0.4092\tvalidation loss: 0.4008\t validation accuracy: 0.9089\n",
      "iteration number: 438\t training loss: 0.4058\tvalidation loss: 0.3980\t validation accuracy: 0.9111\n",
      "iteration number: 439\t training loss: 0.4062\tvalidation loss: 0.3983\t validation accuracy: 0.9044\n",
      "iteration number: 440\t training loss: 0.4033\tvalidation loss: 0.3955\t validation accuracy: 0.9111\n",
      "iteration number: 441\t training loss: 0.4021\tvalidation loss: 0.3930\t validation accuracy: 0.9111\n",
      "iteration number: 442\t training loss: 0.4003\tvalidation loss: 0.3927\t validation accuracy: 0.9089\n",
      "iteration number: 443\t training loss: 0.4033\tvalidation loss: 0.3953\t validation accuracy: 0.9111\n",
      "iteration number: 444\t training loss: 0.4004\tvalidation loss: 0.3936\t validation accuracy: 0.9067\n",
      "iteration number: 445\t training loss: 0.4023\tvalidation loss: 0.3956\t validation accuracy: 0.9044\n",
      "iteration number: 446\t training loss: 0.3997\tvalidation loss: 0.3920\t validation accuracy: 0.9000\n",
      "iteration number: 447\t training loss: 0.3936\tvalidation loss: 0.3872\t validation accuracy: 0.9111\n",
      "iteration number: 448\t training loss: 0.3924\tvalidation loss: 0.3861\t validation accuracy: 0.9133\n",
      "iteration number: 449\t training loss: 0.3933\tvalidation loss: 0.3846\t validation accuracy: 0.9178\n",
      "iteration number: 450\t training loss: 0.3901\tvalidation loss: 0.3849\t validation accuracy: 0.9200\n",
      "iteration number: 451\t training loss: 0.3900\tvalidation loss: 0.3851\t validation accuracy: 0.9089\n",
      "iteration number: 452\t training loss: 0.3897\tvalidation loss: 0.3846\t validation accuracy: 0.9133\n",
      "iteration number: 453\t training loss: 0.3922\tvalidation loss: 0.3863\t validation accuracy: 0.9156\n",
      "iteration number: 454\t training loss: 0.3911\tvalidation loss: 0.3865\t validation accuracy: 0.9178\n",
      "iteration number: 455\t training loss: 0.3845\tvalidation loss: 0.3809\t validation accuracy: 0.9133\n",
      "iteration number: 456\t training loss: 0.3854\tvalidation loss: 0.3816\t validation accuracy: 0.9222\n",
      "iteration number: 457\t training loss: 0.3819\tvalidation loss: 0.3773\t validation accuracy: 0.9178\n",
      "iteration number: 458\t training loss: 0.3815\tvalidation loss: 0.3765\t validation accuracy: 0.9267\n",
      "iteration number: 459\t training loss: 0.3848\tvalidation loss: 0.3838\t validation accuracy: 0.9089\n",
      "iteration number: 460\t training loss: 0.3870\tvalidation loss: 0.3875\t validation accuracy: 0.9022\n",
      "iteration number: 461\t training loss: 0.3802\tvalidation loss: 0.3799\t validation accuracy: 0.9200\n",
      "iteration number: 462\t training loss: 0.3759\tvalidation loss: 0.3743\t validation accuracy: 0.9222\n",
      "iteration number: 463\t training loss: 0.3747\tvalidation loss: 0.3709\t validation accuracy: 0.9200\n",
      "iteration number: 464\t training loss: 0.3742\tvalidation loss: 0.3695\t validation accuracy: 0.9178\n",
      "iteration number: 465\t training loss: 0.3716\tvalidation loss: 0.3677\t validation accuracy: 0.9089\n",
      "iteration number: 466\t training loss: 0.3694\tvalidation loss: 0.3657\t validation accuracy: 0.9133\n",
      "iteration number: 467\t training loss: 0.3678\tvalidation loss: 0.3643\t validation accuracy: 0.9133\n",
      "iteration number: 468\t training loss: 0.3669\tvalidation loss: 0.3644\t validation accuracy: 0.9133\n",
      "iteration number: 469\t training loss: 0.3692\tvalidation loss: 0.3669\t validation accuracy: 0.9133\n",
      "iteration number: 470\t training loss: 0.3713\tvalidation loss: 0.3685\t validation accuracy: 0.9133\n",
      "iteration number: 471\t training loss: 0.3684\tvalidation loss: 0.3654\t validation accuracy: 0.9244\n",
      "iteration number: 472\t training loss: 0.3693\tvalidation loss: 0.3662\t validation accuracy: 0.9111\n",
      "iteration number: 473\t training loss: 0.3615\tvalidation loss: 0.3588\t validation accuracy: 0.9200\n",
      "iteration number: 474\t training loss: 0.3644\tvalidation loss: 0.3605\t validation accuracy: 0.9156\n",
      "iteration number: 475\t training loss: 0.3678\tvalidation loss: 0.3632\t validation accuracy: 0.9044\n",
      "iteration number: 476\t training loss: 0.3653\tvalidation loss: 0.3606\t validation accuracy: 0.9089\n",
      "iteration number: 477\t training loss: 0.3633\tvalidation loss: 0.3584\t validation accuracy: 0.9089\n",
      "iteration number: 478\t training loss: 0.3673\tvalidation loss: 0.3617\t validation accuracy: 0.9156\n",
      "iteration number: 479\t training loss: 0.3624\tvalidation loss: 0.3565\t validation accuracy: 0.9156\n",
      "iteration number: 480\t training loss: 0.3628\tvalidation loss: 0.3574\t validation accuracy: 0.9111\n",
      "iteration number: 481\t training loss: 0.3600\tvalidation loss: 0.3541\t validation accuracy: 0.9156\n",
      "iteration number: 482\t training loss: 0.3553\tvalidation loss: 0.3492\t validation accuracy: 0.9111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 483\t training loss: 0.3583\tvalidation loss: 0.3534\t validation accuracy: 0.9022\n",
      "iteration number: 484\t training loss: 0.3549\tvalidation loss: 0.3504\t validation accuracy: 0.9067\n",
      "iteration number: 485\t training loss: 0.3502\tvalidation loss: 0.3471\t validation accuracy: 0.9111\n",
      "iteration number: 486\t training loss: 0.3493\tvalidation loss: 0.3466\t validation accuracy: 0.9111\n",
      "iteration number: 487\t training loss: 0.3457\tvalidation loss: 0.3436\t validation accuracy: 0.9200\n",
      "iteration number: 488\t training loss: 0.3464\tvalidation loss: 0.3473\t validation accuracy: 0.9200\n",
      "iteration number: 489\t training loss: 0.3462\tvalidation loss: 0.3468\t validation accuracy: 0.9156\n",
      "iteration number: 490\t training loss: 0.3454\tvalidation loss: 0.3470\t validation accuracy: 0.9111\n",
      "iteration number: 491\t training loss: 0.3464\tvalidation loss: 0.3481\t validation accuracy: 0.9133\n",
      "iteration number: 492\t training loss: 0.3442\tvalidation loss: 0.3450\t validation accuracy: 0.9178\n",
      "iteration number: 493\t training loss: 0.3424\tvalidation loss: 0.3430\t validation accuracy: 0.9156\n",
      "iteration number: 494\t training loss: 0.3410\tvalidation loss: 0.3417\t validation accuracy: 0.9200\n",
      "iteration number: 495\t training loss: 0.3392\tvalidation loss: 0.3391\t validation accuracy: 0.9244\n",
      "iteration number: 496\t training loss: 0.3387\tvalidation loss: 0.3381\t validation accuracy: 0.9200\n",
      "iteration number: 497\t training loss: 0.3406\tvalidation loss: 0.3409\t validation accuracy: 0.9267\n",
      "iteration number: 498\t training loss: 0.3401\tvalidation loss: 0.3397\t validation accuracy: 0.9267\n",
      "iteration number: 499\t training loss: 0.3380\tvalidation loss: 0.3371\t validation accuracy: 0.9222\n",
      "iteration number: 500\t training loss: 0.3373\tvalidation loss: 0.3394\t validation accuracy: 0.9200\n",
      "iteration number: 501\t training loss: 0.3376\tvalidation loss: 0.3398\t validation accuracy: 0.9089\n",
      "iteration number: 502\t training loss: 0.3363\tvalidation loss: 0.3377\t validation accuracy: 0.9067\n",
      "iteration number: 503\t training loss: 0.3331\tvalidation loss: 0.3336\t validation accuracy: 0.9244\n",
      "iteration number: 504\t training loss: 0.3335\tvalidation loss: 0.3344\t validation accuracy: 0.9200\n",
      "iteration number: 505\t training loss: 0.3299\tvalidation loss: 0.3316\t validation accuracy: 0.9267\n",
      "iteration number: 506\t training loss: 0.3328\tvalidation loss: 0.3353\t validation accuracy: 0.9222\n",
      "iteration number: 507\t training loss: 0.3304\tvalidation loss: 0.3355\t validation accuracy: 0.9156\n",
      "iteration number: 508\t training loss: 0.3322\tvalidation loss: 0.3381\t validation accuracy: 0.9133\n",
      "iteration number: 509\t training loss: 0.3326\tvalidation loss: 0.3378\t validation accuracy: 0.9111\n",
      "iteration number: 510\t training loss: 0.3306\tvalidation loss: 0.3359\t validation accuracy: 0.9111\n",
      "iteration number: 511\t training loss: 0.3279\tvalidation loss: 0.3333\t validation accuracy: 0.9111\n",
      "iteration number: 512\t training loss: 0.3275\tvalidation loss: 0.3341\t validation accuracy: 0.9156\n",
      "iteration number: 513\t training loss: 0.3295\tvalidation loss: 0.3382\t validation accuracy: 0.9156\n",
      "iteration number: 514\t training loss: 0.3264\tvalidation loss: 0.3345\t validation accuracy: 0.9222\n",
      "iteration number: 515\t training loss: 0.3247\tvalidation loss: 0.3313\t validation accuracy: 0.9156\n",
      "iteration number: 516\t training loss: 0.3211\tvalidation loss: 0.3264\t validation accuracy: 0.9222\n",
      "iteration number: 517\t training loss: 0.3176\tvalidation loss: 0.3224\t validation accuracy: 0.9267\n",
      "iteration number: 518\t training loss: 0.3172\tvalidation loss: 0.3208\t validation accuracy: 0.9244\n",
      "iteration number: 519\t training loss: 0.3179\tvalidation loss: 0.3210\t validation accuracy: 0.9244\n",
      "iteration number: 520\t training loss: 0.3156\tvalidation loss: 0.3195\t validation accuracy: 0.9244\n",
      "iteration number: 521\t training loss: 0.3161\tvalidation loss: 0.3201\t validation accuracy: 0.9267\n",
      "iteration number: 522\t training loss: 0.3123\tvalidation loss: 0.3171\t validation accuracy: 0.9244\n",
      "iteration number: 523\t training loss: 0.3121\tvalidation loss: 0.3185\t validation accuracy: 0.9289\n",
      "iteration number: 524\t training loss: 0.3096\tvalidation loss: 0.3156\t validation accuracy: 0.9333\n",
      "iteration number: 525\t training loss: 0.3108\tvalidation loss: 0.3164\t validation accuracy: 0.9267\n",
      "iteration number: 526\t training loss: 0.3086\tvalidation loss: 0.3135\t validation accuracy: 0.9289\n",
      "iteration number: 527\t training loss: 0.3094\tvalidation loss: 0.3163\t validation accuracy: 0.9267\n",
      "iteration number: 528\t training loss: 0.3114\tvalidation loss: 0.3184\t validation accuracy: 0.9244\n",
      "iteration number: 529\t training loss: 0.3074\tvalidation loss: 0.3138\t validation accuracy: 0.9289\n",
      "iteration number: 530\t training loss: 0.3061\tvalidation loss: 0.3120\t validation accuracy: 0.9311\n",
      "iteration number: 531\t training loss: 0.3075\tvalidation loss: 0.3126\t validation accuracy: 0.9222\n",
      "iteration number: 532\t training loss: 0.3098\tvalidation loss: 0.3152\t validation accuracy: 0.9200\n",
      "iteration number: 533\t training loss: 0.3068\tvalidation loss: 0.3115\t validation accuracy: 0.9289\n",
      "iteration number: 534\t training loss: 0.3048\tvalidation loss: 0.3100\t validation accuracy: 0.9244\n",
      "iteration number: 535\t training loss: 0.3027\tvalidation loss: 0.3098\t validation accuracy: 0.9311\n",
      "iteration number: 536\t training loss: 0.3018\tvalidation loss: 0.3112\t validation accuracy: 0.9244\n",
      "iteration number: 537\t training loss: 0.3004\tvalidation loss: 0.3087\t validation accuracy: 0.9200\n",
      "iteration number: 538\t training loss: 0.2990\tvalidation loss: 0.3080\t validation accuracy: 0.9289\n",
      "iteration number: 539\t training loss: 0.3003\tvalidation loss: 0.3109\t validation accuracy: 0.9267\n",
      "iteration number: 540\t training loss: 0.3022\tvalidation loss: 0.3128\t validation accuracy: 0.9244\n",
      "iteration number: 541\t training loss: 0.2984\tvalidation loss: 0.3087\t validation accuracy: 0.9222\n",
      "iteration number: 542\t training loss: 0.2971\tvalidation loss: 0.3057\t validation accuracy: 0.9244\n",
      "iteration number: 543\t training loss: 0.2969\tvalidation loss: 0.3063\t validation accuracy: 0.9267\n",
      "iteration number: 544\t training loss: 0.2988\tvalidation loss: 0.3092\t validation accuracy: 0.9244\n",
      "iteration number: 545\t training loss: 0.2973\tvalidation loss: 0.3073\t validation accuracy: 0.9244\n",
      "iteration number: 546\t training loss: 0.2949\tvalidation loss: 0.3029\t validation accuracy: 0.9244\n",
      "iteration number: 547\t training loss: 0.2944\tvalidation loss: 0.3023\t validation accuracy: 0.9244\n",
      "iteration number: 548\t training loss: 0.2937\tvalidation loss: 0.3020\t validation accuracy: 0.9222\n",
      "iteration number: 549\t training loss: 0.2937\tvalidation loss: 0.3016\t validation accuracy: 0.9244\n",
      "iteration number: 550\t training loss: 0.2918\tvalidation loss: 0.2995\t validation accuracy: 0.9222\n",
      "iteration number: 551\t training loss: 0.2910\tvalidation loss: 0.2992\t validation accuracy: 0.9289\n",
      "iteration number: 552\t training loss: 0.2923\tvalidation loss: 0.2995\t validation accuracy: 0.9244\n",
      "iteration number: 553\t training loss: 0.2914\tvalidation loss: 0.2991\t validation accuracy: 0.9289\n",
      "iteration number: 554\t training loss: 0.2896\tvalidation loss: 0.2962\t validation accuracy: 0.9244\n",
      "iteration number: 555\t training loss: 0.2876\tvalidation loss: 0.2941\t validation accuracy: 0.9200\n",
      "iteration number: 556\t training loss: 0.2857\tvalidation loss: 0.2931\t validation accuracy: 0.9244\n",
      "iteration number: 557\t training loss: 0.2847\tvalidation loss: 0.2936\t validation accuracy: 0.9267\n",
      "iteration number: 558\t training loss: 0.2857\tvalidation loss: 0.2928\t validation accuracy: 0.9200\n",
      "iteration number: 559\t training loss: 0.2846\tvalidation loss: 0.2916\t validation accuracy: 0.9311\n",
      "iteration number: 560\t training loss: 0.2850\tvalidation loss: 0.2928\t validation accuracy: 0.9267\n",
      "iteration number: 561\t training loss: 0.2845\tvalidation loss: 0.2935\t validation accuracy: 0.9289\n",
      "iteration number: 562\t training loss: 0.2831\tvalidation loss: 0.2931\t validation accuracy: 0.9333\n",
      "iteration number: 563\t training loss: 0.2819\tvalidation loss: 0.2911\t validation accuracy: 0.9333\n",
      "iteration number: 564\t training loss: 0.2856\tvalidation loss: 0.2934\t validation accuracy: 0.9200\n",
      "iteration number: 565\t training loss: 0.2832\tvalidation loss: 0.2919\t validation accuracy: 0.9178\n",
      "iteration number: 566\t training loss: 0.2838\tvalidation loss: 0.2916\t validation accuracy: 0.9289\n",
      "iteration number: 567\t training loss: 0.2797\tvalidation loss: 0.2883\t validation accuracy: 0.9267\n",
      "iteration number: 568\t training loss: 0.2785\tvalidation loss: 0.2869\t validation accuracy: 0.9244\n",
      "iteration number: 569\t training loss: 0.2780\tvalidation loss: 0.2869\t validation accuracy: 0.9267\n",
      "iteration number: 570\t training loss: 0.2799\tvalidation loss: 0.2904\t validation accuracy: 0.9244\n",
      "iteration number: 571\t training loss: 0.2784\tvalidation loss: 0.2889\t validation accuracy: 0.9333\n",
      "iteration number: 572\t training loss: 0.2784\tvalidation loss: 0.2893\t validation accuracy: 0.9333\n",
      "iteration number: 573\t training loss: 0.2764\tvalidation loss: 0.2886\t validation accuracy: 0.9244\n",
      "iteration number: 574\t training loss: 0.2750\tvalidation loss: 0.2889\t validation accuracy: 0.9244\n",
      "iteration number: 575\t training loss: 0.2748\tvalidation loss: 0.2884\t validation accuracy: 0.9244\n",
      "iteration number: 576\t training loss: 0.2747\tvalidation loss: 0.2875\t validation accuracy: 0.9311\n",
      "iteration number: 577\t training loss: 0.2723\tvalidation loss: 0.2860\t validation accuracy: 0.9333\n",
      "iteration number: 578\t training loss: 0.2739\tvalidation loss: 0.2863\t validation accuracy: 0.9333\n",
      "iteration number: 579\t training loss: 0.2752\tvalidation loss: 0.2867\t validation accuracy: 0.9333\n",
      "iteration number: 580\t training loss: 0.2744\tvalidation loss: 0.2863\t validation accuracy: 0.9289\n",
      "iteration number: 581\t training loss: 0.2696\tvalidation loss: 0.2828\t validation accuracy: 0.9378\n",
      "iteration number: 582\t training loss: 0.2726\tvalidation loss: 0.2842\t validation accuracy: 0.9267\n",
      "iteration number: 583\t training loss: 0.2737\tvalidation loss: 0.2850\t validation accuracy: 0.9200\n",
      "iteration number: 584\t training loss: 0.2699\tvalidation loss: 0.2813\t validation accuracy: 0.9333\n",
      "iteration number: 585\t training loss: 0.2668\tvalidation loss: 0.2779\t validation accuracy: 0.9244\n",
      "iteration number: 586\t training loss: 0.2709\tvalidation loss: 0.2807\t validation accuracy: 0.9311\n",
      "iteration number: 587\t training loss: 0.2684\tvalidation loss: 0.2789\t validation accuracy: 0.9311\n",
      "iteration number: 588\t training loss: 0.2635\tvalidation loss: 0.2754\t validation accuracy: 0.9356\n",
      "iteration number: 589\t training loss: 0.2649\tvalidation loss: 0.2765\t validation accuracy: 0.9333\n",
      "iteration number: 590\t training loss: 0.2633\tvalidation loss: 0.2756\t validation accuracy: 0.9267\n",
      "iteration number: 591\t training loss: 0.2647\tvalidation loss: 0.2769\t validation accuracy: 0.9311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 592\t training loss: 0.2662\tvalidation loss: 0.2786\t validation accuracy: 0.9289\n",
      "iteration number: 593\t training loss: 0.2634\tvalidation loss: 0.2763\t validation accuracy: 0.9267\n",
      "iteration number: 594\t training loss: 0.2631\tvalidation loss: 0.2761\t validation accuracy: 0.9289\n",
      "iteration number: 595\t training loss: 0.2602\tvalidation loss: 0.2736\t validation accuracy: 0.9289\n",
      "iteration number: 596\t training loss: 0.2597\tvalidation loss: 0.2730\t validation accuracy: 0.9333\n",
      "iteration number: 597\t training loss: 0.2616\tvalidation loss: 0.2739\t validation accuracy: 0.9311\n",
      "iteration number: 598\t training loss: 0.2614\tvalidation loss: 0.2735\t validation accuracy: 0.9356\n",
      "iteration number: 599\t training loss: 0.2589\tvalidation loss: 0.2719\t validation accuracy: 0.9333\n",
      "iteration number: 600\t training loss: 0.2562\tvalidation loss: 0.2701\t validation accuracy: 0.9378\n",
      "iteration number: 601\t training loss: 0.2568\tvalidation loss: 0.2714\t validation accuracy: 0.9378\n",
      "iteration number: 602\t training loss: 0.2555\tvalidation loss: 0.2707\t validation accuracy: 0.9333\n",
      "iteration number: 603\t training loss: 0.2554\tvalidation loss: 0.2698\t validation accuracy: 0.9356\n",
      "iteration number: 604\t training loss: 0.2591\tvalidation loss: 0.2734\t validation accuracy: 0.9311\n",
      "iteration number: 605\t training loss: 0.2554\tvalidation loss: 0.2704\t validation accuracy: 0.9333\n",
      "iteration number: 606\t training loss: 0.2560\tvalidation loss: 0.2695\t validation accuracy: 0.9311\n",
      "iteration number: 607\t training loss: 0.2534\tvalidation loss: 0.2666\t validation accuracy: 0.9400\n",
      "iteration number: 608\t training loss: 0.2539\tvalidation loss: 0.2657\t validation accuracy: 0.9378\n",
      "iteration number: 609\t training loss: 0.2523\tvalidation loss: 0.2658\t validation accuracy: 0.9356\n",
      "iteration number: 610\t training loss: 0.2511\tvalidation loss: 0.2665\t validation accuracy: 0.9378\n",
      "iteration number: 611\t training loss: 0.2502\tvalidation loss: 0.2641\t validation accuracy: 0.9378\n",
      "iteration number: 612\t training loss: 0.2496\tvalidation loss: 0.2635\t validation accuracy: 0.9378\n",
      "iteration number: 613\t training loss: 0.2494\tvalidation loss: 0.2636\t validation accuracy: 0.9356\n",
      "iteration number: 614\t training loss: 0.2490\tvalidation loss: 0.2638\t validation accuracy: 0.9356\n",
      "iteration number: 615\t training loss: 0.2482\tvalidation loss: 0.2650\t validation accuracy: 0.9311\n",
      "iteration number: 616\t training loss: 0.2468\tvalidation loss: 0.2636\t validation accuracy: 0.9333\n",
      "iteration number: 617\t training loss: 0.2478\tvalidation loss: 0.2657\t validation accuracy: 0.9378\n",
      "iteration number: 618\t training loss: 0.2472\tvalidation loss: 0.2651\t validation accuracy: 0.9356\n",
      "iteration number: 619\t training loss: 0.2477\tvalidation loss: 0.2650\t validation accuracy: 0.9333\n",
      "iteration number: 620\t training loss: 0.2471\tvalidation loss: 0.2653\t validation accuracy: 0.9400\n",
      "iteration number: 621\t training loss: 0.2464\tvalidation loss: 0.2645\t validation accuracy: 0.9400\n",
      "iteration number: 622\t training loss: 0.2480\tvalidation loss: 0.2668\t validation accuracy: 0.9311\n",
      "iteration number: 623\t training loss: 0.2467\tvalidation loss: 0.2655\t validation accuracy: 0.9356\n",
      "iteration number: 624\t training loss: 0.2450\tvalidation loss: 0.2637\t validation accuracy: 0.9378\n",
      "iteration number: 625\t training loss: 0.2457\tvalidation loss: 0.2628\t validation accuracy: 0.9356\n",
      "iteration number: 626\t training loss: 0.2459\tvalidation loss: 0.2629\t validation accuracy: 0.9356\n",
      "iteration number: 627\t training loss: 0.2462\tvalidation loss: 0.2621\t validation accuracy: 0.9333\n",
      "iteration number: 628\t training loss: 0.2434\tvalidation loss: 0.2601\t validation accuracy: 0.9356\n",
      "iteration number: 629\t training loss: 0.2454\tvalidation loss: 0.2638\t validation accuracy: 0.9356\n",
      "iteration number: 630\t training loss: 0.2463\tvalidation loss: 0.2639\t validation accuracy: 0.9333\n",
      "iteration number: 631\t training loss: 0.2415\tvalidation loss: 0.2594\t validation accuracy: 0.9333\n",
      "iteration number: 632\t training loss: 0.2420\tvalidation loss: 0.2598\t validation accuracy: 0.9311\n",
      "iteration number: 633\t training loss: 0.2387\tvalidation loss: 0.2571\t validation accuracy: 0.9378\n",
      "iteration number: 634\t training loss: 0.2386\tvalidation loss: 0.2570\t validation accuracy: 0.9378\n",
      "iteration number: 635\t training loss: 0.2408\tvalidation loss: 0.2614\t validation accuracy: 0.9356\n",
      "iteration number: 636\t training loss: 0.2412\tvalidation loss: 0.2625\t validation accuracy: 0.9333\n",
      "iteration number: 637\t training loss: 0.2408\tvalidation loss: 0.2608\t validation accuracy: 0.9289\n",
      "iteration number: 638\t training loss: 0.2380\tvalidation loss: 0.2565\t validation accuracy: 0.9289\n",
      "iteration number: 639\t training loss: 0.2369\tvalidation loss: 0.2559\t validation accuracy: 0.9311\n",
      "iteration number: 640\t training loss: 0.2372\tvalidation loss: 0.2563\t validation accuracy: 0.9333\n",
      "iteration number: 641\t training loss: 0.2376\tvalidation loss: 0.2563\t validation accuracy: 0.9333\n",
      "iteration number: 642\t training loss: 0.2362\tvalidation loss: 0.2550\t validation accuracy: 0.9400\n",
      "iteration number: 643\t training loss: 0.2368\tvalidation loss: 0.2542\t validation accuracy: 0.9356\n",
      "iteration number: 644\t training loss: 0.2392\tvalidation loss: 0.2568\t validation accuracy: 0.9400\n",
      "iteration number: 645\t training loss: 0.2398\tvalidation loss: 0.2569\t validation accuracy: 0.9378\n",
      "iteration number: 646\t training loss: 0.2338\tvalidation loss: 0.2505\t validation accuracy: 0.9422\n",
      "iteration number: 647\t training loss: 0.2369\tvalidation loss: 0.2539\t validation accuracy: 0.9356\n",
      "iteration number: 648\t training loss: 0.2330\tvalidation loss: 0.2506\t validation accuracy: 0.9400\n",
      "iteration number: 649\t training loss: 0.2342\tvalidation loss: 0.2533\t validation accuracy: 0.9378\n",
      "iteration number: 650\t training loss: 0.2329\tvalidation loss: 0.2527\t validation accuracy: 0.9422\n",
      "iteration number: 651\t training loss: 0.2305\tvalidation loss: 0.2496\t validation accuracy: 0.9444\n",
      "iteration number: 652\t training loss: 0.2299\tvalidation loss: 0.2490\t validation accuracy: 0.9467\n",
      "iteration number: 653\t training loss: 0.2316\tvalidation loss: 0.2503\t validation accuracy: 0.9400\n",
      "iteration number: 654\t training loss: 0.2285\tvalidation loss: 0.2483\t validation accuracy: 0.9444\n",
      "iteration number: 655\t training loss: 0.2279\tvalidation loss: 0.2478\t validation accuracy: 0.9444\n",
      "iteration number: 656\t training loss: 0.2284\tvalidation loss: 0.2484\t validation accuracy: 0.9422\n",
      "iteration number: 657\t training loss: 0.2286\tvalidation loss: 0.2490\t validation accuracy: 0.9467\n",
      "iteration number: 658\t training loss: 0.2269\tvalidation loss: 0.2482\t validation accuracy: 0.9444\n",
      "iteration number: 659\t training loss: 0.2260\tvalidation loss: 0.2462\t validation accuracy: 0.9444\n",
      "iteration number: 660\t training loss: 0.2263\tvalidation loss: 0.2469\t validation accuracy: 0.9444\n",
      "iteration number: 661\t training loss: 0.2293\tvalidation loss: 0.2488\t validation accuracy: 0.9422\n",
      "iteration number: 662\t training loss: 0.2299\tvalidation loss: 0.2483\t validation accuracy: 0.9400\n",
      "iteration number: 663\t training loss: 0.2272\tvalidation loss: 0.2457\t validation accuracy: 0.9378\n",
      "iteration number: 664\t training loss: 0.2260\tvalidation loss: 0.2440\t validation accuracy: 0.9444\n",
      "iteration number: 665\t training loss: 0.2257\tvalidation loss: 0.2443\t validation accuracy: 0.9467\n",
      "iteration number: 666\t training loss: 0.2245\tvalidation loss: 0.2434\t validation accuracy: 0.9444\n",
      "iteration number: 667\t training loss: 0.2230\tvalidation loss: 0.2439\t validation accuracy: 0.9444\n",
      "iteration number: 668\t training loss: 0.2227\tvalidation loss: 0.2444\t validation accuracy: 0.9467\n",
      "iteration number: 669\t training loss: 0.2218\tvalidation loss: 0.2429\t validation accuracy: 0.9467\n",
      "iteration number: 670\t training loss: 0.2213\tvalidation loss: 0.2419\t validation accuracy: 0.9444\n",
      "iteration number: 671\t training loss: 0.2247\tvalidation loss: 0.2457\t validation accuracy: 0.9444\n",
      "iteration number: 672\t training loss: 0.2227\tvalidation loss: 0.2448\t validation accuracy: 0.9400\n",
      "iteration number: 673\t training loss: 0.2202\tvalidation loss: 0.2408\t validation accuracy: 0.9467\n",
      "iteration number: 674\t training loss: 0.2218\tvalidation loss: 0.2420\t validation accuracy: 0.9533\n",
      "iteration number: 675\t training loss: 0.2198\tvalidation loss: 0.2402\t validation accuracy: 0.9467\n",
      "iteration number: 676\t training loss: 0.2210\tvalidation loss: 0.2411\t validation accuracy: 0.9378\n",
      "iteration number: 677\t training loss: 0.2206\tvalidation loss: 0.2411\t validation accuracy: 0.9400\n",
      "iteration number: 678\t training loss: 0.2195\tvalidation loss: 0.2408\t validation accuracy: 0.9400\n",
      "iteration number: 679\t training loss: 0.2194\tvalidation loss: 0.2416\t validation accuracy: 0.9422\n",
      "iteration number: 680\t training loss: 0.2176\tvalidation loss: 0.2395\t validation accuracy: 0.9422\n",
      "iteration number: 681\t training loss: 0.2174\tvalidation loss: 0.2394\t validation accuracy: 0.9422\n",
      "iteration number: 682\t training loss: 0.2167\tvalidation loss: 0.2392\t validation accuracy: 0.9422\n",
      "iteration number: 683\t training loss: 0.2170\tvalidation loss: 0.2416\t validation accuracy: 0.9422\n",
      "iteration number: 684\t training loss: 0.2157\tvalidation loss: 0.2393\t validation accuracy: 0.9400\n",
      "iteration number: 685\t training loss: 0.2173\tvalidation loss: 0.2403\t validation accuracy: 0.9467\n",
      "iteration number: 686\t training loss: 0.2173\tvalidation loss: 0.2402\t validation accuracy: 0.9444\n",
      "iteration number: 687\t training loss: 0.2193\tvalidation loss: 0.2422\t validation accuracy: 0.9422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 688\t training loss: 0.2155\tvalidation loss: 0.2396\t validation accuracy: 0.9511\n",
      "iteration number: 689\t training loss: 0.2141\tvalidation loss: 0.2383\t validation accuracy: 0.9444\n",
      "iteration number: 690\t training loss: 0.2133\tvalidation loss: 0.2378\t validation accuracy: 0.9422\n",
      "iteration number: 691\t training loss: 0.2151\tvalidation loss: 0.2385\t validation accuracy: 0.9444\n",
      "iteration number: 692\t training loss: 0.2130\tvalidation loss: 0.2358\t validation accuracy: 0.9467\n",
      "iteration number: 693\t training loss: 0.2113\tvalidation loss: 0.2343\t validation accuracy: 0.9489\n",
      "iteration number: 694\t training loss: 0.2117\tvalidation loss: 0.2352\t validation accuracy: 0.9444\n",
      "iteration number: 695\t training loss: 0.2129\tvalidation loss: 0.2366\t validation accuracy: 0.9533\n",
      "iteration number: 696\t training loss: 0.2128\tvalidation loss: 0.2367\t validation accuracy: 0.9422\n",
      "iteration number: 697\t training loss: 0.2117\tvalidation loss: 0.2353\t validation accuracy: 0.9489\n",
      "iteration number: 698\t training loss: 0.2104\tvalidation loss: 0.2334\t validation accuracy: 0.9444\n",
      "iteration number: 699\t training loss: 0.2121\tvalidation loss: 0.2352\t validation accuracy: 0.9467\n",
      "iteration number: 700\t training loss: 0.2097\tvalidation loss: 0.2318\t validation accuracy: 0.9444\n",
      "iteration number: 701\t training loss: 0.2092\tvalidation loss: 0.2324\t validation accuracy: 0.9467\n",
      "iteration number: 702\t training loss: 0.2085\tvalidation loss: 0.2321\t validation accuracy: 0.9467\n",
      "iteration number: 703\t training loss: 0.2100\tvalidation loss: 0.2333\t validation accuracy: 0.9467\n",
      "iteration number: 704\t training loss: 0.2084\tvalidation loss: 0.2317\t validation accuracy: 0.9467\n",
      "iteration number: 705\t training loss: 0.2095\tvalidation loss: 0.2340\t validation accuracy: 0.9511\n",
      "iteration number: 706\t training loss: 0.2072\tvalidation loss: 0.2315\t validation accuracy: 0.9467\n",
      "iteration number: 707\t training loss: 0.2076\tvalidation loss: 0.2301\t validation accuracy: 0.9400\n",
      "iteration number: 708\t training loss: 0.2072\tvalidation loss: 0.2298\t validation accuracy: 0.9444\n",
      "iteration number: 709\t training loss: 0.2080\tvalidation loss: 0.2307\t validation accuracy: 0.9489\n",
      "iteration number: 710\t training loss: 0.2071\tvalidation loss: 0.2290\t validation accuracy: 0.9467\n",
      "iteration number: 711\t training loss: 0.2071\tvalidation loss: 0.2296\t validation accuracy: 0.9467\n",
      "iteration number: 712\t training loss: 0.2085\tvalidation loss: 0.2309\t validation accuracy: 0.9467\n",
      "iteration number: 713\t training loss: 0.2072\tvalidation loss: 0.2294\t validation accuracy: 0.9444\n",
      "iteration number: 714\t training loss: 0.2079\tvalidation loss: 0.2305\t validation accuracy: 0.9422\n",
      "iteration number: 715\t training loss: 0.2136\tvalidation loss: 0.2361\t validation accuracy: 0.9422\n",
      "iteration number: 716\t training loss: 0.2122\tvalidation loss: 0.2353\t validation accuracy: 0.9378\n",
      "iteration number: 717\t training loss: 0.2061\tvalidation loss: 0.2291\t validation accuracy: 0.9467\n",
      "iteration number: 718\t training loss: 0.2073\tvalidation loss: 0.2311\t validation accuracy: 0.9444\n",
      "iteration number: 719\t training loss: 0.2052\tvalidation loss: 0.2291\t validation accuracy: 0.9467\n",
      "iteration number: 720\t training loss: 0.2032\tvalidation loss: 0.2273\t validation accuracy: 0.9400\n",
      "iteration number: 721\t training loss: 0.2033\tvalidation loss: 0.2268\t validation accuracy: 0.9422\n",
      "iteration number: 722\t training loss: 0.2027\tvalidation loss: 0.2269\t validation accuracy: 0.9511\n",
      "iteration number: 723\t training loss: 0.2026\tvalidation loss: 0.2269\t validation accuracy: 0.9511\n",
      "iteration number: 724\t training loss: 0.2028\tvalidation loss: 0.2269\t validation accuracy: 0.9489\n",
      "iteration number: 725\t training loss: 0.2037\tvalidation loss: 0.2282\t validation accuracy: 0.9444\n",
      "iteration number: 726\t training loss: 0.2024\tvalidation loss: 0.2256\t validation accuracy: 0.9422\n",
      "iteration number: 727\t training loss: 0.2035\tvalidation loss: 0.2272\t validation accuracy: 0.9511\n",
      "iteration number: 728\t training loss: 0.2032\tvalidation loss: 0.2275\t validation accuracy: 0.9400\n",
      "iteration number: 729\t training loss: 0.2024\tvalidation loss: 0.2272\t validation accuracy: 0.9444\n",
      "iteration number: 730\t training loss: 0.2009\tvalidation loss: 0.2262\t validation accuracy: 0.9444\n",
      "iteration number: 731\t training loss: 0.2013\tvalidation loss: 0.2271\t validation accuracy: 0.9467\n",
      "iteration number: 732\t training loss: 0.2016\tvalidation loss: 0.2270\t validation accuracy: 0.9444\n",
      "iteration number: 733\t training loss: 0.1994\tvalidation loss: 0.2236\t validation accuracy: 0.9444\n",
      "iteration number: 734\t training loss: 0.2016\tvalidation loss: 0.2270\t validation accuracy: 0.9422\n",
      "iteration number: 735\t training loss: 0.1982\tvalidation loss: 0.2236\t validation accuracy: 0.9511\n",
      "iteration number: 736\t training loss: 0.1994\tvalidation loss: 0.2255\t validation accuracy: 0.9511\n",
      "iteration number: 737\t training loss: 0.1972\tvalidation loss: 0.2224\t validation accuracy: 0.9467\n",
      "iteration number: 738\t training loss: 0.2001\tvalidation loss: 0.2250\t validation accuracy: 0.9444\n",
      "iteration number: 739\t training loss: 0.2010\tvalidation loss: 0.2270\t validation accuracy: 0.9489\n",
      "iteration number: 740\t training loss: 0.1994\tvalidation loss: 0.2246\t validation accuracy: 0.9467\n",
      "iteration number: 741\t training loss: 0.1988\tvalidation loss: 0.2244\t validation accuracy: 0.9489\n",
      "iteration number: 742\t training loss: 0.1966\tvalidation loss: 0.2217\t validation accuracy: 0.9489\n",
      "iteration number: 743\t training loss: 0.1959\tvalidation loss: 0.2199\t validation accuracy: 0.9511\n",
      "iteration number: 744\t training loss: 0.1947\tvalidation loss: 0.2190\t validation accuracy: 0.9533\n",
      "iteration number: 745\t training loss: 0.1951\tvalidation loss: 0.2210\t validation accuracy: 0.9533\n",
      "iteration number: 746\t training loss: 0.1935\tvalidation loss: 0.2181\t validation accuracy: 0.9556\n",
      "iteration number: 747\t training loss: 0.1931\tvalidation loss: 0.2179\t validation accuracy: 0.9556\n",
      "iteration number: 748\t training loss: 0.1935\tvalidation loss: 0.2175\t validation accuracy: 0.9489\n",
      "iteration number: 749\t training loss: 0.1937\tvalidation loss: 0.2184\t validation accuracy: 0.9533\n",
      "iteration number: 750\t training loss: 0.1972\tvalidation loss: 0.2212\t validation accuracy: 0.9467\n",
      "iteration number: 751\t training loss: 0.1964\tvalidation loss: 0.2211\t validation accuracy: 0.9578\n",
      "iteration number: 752\t training loss: 0.1978\tvalidation loss: 0.2230\t validation accuracy: 0.9533\n",
      "iteration number: 753\t training loss: 0.1955\tvalidation loss: 0.2215\t validation accuracy: 0.9511\n",
      "iteration number: 754\t training loss: 0.1936\tvalidation loss: 0.2190\t validation accuracy: 0.9511\n",
      "iteration number: 755\t training loss: 0.1925\tvalidation loss: 0.2183\t validation accuracy: 0.9467\n",
      "iteration number: 756\t training loss: 0.1909\tvalidation loss: 0.2170\t validation accuracy: 0.9556\n",
      "iteration number: 757\t training loss: 0.1895\tvalidation loss: 0.2156\t validation accuracy: 0.9533\n",
      "iteration number: 758\t training loss: 0.1898\tvalidation loss: 0.2160\t validation accuracy: 0.9467\n",
      "iteration number: 759\t training loss: 0.1896\tvalidation loss: 0.2177\t validation accuracy: 0.9511\n",
      "iteration number: 760\t training loss: 0.1905\tvalidation loss: 0.2195\t validation accuracy: 0.9533\n",
      "iteration number: 761\t training loss: 0.1911\tvalidation loss: 0.2181\t validation accuracy: 0.9511\n",
      "iteration number: 762\t training loss: 0.1893\tvalidation loss: 0.2158\t validation accuracy: 0.9467\n",
      "iteration number: 763\t training loss: 0.1876\tvalidation loss: 0.2143\t validation accuracy: 0.9556\n",
      "iteration number: 764\t training loss: 0.1880\tvalidation loss: 0.2143\t validation accuracy: 0.9556\n",
      "iteration number: 765\t training loss: 0.1890\tvalidation loss: 0.2159\t validation accuracy: 0.9533\n",
      "iteration number: 766\t training loss: 0.1933\tvalidation loss: 0.2201\t validation accuracy: 0.9489\n",
      "iteration number: 767\t training loss: 0.1894\tvalidation loss: 0.2171\t validation accuracy: 0.9489\n",
      "iteration number: 768\t training loss: 0.1896\tvalidation loss: 0.2180\t validation accuracy: 0.9489\n",
      "iteration number: 769\t training loss: 0.1885\tvalidation loss: 0.2171\t validation accuracy: 0.9533\n",
      "iteration number: 770\t training loss: 0.1872\tvalidation loss: 0.2160\t validation accuracy: 0.9511\n",
      "iteration number: 771\t training loss: 0.1846\tvalidation loss: 0.2141\t validation accuracy: 0.9556\n",
      "iteration number: 772\t training loss: 0.1847\tvalidation loss: 0.2136\t validation accuracy: 0.9578\n",
      "iteration number: 773\t training loss: 0.1841\tvalidation loss: 0.2133\t validation accuracy: 0.9533\n",
      "iteration number: 774\t training loss: 0.1843\tvalidation loss: 0.2142\t validation accuracy: 0.9533\n",
      "iteration number: 775\t training loss: 0.1850\tvalidation loss: 0.2151\t validation accuracy: 0.9511\n",
      "iteration number: 776\t training loss: 0.1857\tvalidation loss: 0.2180\t validation accuracy: 0.9511\n",
      "iteration number: 777\t training loss: 0.1850\tvalidation loss: 0.2176\t validation accuracy: 0.9556\n",
      "iteration number: 778\t training loss: 0.1845\tvalidation loss: 0.2158\t validation accuracy: 0.9556\n",
      "iteration number: 779\t training loss: 0.1839\tvalidation loss: 0.2144\t validation accuracy: 0.9511\n",
      "iteration number: 780\t training loss: 0.1851\tvalidation loss: 0.2154\t validation accuracy: 0.9556\n",
      "iteration number: 781\t training loss: 0.1826\tvalidation loss: 0.2129\t validation accuracy: 0.9533\n",
      "iteration number: 782\t training loss: 0.1817\tvalidation loss: 0.2118\t validation accuracy: 0.9578\n",
      "iteration number: 783\t training loss: 0.1811\tvalidation loss: 0.2115\t validation accuracy: 0.9600\n",
      "iteration number: 784\t training loss: 0.1813\tvalidation loss: 0.2123\t validation accuracy: 0.9556\n",
      "iteration number: 785\t training loss: 0.1813\tvalidation loss: 0.2111\t validation accuracy: 0.9533\n",
      "iteration number: 786\t training loss: 0.1824\tvalidation loss: 0.2111\t validation accuracy: 0.9533\n",
      "iteration number: 787\t training loss: 0.1805\tvalidation loss: 0.2111\t validation accuracy: 0.9533\n",
      "iteration number: 788\t training loss: 0.1798\tvalidation loss: 0.2098\t validation accuracy: 0.9556\n",
      "iteration number: 789\t training loss: 0.1820\tvalidation loss: 0.2115\t validation accuracy: 0.9511\n",
      "iteration number: 790\t training loss: 0.1832\tvalidation loss: 0.2131\t validation accuracy: 0.9467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 791\t training loss: 0.1804\tvalidation loss: 0.2120\t validation accuracy: 0.9489\n",
      "iteration number: 792\t training loss: 0.1800\tvalidation loss: 0.2131\t validation accuracy: 0.9467\n",
      "iteration number: 793\t training loss: 0.1807\tvalidation loss: 0.2138\t validation accuracy: 0.9511\n",
      "iteration number: 794\t training loss: 0.1784\tvalidation loss: 0.2103\t validation accuracy: 0.9556\n",
      "iteration number: 795\t training loss: 0.1818\tvalidation loss: 0.2132\t validation accuracy: 0.9556\n",
      "iteration number: 796\t training loss: 0.1803\tvalidation loss: 0.2121\t validation accuracy: 0.9489\n",
      "iteration number: 797\t training loss: 0.1777\tvalidation loss: 0.2094\t validation accuracy: 0.9533\n",
      "iteration number: 798\t training loss: 0.1773\tvalidation loss: 0.2096\t validation accuracy: 0.9489\n",
      "iteration number: 799\t training loss: 0.1791\tvalidation loss: 0.2112\t validation accuracy: 0.9556\n",
      "iteration number: 800\t training loss: 0.1803\tvalidation loss: 0.2140\t validation accuracy: 0.9489\n",
      "iteration number: 801\t training loss: 0.1776\tvalidation loss: 0.2093\t validation accuracy: 0.9533\n",
      "iteration number: 802\t training loss: 0.1770\tvalidation loss: 0.2094\t validation accuracy: 0.9556\n",
      "iteration number: 803\t training loss: 0.1784\tvalidation loss: 0.2107\t validation accuracy: 0.9556\n",
      "iteration number: 804\t training loss: 0.1775\tvalidation loss: 0.2108\t validation accuracy: 0.9533\n",
      "iteration number: 805\t training loss: 0.1765\tvalidation loss: 0.2095\t validation accuracy: 0.9489\n",
      "iteration number: 806\t training loss: 0.1761\tvalidation loss: 0.2098\t validation accuracy: 0.9578\n",
      "iteration number: 807\t training loss: 0.1751\tvalidation loss: 0.2068\t validation accuracy: 0.9600\n",
      "iteration number: 808\t training loss: 0.1746\tvalidation loss: 0.2051\t validation accuracy: 0.9578\n",
      "iteration number: 809\t training loss: 0.1742\tvalidation loss: 0.2060\t validation accuracy: 0.9578\n",
      "iteration number: 810\t training loss: 0.1758\tvalidation loss: 0.2064\t validation accuracy: 0.9556\n",
      "iteration number: 811\t training loss: 0.1790\tvalidation loss: 0.2107\t validation accuracy: 0.9511\n",
      "iteration number: 812\t training loss: 0.1745\tvalidation loss: 0.2051\t validation accuracy: 0.9600\n",
      "iteration number: 813\t training loss: 0.1740\tvalidation loss: 0.2036\t validation accuracy: 0.9600\n",
      "iteration number: 814\t training loss: 0.1736\tvalidation loss: 0.2046\t validation accuracy: 0.9556\n",
      "iteration number: 815\t training loss: 0.1744\tvalidation loss: 0.2036\t validation accuracy: 0.9578\n",
      "iteration number: 816\t training loss: 0.1765\tvalidation loss: 0.2069\t validation accuracy: 0.9556\n",
      "iteration number: 817\t training loss: 0.1769\tvalidation loss: 0.2080\t validation accuracy: 0.9556\n",
      "iteration number: 818\t training loss: 0.1751\tvalidation loss: 0.2074\t validation accuracy: 0.9533\n",
      "iteration number: 819\t training loss: 0.1786\tvalidation loss: 0.2127\t validation accuracy: 0.9422\n",
      "iteration number: 820\t training loss: 0.1737\tvalidation loss: 0.2056\t validation accuracy: 0.9511\n",
      "iteration number: 821\t training loss: 0.1727\tvalidation loss: 0.2044\t validation accuracy: 0.9511\n",
      "iteration number: 822\t training loss: 0.1745\tvalidation loss: 0.2055\t validation accuracy: 0.9511\n",
      "iteration number: 823\t training loss: 0.1741\tvalidation loss: 0.2047\t validation accuracy: 0.9556\n",
      "iteration number: 824\t training loss: 0.1732\tvalidation loss: 0.2026\t validation accuracy: 0.9556\n",
      "iteration number: 825\t training loss: 0.1720\tvalidation loss: 0.2012\t validation accuracy: 0.9556\n",
      "iteration number: 826\t training loss: 0.1721\tvalidation loss: 0.2023\t validation accuracy: 0.9556\n",
      "iteration number: 827\t training loss: 0.1730\tvalidation loss: 0.2032\t validation accuracy: 0.9533\n",
      "iteration number: 828\t training loss: 0.1706\tvalidation loss: 0.2024\t validation accuracy: 0.9511\n",
      "iteration number: 829\t training loss: 0.1706\tvalidation loss: 0.2031\t validation accuracy: 0.9533\n",
      "iteration number: 830\t training loss: 0.1706\tvalidation loss: 0.2024\t validation accuracy: 0.9533\n",
      "iteration number: 831\t training loss: 0.1718\tvalidation loss: 0.2018\t validation accuracy: 0.9533\n",
      "iteration number: 832\t training loss: 0.1710\tvalidation loss: 0.2026\t validation accuracy: 0.9556\n",
      "iteration number: 833\t training loss: 0.1707\tvalidation loss: 0.2009\t validation accuracy: 0.9533\n",
      "iteration number: 834\t training loss: 0.1706\tvalidation loss: 0.2007\t validation accuracy: 0.9533\n",
      "iteration number: 835\t training loss: 0.1708\tvalidation loss: 0.2005\t validation accuracy: 0.9578\n",
      "iteration number: 836\t training loss: 0.1687\tvalidation loss: 0.1988\t validation accuracy: 0.9578\n",
      "iteration number: 837\t training loss: 0.1702\tvalidation loss: 0.2008\t validation accuracy: 0.9578\n",
      "iteration number: 838\t training loss: 0.1713\tvalidation loss: 0.2023\t validation accuracy: 0.9511\n",
      "iteration number: 839\t training loss: 0.1693\tvalidation loss: 0.2019\t validation accuracy: 0.9578\n",
      "iteration number: 840\t training loss: 0.1691\tvalidation loss: 0.2015\t validation accuracy: 0.9556\n",
      "iteration number: 841\t training loss: 0.1701\tvalidation loss: 0.2018\t validation accuracy: 0.9533\n",
      "iteration number: 842\t training loss: 0.1719\tvalidation loss: 0.2049\t validation accuracy: 0.9489\n",
      "iteration number: 843\t training loss: 0.1677\tvalidation loss: 0.2015\t validation accuracy: 0.9556\n",
      "iteration number: 844\t training loss: 0.1672\tvalidation loss: 0.2006\t validation accuracy: 0.9600\n",
      "iteration number: 845\t training loss: 0.1657\tvalidation loss: 0.2005\t validation accuracy: 0.9556\n",
      "iteration number: 846\t training loss: 0.1658\tvalidation loss: 0.2009\t validation accuracy: 0.9578\n",
      "iteration number: 847\t training loss: 0.1654\tvalidation loss: 0.2000\t validation accuracy: 0.9578\n",
      "iteration number: 848\t training loss: 0.1658\tvalidation loss: 0.2008\t validation accuracy: 0.9600\n",
      "iteration number: 849\t training loss: 0.1693\tvalidation loss: 0.2057\t validation accuracy: 0.9533\n",
      "iteration number: 850\t training loss: 0.1681\tvalidation loss: 0.2045\t validation accuracy: 0.9533\n",
      "iteration number: 851\t training loss: 0.1653\tvalidation loss: 0.2015\t validation accuracy: 0.9556\n",
      "iteration number: 852\t training loss: 0.1642\tvalidation loss: 0.2001\t validation accuracy: 0.9600\n",
      "iteration number: 853\t training loss: 0.1640\tvalidation loss: 0.1991\t validation accuracy: 0.9622\n",
      "iteration number: 854\t training loss: 0.1640\tvalidation loss: 0.1985\t validation accuracy: 0.9600\n",
      "iteration number: 855\t training loss: 0.1637\tvalidation loss: 0.1980\t validation accuracy: 0.9622\n",
      "iteration number: 856\t training loss: 0.1684\tvalidation loss: 0.1994\t validation accuracy: 0.9578\n",
      "iteration number: 857\t training loss: 0.1659\tvalidation loss: 0.1973\t validation accuracy: 0.9556\n",
      "iteration number: 858\t training loss: 0.1652\tvalidation loss: 0.1960\t validation accuracy: 0.9578\n",
      "iteration number: 859\t training loss: 0.1640\tvalidation loss: 0.1954\t validation accuracy: 0.9622\n",
      "iteration number: 860\t training loss: 0.1643\tvalidation loss: 0.1960\t validation accuracy: 0.9600\n",
      "iteration number: 861\t training loss: 0.1636\tvalidation loss: 0.1948\t validation accuracy: 0.9600\n",
      "iteration number: 862\t training loss: 0.1617\tvalidation loss: 0.1939\t validation accuracy: 0.9600\n",
      "iteration number: 863\t training loss: 0.1619\tvalidation loss: 0.1945\t validation accuracy: 0.9600\n",
      "iteration number: 864\t training loss: 0.1649\tvalidation loss: 0.1990\t validation accuracy: 0.9556\n",
      "iteration number: 865\t training loss: 0.1669\tvalidation loss: 0.2038\t validation accuracy: 0.9511\n",
      "iteration number: 866\t training loss: 0.1635\tvalidation loss: 0.1980\t validation accuracy: 0.9578\n",
      "iteration number: 867\t training loss: 0.1614\tvalidation loss: 0.1952\t validation accuracy: 0.9622\n",
      "iteration number: 868\t training loss: 0.1604\tvalidation loss: 0.1944\t validation accuracy: 0.9600\n",
      "iteration number: 869\t training loss: 0.1611\tvalidation loss: 0.1947\t validation accuracy: 0.9600\n",
      "iteration number: 870\t training loss: 0.1600\tvalidation loss: 0.1938\t validation accuracy: 0.9600\n",
      "iteration number: 871\t training loss: 0.1612\tvalidation loss: 0.1968\t validation accuracy: 0.9533\n",
      "iteration number: 872\t training loss: 0.1633\tvalidation loss: 0.1999\t validation accuracy: 0.9556\n",
      "iteration number: 873\t training loss: 0.1596\tvalidation loss: 0.1948\t validation accuracy: 0.9644\n",
      "iteration number: 874\t training loss: 0.1615\tvalidation loss: 0.1968\t validation accuracy: 0.9600\n",
      "iteration number: 875\t training loss: 0.1597\tvalidation loss: 0.1965\t validation accuracy: 0.9644\n",
      "iteration number: 876\t training loss: 0.1586\tvalidation loss: 0.1936\t validation accuracy: 0.9556\n",
      "iteration number: 877\t training loss: 0.1605\tvalidation loss: 0.1970\t validation accuracy: 0.9489\n",
      "iteration number: 878\t training loss: 0.1588\tvalidation loss: 0.1964\t validation accuracy: 0.9600\n",
      "iteration number: 879\t training loss: 0.1588\tvalidation loss: 0.1969\t validation accuracy: 0.9622\n",
      "iteration number: 880\t training loss: 0.1612\tvalidation loss: 0.1992\t validation accuracy: 0.9533\n",
      "iteration number: 881\t training loss: 0.1596\tvalidation loss: 0.1989\t validation accuracy: 0.9533\n",
      "iteration number: 882\t training loss: 0.1578\tvalidation loss: 0.1959\t validation accuracy: 0.9578\n",
      "iteration number: 883\t training loss: 0.1588\tvalidation loss: 0.1961\t validation accuracy: 0.9511\n",
      "iteration number: 884\t training loss: 0.1583\tvalidation loss: 0.1958\t validation accuracy: 0.9556\n",
      "iteration number: 885\t training loss: 0.1579\tvalidation loss: 0.1935\t validation accuracy: 0.9556\n",
      "iteration number: 886\t training loss: 0.1588\tvalidation loss: 0.1936\t validation accuracy: 0.9556\n",
      "iteration number: 887\t training loss: 0.1590\tvalidation loss: 0.1945\t validation accuracy: 0.9556\n",
      "iteration number: 888\t training loss: 0.1579\tvalidation loss: 0.1929\t validation accuracy: 0.9556\n",
      "iteration number: 889\t training loss: 0.1584\tvalidation loss: 0.1941\t validation accuracy: 0.9556\n",
      "iteration number: 890\t training loss: 0.1566\tvalidation loss: 0.1933\t validation accuracy: 0.9578\n",
      "iteration number: 891\t training loss: 0.1570\tvalidation loss: 0.1934\t validation accuracy: 0.9578\n",
      "iteration number: 892\t training loss: 0.1556\tvalidation loss: 0.1927\t validation accuracy: 0.9600\n",
      "iteration number: 893\t training loss: 0.1551\tvalidation loss: 0.1918\t validation accuracy: 0.9622\n",
      "iteration number: 894\t training loss: 0.1556\tvalidation loss: 0.1923\t validation accuracy: 0.9600\n",
      "iteration number: 895\t training loss: 0.1569\tvalidation loss: 0.1933\t validation accuracy: 0.9578\n",
      "iteration number: 896\t training loss: 0.1568\tvalidation loss: 0.1928\t validation accuracy: 0.9600\n",
      "iteration number: 897\t training loss: 0.1565\tvalidation loss: 0.1933\t validation accuracy: 0.9556\n",
      "iteration number: 898\t training loss: 0.1572\tvalidation loss: 0.1936\t validation accuracy: 0.9533\n",
      "iteration number: 899\t training loss: 0.1566\tvalidation loss: 0.1927\t validation accuracy: 0.9556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 900\t training loss: 0.1551\tvalidation loss: 0.1919\t validation accuracy: 0.9556\n",
      "iteration number: 901\t training loss: 0.1554\tvalidation loss: 0.1914\t validation accuracy: 0.9578\n",
      "iteration number: 902\t training loss: 0.1559\tvalidation loss: 0.1927\t validation accuracy: 0.9622\n",
      "iteration number: 903\t training loss: 0.1538\tvalidation loss: 0.1890\t validation accuracy: 0.9600\n",
      "iteration number: 904\t training loss: 0.1528\tvalidation loss: 0.1890\t validation accuracy: 0.9600\n",
      "iteration number: 905\t training loss: 0.1531\tvalidation loss: 0.1889\t validation accuracy: 0.9578\n",
      "iteration number: 906\t training loss: 0.1530\tvalidation loss: 0.1890\t validation accuracy: 0.9578\n",
      "iteration number: 907\t training loss: 0.1536\tvalidation loss: 0.1901\t validation accuracy: 0.9556\n",
      "iteration number: 908\t training loss: 0.1532\tvalidation loss: 0.1899\t validation accuracy: 0.9578\n",
      "iteration number: 909\t training loss: 0.1530\tvalidation loss: 0.1907\t validation accuracy: 0.9622\n",
      "iteration number: 910\t training loss: 0.1521\tvalidation loss: 0.1902\t validation accuracy: 0.9644\n",
      "iteration number: 911\t training loss: 0.1521\tvalidation loss: 0.1891\t validation accuracy: 0.9622\n",
      "iteration number: 912\t training loss: 0.1541\tvalidation loss: 0.1921\t validation accuracy: 0.9556\n",
      "iteration number: 913\t training loss: 0.1555\tvalidation loss: 0.1934\t validation accuracy: 0.9556\n",
      "iteration number: 914\t training loss: 0.1574\tvalidation loss: 0.1971\t validation accuracy: 0.9467\n",
      "iteration number: 915\t training loss: 0.1539\tvalidation loss: 0.1930\t validation accuracy: 0.9533\n",
      "iteration number: 916\t training loss: 0.1549\tvalidation loss: 0.1944\t validation accuracy: 0.9511\n",
      "iteration number: 917\t training loss: 0.1532\tvalidation loss: 0.1918\t validation accuracy: 0.9511\n",
      "iteration number: 918\t training loss: 0.1509\tvalidation loss: 0.1887\t validation accuracy: 0.9600\n",
      "iteration number: 919\t training loss: 0.1519\tvalidation loss: 0.1915\t validation accuracy: 0.9556\n",
      "iteration number: 920\t training loss: 0.1512\tvalidation loss: 0.1922\t validation accuracy: 0.9556\n",
      "iteration number: 921\t training loss: 0.1506\tvalidation loss: 0.1905\t validation accuracy: 0.9578\n",
      "iteration number: 922\t training loss: 0.1527\tvalidation loss: 0.1925\t validation accuracy: 0.9600\n",
      "iteration number: 923\t training loss: 0.1510\tvalidation loss: 0.1905\t validation accuracy: 0.9622\n",
      "iteration number: 924\t training loss: 0.1502\tvalidation loss: 0.1890\t validation accuracy: 0.9622\n",
      "iteration number: 925\t training loss: 0.1488\tvalidation loss: 0.1867\t validation accuracy: 0.9644\n",
      "iteration number: 926\t training loss: 0.1492\tvalidation loss: 0.1876\t validation accuracy: 0.9644\n",
      "iteration number: 927\t training loss: 0.1508\tvalidation loss: 0.1886\t validation accuracy: 0.9600\n",
      "iteration number: 928\t training loss: 0.1514\tvalidation loss: 0.1889\t validation accuracy: 0.9578\n",
      "iteration number: 929\t training loss: 0.1504\tvalidation loss: 0.1897\t validation accuracy: 0.9578\n",
      "iteration number: 930\t training loss: 0.1492\tvalidation loss: 0.1874\t validation accuracy: 0.9622\n",
      "iteration number: 931\t training loss: 0.1523\tvalidation loss: 0.1908\t validation accuracy: 0.9600\n",
      "iteration number: 932\t training loss: 0.1504\tvalidation loss: 0.1896\t validation accuracy: 0.9578\n",
      "iteration number: 933\t training loss: 0.1511\tvalidation loss: 0.1918\t validation accuracy: 0.9578\n",
      "iteration number: 934\t training loss: 0.1504\tvalidation loss: 0.1895\t validation accuracy: 0.9644\n",
      "iteration number: 935\t training loss: 0.1497\tvalidation loss: 0.1894\t validation accuracy: 0.9600\n",
      "iteration number: 936\t training loss: 0.1497\tvalidation loss: 0.1869\t validation accuracy: 0.9556\n",
      "iteration number: 937\t training loss: 0.1489\tvalidation loss: 0.1866\t validation accuracy: 0.9578\n",
      "iteration number: 938\t training loss: 0.1491\tvalidation loss: 0.1873\t validation accuracy: 0.9622\n",
      "iteration number: 939\t training loss: 0.1492\tvalidation loss: 0.1867\t validation accuracy: 0.9622\n",
      "iteration number: 940\t training loss: 0.1474\tvalidation loss: 0.1856\t validation accuracy: 0.9622\n",
      "iteration number: 941\t training loss: 0.1474\tvalidation loss: 0.1859\t validation accuracy: 0.9622\n",
      "iteration number: 942\t training loss: 0.1476\tvalidation loss: 0.1852\t validation accuracy: 0.9644\n",
      "iteration number: 943\t training loss: 0.1459\tvalidation loss: 0.1856\t validation accuracy: 0.9600\n",
      "iteration number: 944\t training loss: 0.1456\tvalidation loss: 0.1831\t validation accuracy: 0.9600\n",
      "iteration number: 945\t training loss: 0.1455\tvalidation loss: 0.1835\t validation accuracy: 0.9622\n",
      "iteration number: 946\t training loss: 0.1460\tvalidation loss: 0.1849\t validation accuracy: 0.9644\n",
      "iteration number: 947\t training loss: 0.1473\tvalidation loss: 0.1867\t validation accuracy: 0.9578\n",
      "iteration number: 948\t training loss: 0.1467\tvalidation loss: 0.1884\t validation accuracy: 0.9533\n",
      "iteration number: 949\t training loss: 0.1472\tvalidation loss: 0.1883\t validation accuracy: 0.9600\n",
      "iteration number: 950\t training loss: 0.1455\tvalidation loss: 0.1862\t validation accuracy: 0.9622\n",
      "iteration number: 951\t training loss: 0.1447\tvalidation loss: 0.1858\t validation accuracy: 0.9622\n",
      "iteration number: 952\t training loss: 0.1445\tvalidation loss: 0.1842\t validation accuracy: 0.9622\n",
      "iteration number: 953\t training loss: 0.1448\tvalidation loss: 0.1865\t validation accuracy: 0.9578\n",
      "iteration number: 954\t training loss: 0.1444\tvalidation loss: 0.1855\t validation accuracy: 0.9578\n",
      "iteration number: 955\t training loss: 0.1447\tvalidation loss: 0.1857\t validation accuracy: 0.9578\n",
      "iteration number: 956\t training loss: 0.1452\tvalidation loss: 0.1849\t validation accuracy: 0.9644\n",
      "iteration number: 957\t training loss: 0.1465\tvalidation loss: 0.1846\t validation accuracy: 0.9622\n",
      "iteration number: 958\t training loss: 0.1466\tvalidation loss: 0.1832\t validation accuracy: 0.9622\n",
      "iteration number: 959\t training loss: 0.1438\tvalidation loss: 0.1803\t validation accuracy: 0.9600\n",
      "iteration number: 960\t training loss: 0.1444\tvalidation loss: 0.1800\t validation accuracy: 0.9622\n",
      "iteration number: 961\t training loss: 0.1455\tvalidation loss: 0.1803\t validation accuracy: 0.9644\n",
      "iteration number: 962\t training loss: 0.1430\tvalidation loss: 0.1799\t validation accuracy: 0.9600\n",
      "iteration number: 963\t training loss: 0.1437\tvalidation loss: 0.1806\t validation accuracy: 0.9689\n",
      "iteration number: 964\t training loss: 0.1442\tvalidation loss: 0.1813\t validation accuracy: 0.9667\n",
      "iteration number: 965\t training loss: 0.1428\tvalidation loss: 0.1807\t validation accuracy: 0.9644\n",
      "iteration number: 966\t training loss: 0.1430\tvalidation loss: 0.1792\t validation accuracy: 0.9644\n",
      "iteration number: 967\t training loss: 0.1426\tvalidation loss: 0.1797\t validation accuracy: 0.9622\n",
      "iteration number: 968\t training loss: 0.1436\tvalidation loss: 0.1805\t validation accuracy: 0.9667\n",
      "iteration number: 969\t training loss: 0.1426\tvalidation loss: 0.1816\t validation accuracy: 0.9644\n",
      "iteration number: 970\t training loss: 0.1428\tvalidation loss: 0.1839\t validation accuracy: 0.9644\n",
      "iteration number: 971\t training loss: 0.1417\tvalidation loss: 0.1810\t validation accuracy: 0.9689\n",
      "iteration number: 972\t training loss: 0.1426\tvalidation loss: 0.1822\t validation accuracy: 0.9600\n",
      "iteration number: 973\t training loss: 0.1425\tvalidation loss: 0.1805\t validation accuracy: 0.9644\n",
      "iteration number: 974\t training loss: 0.1411\tvalidation loss: 0.1804\t validation accuracy: 0.9644\n",
      "iteration number: 975\t training loss: 0.1402\tvalidation loss: 0.1807\t validation accuracy: 0.9644\n",
      "iteration number: 976\t training loss: 0.1400\tvalidation loss: 0.1796\t validation accuracy: 0.9600\n",
      "iteration number: 977\t training loss: 0.1404\tvalidation loss: 0.1801\t validation accuracy: 0.9622\n",
      "iteration number: 978\t training loss: 0.1394\tvalidation loss: 0.1773\t validation accuracy: 0.9644\n",
      "iteration number: 979\t training loss: 0.1395\tvalidation loss: 0.1779\t validation accuracy: 0.9600\n",
      "iteration number: 980\t training loss: 0.1390\tvalidation loss: 0.1777\t validation accuracy: 0.9667\n",
      "iteration number: 981\t training loss: 0.1390\tvalidation loss: 0.1785\t validation accuracy: 0.9644\n",
      "iteration number: 982\t training loss: 0.1405\tvalidation loss: 0.1798\t validation accuracy: 0.9600\n",
      "iteration number: 983\t training loss: 0.1391\tvalidation loss: 0.1781\t validation accuracy: 0.9644\n",
      "iteration number: 984\t training loss: 0.1390\tvalidation loss: 0.1785\t validation accuracy: 0.9622\n",
      "iteration number: 985\t training loss: 0.1389\tvalidation loss: 0.1776\t validation accuracy: 0.9644\n",
      "iteration number: 986\t training loss: 0.1383\tvalidation loss: 0.1781\t validation accuracy: 0.9667\n",
      "iteration number: 987\t training loss: 0.1385\tvalidation loss: 0.1768\t validation accuracy: 0.9622\n",
      "iteration number: 988\t training loss: 0.1381\tvalidation loss: 0.1779\t validation accuracy: 0.9622\n",
      "iteration number: 989\t training loss: 0.1387\tvalidation loss: 0.1798\t validation accuracy: 0.9622\n",
      "iteration number: 990\t training loss: 0.1387\tvalidation loss: 0.1802\t validation accuracy: 0.9600\n",
      "iteration number: 991\t training loss: 0.1386\tvalidation loss: 0.1789\t validation accuracy: 0.9644\n",
      "iteration number: 992\t training loss: 0.1375\tvalidation loss: 0.1783\t validation accuracy: 0.9689\n",
      "iteration number: 993\t training loss: 0.1385\tvalidation loss: 0.1802\t validation accuracy: 0.9600\n",
      "iteration number: 994\t training loss: 0.1382\tvalidation loss: 0.1806\t validation accuracy: 0.9622\n",
      "iteration number: 995\t training loss: 0.1369\tvalidation loss: 0.1776\t validation accuracy: 0.9622\n",
      "iteration number: 996\t training loss: 0.1376\tvalidation loss: 0.1789\t validation accuracy: 0.9644\n",
      "iteration number: 997\t training loss: 0.1378\tvalidation loss: 0.1790\t validation accuracy: 0.9600\n",
      "iteration number: 998\t training loss: 0.1383\tvalidation loss: 0.1805\t validation accuracy: 0.9600\n",
      "iteration number: 999\t training loss: 0.1374\tvalidation loss: 0.1801\t validation accuracy: 0.9644\n",
      "iteration number: 1000\t training loss: 0.1375\tvalidation loss: 0.1810\t validation accuracy: 0.9556\n",
      "iteration number: 1001\t training loss: 0.1394\tvalidation loss: 0.1828\t validation accuracy: 0.9556\n",
      "iteration number: 1002\t training loss: 0.1383\tvalidation loss: 0.1796\t validation accuracy: 0.9622\n",
      "iteration number: 1003\t training loss: 0.1374\tvalidation loss: 0.1781\t validation accuracy: 0.9600\n",
      "iteration number: 1004\t training loss: 0.1365\tvalidation loss: 0.1788\t validation accuracy: 0.9600\n",
      "iteration number: 1005\t training loss: 0.1365\tvalidation loss: 0.1786\t validation accuracy: 0.9622\n",
      "iteration number: 1006\t training loss: 0.1360\tvalidation loss: 0.1777\t validation accuracy: 0.9622\n",
      "iteration number: 1007\t training loss: 0.1357\tvalidation loss: 0.1753\t validation accuracy: 0.9644\n",
      "iteration number: 1008\t training loss: 0.1354\tvalidation loss: 0.1751\t validation accuracy: 0.9644\n",
      "iteration number: 1009\t training loss: 0.1357\tvalidation loss: 0.1744\t validation accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1010\t training loss: 0.1355\tvalidation loss: 0.1741\t validation accuracy: 0.9644\n",
      "iteration number: 1011\t training loss: 0.1352\tvalidation loss: 0.1743\t validation accuracy: 0.9644\n",
      "iteration number: 1012\t training loss: 0.1347\tvalidation loss: 0.1743\t validation accuracy: 0.9622\n",
      "iteration number: 1013\t training loss: 0.1360\tvalidation loss: 0.1750\t validation accuracy: 0.9667\n",
      "iteration number: 1014\t training loss: 0.1353\tvalidation loss: 0.1737\t validation accuracy: 0.9644\n",
      "iteration number: 1015\t training loss: 0.1350\tvalidation loss: 0.1727\t validation accuracy: 0.9667\n",
      "iteration number: 1016\t training loss: 0.1355\tvalidation loss: 0.1742\t validation accuracy: 0.9622\n",
      "iteration number: 1017\t training loss: 0.1366\tvalidation loss: 0.1753\t validation accuracy: 0.9600\n",
      "iteration number: 1018\t training loss: 0.1378\tvalidation loss: 0.1764\t validation accuracy: 0.9622\n",
      "iteration number: 1019\t training loss: 0.1359\tvalidation loss: 0.1757\t validation accuracy: 0.9644\n",
      "iteration number: 1020\t training loss: 0.1360\tvalidation loss: 0.1749\t validation accuracy: 0.9644\n",
      "iteration number: 1021\t training loss: 0.1360\tvalidation loss: 0.1748\t validation accuracy: 0.9644\n",
      "iteration number: 1022\t training loss: 0.1381\tvalidation loss: 0.1776\t validation accuracy: 0.9600\n",
      "iteration number: 1023\t training loss: 0.1381\tvalidation loss: 0.1782\t validation accuracy: 0.9600\n",
      "iteration number: 1024\t training loss: 0.1395\tvalidation loss: 0.1794\t validation accuracy: 0.9600\n",
      "iteration number: 1025\t training loss: 0.1365\tvalidation loss: 0.1768\t validation accuracy: 0.9622\n",
      "iteration number: 1026\t training loss: 0.1343\tvalidation loss: 0.1762\t validation accuracy: 0.9644\n",
      "iteration number: 1027\t training loss: 0.1339\tvalidation loss: 0.1766\t validation accuracy: 0.9667\n",
      "iteration number: 1028\t training loss: 0.1353\tvalidation loss: 0.1794\t validation accuracy: 0.9622\n",
      "iteration number: 1029\t training loss: 0.1342\tvalidation loss: 0.1755\t validation accuracy: 0.9644\n",
      "iteration number: 1030\t training loss: 0.1338\tvalidation loss: 0.1746\t validation accuracy: 0.9644\n",
      "iteration number: 1031\t training loss: 0.1334\tvalidation loss: 0.1747\t validation accuracy: 0.9644\n",
      "iteration number: 1032\t training loss: 0.1332\tvalidation loss: 0.1759\t validation accuracy: 0.9644\n",
      "iteration number: 1033\t training loss: 0.1322\tvalidation loss: 0.1737\t validation accuracy: 0.9667\n",
      "iteration number: 1034\t training loss: 0.1328\tvalidation loss: 0.1741\t validation accuracy: 0.9644\n",
      "iteration number: 1035\t training loss: 0.1330\tvalidation loss: 0.1746\t validation accuracy: 0.9600\n",
      "iteration number: 1036\t training loss: 0.1338\tvalidation loss: 0.1753\t validation accuracy: 0.9600\n",
      "iteration number: 1037\t training loss: 0.1322\tvalidation loss: 0.1731\t validation accuracy: 0.9600\n",
      "iteration number: 1038\t training loss: 0.1334\tvalidation loss: 0.1747\t validation accuracy: 0.9622\n",
      "iteration number: 1039\t training loss: 0.1334\tvalidation loss: 0.1728\t validation accuracy: 0.9600\n",
      "iteration number: 1040\t training loss: 0.1327\tvalidation loss: 0.1727\t validation accuracy: 0.9578\n",
      "iteration number: 1041\t training loss: 0.1319\tvalidation loss: 0.1718\t validation accuracy: 0.9600\n",
      "iteration number: 1042\t training loss: 0.1316\tvalidation loss: 0.1717\t validation accuracy: 0.9644\n",
      "iteration number: 1043\t training loss: 0.1310\tvalidation loss: 0.1728\t validation accuracy: 0.9667\n",
      "iteration number: 1044\t training loss: 0.1319\tvalidation loss: 0.1718\t validation accuracy: 0.9689\n",
      "iteration number: 1045\t training loss: 0.1314\tvalidation loss: 0.1711\t validation accuracy: 0.9667\n",
      "iteration number: 1046\t training loss: 0.1327\tvalidation loss: 0.1774\t validation accuracy: 0.9578\n",
      "iteration number: 1047\t training loss: 0.1309\tvalidation loss: 0.1724\t validation accuracy: 0.9689\n",
      "iteration number: 1048\t training loss: 0.1311\tvalidation loss: 0.1711\t validation accuracy: 0.9622\n",
      "iteration number: 1049\t training loss: 0.1302\tvalidation loss: 0.1724\t validation accuracy: 0.9622\n",
      "iteration number: 1050\t training loss: 0.1304\tvalidation loss: 0.1714\t validation accuracy: 0.9644\n",
      "iteration number: 1051\t training loss: 0.1302\tvalidation loss: 0.1731\t validation accuracy: 0.9622\n",
      "iteration number: 1052\t training loss: 0.1291\tvalidation loss: 0.1718\t validation accuracy: 0.9689\n",
      "iteration number: 1053\t training loss: 0.1285\tvalidation loss: 0.1712\t validation accuracy: 0.9644\n",
      "iteration number: 1054\t training loss: 0.1289\tvalidation loss: 0.1725\t validation accuracy: 0.9644\n",
      "iteration number: 1055\t training loss: 0.1295\tvalidation loss: 0.1740\t validation accuracy: 0.9622\n",
      "iteration number: 1056\t training loss: 0.1286\tvalidation loss: 0.1733\t validation accuracy: 0.9644\n",
      "iteration number: 1057\t training loss: 0.1281\tvalidation loss: 0.1715\t validation accuracy: 0.9644\n",
      "iteration number: 1058\t training loss: 0.1291\tvalidation loss: 0.1714\t validation accuracy: 0.9667\n",
      "iteration number: 1059\t training loss: 0.1291\tvalidation loss: 0.1703\t validation accuracy: 0.9644\n",
      "iteration number: 1060\t training loss: 0.1282\tvalidation loss: 0.1703\t validation accuracy: 0.9622\n",
      "iteration number: 1061\t training loss: 0.1285\tvalidation loss: 0.1693\t validation accuracy: 0.9600\n",
      "iteration number: 1062\t training loss: 0.1279\tvalidation loss: 0.1695\t validation accuracy: 0.9644\n",
      "iteration number: 1063\t training loss: 0.1286\tvalidation loss: 0.1726\t validation accuracy: 0.9667\n",
      "iteration number: 1064\t training loss: 0.1279\tvalidation loss: 0.1712\t validation accuracy: 0.9667\n",
      "iteration number: 1065\t training loss: 0.1300\tvalidation loss: 0.1755\t validation accuracy: 0.9556\n",
      "iteration number: 1066\t training loss: 0.1277\tvalidation loss: 0.1736\t validation accuracy: 0.9622\n",
      "iteration number: 1067\t training loss: 0.1285\tvalidation loss: 0.1750\t validation accuracy: 0.9600\n",
      "iteration number: 1068\t training loss: 0.1272\tvalidation loss: 0.1716\t validation accuracy: 0.9600\n",
      "iteration number: 1069\t training loss: 0.1269\tvalidation loss: 0.1714\t validation accuracy: 0.9667\n",
      "iteration number: 1070\t training loss: 0.1272\tvalidation loss: 0.1711\t validation accuracy: 0.9644\n",
      "iteration number: 1071\t training loss: 0.1267\tvalidation loss: 0.1716\t validation accuracy: 0.9622\n",
      "iteration number: 1072\t training loss: 0.1268\tvalidation loss: 0.1704\t validation accuracy: 0.9600\n",
      "iteration number: 1073\t training loss: 0.1272\tvalidation loss: 0.1702\t validation accuracy: 0.9600\n",
      "iteration number: 1074\t training loss: 0.1274\tvalidation loss: 0.1705\t validation accuracy: 0.9600\n",
      "iteration number: 1075\t training loss: 0.1289\tvalidation loss: 0.1711\t validation accuracy: 0.9600\n",
      "iteration number: 1076\t training loss: 0.1270\tvalidation loss: 0.1703\t validation accuracy: 0.9622\n",
      "iteration number: 1077\t training loss: 0.1282\tvalidation loss: 0.1714\t validation accuracy: 0.9578\n",
      "iteration number: 1078\t training loss: 0.1279\tvalidation loss: 0.1715\t validation accuracy: 0.9600\n",
      "iteration number: 1079\t training loss: 0.1295\tvalidation loss: 0.1718\t validation accuracy: 0.9622\n",
      "iteration number: 1080\t training loss: 0.1284\tvalidation loss: 0.1717\t validation accuracy: 0.9600\n",
      "iteration number: 1081\t training loss: 0.1296\tvalidation loss: 0.1735\t validation accuracy: 0.9578\n",
      "iteration number: 1082\t training loss: 0.1270\tvalidation loss: 0.1704\t validation accuracy: 0.9644\n",
      "iteration number: 1083\t training loss: 0.1266\tvalidation loss: 0.1710\t validation accuracy: 0.9644\n",
      "iteration number: 1084\t training loss: 0.1265\tvalidation loss: 0.1721\t validation accuracy: 0.9689\n",
      "iteration number: 1085\t training loss: 0.1264\tvalidation loss: 0.1692\t validation accuracy: 0.9689\n",
      "iteration number: 1086\t training loss: 0.1252\tvalidation loss: 0.1675\t validation accuracy: 0.9689\n",
      "iteration number: 1087\t training loss: 0.1255\tvalidation loss: 0.1684\t validation accuracy: 0.9600\n",
      "iteration number: 1088\t training loss: 0.1262\tvalidation loss: 0.1699\t validation accuracy: 0.9600\n",
      "iteration number: 1089\t training loss: 0.1278\tvalidation loss: 0.1706\t validation accuracy: 0.9578\n",
      "iteration number: 1090\t training loss: 0.1243\tvalidation loss: 0.1681\t validation accuracy: 0.9644\n",
      "iteration number: 1091\t training loss: 0.1258\tvalidation loss: 0.1723\t validation accuracy: 0.9600\n",
      "iteration number: 1092\t training loss: 0.1269\tvalidation loss: 0.1737\t validation accuracy: 0.9556\n",
      "iteration number: 1093\t training loss: 0.1264\tvalidation loss: 0.1749\t validation accuracy: 0.9533\n",
      "iteration number: 1094\t training loss: 0.1246\tvalidation loss: 0.1701\t validation accuracy: 0.9644\n",
      "iteration number: 1095\t training loss: 0.1239\tvalidation loss: 0.1703\t validation accuracy: 0.9667\n",
      "iteration number: 1096\t training loss: 0.1249\tvalidation loss: 0.1706\t validation accuracy: 0.9644\n",
      "iteration number: 1097\t training loss: 0.1246\tvalidation loss: 0.1698\t validation accuracy: 0.9622\n",
      "iteration number: 1098\t training loss: 0.1249\tvalidation loss: 0.1685\t validation accuracy: 0.9622\n",
      "iteration number: 1099\t training loss: 0.1240\tvalidation loss: 0.1685\t validation accuracy: 0.9644\n",
      "iteration number: 1100\t training loss: 0.1237\tvalidation loss: 0.1684\t validation accuracy: 0.9600\n",
      "iteration number: 1101\t training loss: 0.1228\tvalidation loss: 0.1681\t validation accuracy: 0.9644\n",
      "iteration number: 1102\t training loss: 0.1224\tvalidation loss: 0.1684\t validation accuracy: 0.9667\n",
      "iteration number: 1103\t training loss: 0.1232\tvalidation loss: 0.1680\t validation accuracy: 0.9644\n",
      "iteration number: 1104\t training loss: 0.1226\tvalidation loss: 0.1680\t validation accuracy: 0.9667\n",
      "iteration number: 1105\t training loss: 0.1245\tvalidation loss: 0.1721\t validation accuracy: 0.9622\n",
      "iteration number: 1106\t training loss: 0.1230\tvalidation loss: 0.1688\t validation accuracy: 0.9667\n",
      "iteration number: 1107\t training loss: 0.1229\tvalidation loss: 0.1682\t validation accuracy: 0.9667\n",
      "iteration number: 1108\t training loss: 0.1222\tvalidation loss: 0.1670\t validation accuracy: 0.9667\n",
      "iteration number: 1109\t training loss: 0.1220\tvalidation loss: 0.1661\t validation accuracy: 0.9622\n",
      "iteration number: 1110\t training loss: 0.1218\tvalidation loss: 0.1671\t validation accuracy: 0.9600\n",
      "iteration number: 1111\t training loss: 0.1224\tvalidation loss: 0.1671\t validation accuracy: 0.9600\n",
      "iteration number: 1112\t training loss: 0.1220\tvalidation loss: 0.1659\t validation accuracy: 0.9600\n",
      "iteration number: 1113\t training loss: 0.1244\tvalidation loss: 0.1650\t validation accuracy: 0.9644\n",
      "iteration number: 1114\t training loss: 0.1237\tvalidation loss: 0.1644\t validation accuracy: 0.9667\n",
      "iteration number: 1115\t training loss: 0.1233\tvalidation loss: 0.1642\t validation accuracy: 0.9667\n",
      "iteration number: 1116\t training loss: 0.1234\tvalidation loss: 0.1635\t validation accuracy: 0.9667\n",
      "iteration number: 1117\t training loss: 0.1222\tvalidation loss: 0.1648\t validation accuracy: 0.9711\n",
      "iteration number: 1118\t training loss: 0.1220\tvalidation loss: 0.1665\t validation accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1119\t training loss: 0.1209\tvalidation loss: 0.1653\t validation accuracy: 0.9689\n",
      "iteration number: 1120\t training loss: 0.1206\tvalidation loss: 0.1656\t validation accuracy: 0.9667\n",
      "iteration number: 1121\t training loss: 0.1212\tvalidation loss: 0.1666\t validation accuracy: 0.9644\n",
      "iteration number: 1122\t training loss: 0.1204\tvalidation loss: 0.1640\t validation accuracy: 0.9644\n",
      "iteration number: 1123\t training loss: 0.1200\tvalidation loss: 0.1648\t validation accuracy: 0.9667\n",
      "iteration number: 1124\t training loss: 0.1198\tvalidation loss: 0.1648\t validation accuracy: 0.9667\n",
      "iteration number: 1125\t training loss: 0.1198\tvalidation loss: 0.1644\t validation accuracy: 0.9644\n",
      "iteration number: 1126\t training loss: 0.1198\tvalidation loss: 0.1654\t validation accuracy: 0.9644\n",
      "iteration number: 1127\t training loss: 0.1195\tvalidation loss: 0.1642\t validation accuracy: 0.9667\n",
      "iteration number: 1128\t training loss: 0.1201\tvalidation loss: 0.1647\t validation accuracy: 0.9644\n",
      "iteration number: 1129\t training loss: 0.1204\tvalidation loss: 0.1634\t validation accuracy: 0.9600\n",
      "iteration number: 1130\t training loss: 0.1205\tvalidation loss: 0.1636\t validation accuracy: 0.9622\n",
      "iteration number: 1131\t training loss: 0.1211\tvalidation loss: 0.1644\t validation accuracy: 0.9644\n",
      "iteration number: 1132\t training loss: 0.1210\tvalidation loss: 0.1647\t validation accuracy: 0.9644\n",
      "iteration number: 1133\t training loss: 0.1211\tvalidation loss: 0.1655\t validation accuracy: 0.9622\n",
      "iteration number: 1134\t training loss: 0.1199\tvalidation loss: 0.1661\t validation accuracy: 0.9644\n",
      "iteration number: 1135\t training loss: 0.1192\tvalidation loss: 0.1649\t validation accuracy: 0.9667\n",
      "iteration number: 1136\t training loss: 0.1192\tvalidation loss: 0.1658\t validation accuracy: 0.9644\n",
      "iteration number: 1137\t training loss: 0.1198\tvalidation loss: 0.1662\t validation accuracy: 0.9600\n",
      "iteration number: 1138\t training loss: 0.1203\tvalidation loss: 0.1667\t validation accuracy: 0.9644\n",
      "iteration number: 1139\t training loss: 0.1196\tvalidation loss: 0.1681\t validation accuracy: 0.9622\n",
      "iteration number: 1140\t training loss: 0.1181\tvalidation loss: 0.1657\t validation accuracy: 0.9689\n",
      "iteration number: 1141\t training loss: 0.1208\tvalidation loss: 0.1666\t validation accuracy: 0.9667\n",
      "iteration number: 1142\t training loss: 0.1222\tvalidation loss: 0.1670\t validation accuracy: 0.9667\n",
      "iteration number: 1143\t training loss: 0.1214\tvalidation loss: 0.1653\t validation accuracy: 0.9667\n",
      "iteration number: 1144\t training loss: 0.1199\tvalidation loss: 0.1638\t validation accuracy: 0.9667\n",
      "iteration number: 1145\t training loss: 0.1199\tvalidation loss: 0.1640\t validation accuracy: 0.9667\n",
      "iteration number: 1146\t training loss: 0.1182\tvalidation loss: 0.1637\t validation accuracy: 0.9689\n",
      "iteration number: 1147\t training loss: 0.1195\tvalidation loss: 0.1674\t validation accuracy: 0.9622\n",
      "iteration number: 1148\t training loss: 0.1188\tvalidation loss: 0.1676\t validation accuracy: 0.9622\n",
      "iteration number: 1149\t training loss: 0.1215\tvalidation loss: 0.1726\t validation accuracy: 0.9556\n",
      "iteration number: 1150\t training loss: 0.1172\tvalidation loss: 0.1645\t validation accuracy: 0.9667\n",
      "iteration number: 1151\t training loss: 0.1178\tvalidation loss: 0.1633\t validation accuracy: 0.9644\n",
      "iteration number: 1152\t training loss: 0.1170\tvalidation loss: 0.1618\t validation accuracy: 0.9644\n",
      "iteration number: 1153\t training loss: 0.1168\tvalidation loss: 0.1626\t validation accuracy: 0.9667\n",
      "iteration number: 1154\t training loss: 0.1161\tvalidation loss: 0.1613\t validation accuracy: 0.9689\n",
      "iteration number: 1155\t training loss: 0.1166\tvalidation loss: 0.1605\t validation accuracy: 0.9644\n",
      "iteration number: 1156\t training loss: 0.1172\tvalidation loss: 0.1613\t validation accuracy: 0.9667\n",
      "iteration number: 1157\t training loss: 0.1157\tvalidation loss: 0.1627\t validation accuracy: 0.9667\n",
      "iteration number: 1158\t training loss: 0.1156\tvalidation loss: 0.1628\t validation accuracy: 0.9667\n",
      "iteration number: 1159\t training loss: 0.1166\tvalidation loss: 0.1620\t validation accuracy: 0.9622\n",
      "iteration number: 1160\t training loss: 0.1166\tvalidation loss: 0.1625\t validation accuracy: 0.9622\n",
      "iteration number: 1161\t training loss: 0.1168\tvalidation loss: 0.1621\t validation accuracy: 0.9622\n",
      "iteration number: 1162\t training loss: 0.1158\tvalidation loss: 0.1618\t validation accuracy: 0.9689\n",
      "iteration number: 1163\t training loss: 0.1174\tvalidation loss: 0.1641\t validation accuracy: 0.9644\n",
      "iteration number: 1164\t training loss: 0.1179\tvalidation loss: 0.1649\t validation accuracy: 0.9600\n",
      "iteration number: 1165\t training loss: 0.1152\tvalidation loss: 0.1623\t validation accuracy: 0.9689\n",
      "iteration number: 1166\t training loss: 0.1156\tvalidation loss: 0.1626\t validation accuracy: 0.9667\n",
      "iteration number: 1167\t training loss: 0.1160\tvalidation loss: 0.1611\t validation accuracy: 0.9689\n",
      "iteration number: 1168\t training loss: 0.1167\tvalidation loss: 0.1651\t validation accuracy: 0.9622\n",
      "iteration number: 1169\t training loss: 0.1159\tvalidation loss: 0.1648\t validation accuracy: 0.9600\n",
      "iteration number: 1170\t training loss: 0.1159\tvalidation loss: 0.1655\t validation accuracy: 0.9600\n",
      "iteration number: 1171\t training loss: 0.1157\tvalidation loss: 0.1670\t validation accuracy: 0.9578\n",
      "iteration number: 1172\t training loss: 0.1148\tvalidation loss: 0.1660\t validation accuracy: 0.9600\n",
      "iteration number: 1173\t training loss: 0.1155\tvalidation loss: 0.1674\t validation accuracy: 0.9600\n",
      "iteration number: 1174\t training loss: 0.1143\tvalidation loss: 0.1645\t validation accuracy: 0.9622\n",
      "iteration number: 1175\t training loss: 0.1155\tvalidation loss: 0.1678\t validation accuracy: 0.9622\n",
      "iteration number: 1176\t training loss: 0.1142\tvalidation loss: 0.1646\t validation accuracy: 0.9622\n",
      "iteration number: 1177\t training loss: 0.1169\tvalidation loss: 0.1694\t validation accuracy: 0.9578\n",
      "iteration number: 1178\t training loss: 0.1171\tvalidation loss: 0.1691\t validation accuracy: 0.9556\n",
      "iteration number: 1179\t training loss: 0.1166\tvalidation loss: 0.1692\t validation accuracy: 0.9578\n",
      "iteration number: 1180\t training loss: 0.1154\tvalidation loss: 0.1669\t validation accuracy: 0.9578\n",
      "iteration number: 1181\t training loss: 0.1157\tvalidation loss: 0.1667\t validation accuracy: 0.9556\n",
      "iteration number: 1182\t training loss: 0.1146\tvalidation loss: 0.1622\t validation accuracy: 0.9689\n",
      "iteration number: 1183\t training loss: 0.1144\tvalidation loss: 0.1622\t validation accuracy: 0.9689\n",
      "iteration number: 1184\t training loss: 0.1144\tvalidation loss: 0.1628\t validation accuracy: 0.9622\n",
      "iteration number: 1185\t training loss: 0.1138\tvalidation loss: 0.1604\t validation accuracy: 0.9667\n",
      "iteration number: 1186\t training loss: 0.1134\tvalidation loss: 0.1604\t validation accuracy: 0.9667\n",
      "iteration number: 1187\t training loss: 0.1168\tvalidation loss: 0.1619\t validation accuracy: 0.9667\n",
      "iteration number: 1188\t training loss: 0.1179\tvalidation loss: 0.1638\t validation accuracy: 0.9667\n",
      "iteration number: 1189\t training loss: 0.1151\tvalidation loss: 0.1632\t validation accuracy: 0.9622\n",
      "iteration number: 1190\t training loss: 0.1132\tvalidation loss: 0.1589\t validation accuracy: 0.9644\n",
      "iteration number: 1191\t training loss: 0.1143\tvalidation loss: 0.1594\t validation accuracy: 0.9644\n",
      "iteration number: 1192\t training loss: 0.1140\tvalidation loss: 0.1604\t validation accuracy: 0.9622\n",
      "iteration number: 1193\t training loss: 0.1129\tvalidation loss: 0.1593\t validation accuracy: 0.9667\n",
      "iteration number: 1194\t training loss: 0.1136\tvalidation loss: 0.1592\t validation accuracy: 0.9644\n",
      "iteration number: 1195\t training loss: 0.1138\tvalidation loss: 0.1592\t validation accuracy: 0.9644\n",
      "iteration number: 1196\t training loss: 0.1140\tvalidation loss: 0.1607\t validation accuracy: 0.9689\n",
      "iteration number: 1197\t training loss: 0.1142\tvalidation loss: 0.1609\t validation accuracy: 0.9667\n",
      "iteration number: 1198\t training loss: 0.1126\tvalidation loss: 0.1602\t validation accuracy: 0.9667\n",
      "iteration number: 1199\t training loss: 0.1140\tvalidation loss: 0.1608\t validation accuracy: 0.9689\n",
      "iteration number: 1200\t training loss: 0.1130\tvalidation loss: 0.1629\t validation accuracy: 0.9667\n",
      "iteration number: 1201\t training loss: 0.1123\tvalidation loss: 0.1612\t validation accuracy: 0.9667\n",
      "iteration number: 1202\t training loss: 0.1127\tvalidation loss: 0.1620\t validation accuracy: 0.9644\n",
      "iteration number: 1203\t training loss: 0.1129\tvalidation loss: 0.1619\t validation accuracy: 0.9644\n",
      "iteration number: 1204\t training loss: 0.1134\tvalidation loss: 0.1626\t validation accuracy: 0.9667\n",
      "iteration number: 1205\t training loss: 0.1130\tvalidation loss: 0.1613\t validation accuracy: 0.9689\n",
      "iteration number: 1206\t training loss: 0.1120\tvalidation loss: 0.1600\t validation accuracy: 0.9689\n",
      "iteration number: 1207\t training loss: 0.1114\tvalidation loss: 0.1604\t validation accuracy: 0.9667\n",
      "iteration number: 1208\t training loss: 0.1111\tvalidation loss: 0.1597\t validation accuracy: 0.9689\n",
      "iteration number: 1209\t training loss: 0.1098\tvalidation loss: 0.1594\t validation accuracy: 0.9667\n",
      "iteration number: 1210\t training loss: 0.1104\tvalidation loss: 0.1587\t validation accuracy: 0.9689\n",
      "iteration number: 1211\t training loss: 0.1110\tvalidation loss: 0.1632\t validation accuracy: 0.9644\n",
      "iteration number: 1212\t training loss: 0.1121\tvalidation loss: 0.1657\t validation accuracy: 0.9622\n",
      "iteration number: 1213\t training loss: 0.1116\tvalidation loss: 0.1637\t validation accuracy: 0.9622\n",
      "iteration number: 1214\t training loss: 0.1112\tvalidation loss: 0.1629\t validation accuracy: 0.9644\n",
      "iteration number: 1215\t training loss: 0.1110\tvalidation loss: 0.1621\t validation accuracy: 0.9644\n",
      "iteration number: 1216\t training loss: 0.1130\tvalidation loss: 0.1662\t validation accuracy: 0.9622\n",
      "iteration number: 1217\t training loss: 0.1120\tvalidation loss: 0.1647\t validation accuracy: 0.9622\n",
      "iteration number: 1218\t training loss: 0.1107\tvalidation loss: 0.1630\t validation accuracy: 0.9600\n",
      "iteration number: 1219\t training loss: 0.1120\tvalidation loss: 0.1659\t validation accuracy: 0.9600\n",
      "iteration number: 1220\t training loss: 0.1115\tvalidation loss: 0.1630\t validation accuracy: 0.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1221\t training loss: 0.1110\tvalidation loss: 0.1611\t validation accuracy: 0.9622\n",
      "iteration number: 1222\t training loss: 0.1104\tvalidation loss: 0.1601\t validation accuracy: 0.9622\n",
      "iteration number: 1223\t training loss: 0.1100\tvalidation loss: 0.1575\t validation accuracy: 0.9644\n",
      "iteration number: 1224\t training loss: 0.1101\tvalidation loss: 0.1560\t validation accuracy: 0.9667\n",
      "iteration number: 1225\t training loss: 0.1143\tvalidation loss: 0.1620\t validation accuracy: 0.9556\n",
      "iteration number: 1226\t training loss: 0.1095\tvalidation loss: 0.1565\t validation accuracy: 0.9689\n",
      "iteration number: 1227\t training loss: 0.1095\tvalidation loss: 0.1550\t validation accuracy: 0.9689\n",
      "iteration number: 1228\t training loss: 0.1083\tvalidation loss: 0.1543\t validation accuracy: 0.9711\n",
      "iteration number: 1229\t training loss: 0.1091\tvalidation loss: 0.1562\t validation accuracy: 0.9689\n",
      "iteration number: 1230\t training loss: 0.1087\tvalidation loss: 0.1563\t validation accuracy: 0.9689\n",
      "iteration number: 1231\t training loss: 0.1083\tvalidation loss: 0.1537\t validation accuracy: 0.9689\n",
      "iteration number: 1232\t training loss: 0.1073\tvalidation loss: 0.1562\t validation accuracy: 0.9689\n",
      "iteration number: 1233\t training loss: 0.1078\tvalidation loss: 0.1564\t validation accuracy: 0.9689\n",
      "iteration number: 1234\t training loss: 0.1075\tvalidation loss: 0.1571\t validation accuracy: 0.9689\n",
      "iteration number: 1235\t training loss: 0.1093\tvalidation loss: 0.1583\t validation accuracy: 0.9667\n",
      "iteration number: 1236\t training loss: 0.1095\tvalidation loss: 0.1594\t validation accuracy: 0.9600\n",
      "iteration number: 1237\t training loss: 0.1105\tvalidation loss: 0.1586\t validation accuracy: 0.9644\n",
      "iteration number: 1238\t training loss: 0.1113\tvalidation loss: 0.1594\t validation accuracy: 0.9600\n",
      "iteration number: 1239\t training loss: 0.1093\tvalidation loss: 0.1587\t validation accuracy: 0.9600\n",
      "iteration number: 1240\t training loss: 0.1094\tvalidation loss: 0.1576\t validation accuracy: 0.9622\n",
      "iteration number: 1241\t training loss: 0.1094\tvalidation loss: 0.1570\t validation accuracy: 0.9622\n",
      "iteration number: 1242\t training loss: 0.1090\tvalidation loss: 0.1555\t validation accuracy: 0.9622\n",
      "iteration number: 1243\t training loss: 0.1097\tvalidation loss: 0.1577\t validation accuracy: 0.9622\n",
      "iteration number: 1244\t training loss: 0.1080\tvalidation loss: 0.1561\t validation accuracy: 0.9644\n",
      "iteration number: 1245\t training loss: 0.1091\tvalidation loss: 0.1578\t validation accuracy: 0.9600\n",
      "iteration number: 1246\t training loss: 0.1070\tvalidation loss: 0.1554\t validation accuracy: 0.9711\n",
      "iteration number: 1247\t training loss: 0.1071\tvalidation loss: 0.1558\t validation accuracy: 0.9667\n",
      "iteration number: 1248\t training loss: 0.1069\tvalidation loss: 0.1564\t validation accuracy: 0.9667\n",
      "iteration number: 1249\t training loss: 0.1062\tvalidation loss: 0.1549\t validation accuracy: 0.9667\n",
      "iteration number: 1250\t training loss: 0.1065\tvalidation loss: 0.1573\t validation accuracy: 0.9644\n",
      "iteration number: 1251\t training loss: 0.1059\tvalidation loss: 0.1571\t validation accuracy: 0.9644\n",
      "iteration number: 1252\t training loss: 0.1056\tvalidation loss: 0.1555\t validation accuracy: 0.9644\n",
      "iteration number: 1253\t training loss: 0.1068\tvalidation loss: 0.1569\t validation accuracy: 0.9622\n",
      "iteration number: 1254\t training loss: 0.1080\tvalidation loss: 0.1594\t validation accuracy: 0.9600\n",
      "iteration number: 1255\t training loss: 0.1053\tvalidation loss: 0.1546\t validation accuracy: 0.9644\n",
      "iteration number: 1256\t training loss: 0.1069\tvalidation loss: 0.1575\t validation accuracy: 0.9622\n",
      "iteration number: 1257\t training loss: 0.1081\tvalidation loss: 0.1592\t validation accuracy: 0.9644\n",
      "iteration number: 1258\t training loss: 0.1064\tvalidation loss: 0.1573\t validation accuracy: 0.9667\n",
      "iteration number: 1259\t training loss: 0.1073\tvalidation loss: 0.1579\t validation accuracy: 0.9667\n",
      "iteration number: 1260\t training loss: 0.1057\tvalidation loss: 0.1563\t validation accuracy: 0.9644\n",
      "iteration number: 1261\t training loss: 0.1060\tvalidation loss: 0.1550\t validation accuracy: 0.9711\n",
      "iteration number: 1262\t training loss: 0.1048\tvalidation loss: 0.1531\t validation accuracy: 0.9689\n",
      "iteration number: 1263\t training loss: 0.1054\tvalidation loss: 0.1567\t validation accuracy: 0.9644\n",
      "iteration number: 1264\t training loss: 0.1051\tvalidation loss: 0.1547\t validation accuracy: 0.9689\n",
      "iteration number: 1265\t training loss: 0.1064\tvalidation loss: 0.1573\t validation accuracy: 0.9622\n",
      "iteration number: 1266\t training loss: 0.1050\tvalidation loss: 0.1528\t validation accuracy: 0.9689\n",
      "iteration number: 1267\t training loss: 0.1048\tvalidation loss: 0.1541\t validation accuracy: 0.9667\n",
      "iteration number: 1268\t training loss: 0.1045\tvalidation loss: 0.1539\t validation accuracy: 0.9667\n",
      "iteration number: 1269\t training loss: 0.1061\tvalidation loss: 0.1586\t validation accuracy: 0.9644\n",
      "iteration number: 1270\t training loss: 0.1048\tvalidation loss: 0.1559\t validation accuracy: 0.9644\n",
      "iteration number: 1271\t training loss: 0.1041\tvalidation loss: 0.1556\t validation accuracy: 0.9667\n",
      "iteration number: 1272\t training loss: 0.1038\tvalidation loss: 0.1543\t validation accuracy: 0.9644\n",
      "iteration number: 1273\t training loss: 0.1052\tvalidation loss: 0.1553\t validation accuracy: 0.9644\n",
      "iteration number: 1274\t training loss: 0.1048\tvalidation loss: 0.1553\t validation accuracy: 0.9644\n",
      "iteration number: 1275\t training loss: 0.1049\tvalidation loss: 0.1542\t validation accuracy: 0.9622\n",
      "iteration number: 1276\t training loss: 0.1051\tvalidation loss: 0.1536\t validation accuracy: 0.9622\n",
      "iteration number: 1277\t training loss: 0.1063\tvalidation loss: 0.1555\t validation accuracy: 0.9600\n",
      "iteration number: 1278\t training loss: 0.1074\tvalidation loss: 0.1560\t validation accuracy: 0.9600\n",
      "iteration number: 1279\t training loss: 0.1073\tvalidation loss: 0.1561\t validation accuracy: 0.9622\n",
      "iteration number: 1280\t training loss: 0.1046\tvalidation loss: 0.1550\t validation accuracy: 0.9667\n",
      "iteration number: 1281\t training loss: 0.1039\tvalidation loss: 0.1548\t validation accuracy: 0.9667\n",
      "iteration number: 1282\t training loss: 0.1028\tvalidation loss: 0.1528\t validation accuracy: 0.9689\n",
      "iteration number: 1283\t training loss: 0.1034\tvalidation loss: 0.1518\t validation accuracy: 0.9667\n",
      "iteration number: 1284\t training loss: 0.1028\tvalidation loss: 0.1524\t validation accuracy: 0.9689\n",
      "iteration number: 1285\t training loss: 0.1025\tvalidation loss: 0.1537\t validation accuracy: 0.9667\n",
      "iteration number: 1286\t training loss: 0.1023\tvalidation loss: 0.1536\t validation accuracy: 0.9667\n",
      "iteration number: 1287\t training loss: 0.1023\tvalidation loss: 0.1527\t validation accuracy: 0.9667\n",
      "iteration number: 1288\t training loss: 0.1023\tvalidation loss: 0.1536\t validation accuracy: 0.9667\n",
      "iteration number: 1289\t training loss: 0.1043\tvalidation loss: 0.1588\t validation accuracy: 0.9644\n",
      "iteration number: 1290\t training loss: 0.1060\tvalidation loss: 0.1622\t validation accuracy: 0.9578\n",
      "iteration number: 1291\t training loss: 0.1101\tvalidation loss: 0.1688\t validation accuracy: 0.9533\n",
      "iteration number: 1292\t training loss: 0.1099\tvalidation loss: 0.1691\t validation accuracy: 0.9511\n",
      "iteration number: 1293\t training loss: 0.1060\tvalidation loss: 0.1624\t validation accuracy: 0.9578\n",
      "iteration number: 1294\t training loss: 0.1039\tvalidation loss: 0.1580\t validation accuracy: 0.9644\n",
      "iteration number: 1295\t training loss: 0.1057\tvalidation loss: 0.1600\t validation accuracy: 0.9600\n",
      "iteration number: 1296\t training loss: 0.1028\tvalidation loss: 0.1552\t validation accuracy: 0.9644\n",
      "iteration number: 1297\t training loss: 0.1025\tvalidation loss: 0.1537\t validation accuracy: 0.9622\n",
      "iteration number: 1298\t training loss: 0.1026\tvalidation loss: 0.1540\t validation accuracy: 0.9622\n",
      "iteration number: 1299\t training loss: 0.1033\tvalidation loss: 0.1562\t validation accuracy: 0.9622\n",
      "iteration number: 1300\t training loss: 0.1047\tvalidation loss: 0.1582\t validation accuracy: 0.9600\n",
      "iteration number: 1301\t training loss: 0.1042\tvalidation loss: 0.1582\t validation accuracy: 0.9578\n",
      "iteration number: 1302\t training loss: 0.1027\tvalidation loss: 0.1558\t validation accuracy: 0.9622\n",
      "iteration number: 1303\t training loss: 0.1034\tvalidation loss: 0.1566\t validation accuracy: 0.9600\n",
      "iteration number: 1304\t training loss: 0.1033\tvalidation loss: 0.1559\t validation accuracy: 0.9600\n",
      "iteration number: 1305\t training loss: 0.1008\tvalidation loss: 0.1523\t validation accuracy: 0.9689\n",
      "iteration number: 1306\t training loss: 0.1010\tvalidation loss: 0.1498\t validation accuracy: 0.9689\n",
      "iteration number: 1307\t training loss: 0.1010\tvalidation loss: 0.1492\t validation accuracy: 0.9711\n",
      "iteration number: 1308\t training loss: 0.1007\tvalidation loss: 0.1491\t validation accuracy: 0.9689\n",
      "iteration number: 1309\t training loss: 0.1008\tvalidation loss: 0.1492\t validation accuracy: 0.9689\n",
      "iteration number: 1310\t training loss: 0.1011\tvalidation loss: 0.1497\t validation accuracy: 0.9689\n",
      "iteration number: 1311\t training loss: 0.1027\tvalidation loss: 0.1546\t validation accuracy: 0.9667\n",
      "iteration number: 1312\t training loss: 0.1049\tvalidation loss: 0.1581\t validation accuracy: 0.9600\n",
      "iteration number: 1313\t training loss: 0.1023\tvalidation loss: 0.1528\t validation accuracy: 0.9667\n",
      "iteration number: 1314\t training loss: 0.1009\tvalidation loss: 0.1503\t validation accuracy: 0.9689\n",
      "iteration number: 1315\t training loss: 0.1010\tvalidation loss: 0.1501\t validation accuracy: 0.9689\n",
      "iteration number: 1316\t training loss: 0.1012\tvalidation loss: 0.1497\t validation accuracy: 0.9689\n",
      "iteration number: 1317\t training loss: 0.1005\tvalidation loss: 0.1509\t validation accuracy: 0.9667\n",
      "iteration number: 1318\t training loss: 0.1005\tvalidation loss: 0.1501\t validation accuracy: 0.9711\n",
      "iteration number: 1319\t training loss: 0.1015\tvalidation loss: 0.1492\t validation accuracy: 0.9711\n",
      "iteration number: 1320\t training loss: 0.1008\tvalidation loss: 0.1493\t validation accuracy: 0.9711\n",
      "iteration number: 1321\t training loss: 0.1010\tvalidation loss: 0.1521\t validation accuracy: 0.9667\n",
      "iteration number: 1322\t training loss: 0.1017\tvalidation loss: 0.1539\t validation accuracy: 0.9644\n",
      "iteration number: 1323\t training loss: 0.1011\tvalidation loss: 0.1527\t validation accuracy: 0.9644\n",
      "iteration number: 1324\t training loss: 0.1010\tvalidation loss: 0.1512\t validation accuracy: 0.9689\n",
      "iteration number: 1325\t training loss: 0.1020\tvalidation loss: 0.1521\t validation accuracy: 0.9667\n",
      "iteration number: 1326\t training loss: 0.1014\tvalidation loss: 0.1519\t validation accuracy: 0.9667\n",
      "iteration number: 1327\t training loss: 0.1002\tvalidation loss: 0.1513\t validation accuracy: 0.9689\n",
      "iteration number: 1328\t training loss: 0.1018\tvalidation loss: 0.1528\t validation accuracy: 0.9600\n",
      "iteration number: 1329\t training loss: 0.1013\tvalidation loss: 0.1519\t validation accuracy: 0.9622\n",
      "iteration number: 1330\t training loss: 0.1018\tvalidation loss: 0.1529\t validation accuracy: 0.9600\n",
      "iteration number: 1331\t training loss: 0.1012\tvalidation loss: 0.1509\t validation accuracy: 0.9644\n",
      "iteration number: 1332\t training loss: 0.1001\tvalidation loss: 0.1523\t validation accuracy: 0.9644\n",
      "iteration number: 1333\t training loss: 0.0992\tvalidation loss: 0.1523\t validation accuracy: 0.9644\n",
      "iteration number: 1334\t training loss: 0.0987\tvalidation loss: 0.1516\t validation accuracy: 0.9644\n",
      "iteration number: 1335\t training loss: 0.0985\tvalidation loss: 0.1514\t validation accuracy: 0.9667\n",
      "iteration number: 1336\t training loss: 0.0990\tvalidation loss: 0.1528\t validation accuracy: 0.9667\n",
      "iteration number: 1337\t training loss: 0.0979\tvalidation loss: 0.1494\t validation accuracy: 0.9711\n",
      "iteration number: 1338\t training loss: 0.0981\tvalidation loss: 0.1494\t validation accuracy: 0.9689\n",
      "iteration number: 1339\t training loss: 0.0992\tvalidation loss: 0.1498\t validation accuracy: 0.9711\n",
      "iteration number: 1340\t training loss: 0.0981\tvalidation loss: 0.1480\t validation accuracy: 0.9711\n",
      "iteration number: 1341\t training loss: 0.0980\tvalidation loss: 0.1482\t validation accuracy: 0.9711\n",
      "iteration number: 1342\t training loss: 0.0981\tvalidation loss: 0.1482\t validation accuracy: 0.9711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1343\t training loss: 0.0984\tvalidation loss: 0.1509\t validation accuracy: 0.9689\n",
      "iteration number: 1344\t training loss: 0.0978\tvalidation loss: 0.1505\t validation accuracy: 0.9711\n",
      "iteration number: 1345\t training loss: 0.0979\tvalidation loss: 0.1498\t validation accuracy: 0.9711\n",
      "iteration number: 1346\t training loss: 0.0985\tvalidation loss: 0.1522\t validation accuracy: 0.9644\n",
      "iteration number: 1347\t training loss: 0.0988\tvalidation loss: 0.1532\t validation accuracy: 0.9667\n",
      "iteration number: 1348\t training loss: 0.0987\tvalidation loss: 0.1523\t validation accuracy: 0.9667\n",
      "iteration number: 1349\t training loss: 0.0983\tvalidation loss: 0.1505\t validation accuracy: 0.9689\n",
      "iteration number: 1350\t training loss: 0.0989\tvalidation loss: 0.1503\t validation accuracy: 0.9711\n",
      "iteration number: 1351\t training loss: 0.0992\tvalidation loss: 0.1526\t validation accuracy: 0.9711\n",
      "iteration number: 1352\t training loss: 0.1015\tvalidation loss: 0.1550\t validation accuracy: 0.9689\n",
      "iteration number: 1353\t training loss: 0.0991\tvalidation loss: 0.1514\t validation accuracy: 0.9689\n",
      "iteration number: 1354\t training loss: 0.0989\tvalidation loss: 0.1521\t validation accuracy: 0.9667\n",
      "iteration number: 1355\t training loss: 0.0987\tvalidation loss: 0.1511\t validation accuracy: 0.9711\n",
      "iteration number: 1356\t training loss: 0.0990\tvalidation loss: 0.1510\t validation accuracy: 0.9711\n",
      "iteration number: 1357\t training loss: 0.0989\tvalidation loss: 0.1502\t validation accuracy: 0.9733\n",
      "iteration number: 1358\t training loss: 0.0983\tvalidation loss: 0.1505\t validation accuracy: 0.9711\n",
      "iteration number: 1359\t training loss: 0.0982\tvalidation loss: 0.1496\t validation accuracy: 0.9689\n",
      "iteration number: 1360\t training loss: 0.0986\tvalidation loss: 0.1500\t validation accuracy: 0.9689\n",
      "iteration number: 1361\t training loss: 0.0982\tvalidation loss: 0.1499\t validation accuracy: 0.9689\n",
      "iteration number: 1362\t training loss: 0.0988\tvalidation loss: 0.1508\t validation accuracy: 0.9689\n",
      "iteration number: 1363\t training loss: 0.0975\tvalidation loss: 0.1497\t validation accuracy: 0.9689\n",
      "iteration number: 1364\t training loss: 0.0969\tvalidation loss: 0.1485\t validation accuracy: 0.9689\n",
      "iteration number: 1365\t training loss: 0.0968\tvalidation loss: 0.1480\t validation accuracy: 0.9711\n",
      "iteration number: 1366\t training loss: 0.0966\tvalidation loss: 0.1460\t validation accuracy: 0.9689\n",
      "iteration number: 1367\t training loss: 0.0960\tvalidation loss: 0.1468\t validation accuracy: 0.9689\n",
      "iteration number: 1368\t training loss: 0.0961\tvalidation loss: 0.1477\t validation accuracy: 0.9689\n",
      "iteration number: 1369\t training loss: 0.0962\tvalidation loss: 0.1473\t validation accuracy: 0.9711\n",
      "iteration number: 1370\t training loss: 0.0960\tvalidation loss: 0.1500\t validation accuracy: 0.9644\n",
      "iteration number: 1371\t training loss: 0.0961\tvalidation loss: 0.1500\t validation accuracy: 0.9667\n",
      "iteration number: 1372\t training loss: 0.0964\tvalidation loss: 0.1492\t validation accuracy: 0.9644\n",
      "iteration number: 1373\t training loss: 0.0961\tvalidation loss: 0.1495\t validation accuracy: 0.9667\n",
      "iteration number: 1374\t training loss: 0.0960\tvalidation loss: 0.1510\t validation accuracy: 0.9622\n",
      "iteration number: 1375\t training loss: 0.0960\tvalidation loss: 0.1512\t validation accuracy: 0.9644\n",
      "iteration number: 1376\t training loss: 0.0956\tvalidation loss: 0.1487\t validation accuracy: 0.9667\n",
      "iteration number: 1377\t training loss: 0.0959\tvalidation loss: 0.1483\t validation accuracy: 0.9667\n",
      "iteration number: 1378\t training loss: 0.0955\tvalidation loss: 0.1490\t validation accuracy: 0.9667\n",
      "iteration number: 1379\t training loss: 0.0958\tvalidation loss: 0.1497\t validation accuracy: 0.9644\n",
      "iteration number: 1380\t training loss: 0.0957\tvalidation loss: 0.1491\t validation accuracy: 0.9667\n",
      "iteration number: 1381\t training loss: 0.0961\tvalidation loss: 0.1493\t validation accuracy: 0.9667\n",
      "iteration number: 1382\t training loss: 0.0958\tvalidation loss: 0.1477\t validation accuracy: 0.9667\n",
      "iteration number: 1383\t training loss: 0.0961\tvalidation loss: 0.1477\t validation accuracy: 0.9667\n",
      "iteration number: 1384\t training loss: 0.0951\tvalidation loss: 0.1489\t validation accuracy: 0.9644\n",
      "iteration number: 1385\t training loss: 0.0945\tvalidation loss: 0.1481\t validation accuracy: 0.9667\n",
      "iteration number: 1386\t training loss: 0.0951\tvalidation loss: 0.1496\t validation accuracy: 0.9667\n",
      "iteration number: 1387\t training loss: 0.0957\tvalidation loss: 0.1524\t validation accuracy: 0.9667\n",
      "iteration number: 1388\t training loss: 0.0959\tvalidation loss: 0.1535\t validation accuracy: 0.9644\n",
      "iteration number: 1389\t training loss: 0.0959\tvalidation loss: 0.1541\t validation accuracy: 0.9644\n",
      "iteration number: 1390\t training loss: 0.0952\tvalidation loss: 0.1525\t validation accuracy: 0.9644\n",
      "iteration number: 1391\t training loss: 0.0951\tvalidation loss: 0.1519\t validation accuracy: 0.9644\n",
      "iteration number: 1392\t training loss: 0.0946\tvalidation loss: 0.1489\t validation accuracy: 0.9689\n",
      "iteration number: 1393\t training loss: 0.0966\tvalidation loss: 0.1480\t validation accuracy: 0.9667\n",
      "iteration number: 1394\t training loss: 0.0954\tvalidation loss: 0.1464\t validation accuracy: 0.9667\n",
      "iteration number: 1395\t training loss: 0.0943\tvalidation loss: 0.1457\t validation accuracy: 0.9689\n",
      "iteration number: 1396\t training loss: 0.0950\tvalidation loss: 0.1460\t validation accuracy: 0.9689\n",
      "iteration number: 1397\t training loss: 0.0947\tvalidation loss: 0.1463\t validation accuracy: 0.9689\n",
      "iteration number: 1398\t training loss: 0.0952\tvalidation loss: 0.1487\t validation accuracy: 0.9667\n",
      "iteration number: 1399\t training loss: 0.0944\tvalidation loss: 0.1485\t validation accuracy: 0.9667\n",
      "iteration number: 1400\t training loss: 0.0931\tvalidation loss: 0.1470\t validation accuracy: 0.9689\n",
      "iteration number: 1401\t training loss: 0.0939\tvalidation loss: 0.1472\t validation accuracy: 0.9667\n",
      "iteration number: 1402\t training loss: 0.0934\tvalidation loss: 0.1473\t validation accuracy: 0.9667\n",
      "iteration number: 1403\t training loss: 0.0948\tvalidation loss: 0.1491\t validation accuracy: 0.9667\n",
      "iteration number: 1404\t training loss: 0.0939\tvalidation loss: 0.1473\t validation accuracy: 0.9689\n",
      "iteration number: 1405\t training loss: 0.0943\tvalidation loss: 0.1477\t validation accuracy: 0.9667\n",
      "iteration number: 1406\t training loss: 0.0943\tvalidation loss: 0.1484\t validation accuracy: 0.9644\n",
      "iteration number: 1407\t training loss: 0.0933\tvalidation loss: 0.1476\t validation accuracy: 0.9689\n",
      "iteration number: 1408\t training loss: 0.0934\tvalidation loss: 0.1462\t validation accuracy: 0.9667\n",
      "iteration number: 1409\t training loss: 0.0932\tvalidation loss: 0.1467\t validation accuracy: 0.9689\n",
      "iteration number: 1410\t training loss: 0.0927\tvalidation loss: 0.1444\t validation accuracy: 0.9733\n",
      "iteration number: 1411\t training loss: 0.0928\tvalidation loss: 0.1447\t validation accuracy: 0.9667\n",
      "iteration number: 1412\t training loss: 0.0937\tvalidation loss: 0.1458\t validation accuracy: 0.9667\n",
      "iteration number: 1413\t training loss: 0.0929\tvalidation loss: 0.1443\t validation accuracy: 0.9667\n",
      "iteration number: 1414\t training loss: 0.0920\tvalidation loss: 0.1451\t validation accuracy: 0.9667\n",
      "iteration number: 1415\t training loss: 0.0920\tvalidation loss: 0.1449\t validation accuracy: 0.9689\n",
      "iteration number: 1416\t training loss: 0.0919\tvalidation loss: 0.1456\t validation accuracy: 0.9667\n",
      "iteration number: 1417\t training loss: 0.0922\tvalidation loss: 0.1471\t validation accuracy: 0.9644\n",
      "iteration number: 1418\t training loss: 0.0924\tvalidation loss: 0.1457\t validation accuracy: 0.9689\n",
      "iteration number: 1419\t training loss: 0.0928\tvalidation loss: 0.1460\t validation accuracy: 0.9689\n",
      "iteration number: 1420\t training loss: 0.0924\tvalidation loss: 0.1454\t validation accuracy: 0.9689\n",
      "iteration number: 1421\t training loss: 0.0918\tvalidation loss: 0.1454\t validation accuracy: 0.9667\n",
      "iteration number: 1422\t training loss: 0.0926\tvalidation loss: 0.1467\t validation accuracy: 0.9689\n",
      "iteration number: 1423\t training loss: 0.0922\tvalidation loss: 0.1458\t validation accuracy: 0.9689\n",
      "iteration number: 1424\t training loss: 0.0923\tvalidation loss: 0.1449\t validation accuracy: 0.9689\n",
      "iteration number: 1425\t training loss: 0.0929\tvalidation loss: 0.1477\t validation accuracy: 0.9667\n",
      "iteration number: 1426\t training loss: 0.0911\tvalidation loss: 0.1437\t validation accuracy: 0.9711\n",
      "iteration number: 1427\t training loss: 0.0920\tvalidation loss: 0.1426\t validation accuracy: 0.9711\n",
      "iteration number: 1428\t training loss: 0.0924\tvalidation loss: 0.1436\t validation accuracy: 0.9689\n",
      "iteration number: 1429\t training loss: 0.0913\tvalidation loss: 0.1443\t validation accuracy: 0.9711\n",
      "iteration number: 1430\t training loss: 0.0914\tvalidation loss: 0.1453\t validation accuracy: 0.9711\n",
      "iteration number: 1431\t training loss: 0.0926\tvalidation loss: 0.1471\t validation accuracy: 0.9689\n",
      "iteration number: 1432\t training loss: 0.0913\tvalidation loss: 0.1449\t validation accuracy: 0.9711\n",
      "iteration number: 1433\t training loss: 0.0909\tvalidation loss: 0.1450\t validation accuracy: 0.9711\n",
      "iteration number: 1434\t training loss: 0.0913\tvalidation loss: 0.1447\t validation accuracy: 0.9689\n",
      "iteration number: 1435\t training loss: 0.0908\tvalidation loss: 0.1445\t validation accuracy: 0.9689\n",
      "iteration number: 1436\t training loss: 0.0917\tvalidation loss: 0.1482\t validation accuracy: 0.9667\n",
      "iteration number: 1437\t training loss: 0.0914\tvalidation loss: 0.1474\t validation accuracy: 0.9667\n",
      "iteration number: 1438\t training loss: 0.0921\tvalidation loss: 0.1477\t validation accuracy: 0.9667\n",
      "iteration number: 1439\t training loss: 0.0930\tvalidation loss: 0.1460\t validation accuracy: 0.9667\n",
      "iteration number: 1440\t training loss: 0.0914\tvalidation loss: 0.1464\t validation accuracy: 0.9667\n",
      "iteration number: 1441\t training loss: 0.0919\tvalidation loss: 0.1471\t validation accuracy: 0.9667\n",
      "iteration number: 1442\t training loss: 0.0919\tvalidation loss: 0.1467\t validation accuracy: 0.9689\n",
      "iteration number: 1443\t training loss: 0.0911\tvalidation loss: 0.1485\t validation accuracy: 0.9667\n",
      "iteration number: 1444\t training loss: 0.0934\tvalidation loss: 0.1521\t validation accuracy: 0.9600\n",
      "iteration number: 1445\t training loss: 0.0933\tvalidation loss: 0.1512\t validation accuracy: 0.9622\n",
      "iteration number: 1446\t training loss: 0.0903\tvalidation loss: 0.1463\t validation accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1447\t training loss: 0.0906\tvalidation loss: 0.1485\t validation accuracy: 0.9644\n",
      "iteration number: 1448\t training loss: 0.0916\tvalidation loss: 0.1509\t validation accuracy: 0.9644\n",
      "iteration number: 1449\t training loss: 0.0914\tvalidation loss: 0.1511\t validation accuracy: 0.9667\n",
      "iteration number: 1450\t training loss: 0.0906\tvalidation loss: 0.1482\t validation accuracy: 0.9689\n",
      "iteration number: 1451\t training loss: 0.0898\tvalidation loss: 0.1463\t validation accuracy: 0.9689\n",
      "iteration number: 1452\t training loss: 0.0902\tvalidation loss: 0.1487\t validation accuracy: 0.9667\n",
      "iteration number: 1453\t training loss: 0.0897\tvalidation loss: 0.1479\t validation accuracy: 0.9667\n",
      "iteration number: 1454\t training loss: 0.0898\tvalidation loss: 0.1485\t validation accuracy: 0.9667\n",
      "iteration number: 1455\t training loss: 0.0894\tvalidation loss: 0.1469\t validation accuracy: 0.9667\n",
      "iteration number: 1456\t training loss: 0.0895\tvalidation loss: 0.1461\t validation accuracy: 0.9667\n",
      "iteration number: 1457\t training loss: 0.0892\tvalidation loss: 0.1461\t validation accuracy: 0.9667\n",
      "iteration number: 1458\t training loss: 0.0894\tvalidation loss: 0.1464\t validation accuracy: 0.9667\n",
      "iteration number: 1459\t training loss: 0.0901\tvalidation loss: 0.1490\t validation accuracy: 0.9667\n",
      "iteration number: 1460\t training loss: 0.0903\tvalidation loss: 0.1474\t validation accuracy: 0.9689\n",
      "iteration number: 1461\t training loss: 0.0894\tvalidation loss: 0.1472\t validation accuracy: 0.9689\n",
      "iteration number: 1462\t training loss: 0.0887\tvalidation loss: 0.1460\t validation accuracy: 0.9689\n",
      "iteration number: 1463\t training loss: 0.0886\tvalidation loss: 0.1433\t validation accuracy: 0.9711\n",
      "iteration number: 1464\t training loss: 0.0893\tvalidation loss: 0.1436\t validation accuracy: 0.9711\n",
      "iteration number: 1465\t training loss: 0.0898\tvalidation loss: 0.1442\t validation accuracy: 0.9689\n",
      "iteration number: 1466\t training loss: 0.0883\tvalidation loss: 0.1436\t validation accuracy: 0.9689\n",
      "iteration number: 1467\t training loss: 0.0884\tvalidation loss: 0.1442\t validation accuracy: 0.9711\n",
      "iteration number: 1468\t training loss: 0.0886\tvalidation loss: 0.1451\t validation accuracy: 0.9689\n",
      "iteration number: 1469\t training loss: 0.0904\tvalidation loss: 0.1465\t validation accuracy: 0.9644\n",
      "iteration number: 1470\t training loss: 0.0901\tvalidation loss: 0.1459\t validation accuracy: 0.9644\n",
      "iteration number: 1471\t training loss: 0.0895\tvalidation loss: 0.1455\t validation accuracy: 0.9667\n",
      "iteration number: 1472\t training loss: 0.0900\tvalidation loss: 0.1469\t validation accuracy: 0.9667\n",
      "iteration number: 1473\t training loss: 0.0888\tvalidation loss: 0.1461\t validation accuracy: 0.9711\n",
      "iteration number: 1474\t training loss: 0.0892\tvalidation loss: 0.1458\t validation accuracy: 0.9711\n",
      "iteration number: 1475\t training loss: 0.0891\tvalidation loss: 0.1456\t validation accuracy: 0.9689\n",
      "iteration number: 1476\t training loss: 0.0878\tvalidation loss: 0.1451\t validation accuracy: 0.9711\n",
      "iteration number: 1477\t training loss: 0.0879\tvalidation loss: 0.1461\t validation accuracy: 0.9667\n",
      "iteration number: 1478\t training loss: 0.0879\tvalidation loss: 0.1461\t validation accuracy: 0.9644\n",
      "iteration number: 1479\t training loss: 0.0872\tvalidation loss: 0.1462\t validation accuracy: 0.9689\n",
      "iteration number: 1480\t training loss: 0.0875\tvalidation loss: 0.1462\t validation accuracy: 0.9667\n",
      "iteration number: 1481\t training loss: 0.0892\tvalidation loss: 0.1513\t validation accuracy: 0.9667\n",
      "iteration number: 1482\t training loss: 0.0893\tvalidation loss: 0.1514\t validation accuracy: 0.9622\n",
      "iteration number: 1483\t training loss: 0.0880\tvalidation loss: 0.1474\t validation accuracy: 0.9667\n",
      "iteration number: 1484\t training loss: 0.0886\tvalidation loss: 0.1474\t validation accuracy: 0.9667\n",
      "iteration number: 1485\t training loss: 0.0878\tvalidation loss: 0.1468\t validation accuracy: 0.9689\n",
      "iteration number: 1486\t training loss: 0.0875\tvalidation loss: 0.1457\t validation accuracy: 0.9689\n",
      "iteration number: 1487\t training loss: 0.0871\tvalidation loss: 0.1458\t validation accuracy: 0.9667\n",
      "iteration number: 1488\t training loss: 0.0868\tvalidation loss: 0.1453\t validation accuracy: 0.9667\n",
      "iteration number: 1489\t training loss: 0.0871\tvalidation loss: 0.1462\t validation accuracy: 0.9667\n",
      "iteration number: 1490\t training loss: 0.0895\tvalidation loss: 0.1488\t validation accuracy: 0.9644\n",
      "iteration number: 1491\t training loss: 0.0897\tvalidation loss: 0.1501\t validation accuracy: 0.9622\n",
      "iteration number: 1492\t training loss: 0.0868\tvalidation loss: 0.1433\t validation accuracy: 0.9689\n",
      "iteration number: 1493\t training loss: 0.0868\tvalidation loss: 0.1454\t validation accuracy: 0.9667\n",
      "iteration number: 1494\t training loss: 0.0868\tvalidation loss: 0.1449\t validation accuracy: 0.9689\n",
      "iteration number: 1495\t training loss: 0.0876\tvalidation loss: 0.1457\t validation accuracy: 0.9689\n",
      "iteration number: 1496\t training loss: 0.0874\tvalidation loss: 0.1447\t validation accuracy: 0.9689\n",
      "iteration number: 1497\t training loss: 0.0877\tvalidation loss: 0.1450\t validation accuracy: 0.9689\n",
      "iteration number: 1498\t training loss: 0.0883\tvalidation loss: 0.1454\t validation accuracy: 0.9689\n",
      "iteration number: 1499\t training loss: 0.0891\tvalidation loss: 0.1468\t validation accuracy: 0.9667\n",
      "iteration number: 1500\t training loss: 0.0875\tvalidation loss: 0.1455\t validation accuracy: 0.9667\n",
      "iteration number: 1501\t training loss: 0.0872\tvalidation loss: 0.1452\t validation accuracy: 0.9667\n",
      "iteration number: 1502\t training loss: 0.0867\tvalidation loss: 0.1443\t validation accuracy: 0.9689\n",
      "iteration number: 1503\t training loss: 0.0865\tvalidation loss: 0.1444\t validation accuracy: 0.9667\n",
      "iteration number: 1504\t training loss: 0.0859\tvalidation loss: 0.1436\t validation accuracy: 0.9667\n",
      "iteration number: 1505\t training loss: 0.0875\tvalidation loss: 0.1462\t validation accuracy: 0.9667\n",
      "iteration number: 1506\t training loss: 0.0872\tvalidation loss: 0.1458\t validation accuracy: 0.9667\n",
      "iteration number: 1507\t training loss: 0.0867\tvalidation loss: 0.1466\t validation accuracy: 0.9667\n",
      "iteration number: 1508\t training loss: 0.0878\tvalidation loss: 0.1479\t validation accuracy: 0.9667\n",
      "iteration number: 1509\t training loss: 0.0881\tvalidation loss: 0.1471\t validation accuracy: 0.9667\n",
      "iteration number: 1510\t training loss: 0.0888\tvalidation loss: 0.1490\t validation accuracy: 0.9622\n",
      "iteration number: 1511\t training loss: 0.0910\tvalidation loss: 0.1518\t validation accuracy: 0.9600\n",
      "iteration number: 1512\t training loss: 0.0868\tvalidation loss: 0.1448\t validation accuracy: 0.9667\n",
      "iteration number: 1513\t training loss: 0.0860\tvalidation loss: 0.1430\t validation accuracy: 0.9689\n",
      "iteration number: 1514\t training loss: 0.0860\tvalidation loss: 0.1417\t validation accuracy: 0.9689\n",
      "iteration number: 1515\t training loss: 0.0858\tvalidation loss: 0.1415\t validation accuracy: 0.9689\n",
      "iteration number: 1516\t training loss: 0.0852\tvalidation loss: 0.1414\t validation accuracy: 0.9689\n",
      "iteration number: 1517\t training loss: 0.0855\tvalidation loss: 0.1432\t validation accuracy: 0.9689\n",
      "iteration number: 1518\t training loss: 0.0853\tvalidation loss: 0.1425\t validation accuracy: 0.9689\n",
      "iteration number: 1519\t training loss: 0.0852\tvalidation loss: 0.1423\t validation accuracy: 0.9667\n",
      "iteration number: 1520\t training loss: 0.0849\tvalidation loss: 0.1405\t validation accuracy: 0.9711\n",
      "iteration number: 1521\t training loss: 0.0856\tvalidation loss: 0.1427\t validation accuracy: 0.9689\n",
      "iteration number: 1522\t training loss: 0.0863\tvalidation loss: 0.1404\t validation accuracy: 0.9689\n",
      "iteration number: 1523\t training loss: 0.0865\tvalidation loss: 0.1407\t validation accuracy: 0.9689\n",
      "iteration number: 1524\t training loss: 0.0858\tvalidation loss: 0.1405\t validation accuracy: 0.9667\n",
      "iteration number: 1525\t training loss: 0.0867\tvalidation loss: 0.1415\t validation accuracy: 0.9667\n",
      "iteration number: 1526\t training loss: 0.0855\tvalidation loss: 0.1411\t validation accuracy: 0.9689\n",
      "iteration number: 1527\t training loss: 0.0870\tvalidation loss: 0.1463\t validation accuracy: 0.9644\n",
      "iteration number: 1528\t training loss: 0.0863\tvalidation loss: 0.1452\t validation accuracy: 0.9644\n",
      "iteration number: 1529\t training loss: 0.0863\tvalidation loss: 0.1447\t validation accuracy: 0.9667\n",
      "iteration number: 1530\t training loss: 0.0867\tvalidation loss: 0.1464\t validation accuracy: 0.9644\n",
      "iteration number: 1531\t training loss: 0.0900\tvalidation loss: 0.1517\t validation accuracy: 0.9600\n",
      "iteration number: 1532\t training loss: 0.0866\tvalidation loss: 0.1468\t validation accuracy: 0.9644\n",
      "iteration number: 1533\t training loss: 0.0854\tvalidation loss: 0.1451\t validation accuracy: 0.9667\n",
      "iteration number: 1534\t training loss: 0.0851\tvalidation loss: 0.1441\t validation accuracy: 0.9667\n",
      "iteration number: 1535\t training loss: 0.0858\tvalidation loss: 0.1455\t validation accuracy: 0.9689\n",
      "iteration number: 1536\t training loss: 0.0847\tvalidation loss: 0.1433\t validation accuracy: 0.9689\n",
      "iteration number: 1537\t training loss: 0.0854\tvalidation loss: 0.1447\t validation accuracy: 0.9689\n",
      "iteration number: 1538\t training loss: 0.0850\tvalidation loss: 0.1447\t validation accuracy: 0.9689\n",
      "iteration number: 1539\t training loss: 0.0845\tvalidation loss: 0.1432\t validation accuracy: 0.9689\n",
      "iteration number: 1540\t training loss: 0.0855\tvalidation loss: 0.1461\t validation accuracy: 0.9667\n",
      "iteration number: 1541\t training loss: 0.0866\tvalidation loss: 0.1490\t validation accuracy: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1542\t training loss: 0.0854\tvalidation loss: 0.1467\t validation accuracy: 0.9667\n",
      "iteration number: 1543\t training loss: 0.0868\tvalidation loss: 0.1489\t validation accuracy: 0.9600\n",
      "iteration number: 1544\t training loss: 0.0845\tvalidation loss: 0.1444\t validation accuracy: 0.9667\n",
      "iteration number: 1545\t training loss: 0.0836\tvalidation loss: 0.1421\t validation accuracy: 0.9667\n",
      "iteration number: 1546\t training loss: 0.0837\tvalidation loss: 0.1419\t validation accuracy: 0.9689\n",
      "iteration number: 1547\t training loss: 0.0834\tvalidation loss: 0.1424\t validation accuracy: 0.9689\n",
      "iteration number: 1548\t training loss: 0.0836\tvalidation loss: 0.1418\t validation accuracy: 0.9711\n",
      "iteration number: 1549\t training loss: 0.0831\tvalidation loss: 0.1412\t validation accuracy: 0.9711\n",
      "iteration number: 1550\t training loss: 0.0830\tvalidation loss: 0.1411\t validation accuracy: 0.9711\n",
      "iteration number: 1551\t training loss: 0.0832\tvalidation loss: 0.1422\t validation accuracy: 0.9689\n",
      "iteration number: 1552\t training loss: 0.0836\tvalidation loss: 0.1437\t validation accuracy: 0.9689\n",
      "iteration number: 1553\t training loss: 0.0844\tvalidation loss: 0.1445\t validation accuracy: 0.9689\n",
      "iteration number: 1554\t training loss: 0.0839\tvalidation loss: 0.1429\t validation accuracy: 0.9689\n",
      "iteration number: 1555\t training loss: 0.0836\tvalidation loss: 0.1419\t validation accuracy: 0.9711\n",
      "iteration number: 1556\t training loss: 0.0839\tvalidation loss: 0.1408\t validation accuracy: 0.9711\n",
      "iteration number: 1557\t training loss: 0.0835\tvalidation loss: 0.1413\t validation accuracy: 0.9689\n",
      "iteration number: 1558\t training loss: 0.0843\tvalidation loss: 0.1421\t validation accuracy: 0.9667\n",
      "iteration number: 1559\t training loss: 0.0842\tvalidation loss: 0.1412\t validation accuracy: 0.9667\n",
      "iteration number: 1560\t training loss: 0.0832\tvalidation loss: 0.1407\t validation accuracy: 0.9689\n",
      "iteration number: 1561\t training loss: 0.0829\tvalidation loss: 0.1400\t validation accuracy: 0.9689\n",
      "iteration number: 1562\t training loss: 0.0832\tvalidation loss: 0.1403\t validation accuracy: 0.9689\n",
      "iteration number: 1563\t training loss: 0.0840\tvalidation loss: 0.1408\t validation accuracy: 0.9667\n",
      "iteration number: 1564\t training loss: 0.0834\tvalidation loss: 0.1400\t validation accuracy: 0.9689\n",
      "iteration number: 1565\t training loss: 0.0836\tvalidation loss: 0.1418\t validation accuracy: 0.9689\n",
      "iteration number: 1566\t training loss: 0.0850\tvalidation loss: 0.1430\t validation accuracy: 0.9644\n",
      "iteration number: 1567\t training loss: 0.0862\tvalidation loss: 0.1445\t validation accuracy: 0.9622\n",
      "iteration number: 1568\t training loss: 0.0846\tvalidation loss: 0.1430\t validation accuracy: 0.9667\n",
      "iteration number: 1569\t training loss: 0.0839\tvalidation loss: 0.1427\t validation accuracy: 0.9689\n",
      "iteration number: 1570\t training loss: 0.0843\tvalidation loss: 0.1427\t validation accuracy: 0.9689\n",
      "iteration number: 1571\t training loss: 0.0844\tvalidation loss: 0.1416\t validation accuracy: 0.9689\n",
      "iteration number: 1572\t training loss: 0.0837\tvalidation loss: 0.1425\t validation accuracy: 0.9667\n",
      "iteration number: 1573\t training loss: 0.0827\tvalidation loss: 0.1409\t validation accuracy: 0.9689\n",
      "iteration number: 1574\t training loss: 0.0822\tvalidation loss: 0.1400\t validation accuracy: 0.9689\n",
      "iteration number: 1575\t training loss: 0.0819\tvalidation loss: 0.1415\t validation accuracy: 0.9711\n",
      "iteration number: 1576\t training loss: 0.0820\tvalidation loss: 0.1413\t validation accuracy: 0.9711\n",
      "iteration number: 1577\t training loss: 0.0824\tvalidation loss: 0.1430\t validation accuracy: 0.9667\n",
      "iteration number: 1578\t training loss: 0.0819\tvalidation loss: 0.1410\t validation accuracy: 0.9689\n",
      "iteration number: 1579\t training loss: 0.0814\tvalidation loss: 0.1412\t validation accuracy: 0.9689\n",
      "iteration number: 1580\t training loss: 0.0817\tvalidation loss: 0.1426\t validation accuracy: 0.9667\n",
      "iteration number: 1581\t training loss: 0.0814\tvalidation loss: 0.1417\t validation accuracy: 0.9689\n",
      "iteration number: 1582\t training loss: 0.0820\tvalidation loss: 0.1428\t validation accuracy: 0.9689\n",
      "iteration number: 1583\t training loss: 0.0822\tvalidation loss: 0.1410\t validation accuracy: 0.9667\n",
      "iteration number: 1584\t training loss: 0.0821\tvalidation loss: 0.1401\t validation accuracy: 0.9689\n",
      "iteration number: 1585\t training loss: 0.0817\tvalidation loss: 0.1397\t validation accuracy: 0.9689\n",
      "iteration number: 1586\t training loss: 0.0816\tvalidation loss: 0.1396\t validation accuracy: 0.9689\n",
      "iteration number: 1587\t training loss: 0.0819\tvalidation loss: 0.1389\t validation accuracy: 0.9689\n",
      "iteration number: 1588\t training loss: 0.0817\tvalidation loss: 0.1382\t validation accuracy: 0.9689\n",
      "iteration number: 1589\t training loss: 0.0825\tvalidation loss: 0.1399\t validation accuracy: 0.9689\n",
      "iteration number: 1590\t training loss: 0.0823\tvalidation loss: 0.1410\t validation accuracy: 0.9689\n",
      "iteration number: 1591\t training loss: 0.0821\tvalidation loss: 0.1404\t validation accuracy: 0.9667\n",
      "iteration number: 1592\t training loss: 0.0821\tvalidation loss: 0.1397\t validation accuracy: 0.9689\n",
      "iteration number: 1593\t training loss: 0.0822\tvalidation loss: 0.1395\t validation accuracy: 0.9689\n",
      "iteration number: 1594\t training loss: 0.0826\tvalidation loss: 0.1396\t validation accuracy: 0.9689\n",
      "iteration number: 1595\t training loss: 0.0831\tvalidation loss: 0.1397\t validation accuracy: 0.9711\n",
      "iteration number: 1596\t training loss: 0.0822\tvalidation loss: 0.1392\t validation accuracy: 0.9689\n",
      "iteration number: 1597\t training loss: 0.0820\tvalidation loss: 0.1404\t validation accuracy: 0.9689\n",
      "iteration number: 1598\t training loss: 0.0813\tvalidation loss: 0.1410\t validation accuracy: 0.9667\n",
      "iteration number: 1599\t training loss: 0.0822\tvalidation loss: 0.1402\t validation accuracy: 0.9689\n",
      "iteration number: 1600\t training loss: 0.0808\tvalidation loss: 0.1384\t validation accuracy: 0.9689\n",
      "iteration number: 1601\t training loss: 0.0812\tvalidation loss: 0.1371\t validation accuracy: 0.9711\n",
      "iteration number: 1602\t training loss: 0.0813\tvalidation loss: 0.1385\t validation accuracy: 0.9733\n",
      "iteration number: 1603\t training loss: 0.0814\tvalidation loss: 0.1394\t validation accuracy: 0.9711\n",
      "iteration number: 1604\t training loss: 0.0824\tvalidation loss: 0.1409\t validation accuracy: 0.9711\n",
      "iteration number: 1605\t training loss: 0.0841\tvalidation loss: 0.1432\t validation accuracy: 0.9667\n",
      "iteration number: 1606\t training loss: 0.0849\tvalidation loss: 0.1420\t validation accuracy: 0.9689\n",
      "iteration number: 1607\t training loss: 0.0816\tvalidation loss: 0.1389\t validation accuracy: 0.9711\n",
      "iteration number: 1608\t training loss: 0.0810\tvalidation loss: 0.1378\t validation accuracy: 0.9711\n",
      "iteration number: 1609\t training loss: 0.0816\tvalidation loss: 0.1397\t validation accuracy: 0.9689\n",
      "iteration number: 1610\t training loss: 0.0807\tvalidation loss: 0.1408\t validation accuracy: 0.9689\n",
      "iteration number: 1611\t training loss: 0.0806\tvalidation loss: 0.1405\t validation accuracy: 0.9667\n",
      "iteration number: 1612\t training loss: 0.0807\tvalidation loss: 0.1399\t validation accuracy: 0.9689\n",
      "iteration number: 1613\t training loss: 0.0802\tvalidation loss: 0.1384\t validation accuracy: 0.9689\n",
      "iteration number: 1614\t training loss: 0.0803\tvalidation loss: 0.1385\t validation accuracy: 0.9667\n",
      "iteration number: 1615\t training loss: 0.0817\tvalidation loss: 0.1420\t validation accuracy: 0.9644\n",
      "iteration number: 1616\t training loss: 0.0806\tvalidation loss: 0.1387\t validation accuracy: 0.9667\n",
      "iteration number: 1617\t training loss: 0.0795\tvalidation loss: 0.1380\t validation accuracy: 0.9667\n",
      "iteration number: 1618\t training loss: 0.0796\tvalidation loss: 0.1379\t validation accuracy: 0.9667\n",
      "iteration number: 1619\t training loss: 0.0798\tvalidation loss: 0.1393\t validation accuracy: 0.9667\n",
      "iteration number: 1620\t training loss: 0.0797\tvalidation loss: 0.1374\t validation accuracy: 0.9689\n",
      "iteration number: 1621\t training loss: 0.0791\tvalidation loss: 0.1382\t validation accuracy: 0.9689\n",
      "iteration number: 1622\t training loss: 0.0792\tvalidation loss: 0.1393\t validation accuracy: 0.9667\n",
      "iteration number: 1623\t training loss: 0.0794\tvalidation loss: 0.1409\t validation accuracy: 0.9667\n",
      "iteration number: 1624\t training loss: 0.0797\tvalidation loss: 0.1409\t validation accuracy: 0.9689\n",
      "iteration number: 1625\t training loss: 0.0787\tvalidation loss: 0.1393\t validation accuracy: 0.9689\n",
      "iteration number: 1626\t training loss: 0.0785\tvalidation loss: 0.1393\t validation accuracy: 0.9689\n",
      "iteration number: 1627\t training loss: 0.0785\tvalidation loss: 0.1385\t validation accuracy: 0.9667\n",
      "iteration number: 1628\t training loss: 0.0788\tvalidation loss: 0.1385\t validation accuracy: 0.9667\n",
      "iteration number: 1629\t training loss: 0.0788\tvalidation loss: 0.1373\t validation accuracy: 0.9689\n",
      "iteration number: 1630\t training loss: 0.0789\tvalidation loss: 0.1369\t validation accuracy: 0.9711\n",
      "iteration number: 1631\t training loss: 0.0796\tvalidation loss: 0.1366\t validation accuracy: 0.9711\n",
      "iteration number: 1632\t training loss: 0.0792\tvalidation loss: 0.1369\t validation accuracy: 0.9711\n",
      "iteration number: 1633\t training loss: 0.0796\tvalidation loss: 0.1367\t validation accuracy: 0.9733\n",
      "iteration number: 1634\t training loss: 0.0788\tvalidation loss: 0.1359\t validation accuracy: 0.9711\n",
      "iteration number: 1635\t training loss: 0.0804\tvalidation loss: 0.1395\t validation accuracy: 0.9711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1636\t training loss: 0.0800\tvalidation loss: 0.1367\t validation accuracy: 0.9711\n",
      "iteration number: 1637\t training loss: 0.0799\tvalidation loss: 0.1361\t validation accuracy: 0.9711\n",
      "iteration number: 1638\t training loss: 0.0796\tvalidation loss: 0.1370\t validation accuracy: 0.9711\n",
      "iteration number: 1639\t training loss: 0.0795\tvalidation loss: 0.1368\t validation accuracy: 0.9711\n",
      "iteration number: 1640\t training loss: 0.0792\tvalidation loss: 0.1362\t validation accuracy: 0.9689\n",
      "iteration number: 1641\t training loss: 0.0787\tvalidation loss: 0.1363\t validation accuracy: 0.9689\n",
      "iteration number: 1642\t training loss: 0.0789\tvalidation loss: 0.1362\t validation accuracy: 0.9689\n",
      "iteration number: 1643\t training loss: 0.0787\tvalidation loss: 0.1369\t validation accuracy: 0.9689\n",
      "iteration number: 1644\t training loss: 0.0788\tvalidation loss: 0.1375\t validation accuracy: 0.9644\n",
      "iteration number: 1645\t training loss: 0.0790\tvalidation loss: 0.1374\t validation accuracy: 0.9667\n",
      "iteration number: 1646\t training loss: 0.0784\tvalidation loss: 0.1365\t validation accuracy: 0.9689\n",
      "iteration number: 1647\t training loss: 0.0793\tvalidation loss: 0.1365\t validation accuracy: 0.9689\n",
      "iteration number: 1648\t training loss: 0.0788\tvalidation loss: 0.1374\t validation accuracy: 0.9667\n",
      "iteration number: 1649\t training loss: 0.0791\tvalidation loss: 0.1380\t validation accuracy: 0.9689\n",
      "iteration number: 1650\t training loss: 0.0792\tvalidation loss: 0.1383\t validation accuracy: 0.9689\n",
      "iteration number: 1651\t training loss: 0.0787\tvalidation loss: 0.1406\t validation accuracy: 0.9667\n",
      "iteration number: 1652\t training loss: 0.0798\tvalidation loss: 0.1429\t validation accuracy: 0.9644\n",
      "iteration number: 1653\t training loss: 0.0788\tvalidation loss: 0.1413\t validation accuracy: 0.9689\n",
      "iteration number: 1654\t training loss: 0.0778\tvalidation loss: 0.1389\t validation accuracy: 0.9689\n",
      "iteration number: 1655\t training loss: 0.0777\tvalidation loss: 0.1387\t validation accuracy: 0.9667\n",
      "iteration number: 1656\t training loss: 0.0782\tvalidation loss: 0.1413\t validation accuracy: 0.9689\n",
      "iteration number: 1657\t training loss: 0.0775\tvalidation loss: 0.1396\t validation accuracy: 0.9689\n",
      "iteration number: 1658\t training loss: 0.0792\tvalidation loss: 0.1429\t validation accuracy: 0.9667\n",
      "iteration number: 1659\t training loss: 0.0790\tvalidation loss: 0.1415\t validation accuracy: 0.9689\n",
      "iteration number: 1660\t training loss: 0.0787\tvalidation loss: 0.1402\t validation accuracy: 0.9733\n",
      "iteration number: 1661\t training loss: 0.0780\tvalidation loss: 0.1408\t validation accuracy: 0.9689\n",
      "iteration number: 1662\t training loss: 0.0780\tvalidation loss: 0.1401\t validation accuracy: 0.9667\n",
      "iteration number: 1663\t training loss: 0.0778\tvalidation loss: 0.1401\t validation accuracy: 0.9667\n",
      "iteration number: 1664\t training loss: 0.0769\tvalidation loss: 0.1387\t validation accuracy: 0.9667\n",
      "iteration number: 1665\t training loss: 0.0771\tvalidation loss: 0.1373\t validation accuracy: 0.9689\n",
      "iteration number: 1666\t training loss: 0.0778\tvalidation loss: 0.1395\t validation accuracy: 0.9689\n",
      "iteration number: 1667\t training loss: 0.0770\tvalidation loss: 0.1394\t validation accuracy: 0.9689\n",
      "iteration number: 1668\t training loss: 0.0773\tvalidation loss: 0.1391\t validation accuracy: 0.9689\n",
      "iteration number: 1669\t training loss: 0.0785\tvalidation loss: 0.1411\t validation accuracy: 0.9667\n",
      "iteration number: 1670\t training loss: 0.0772\tvalidation loss: 0.1419\t validation accuracy: 0.9689\n",
      "iteration number: 1671\t training loss: 0.0772\tvalidation loss: 0.1422\t validation accuracy: 0.9689\n",
      "iteration number: 1672\t training loss: 0.0769\tvalidation loss: 0.1411\t validation accuracy: 0.9689\n",
      "iteration number: 1673\t training loss: 0.0766\tvalidation loss: 0.1407\t validation accuracy: 0.9689\n",
      "iteration number: 1674\t training loss: 0.0761\tvalidation loss: 0.1413\t validation accuracy: 0.9644\n",
      "iteration number: 1675\t training loss: 0.0764\tvalidation loss: 0.1404\t validation accuracy: 0.9667\n",
      "iteration number: 1676\t training loss: 0.0763\tvalidation loss: 0.1405\t validation accuracy: 0.9667\n",
      "iteration number: 1677\t training loss: 0.0756\tvalidation loss: 0.1391\t validation accuracy: 0.9689\n",
      "iteration number: 1678\t training loss: 0.0757\tvalidation loss: 0.1378\t validation accuracy: 0.9711\n",
      "iteration number: 1679\t training loss: 0.0760\tvalidation loss: 0.1390\t validation accuracy: 0.9689\n",
      "iteration number: 1680\t training loss: 0.0762\tvalidation loss: 0.1402\t validation accuracy: 0.9667\n",
      "iteration number: 1681\t training loss: 0.0791\tvalidation loss: 0.1408\t validation accuracy: 0.9667\n",
      "iteration number: 1682\t training loss: 0.0768\tvalidation loss: 0.1389\t validation accuracy: 0.9711\n",
      "iteration number: 1683\t training loss: 0.0786\tvalidation loss: 0.1411\t validation accuracy: 0.9667\n",
      "iteration number: 1684\t training loss: 0.0785\tvalidation loss: 0.1393\t validation accuracy: 0.9644\n",
      "iteration number: 1685\t training loss: 0.0776\tvalidation loss: 0.1365\t validation accuracy: 0.9667\n",
      "iteration number: 1686\t training loss: 0.0788\tvalidation loss: 0.1367\t validation accuracy: 0.9711\n",
      "iteration number: 1687\t training loss: 0.0791\tvalidation loss: 0.1376\t validation accuracy: 0.9667\n",
      "iteration number: 1688\t training loss: 0.0777\tvalidation loss: 0.1387\t validation accuracy: 0.9667\n",
      "iteration number: 1689\t training loss: 0.0762\tvalidation loss: 0.1381\t validation accuracy: 0.9667\n",
      "iteration number: 1690\t training loss: 0.0750\tvalidation loss: 0.1380\t validation accuracy: 0.9667\n",
      "iteration number: 1691\t training loss: 0.0751\tvalidation loss: 0.1386\t validation accuracy: 0.9644\n",
      "iteration number: 1692\t training loss: 0.0753\tvalidation loss: 0.1392\t validation accuracy: 0.9644\n",
      "iteration number: 1693\t training loss: 0.0756\tvalidation loss: 0.1394\t validation accuracy: 0.9644\n",
      "iteration number: 1694\t training loss: 0.0754\tvalidation loss: 0.1379\t validation accuracy: 0.9689\n",
      "iteration number: 1695\t training loss: 0.0753\tvalidation loss: 0.1371\t validation accuracy: 0.9689\n",
      "iteration number: 1696\t training loss: 0.0749\tvalidation loss: 0.1373\t validation accuracy: 0.9689\n",
      "iteration number: 1697\t training loss: 0.0760\tvalidation loss: 0.1385\t validation accuracy: 0.9689\n",
      "iteration number: 1698\t training loss: 0.0757\tvalidation loss: 0.1407\t validation accuracy: 0.9644\n",
      "iteration number: 1699\t training loss: 0.0750\tvalidation loss: 0.1390\t validation accuracy: 0.9644\n",
      "iteration number: 1700\t training loss: 0.0750\tvalidation loss: 0.1385\t validation accuracy: 0.9644\n",
      "iteration number: 1701\t training loss: 0.0747\tvalidation loss: 0.1378\t validation accuracy: 0.9644\n",
      "iteration number: 1702\t training loss: 0.0744\tvalidation loss: 0.1364\t validation accuracy: 0.9711\n",
      "iteration number: 1703\t training loss: 0.0748\tvalidation loss: 0.1349\t validation accuracy: 0.9711\n",
      "iteration number: 1704\t training loss: 0.0746\tvalidation loss: 0.1356\t validation accuracy: 0.9689\n",
      "iteration number: 1705\t training loss: 0.0745\tvalidation loss: 0.1370\t validation accuracy: 0.9667\n",
      "iteration number: 1706\t training loss: 0.0774\tvalidation loss: 0.1450\t validation accuracy: 0.9644\n",
      "iteration number: 1707\t training loss: 0.0749\tvalidation loss: 0.1393\t validation accuracy: 0.9644\n",
      "iteration number: 1708\t training loss: 0.0753\tvalidation loss: 0.1379\t validation accuracy: 0.9689\n",
      "iteration number: 1709\t training loss: 0.0747\tvalidation loss: 0.1367\t validation accuracy: 0.9689\n",
      "iteration number: 1710\t training loss: 0.0747\tvalidation loss: 0.1370\t validation accuracy: 0.9689\n",
      "iteration number: 1711\t training loss: 0.0748\tvalidation loss: 0.1378\t validation accuracy: 0.9667\n",
      "iteration number: 1712\t training loss: 0.0740\tvalidation loss: 0.1377\t validation accuracy: 0.9689\n",
      "iteration number: 1713\t training loss: 0.0743\tvalidation loss: 0.1377\t validation accuracy: 0.9689\n",
      "iteration number: 1714\t training loss: 0.0737\tvalidation loss: 0.1379\t validation accuracy: 0.9689\n",
      "iteration number: 1715\t training loss: 0.0742\tvalidation loss: 0.1396\t validation accuracy: 0.9667\n",
      "iteration number: 1716\t training loss: 0.0740\tvalidation loss: 0.1371\t validation accuracy: 0.9689\n",
      "iteration number: 1717\t training loss: 0.0738\tvalidation loss: 0.1366\t validation accuracy: 0.9689\n",
      "iteration number: 1718\t training loss: 0.0738\tvalidation loss: 0.1363\t validation accuracy: 0.9689\n",
      "iteration number: 1719\t training loss: 0.0742\tvalidation loss: 0.1370\t validation accuracy: 0.9689\n",
      "iteration number: 1720\t training loss: 0.0747\tvalidation loss: 0.1361\t validation accuracy: 0.9689\n",
      "iteration number: 1721\t training loss: 0.0739\tvalidation loss: 0.1380\t validation accuracy: 0.9689\n",
      "iteration number: 1722\t training loss: 0.0738\tvalidation loss: 0.1379\t validation accuracy: 0.9667\n",
      "iteration number: 1723\t training loss: 0.0747\tvalidation loss: 0.1378\t validation accuracy: 0.9689\n",
      "iteration number: 1724\t training loss: 0.0751\tvalidation loss: 0.1378\t validation accuracy: 0.9689\n",
      "iteration number: 1725\t training loss: 0.0755\tvalidation loss: 0.1377\t validation accuracy: 0.9689\n",
      "iteration number: 1726\t training loss: 0.0753\tvalidation loss: 0.1374\t validation accuracy: 0.9689\n",
      "iteration number: 1727\t training loss: 0.0752\tvalidation loss: 0.1365\t validation accuracy: 0.9733\n",
      "iteration number: 1728\t training loss: 0.0759\tvalidation loss: 0.1369\t validation accuracy: 0.9733\n",
      "iteration number: 1729\t training loss: 0.0746\tvalidation loss: 0.1349\t validation accuracy: 0.9756\n",
      "iteration number: 1730\t training loss: 0.0743\tvalidation loss: 0.1353\t validation accuracy: 0.9711\n",
      "iteration number: 1731\t training loss: 0.0755\tvalidation loss: 0.1350\t validation accuracy: 0.9756\n",
      "iteration number: 1732\t training loss: 0.0744\tvalidation loss: 0.1353\t validation accuracy: 0.9733\n",
      "iteration number: 1733\t training loss: 0.0736\tvalidation loss: 0.1360\t validation accuracy: 0.9711\n",
      "iteration number: 1734\t training loss: 0.0736\tvalidation loss: 0.1357\t validation accuracy: 0.9733\n",
      "iteration number: 1735\t training loss: 0.0736\tvalidation loss: 0.1355\t validation accuracy: 0.9733\n",
      "iteration number: 1736\t training loss: 0.0732\tvalidation loss: 0.1359\t validation accuracy: 0.9711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1737\t training loss: 0.0737\tvalidation loss: 0.1361\t validation accuracy: 0.9689\n",
      "iteration number: 1738\t training loss: 0.0739\tvalidation loss: 0.1360\t validation accuracy: 0.9711\n",
      "iteration number: 1739\t training loss: 0.0731\tvalidation loss: 0.1353\t validation accuracy: 0.9711\n",
      "iteration number: 1740\t training loss: 0.0733\tvalidation loss: 0.1346\t validation accuracy: 0.9689\n",
      "iteration number: 1741\t training loss: 0.0731\tvalidation loss: 0.1350\t validation accuracy: 0.9689\n",
      "iteration number: 1742\t training loss: 0.0732\tvalidation loss: 0.1345\t validation accuracy: 0.9689\n",
      "iteration number: 1743\t training loss: 0.0732\tvalidation loss: 0.1334\t validation accuracy: 0.9711\n",
      "iteration number: 1744\t training loss: 0.0726\tvalidation loss: 0.1341\t validation accuracy: 0.9689\n",
      "iteration number: 1745\t training loss: 0.0727\tvalidation loss: 0.1363\t validation accuracy: 0.9667\n",
      "iteration number: 1746\t training loss: 0.0727\tvalidation loss: 0.1352\t validation accuracy: 0.9667\n",
      "iteration number: 1747\t training loss: 0.0725\tvalidation loss: 0.1350\t validation accuracy: 0.9667\n",
      "iteration number: 1748\t training loss: 0.0724\tvalidation loss: 0.1345\t validation accuracy: 0.9689\n",
      "iteration number: 1749\t training loss: 0.0724\tvalidation loss: 0.1351\t validation accuracy: 0.9689\n",
      "iteration number: 1750\t training loss: 0.0723\tvalidation loss: 0.1357\t validation accuracy: 0.9667\n",
      "iteration number: 1751\t training loss: 0.0733\tvalidation loss: 0.1359\t validation accuracy: 0.9667\n",
      "iteration number: 1752\t training loss: 0.0723\tvalidation loss: 0.1347\t validation accuracy: 0.9689\n",
      "iteration number: 1753\t training loss: 0.0726\tvalidation loss: 0.1346\t validation accuracy: 0.9689\n",
      "iteration number: 1754\t training loss: 0.0731\tvalidation loss: 0.1334\t validation accuracy: 0.9689\n",
      "iteration number: 1755\t training loss: 0.0731\tvalidation loss: 0.1338\t validation accuracy: 0.9711\n",
      "iteration number: 1756\t training loss: 0.0725\tvalidation loss: 0.1341\t validation accuracy: 0.9667\n",
      "iteration number: 1757\t training loss: 0.0719\tvalidation loss: 0.1348\t validation accuracy: 0.9667\n",
      "iteration number: 1758\t training loss: 0.0725\tvalidation loss: 0.1376\t validation accuracy: 0.9667\n",
      "iteration number: 1759\t training loss: 0.0724\tvalidation loss: 0.1360\t validation accuracy: 0.9689\n",
      "iteration number: 1760\t training loss: 0.0722\tvalidation loss: 0.1361\t validation accuracy: 0.9689\n",
      "iteration number: 1761\t training loss: 0.0726\tvalidation loss: 0.1367\t validation accuracy: 0.9667\n",
      "iteration number: 1762\t training loss: 0.0735\tvalidation loss: 0.1363\t validation accuracy: 0.9667\n",
      "iteration number: 1763\t training loss: 0.0725\tvalidation loss: 0.1359\t validation accuracy: 0.9667\n",
      "iteration number: 1764\t training loss: 0.0724\tvalidation loss: 0.1373\t validation accuracy: 0.9667\n",
      "iteration number: 1765\t training loss: 0.0722\tvalidation loss: 0.1366\t validation accuracy: 0.9689\n",
      "iteration number: 1766\t training loss: 0.0717\tvalidation loss: 0.1368\t validation accuracy: 0.9667\n",
      "iteration number: 1767\t training loss: 0.0721\tvalidation loss: 0.1379\t validation accuracy: 0.9667\n",
      "iteration number: 1768\t training loss: 0.0728\tvalidation loss: 0.1406\t validation accuracy: 0.9689\n",
      "iteration number: 1769\t training loss: 0.0742\tvalidation loss: 0.1434\t validation accuracy: 0.9689\n",
      "iteration number: 1770\t training loss: 0.0727\tvalidation loss: 0.1405\t validation accuracy: 0.9667\n",
      "iteration number: 1771\t training loss: 0.0740\tvalidation loss: 0.1433\t validation accuracy: 0.9667\n",
      "iteration number: 1772\t training loss: 0.0752\tvalidation loss: 0.1450\t validation accuracy: 0.9644\n",
      "iteration number: 1773\t training loss: 0.0733\tvalidation loss: 0.1419\t validation accuracy: 0.9644\n",
      "iteration number: 1774\t training loss: 0.0752\tvalidation loss: 0.1455\t validation accuracy: 0.9667\n",
      "iteration number: 1775\t training loss: 0.0736\tvalidation loss: 0.1413\t validation accuracy: 0.9667\n",
      "iteration number: 1776\t training loss: 0.0779\tvalidation loss: 0.1492\t validation accuracy: 0.9600\n",
      "iteration number: 1777\t training loss: 0.0786\tvalidation loss: 0.1500\t validation accuracy: 0.9578\n",
      "iteration number: 1778\t training loss: 0.0754\tvalidation loss: 0.1451\t validation accuracy: 0.9644\n",
      "iteration number: 1779\t training loss: 0.0730\tvalidation loss: 0.1411\t validation accuracy: 0.9667\n",
      "iteration number: 1780\t training loss: 0.0720\tvalidation loss: 0.1365\t validation accuracy: 0.9689\n",
      "iteration number: 1781\t training loss: 0.0727\tvalidation loss: 0.1385\t validation accuracy: 0.9667\n",
      "iteration number: 1782\t training loss: 0.0720\tvalidation loss: 0.1381\t validation accuracy: 0.9667\n",
      "iteration number: 1783\t training loss: 0.0709\tvalidation loss: 0.1370\t validation accuracy: 0.9689\n",
      "iteration number: 1784\t training loss: 0.0708\tvalidation loss: 0.1364\t validation accuracy: 0.9689\n",
      "iteration number: 1785\t training loss: 0.0717\tvalidation loss: 0.1364\t validation accuracy: 0.9689\n",
      "iteration number: 1786\t training loss: 0.0719\tvalidation loss: 0.1375\t validation accuracy: 0.9689\n",
      "iteration number: 1787\t training loss: 0.0715\tvalidation loss: 0.1376\t validation accuracy: 0.9667\n",
      "iteration number: 1788\t training loss: 0.0720\tvalidation loss: 0.1381\t validation accuracy: 0.9667\n",
      "iteration number: 1789\t training loss: 0.0710\tvalidation loss: 0.1357\t validation accuracy: 0.9689\n",
      "iteration number: 1790\t training loss: 0.0709\tvalidation loss: 0.1347\t validation accuracy: 0.9689\n",
      "iteration number: 1791\t training loss: 0.0710\tvalidation loss: 0.1354\t validation accuracy: 0.9689\n",
      "iteration number: 1792\t training loss: 0.0711\tvalidation loss: 0.1367\t validation accuracy: 0.9689\n",
      "iteration number: 1793\t training loss: 0.0705\tvalidation loss: 0.1354\t validation accuracy: 0.9689\n",
      "iteration number: 1794\t training loss: 0.0713\tvalidation loss: 0.1367\t validation accuracy: 0.9689\n",
      "iteration number: 1795\t training loss: 0.0713\tvalidation loss: 0.1364\t validation accuracy: 0.9689\n",
      "iteration number: 1796\t training loss: 0.0705\tvalidation loss: 0.1359\t validation accuracy: 0.9689\n",
      "iteration number: 1797\t training loss: 0.0722\tvalidation loss: 0.1348\t validation accuracy: 0.9711\n",
      "iteration number: 1798\t training loss: 0.0723\tvalidation loss: 0.1354\t validation accuracy: 0.9667\n",
      "iteration number: 1799\t training loss: 0.0725\tvalidation loss: 0.1365\t validation accuracy: 0.9667\n",
      "iteration number: 1800\t training loss: 0.0707\tvalidation loss: 0.1356\t validation accuracy: 0.9711\n",
      "iteration number: 1801\t training loss: 0.0705\tvalidation loss: 0.1382\t validation accuracy: 0.9689\n",
      "iteration number: 1802\t training loss: 0.0717\tvalidation loss: 0.1414\t validation accuracy: 0.9667\n",
      "iteration number: 1803\t training loss: 0.0703\tvalidation loss: 0.1376\t validation accuracy: 0.9667\n",
      "iteration number: 1804\t training loss: 0.0701\tvalidation loss: 0.1361\t validation accuracy: 0.9689\n",
      "iteration number: 1805\t training loss: 0.0703\tvalidation loss: 0.1365\t validation accuracy: 0.9689\n",
      "iteration number: 1806\t training loss: 0.0703\tvalidation loss: 0.1369\t validation accuracy: 0.9689\n",
      "iteration number: 1807\t training loss: 0.0711\tvalidation loss: 0.1377\t validation accuracy: 0.9689\n",
      "iteration number: 1808\t training loss: 0.0723\tvalidation loss: 0.1392\t validation accuracy: 0.9689\n",
      "iteration number: 1809\t training loss: 0.0720\tvalidation loss: 0.1384\t validation accuracy: 0.9689\n",
      "iteration number: 1810\t training loss: 0.0707\tvalidation loss: 0.1385\t validation accuracy: 0.9711\n",
      "iteration number: 1811\t training loss: 0.0702\tvalidation loss: 0.1372\t validation accuracy: 0.9711\n",
      "iteration number: 1812\t training loss: 0.0698\tvalidation loss: 0.1364\t validation accuracy: 0.9711\n",
      "iteration number: 1813\t training loss: 0.0697\tvalidation loss: 0.1358\t validation accuracy: 0.9711\n",
      "iteration number: 1814\t training loss: 0.0695\tvalidation loss: 0.1348\t validation accuracy: 0.9689\n",
      "iteration number: 1815\t training loss: 0.0696\tvalidation loss: 0.1347\t validation accuracy: 0.9689\n",
      "iteration number: 1816\t training loss: 0.0695\tvalidation loss: 0.1368\t validation accuracy: 0.9667\n",
      "iteration number: 1817\t training loss: 0.0696\tvalidation loss: 0.1379\t validation accuracy: 0.9667\n",
      "iteration number: 1818\t training loss: 0.0693\tvalidation loss: 0.1371\t validation accuracy: 0.9689\n",
      "iteration number: 1819\t training loss: 0.0695\tvalidation loss: 0.1380\t validation accuracy: 0.9689\n",
      "iteration number: 1820\t training loss: 0.0705\tvalidation loss: 0.1385\t validation accuracy: 0.9689\n",
      "iteration number: 1821\t training loss: 0.0727\tvalidation loss: 0.1392\t validation accuracy: 0.9689\n",
      "iteration number: 1822\t training loss: 0.0738\tvalidation loss: 0.1410\t validation accuracy: 0.9644\n",
      "iteration number: 1823\t training loss: 0.0729\tvalidation loss: 0.1406\t validation accuracy: 0.9644\n",
      "iteration number: 1824\t training loss: 0.0713\tvalidation loss: 0.1378\t validation accuracy: 0.9667\n",
      "iteration number: 1825\t training loss: 0.0705\tvalidation loss: 0.1360\t validation accuracy: 0.9689\n",
      "iteration number: 1826\t training loss: 0.0690\tvalidation loss: 0.1365\t validation accuracy: 0.9667\n",
      "iteration number: 1827\t training loss: 0.0692\tvalidation loss: 0.1364\t validation accuracy: 0.9667\n",
      "iteration number: 1828\t training loss: 0.0687\tvalidation loss: 0.1368\t validation accuracy: 0.9667\n",
      "iteration number: 1829\t training loss: 0.0692\tvalidation loss: 0.1372\t validation accuracy: 0.9689\n",
      "iteration number: 1830\t training loss: 0.0695\tvalidation loss: 0.1379\t validation accuracy: 0.9711\n",
      "iteration number: 1831\t training loss: 0.0689\tvalidation loss: 0.1374\t validation accuracy: 0.9689\n",
      "iteration number: 1832\t training loss: 0.0695\tvalidation loss: 0.1384\t validation accuracy: 0.9689\n",
      "iteration number: 1833\t training loss: 0.0698\tvalidation loss: 0.1378\t validation accuracy: 0.9689\n",
      "iteration number: 1834\t training loss: 0.0701\tvalidation loss: 0.1375\t validation accuracy: 0.9689\n",
      "iteration number: 1835\t training loss: 0.0693\tvalidation loss: 0.1364\t validation accuracy: 0.9689\n",
      "iteration number: 1836\t training loss: 0.0686\tvalidation loss: 0.1346\t validation accuracy: 0.9689\n",
      "iteration number: 1837\t training loss: 0.0689\tvalidation loss: 0.1328\t validation accuracy: 0.9711\n",
      "iteration number: 1838\t training loss: 0.0686\tvalidation loss: 0.1321\t validation accuracy: 0.9733\n",
      "iteration number: 1839\t training loss: 0.0686\tvalidation loss: 0.1326\t validation accuracy: 0.9733\n",
      "iteration number: 1840\t training loss: 0.0684\tvalidation loss: 0.1325\t validation accuracy: 0.9733\n",
      "iteration number: 1841\t training loss: 0.0681\tvalidation loss: 0.1332\t validation accuracy: 0.9711\n",
      "iteration number: 1842\t training loss: 0.0683\tvalidation loss: 0.1336\t validation accuracy: 0.9689\n",
      "iteration number: 1843\t training loss: 0.0692\tvalidation loss: 0.1346\t validation accuracy: 0.9667\n",
      "iteration number: 1844\t training loss: 0.0688\tvalidation loss: 0.1357\t validation accuracy: 0.9667\n",
      "iteration number: 1845\t training loss: 0.0681\tvalidation loss: 0.1328\t validation accuracy: 0.9689\n",
      "iteration number: 1846\t training loss: 0.0685\tvalidation loss: 0.1334\t validation accuracy: 0.9667\n",
      "iteration number: 1847\t training loss: 0.0691\tvalidation loss: 0.1349\t validation accuracy: 0.9667\n",
      "iteration number: 1848\t training loss: 0.0693\tvalidation loss: 0.1352\t validation accuracy: 0.9667\n",
      "iteration number: 1849\t training loss: 0.0701\tvalidation loss: 0.1378\t validation accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1850\t training loss: 0.0683\tvalidation loss: 0.1353\t validation accuracy: 0.9667\n",
      "iteration number: 1851\t training loss: 0.0680\tvalidation loss: 0.1347\t validation accuracy: 0.9667\n",
      "iteration number: 1852\t training loss: 0.0676\tvalidation loss: 0.1335\t validation accuracy: 0.9667\n",
      "iteration number: 1853\t training loss: 0.0690\tvalidation loss: 0.1328\t validation accuracy: 0.9689\n",
      "iteration number: 1854\t training loss: 0.0685\tvalidation loss: 0.1336\t validation accuracy: 0.9689\n",
      "iteration number: 1855\t training loss: 0.0686\tvalidation loss: 0.1334\t validation accuracy: 0.9689\n",
      "iteration number: 1856\t training loss: 0.0674\tvalidation loss: 0.1348\t validation accuracy: 0.9667\n",
      "iteration number: 1857\t training loss: 0.0705\tvalidation loss: 0.1386\t validation accuracy: 0.9644\n",
      "iteration number: 1858\t training loss: 0.0676\tvalidation loss: 0.1349\t validation accuracy: 0.9667\n",
      "iteration number: 1859\t training loss: 0.0674\tvalidation loss: 0.1336\t validation accuracy: 0.9667\n",
      "iteration number: 1860\t training loss: 0.0671\tvalidation loss: 0.1327\t validation accuracy: 0.9689\n",
      "iteration number: 1861\t training loss: 0.0672\tvalidation loss: 0.1324\t validation accuracy: 0.9689\n",
      "iteration number: 1862\t training loss: 0.0677\tvalidation loss: 0.1340\t validation accuracy: 0.9667\n",
      "iteration number: 1863\t training loss: 0.0679\tvalidation loss: 0.1326\t validation accuracy: 0.9689\n",
      "iteration number: 1864\t training loss: 0.0685\tvalidation loss: 0.1337\t validation accuracy: 0.9689\n",
      "iteration number: 1865\t training loss: 0.0682\tvalidation loss: 0.1322\t validation accuracy: 0.9689\n",
      "iteration number: 1866\t training loss: 0.0677\tvalidation loss: 0.1308\t validation accuracy: 0.9689\n",
      "iteration number: 1867\t training loss: 0.0692\tvalidation loss: 0.1314\t validation accuracy: 0.9689\n",
      "iteration number: 1868\t training loss: 0.0702\tvalidation loss: 0.1319\t validation accuracy: 0.9711\n",
      "iteration number: 1869\t training loss: 0.0682\tvalidation loss: 0.1316\t validation accuracy: 0.9689\n",
      "iteration number: 1870\t training loss: 0.0693\tvalidation loss: 0.1322\t validation accuracy: 0.9689\n",
      "iteration number: 1871\t training loss: 0.0682\tvalidation loss: 0.1322\t validation accuracy: 0.9667\n",
      "iteration number: 1872\t training loss: 0.0697\tvalidation loss: 0.1345\t validation accuracy: 0.9667\n",
      "iteration number: 1873\t training loss: 0.0705\tvalidation loss: 0.1381\t validation accuracy: 0.9644\n",
      "iteration number: 1874\t training loss: 0.0703\tvalidation loss: 0.1372\t validation accuracy: 0.9667\n",
      "iteration number: 1875\t training loss: 0.0710\tvalidation loss: 0.1389\t validation accuracy: 0.9600\n",
      "iteration number: 1876\t training loss: 0.0684\tvalidation loss: 0.1337\t validation accuracy: 0.9667\n",
      "iteration number: 1877\t training loss: 0.0681\tvalidation loss: 0.1338\t validation accuracy: 0.9689\n",
      "iteration number: 1878\t training loss: 0.0682\tvalidation loss: 0.1340\t validation accuracy: 0.9667\n",
      "iteration number: 1879\t training loss: 0.0680\tvalidation loss: 0.1346\t validation accuracy: 0.9644\n",
      "iteration number: 1880\t training loss: 0.0671\tvalidation loss: 0.1335\t validation accuracy: 0.9667\n",
      "iteration number: 1881\t training loss: 0.0688\tvalidation loss: 0.1372\t validation accuracy: 0.9667\n",
      "iteration number: 1882\t training loss: 0.0698\tvalidation loss: 0.1399\t validation accuracy: 0.9667\n",
      "iteration number: 1883\t training loss: 0.0692\tvalidation loss: 0.1387\t validation accuracy: 0.9667\n",
      "iteration number: 1884\t training loss: 0.0713\tvalidation loss: 0.1427\t validation accuracy: 0.9600\n",
      "iteration number: 1885\t training loss: 0.0689\tvalidation loss: 0.1386\t validation accuracy: 0.9667\n",
      "iteration number: 1886\t training loss: 0.0686\tvalidation loss: 0.1372\t validation accuracy: 0.9667\n",
      "iteration number: 1887\t training loss: 0.0671\tvalidation loss: 0.1329\t validation accuracy: 0.9689\n",
      "iteration number: 1888\t training loss: 0.0679\tvalidation loss: 0.1321\t validation accuracy: 0.9689\n",
      "iteration number: 1889\t training loss: 0.0678\tvalidation loss: 0.1324\t validation accuracy: 0.9711\n",
      "iteration number: 1890\t training loss: 0.0677\tvalidation loss: 0.1333\t validation accuracy: 0.9711\n",
      "iteration number: 1891\t training loss: 0.0676\tvalidation loss: 0.1327\t validation accuracy: 0.9711\n",
      "iteration number: 1892\t training loss: 0.0676\tvalidation loss: 0.1333\t validation accuracy: 0.9667\n",
      "iteration number: 1893\t training loss: 0.0675\tvalidation loss: 0.1330\t validation accuracy: 0.9667\n",
      "iteration number: 1894\t training loss: 0.0671\tvalidation loss: 0.1331\t validation accuracy: 0.9689\n",
      "iteration number: 1895\t training loss: 0.0669\tvalidation loss: 0.1323\t validation accuracy: 0.9733\n",
      "iteration number: 1896\t training loss: 0.0669\tvalidation loss: 0.1319\t validation accuracy: 0.9733\n",
      "iteration number: 1897\t training loss: 0.0673\tvalidation loss: 0.1314\t validation accuracy: 0.9711\n",
      "iteration number: 1898\t training loss: 0.0667\tvalidation loss: 0.1320\t validation accuracy: 0.9667\n",
      "iteration number: 1899\t training loss: 0.0666\tvalidation loss: 0.1322\t validation accuracy: 0.9711\n",
      "iteration number: 1900\t training loss: 0.0677\tvalidation loss: 0.1352\t validation accuracy: 0.9689\n",
      "iteration number: 1901\t training loss: 0.0684\tvalidation loss: 0.1370\t validation accuracy: 0.9689\n",
      "iteration number: 1902\t training loss: 0.0708\tvalidation loss: 0.1425\t validation accuracy: 0.9600\n",
      "iteration number: 1903\t training loss: 0.0673\tvalidation loss: 0.1363\t validation accuracy: 0.9667\n",
      "iteration number: 1904\t training loss: 0.0658\tvalidation loss: 0.1337\t validation accuracy: 0.9711\n",
      "iteration number: 1905\t training loss: 0.0658\tvalidation loss: 0.1346\t validation accuracy: 0.9711\n",
      "iteration number: 1906\t training loss: 0.0668\tvalidation loss: 0.1376\t validation accuracy: 0.9667\n",
      "iteration number: 1907\t training loss: 0.0669\tvalidation loss: 0.1381\t validation accuracy: 0.9644\n",
      "iteration number: 1908\t training loss: 0.0655\tvalidation loss: 0.1349\t validation accuracy: 0.9689\n",
      "iteration number: 1909\t training loss: 0.0658\tvalidation loss: 0.1355\t validation accuracy: 0.9689\n",
      "iteration number: 1910\t training loss: 0.0662\tvalidation loss: 0.1365\t validation accuracy: 0.9689\n",
      "iteration number: 1911\t training loss: 0.0662\tvalidation loss: 0.1373\t validation accuracy: 0.9667\n",
      "iteration number: 1912\t training loss: 0.0662\tvalidation loss: 0.1375\t validation accuracy: 0.9644\n",
      "iteration number: 1913\t training loss: 0.0666\tvalidation loss: 0.1369\t validation accuracy: 0.9644\n",
      "iteration number: 1914\t training loss: 0.0675\tvalidation loss: 0.1367\t validation accuracy: 0.9622\n",
      "iteration number: 1915\t training loss: 0.0669\tvalidation loss: 0.1345\t validation accuracy: 0.9667\n",
      "iteration number: 1916\t training loss: 0.0675\tvalidation loss: 0.1350\t validation accuracy: 0.9644\n",
      "iteration number: 1917\t training loss: 0.0672\tvalidation loss: 0.1356\t validation accuracy: 0.9622\n",
      "iteration number: 1918\t training loss: 0.0665\tvalidation loss: 0.1355\t validation accuracy: 0.9644\n",
      "iteration number: 1919\t training loss: 0.0664\tvalidation loss: 0.1347\t validation accuracy: 0.9644\n",
      "iteration number: 1920\t training loss: 0.0660\tvalidation loss: 0.1342\t validation accuracy: 0.9667\n",
      "iteration number: 1921\t training loss: 0.0660\tvalidation loss: 0.1350\t validation accuracy: 0.9644\n",
      "iteration number: 1922\t training loss: 0.0668\tvalidation loss: 0.1365\t validation accuracy: 0.9667\n",
      "iteration number: 1923\t training loss: 0.0666\tvalidation loss: 0.1377\t validation accuracy: 0.9667\n",
      "iteration number: 1924\t training loss: 0.0657\tvalidation loss: 0.1367\t validation accuracy: 0.9667\n",
      "iteration number: 1925\t training loss: 0.0652\tvalidation loss: 0.1357\t validation accuracy: 0.9667\n",
      "iteration number: 1926\t training loss: 0.0653\tvalidation loss: 0.1355\t validation accuracy: 0.9667\n",
      "iteration number: 1927\t training loss: 0.0655\tvalidation loss: 0.1366\t validation accuracy: 0.9667\n",
      "iteration number: 1928\t training loss: 0.0661\tvalidation loss: 0.1372\t validation accuracy: 0.9667\n",
      "iteration number: 1929\t training loss: 0.0655\tvalidation loss: 0.1374\t validation accuracy: 0.9644\n",
      "iteration number: 1930\t training loss: 0.0656\tvalidation loss: 0.1367\t validation accuracy: 0.9644\n",
      "iteration number: 1931\t training loss: 0.0663\tvalidation loss: 0.1382\t validation accuracy: 0.9644\n",
      "iteration number: 1932\t training loss: 0.0669\tvalidation loss: 0.1401\t validation accuracy: 0.9600\n",
      "iteration number: 1933\t training loss: 0.0655\tvalidation loss: 0.1376\t validation accuracy: 0.9600\n",
      "iteration number: 1934\t training loss: 0.0654\tvalidation loss: 0.1367\t validation accuracy: 0.9622\n",
      "iteration number: 1935\t training loss: 0.0655\tvalidation loss: 0.1380\t validation accuracy: 0.9578\n",
      "iteration number: 1936\t training loss: 0.0646\tvalidation loss: 0.1356\t validation accuracy: 0.9644\n",
      "iteration number: 1937\t training loss: 0.0647\tvalidation loss: 0.1358\t validation accuracy: 0.9644\n",
      "iteration number: 1938\t training loss: 0.0656\tvalidation loss: 0.1363\t validation accuracy: 0.9689\n",
      "iteration number: 1939\t training loss: 0.0649\tvalidation loss: 0.1349\t validation accuracy: 0.9689\n",
      "iteration number: 1940\t training loss: 0.0648\tvalidation loss: 0.1337\t validation accuracy: 0.9644\n",
      "iteration number: 1941\t training loss: 0.0655\tvalidation loss: 0.1346\t validation accuracy: 0.9600\n",
      "iteration number: 1942\t training loss: 0.0649\tvalidation loss: 0.1339\t validation accuracy: 0.9600\n",
      "iteration number: 1943\t training loss: 0.0644\tvalidation loss: 0.1332\t validation accuracy: 0.9622\n",
      "iteration number: 1944\t training loss: 0.0642\tvalidation loss: 0.1341\t validation accuracy: 0.9667\n",
      "iteration number: 1945\t training loss: 0.0638\tvalidation loss: 0.1330\t validation accuracy: 0.9689\n",
      "iteration number: 1946\t training loss: 0.0638\tvalidation loss: 0.1327\t validation accuracy: 0.9711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1947\t training loss: 0.0642\tvalidation loss: 0.1329\t validation accuracy: 0.9689\n",
      "iteration number: 1948\t training loss: 0.0644\tvalidation loss: 0.1334\t validation accuracy: 0.9711\n",
      "iteration number: 1949\t training loss: 0.0643\tvalidation loss: 0.1346\t validation accuracy: 0.9667\n",
      "iteration number: 1950\t training loss: 0.0656\tvalidation loss: 0.1383\t validation accuracy: 0.9689\n",
      "iteration number: 1951\t training loss: 0.0667\tvalidation loss: 0.1392\t validation accuracy: 0.9667\n",
      "iteration number: 1952\t training loss: 0.0659\tvalidation loss: 0.1394\t validation accuracy: 0.9644\n",
      "iteration number: 1953\t training loss: 0.0649\tvalidation loss: 0.1358\t validation accuracy: 0.9667\n",
      "iteration number: 1954\t training loss: 0.0653\tvalidation loss: 0.1380\t validation accuracy: 0.9644\n",
      "iteration number: 1955\t training loss: 0.0641\tvalidation loss: 0.1352\t validation accuracy: 0.9644\n",
      "iteration number: 1956\t training loss: 0.0648\tvalidation loss: 0.1352\t validation accuracy: 0.9644\n",
      "iteration number: 1957\t training loss: 0.0655\tvalidation loss: 0.1360\t validation accuracy: 0.9644\n",
      "iteration number: 1958\t training loss: 0.0638\tvalidation loss: 0.1331\t validation accuracy: 0.9689\n",
      "iteration number: 1959\t training loss: 0.0637\tvalidation loss: 0.1321\t validation accuracy: 0.9689\n",
      "iteration number: 1960\t training loss: 0.0645\tvalidation loss: 0.1312\t validation accuracy: 0.9667\n",
      "iteration number: 1961\t training loss: 0.0639\tvalidation loss: 0.1297\t validation accuracy: 0.9711\n",
      "iteration number: 1962\t training loss: 0.0633\tvalidation loss: 0.1312\t validation accuracy: 0.9689\n",
      "iteration number: 1963\t training loss: 0.0629\tvalidation loss: 0.1309\t validation accuracy: 0.9711\n",
      "iteration number: 1964\t training loss: 0.0633\tvalidation loss: 0.1308\t validation accuracy: 0.9711\n",
      "iteration number: 1965\t training loss: 0.0640\tvalidation loss: 0.1310\t validation accuracy: 0.9689\n",
      "iteration number: 1966\t training loss: 0.0645\tvalidation loss: 0.1315\t validation accuracy: 0.9711\n",
      "iteration number: 1967\t training loss: 0.0657\tvalidation loss: 0.1328\t validation accuracy: 0.9644\n",
      "iteration number: 1968\t training loss: 0.0648\tvalidation loss: 0.1323\t validation accuracy: 0.9689\n",
      "iteration number: 1969\t training loss: 0.0658\tvalidation loss: 0.1348\t validation accuracy: 0.9667\n",
      "iteration number: 1970\t training loss: 0.0656\tvalidation loss: 0.1344\t validation accuracy: 0.9667\n",
      "iteration number: 1971\t training loss: 0.0654\tvalidation loss: 0.1342\t validation accuracy: 0.9667\n",
      "iteration number: 1972\t training loss: 0.0640\tvalidation loss: 0.1329\t validation accuracy: 0.9689\n",
      "iteration number: 1973\t training loss: 0.0652\tvalidation loss: 0.1334\t validation accuracy: 0.9644\n",
      "iteration number: 1974\t training loss: 0.0640\tvalidation loss: 0.1322\t validation accuracy: 0.9667\n",
      "iteration number: 1975\t training loss: 0.0627\tvalidation loss: 0.1300\t validation accuracy: 0.9689\n",
      "iteration number: 1976\t training loss: 0.0626\tvalidation loss: 0.1304\t validation accuracy: 0.9711\n",
      "iteration number: 1977\t training loss: 0.0628\tvalidation loss: 0.1298\t validation accuracy: 0.9711\n",
      "iteration number: 1978\t training loss: 0.0629\tvalidation loss: 0.1313\t validation accuracy: 0.9711\n",
      "iteration number: 1979\t training loss: 0.0640\tvalidation loss: 0.1335\t validation accuracy: 0.9711\n",
      "iteration number: 1980\t training loss: 0.0643\tvalidation loss: 0.1339\t validation accuracy: 0.9711\n",
      "iteration number: 1981\t training loss: 0.0635\tvalidation loss: 0.1341\t validation accuracy: 0.9689\n",
      "iteration number: 1982\t training loss: 0.0641\tvalidation loss: 0.1355\t validation accuracy: 0.9689\n",
      "iteration number: 1983\t training loss: 0.0638\tvalidation loss: 0.1345\t validation accuracy: 0.9711\n",
      "iteration number: 1984\t training loss: 0.0645\tvalidation loss: 0.1348\t validation accuracy: 0.9667\n",
      "iteration number: 1985\t training loss: 0.0634\tvalidation loss: 0.1340\t validation accuracy: 0.9689\n",
      "iteration number: 1986\t training loss: 0.0632\tvalidation loss: 0.1314\t validation accuracy: 0.9667\n",
      "iteration number: 1987\t training loss: 0.0629\tvalidation loss: 0.1310\t validation accuracy: 0.9667\n",
      "iteration number: 1988\t training loss: 0.0631\tvalidation loss: 0.1303\t validation accuracy: 0.9644\n",
      "iteration number: 1989\t training loss: 0.0628\tvalidation loss: 0.1294\t validation accuracy: 0.9689\n",
      "iteration number: 1990\t training loss: 0.0641\tvalidation loss: 0.1305\t validation accuracy: 0.9689\n",
      "iteration number: 1991\t training loss: 0.0629\tvalidation loss: 0.1290\t validation accuracy: 0.9711\n",
      "iteration number: 1992\t training loss: 0.0635\tvalidation loss: 0.1296\t validation accuracy: 0.9711\n",
      "iteration number: 1993\t training loss: 0.0653\tvalidation loss: 0.1316\t validation accuracy: 0.9689\n",
      "iteration number: 1994\t training loss: 0.0652\tvalidation loss: 0.1306\t validation accuracy: 0.9711\n",
      "iteration number: 1995\t training loss: 0.0645\tvalidation loss: 0.1289\t validation accuracy: 0.9711\n",
      "iteration number: 1996\t training loss: 0.0635\tvalidation loss: 0.1302\t validation accuracy: 0.9733\n",
      "iteration number: 1997\t training loss: 0.0640\tvalidation loss: 0.1295\t validation accuracy: 0.9733\n",
      "iteration number: 1998\t training loss: 0.0636\tvalidation loss: 0.1295\t validation accuracy: 0.9733\n",
      "iteration number: 1999\t training loss: 0.0649\tvalidation loss: 0.1308\t validation accuracy: 0.9711\n",
      "iteration number: 2000\t training loss: 0.0640\tvalidation loss: 0.1298\t validation accuracy: 0.9689\n",
      "iteration number: 2001\t training loss: 0.0633\tvalidation loss: 0.1298\t validation accuracy: 0.9689\n",
      "iteration number: 2002\t training loss: 0.0624\tvalidation loss: 0.1297\t validation accuracy: 0.9689\n",
      "iteration number: 2003\t training loss: 0.0627\tvalidation loss: 0.1304\t validation accuracy: 0.9689\n",
      "iteration number: 2004\t training loss: 0.0621\tvalidation loss: 0.1308\t validation accuracy: 0.9689\n",
      "iteration number: 2005\t training loss: 0.0629\tvalidation loss: 0.1339\t validation accuracy: 0.9667\n",
      "iteration number: 2006\t training loss: 0.0623\tvalidation loss: 0.1314\t validation accuracy: 0.9667\n",
      "iteration number: 2007\t training loss: 0.0613\tvalidation loss: 0.1307\t validation accuracy: 0.9689\n",
      "iteration number: 2008\t training loss: 0.0617\tvalidation loss: 0.1316\t validation accuracy: 0.9667\n",
      "iteration number: 2009\t training loss: 0.0614\tvalidation loss: 0.1311\t validation accuracy: 0.9689\n",
      "iteration number: 2010\t training loss: 0.0613\tvalidation loss: 0.1307\t validation accuracy: 0.9689\n",
      "iteration number: 2011\t training loss: 0.0624\tvalidation loss: 0.1319\t validation accuracy: 0.9711\n",
      "iteration number: 2012\t training loss: 0.0613\tvalidation loss: 0.1313\t validation accuracy: 0.9711\n",
      "iteration number: 2013\t training loss: 0.0619\tvalidation loss: 0.1328\t validation accuracy: 0.9689\n",
      "iteration number: 2014\t training loss: 0.0631\tvalidation loss: 0.1366\t validation accuracy: 0.9667\n",
      "iteration number: 2015\t training loss: 0.0622\tvalidation loss: 0.1346\t validation accuracy: 0.9667\n",
      "iteration number: 2016\t training loss: 0.0619\tvalidation loss: 0.1339\t validation accuracy: 0.9689\n",
      "iteration number: 2017\t training loss: 0.0623\tvalidation loss: 0.1342\t validation accuracy: 0.9689\n",
      "iteration number: 2018\t training loss: 0.0615\tvalidation loss: 0.1313\t validation accuracy: 0.9711\n",
      "iteration number: 2019\t training loss: 0.0617\tvalidation loss: 0.1305\t validation accuracy: 0.9689\n",
      "iteration number: 2020\t training loss: 0.0620\tvalidation loss: 0.1311\t validation accuracy: 0.9711\n",
      "iteration number: 2021\t training loss: 0.0617\tvalidation loss: 0.1299\t validation accuracy: 0.9711\n",
      "iteration number: 2022\t training loss: 0.0615\tvalidation loss: 0.1291\t validation accuracy: 0.9711\n",
      "iteration number: 2023\t training loss: 0.0614\tvalidation loss: 0.1296\t validation accuracy: 0.9711\n",
      "iteration number: 2024\t training loss: 0.0613\tvalidation loss: 0.1295\t validation accuracy: 0.9711\n",
      "iteration number: 2025\t training loss: 0.0614\tvalidation loss: 0.1288\t validation accuracy: 0.9711\n",
      "iteration number: 2026\t training loss: 0.0608\tvalidation loss: 0.1299\t validation accuracy: 0.9711\n",
      "iteration number: 2027\t training loss: 0.0615\tvalidation loss: 0.1300\t validation accuracy: 0.9711\n",
      "iteration number: 2028\t training loss: 0.0616\tvalidation loss: 0.1312\t validation accuracy: 0.9689\n",
      "iteration number: 2029\t training loss: 0.0616\tvalidation loss: 0.1306\t validation accuracy: 0.9689\n",
      "iteration number: 2030\t training loss: 0.0617\tvalidation loss: 0.1305\t validation accuracy: 0.9689\n",
      "iteration number: 2031\t training loss: 0.0609\tvalidation loss: 0.1297\t validation accuracy: 0.9711\n",
      "iteration number: 2032\t training loss: 0.0608\tvalidation loss: 0.1289\t validation accuracy: 0.9711\n",
      "iteration number: 2033\t training loss: 0.0608\tvalidation loss: 0.1287\t validation accuracy: 0.9733\n",
      "iteration number: 2034\t training loss: 0.0609\tvalidation loss: 0.1284\t validation accuracy: 0.9711\n",
      "iteration number: 2035\t training loss: 0.0607\tvalidation loss: 0.1294\t validation accuracy: 0.9667\n",
      "iteration number: 2036\t training loss: 0.0617\tvalidation loss: 0.1322\t validation accuracy: 0.9667\n",
      "iteration number: 2037\t training loss: 0.0618\tvalidation loss: 0.1326\t validation accuracy: 0.9667\n",
      "iteration number: 2038\t training loss: 0.0631\tvalidation loss: 0.1347\t validation accuracy: 0.9689\n",
      "iteration number: 2039\t training loss: 0.0633\tvalidation loss: 0.1371\t validation accuracy: 0.9667\n",
      "iteration number: 2040\t training loss: 0.0633\tvalidation loss: 0.1368\t validation accuracy: 0.9667\n",
      "iteration number: 2041\t training loss: 0.0610\tvalidation loss: 0.1317\t validation accuracy: 0.9689\n",
      "iteration number: 2042\t training loss: 0.0612\tvalidation loss: 0.1320\t validation accuracy: 0.9667\n",
      "iteration number: 2043\t training loss: 0.0613\tvalidation loss: 0.1319\t validation accuracy: 0.9689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2044\t training loss: 0.0607\tvalidation loss: 0.1311\t validation accuracy: 0.9667\n",
      "iteration number: 2045\t training loss: 0.0609\tvalidation loss: 0.1307\t validation accuracy: 0.9689\n",
      "iteration number: 2046\t training loss: 0.0618\tvalidation loss: 0.1328\t validation accuracy: 0.9689\n",
      "iteration number: 2047\t training loss: 0.0610\tvalidation loss: 0.1319\t validation accuracy: 0.9689\n",
      "iteration number: 2048\t training loss: 0.0604\tvalidation loss: 0.1303\t validation accuracy: 0.9689\n",
      "iteration number: 2049\t training loss: 0.0608\tvalidation loss: 0.1286\t validation accuracy: 0.9667\n",
      "iteration number: 2050\t training loss: 0.0606\tvalidation loss: 0.1294\t validation accuracy: 0.9689\n",
      "iteration number: 2051\t training loss: 0.0606\tvalidation loss: 0.1301\t validation accuracy: 0.9689\n",
      "iteration number: 2052\t training loss: 0.0604\tvalidation loss: 0.1297\t validation accuracy: 0.9711\n",
      "iteration number: 2053\t training loss: 0.0604\tvalidation loss: 0.1297\t validation accuracy: 0.9733\n",
      "iteration number: 2054\t training loss: 0.0610\tvalidation loss: 0.1315\t validation accuracy: 0.9711\n",
      "iteration number: 2055\t training loss: 0.0609\tvalidation loss: 0.1303\t validation accuracy: 0.9711\n",
      "iteration number: 2056\t training loss: 0.0601\tvalidation loss: 0.1291\t validation accuracy: 0.9733\n",
      "iteration number: 2057\t training loss: 0.0602\tvalidation loss: 0.1294\t validation accuracy: 0.9733\n",
      "iteration number: 2058\t training loss: 0.0605\tvalidation loss: 0.1314\t validation accuracy: 0.9733\n",
      "iteration number: 2059\t training loss: 0.0608\tvalidation loss: 0.1301\t validation accuracy: 0.9733\n",
      "iteration number: 2060\t training loss: 0.0625\tvalidation loss: 0.1313\t validation accuracy: 0.9711\n",
      "iteration number: 2061\t training loss: 0.0623\tvalidation loss: 0.1310\t validation accuracy: 0.9711\n",
      "iteration number: 2062\t training loss: 0.0637\tvalidation loss: 0.1327\t validation accuracy: 0.9667\n",
      "iteration number: 2063\t training loss: 0.0610\tvalidation loss: 0.1311\t validation accuracy: 0.9711\n",
      "iteration number: 2064\t training loss: 0.0607\tvalidation loss: 0.1304\t validation accuracy: 0.9711\n",
      "iteration number: 2065\t training loss: 0.0611\tvalidation loss: 0.1314\t validation accuracy: 0.9711\n",
      "iteration number: 2066\t training loss: 0.0608\tvalidation loss: 0.1313\t validation accuracy: 0.9667\n",
      "iteration number: 2067\t training loss: 0.0605\tvalidation loss: 0.1311\t validation accuracy: 0.9689\n",
      "iteration number: 2068\t training loss: 0.0602\tvalidation loss: 0.1311\t validation accuracy: 0.9689\n",
      "iteration number: 2069\t training loss: 0.0607\tvalidation loss: 0.1322\t validation accuracy: 0.9711\n",
      "iteration number: 2070\t training loss: 0.0605\tvalidation loss: 0.1310\t validation accuracy: 0.9711\n",
      "iteration number: 2071\t training loss: 0.0606\tvalidation loss: 0.1302\t validation accuracy: 0.9667\n",
      "iteration number: 2072\t training loss: 0.0611\tvalidation loss: 0.1325\t validation accuracy: 0.9689\n",
      "iteration number: 2073\t training loss: 0.0612\tvalidation loss: 0.1336\t validation accuracy: 0.9667\n",
      "iteration number: 2074\t training loss: 0.0609\tvalidation loss: 0.1320\t validation accuracy: 0.9667\n",
      "iteration number: 2075\t training loss: 0.0604\tvalidation loss: 0.1315\t validation accuracy: 0.9667\n",
      "iteration number: 2076\t training loss: 0.0605\tvalidation loss: 0.1320\t validation accuracy: 0.9644\n",
      "iteration number: 2077\t training loss: 0.0611\tvalidation loss: 0.1337\t validation accuracy: 0.9644\n",
      "iteration number: 2078\t training loss: 0.0596\tvalidation loss: 0.1297\t validation accuracy: 0.9667\n",
      "iteration number: 2079\t training loss: 0.0598\tvalidation loss: 0.1292\t validation accuracy: 0.9667\n",
      "iteration number: 2080\t training loss: 0.0603\tvalidation loss: 0.1295\t validation accuracy: 0.9711\n",
      "iteration number: 2081\t training loss: 0.0605\tvalidation loss: 0.1299\t validation accuracy: 0.9711\n",
      "iteration number: 2082\t training loss: 0.0599\tvalidation loss: 0.1291\t validation accuracy: 0.9733\n",
      "iteration number: 2083\t training loss: 0.0597\tvalidation loss: 0.1279\t validation accuracy: 0.9733\n",
      "iteration number: 2084\t training loss: 0.0598\tvalidation loss: 0.1288\t validation accuracy: 0.9733\n",
      "iteration number: 2085\t training loss: 0.0599\tvalidation loss: 0.1299\t validation accuracy: 0.9711\n",
      "iteration number: 2086\t training loss: 0.0602\tvalidation loss: 0.1288\t validation accuracy: 0.9711\n",
      "iteration number: 2087\t training loss: 0.0596\tvalidation loss: 0.1288\t validation accuracy: 0.9711\n",
      "iteration number: 2088\t training loss: 0.0593\tvalidation loss: 0.1289\t validation accuracy: 0.9733\n",
      "iteration number: 2089\t training loss: 0.0592\tvalidation loss: 0.1292\t validation accuracy: 0.9733\n",
      "iteration number: 2090\t training loss: 0.0596\tvalidation loss: 0.1292\t validation accuracy: 0.9733\n",
      "iteration number: 2091\t training loss: 0.0590\tvalidation loss: 0.1293\t validation accuracy: 0.9711\n",
      "iteration number: 2092\t training loss: 0.0589\tvalidation loss: 0.1291\t validation accuracy: 0.9711\n",
      "iteration number: 2093\t training loss: 0.0588\tvalidation loss: 0.1289\t validation accuracy: 0.9733\n",
      "iteration number: 2094\t training loss: 0.0589\tvalidation loss: 0.1293\t validation accuracy: 0.9711\n",
      "iteration number: 2095\t training loss: 0.0587\tvalidation loss: 0.1295\t validation accuracy: 0.9711\n",
      "iteration number: 2096\t training loss: 0.0585\tvalidation loss: 0.1285\t validation accuracy: 0.9711\n",
      "iteration number: 2097\t training loss: 0.0590\tvalidation loss: 0.1297\t validation accuracy: 0.9711\n",
      "iteration number: 2098\t training loss: 0.0590\tvalidation loss: 0.1292\t validation accuracy: 0.9711\n",
      "iteration number: 2099\t training loss: 0.0600\tvalidation loss: 0.1304\t validation accuracy: 0.9689\n",
      "iteration number: 2100\t training loss: 0.0601\tvalidation loss: 0.1301\t validation accuracy: 0.9733\n",
      "iteration number: 2101\t training loss: 0.0607\tvalidation loss: 0.1313\t validation accuracy: 0.9711\n",
      "iteration number: 2102\t training loss: 0.0589\tvalidation loss: 0.1299\t validation accuracy: 0.9733\n",
      "iteration number: 2103\t training loss: 0.0588\tvalidation loss: 0.1281\t validation accuracy: 0.9733\n",
      "iteration number: 2104\t training loss: 0.0592\tvalidation loss: 0.1283\t validation accuracy: 0.9711\n",
      "iteration number: 2105\t training loss: 0.0599\tvalidation loss: 0.1277\t validation accuracy: 0.9733\n",
      "iteration number: 2106\t training loss: 0.0606\tvalidation loss: 0.1280\t validation accuracy: 0.9711\n",
      "iteration number: 2107\t training loss: 0.0594\tvalidation loss: 0.1262\t validation accuracy: 0.9733\n",
      "iteration number: 2108\t training loss: 0.0592\tvalidation loss: 0.1270\t validation accuracy: 0.9733\n",
      "iteration number: 2109\t training loss: 0.0591\tvalidation loss: 0.1280\t validation accuracy: 0.9733\n",
      "iteration number: 2110\t training loss: 0.0591\tvalidation loss: 0.1281\t validation accuracy: 0.9689\n",
      "iteration number: 2111\t training loss: 0.0591\tvalidation loss: 0.1287\t validation accuracy: 0.9689\n",
      "iteration number: 2112\t training loss: 0.0592\tvalidation loss: 0.1298\t validation accuracy: 0.9689\n",
      "iteration number: 2113\t training loss: 0.0591\tvalidation loss: 0.1305\t validation accuracy: 0.9689\n",
      "iteration number: 2114\t training loss: 0.0592\tvalidation loss: 0.1301\t validation accuracy: 0.9667\n",
      "iteration number: 2115\t training loss: 0.0590\tvalidation loss: 0.1296\t validation accuracy: 0.9689\n",
      "iteration number: 2116\t training loss: 0.0587\tvalidation loss: 0.1288\t validation accuracy: 0.9689\n",
      "iteration number: 2117\t training loss: 0.0585\tvalidation loss: 0.1277\t validation accuracy: 0.9711\n",
      "iteration number: 2118\t training loss: 0.0578\tvalidation loss: 0.1278\t validation accuracy: 0.9711\n",
      "iteration number: 2119\t training loss: 0.0590\tvalidation loss: 0.1295\t validation accuracy: 0.9711\n",
      "iteration number: 2120\t training loss: 0.0584\tvalidation loss: 0.1290\t validation accuracy: 0.9667\n",
      "iteration number: 2121\t training loss: 0.0584\tvalidation loss: 0.1292\t validation accuracy: 0.9667\n",
      "iteration number: 2122\t training loss: 0.0578\tvalidation loss: 0.1271\t validation accuracy: 0.9711\n",
      "iteration number: 2123\t training loss: 0.0583\tvalidation loss: 0.1261\t validation accuracy: 0.9733\n",
      "iteration number: 2124\t training loss: 0.0585\tvalidation loss: 0.1268\t validation accuracy: 0.9733\n",
      "iteration number: 2125\t training loss: 0.0582\tvalidation loss: 0.1260\t validation accuracy: 0.9733\n",
      "iteration number: 2126\t training loss: 0.0581\tvalidation loss: 0.1260\t validation accuracy: 0.9733\n",
      "iteration number: 2127\t training loss: 0.0587\tvalidation loss: 0.1267\t validation accuracy: 0.9733\n",
      "iteration number: 2128\t training loss: 0.0585\tvalidation loss: 0.1256\t validation accuracy: 0.9733\n",
      "iteration number: 2129\t training loss: 0.0586\tvalidation loss: 0.1260\t validation accuracy: 0.9733\n",
      "iteration number: 2130\t training loss: 0.0583\tvalidation loss: 0.1252\t validation accuracy: 0.9711\n",
      "iteration number: 2131\t training loss: 0.0578\tvalidation loss: 0.1245\t validation accuracy: 0.9733\n",
      "iteration number: 2132\t training loss: 0.0581\tvalidation loss: 0.1261\t validation accuracy: 0.9733\n",
      "iteration number: 2133\t training loss: 0.0578\tvalidation loss: 0.1266\t validation accuracy: 0.9711\n",
      "iteration number: 2134\t training loss: 0.0593\tvalidation loss: 0.1296\t validation accuracy: 0.9689\n",
      "iteration number: 2135\t training loss: 0.0586\tvalidation loss: 0.1271\t validation accuracy: 0.9711\n",
      "iteration number: 2136\t training loss: 0.0578\tvalidation loss: 0.1263\t validation accuracy: 0.9733\n",
      "iteration number: 2137\t training loss: 0.0579\tvalidation loss: 0.1254\t validation accuracy: 0.9733\n",
      "iteration number: 2138\t training loss: 0.0580\tvalidation loss: 0.1254\t validation accuracy: 0.9733\n",
      "iteration number: 2139\t training loss: 0.0577\tvalidation loss: 0.1249\t validation accuracy: 0.9733\n",
      "iteration number: 2140\t training loss: 0.0573\tvalidation loss: 0.1256\t validation accuracy: 0.9711\n",
      "iteration number: 2141\t training loss: 0.0571\tvalidation loss: 0.1262\t validation accuracy: 0.9711\n",
      "iteration number: 2142\t training loss: 0.0573\tvalidation loss: 0.1263\t validation accuracy: 0.9711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 2143\t training loss: 0.0570\tvalidation loss: 0.1251\t validation accuracy: 0.9711\n",
      "iteration number: 2144\t training loss: 0.0570\tvalidation loss: 0.1258\t validation accuracy: 0.9711\n",
      "iteration number: 2145\t training loss: 0.0575\tvalidation loss: 0.1254\t validation accuracy: 0.9711\n",
      "iteration number: 2146\t training loss: 0.0576\tvalidation loss: 0.1258\t validation accuracy: 0.9689\n",
      "iteration number: 2147\t training loss: 0.0571\tvalidation loss: 0.1251\t validation accuracy: 0.9689\n",
      "iteration number: 2148\t training loss: 0.0569\tvalidation loss: 0.1247\t validation accuracy: 0.9711\n",
      "iteration number: 2149\t training loss: 0.0569\tvalidation loss: 0.1267\t validation accuracy: 0.9689\n",
      "iteration number: 2150\t training loss: 0.0567\tvalidation loss: 0.1261\t validation accuracy: 0.9711\n",
      "iteration number: 2151\t training loss: 0.0579\tvalidation loss: 0.1281\t validation accuracy: 0.9689\n",
      "iteration number: 2152\t training loss: 0.0575\tvalidation loss: 0.1249\t validation accuracy: 0.9733\n",
      "iteration number: 2153\t training loss: 0.0574\tvalidation loss: 0.1253\t validation accuracy: 0.9733\n",
      "iteration number: 2154\t training loss: 0.0577\tvalidation loss: 0.1265\t validation accuracy: 0.9711\n",
      "iteration number: 2155\t training loss: 0.0581\tvalidation loss: 0.1271\t validation accuracy: 0.9711\n",
      "iteration number: 2156\t training loss: 0.0583\tvalidation loss: 0.1271\t validation accuracy: 0.9733\n",
      "iteration number: 2157\t training loss: 0.0578\tvalidation loss: 0.1273\t validation accuracy: 0.9689\n",
      "iteration number: 2158\t training loss: 0.0576\tvalidation loss: 0.1264\t validation accuracy: 0.9711\n",
      "iteration number: 2159\t training loss: 0.0573\tvalidation loss: 0.1280\t validation accuracy: 0.9689\n",
      "iteration number: 2160\t training loss: 0.0585\tvalidation loss: 0.1306\t validation accuracy: 0.9711\n",
      "iteration number: 2161\t training loss: 0.0584\tvalidation loss: 0.1298\t validation accuracy: 0.9711\n",
      "iteration number: 2162\t training loss: 0.0569\tvalidation loss: 0.1283\t validation accuracy: 0.9689\n",
      "iteration number: 2163\t training loss: 0.0569\tvalidation loss: 0.1279\t validation accuracy: 0.9689\n",
      "iteration number: 2164\t training loss: 0.0564\tvalidation loss: 0.1274\t validation accuracy: 0.9689\n",
      "iteration number: 2165\t training loss: 0.0568\tvalidation loss: 0.1295\t validation accuracy: 0.9667\n",
      "iteration number: 2166\t training loss: 0.0572\tvalidation loss: 0.1300\t validation accuracy: 0.9667\n",
      "iteration number: 2167\t training loss: 0.0575\tvalidation loss: 0.1298\t validation accuracy: 0.9644\n",
      "iteration number: 2168\t training loss: 0.0573\tvalidation loss: 0.1307\t validation accuracy: 0.9644\n",
      "iteration number: 2169\t training loss: 0.0563\tvalidation loss: 0.1253\t validation accuracy: 0.9689\n",
      "iteration number: 2170\t training loss: 0.0561\tvalidation loss: 0.1262\t validation accuracy: 0.9711\n",
      "iteration number: 2171\t training loss: 0.0561\tvalidation loss: 0.1257\t validation accuracy: 0.9733\n",
      "iteration number: 2172\t training loss: 0.0561\tvalidation loss: 0.1251\t validation accuracy: 0.9733\n",
      "iteration number: 2173\t training loss: 0.0564\tvalidation loss: 0.1254\t validation accuracy: 0.9711\n",
      "iteration number: 2174\t training loss: 0.0562\tvalidation loss: 0.1257\t validation accuracy: 0.9711\n",
      "iteration number: 2175\t training loss: 0.0565\tvalidation loss: 0.1245\t validation accuracy: 0.9711\n",
      "iteration number: 2176\t training loss: 0.0577\tvalidation loss: 0.1260\t validation accuracy: 0.9711\n",
      "iteration number: 2177\t training loss: 0.0569\tvalidation loss: 0.1260\t validation accuracy: 0.9733\n",
      "iteration number: 2178\t training loss: 0.0569\tvalidation loss: 0.1249\t validation accuracy: 0.9733\n",
      "iteration number: 2179\t training loss: 0.0574\tvalidation loss: 0.1282\t validation accuracy: 0.9711\n",
      "iteration number: 2180\t training loss: 0.0566\tvalidation loss: 0.1270\t validation accuracy: 0.9733\n",
      "iteration number: 2181\t training loss: 0.0561\tvalidation loss: 0.1279\t validation accuracy: 0.9711\n",
      "iteration number: 2182\t training loss: 0.0559\tvalidation loss: 0.1281\t validation accuracy: 0.9711\n",
      "iteration number: 2183\t training loss: 0.0557\tvalidation loss: 0.1285\t validation accuracy: 0.9711\n",
      "iteration number: 2184\t training loss: 0.0559\tvalidation loss: 0.1282\t validation accuracy: 0.9711\n",
      "iteration number: 2185\t training loss: 0.0568\tvalidation loss: 0.1295\t validation accuracy: 0.9689\n",
      "iteration number: 2186\t training loss: 0.0572\tvalidation loss: 0.1298\t validation accuracy: 0.9711\n",
      "iteration number: 2187\t training loss: 0.0565\tvalidation loss: 0.1302\t validation accuracy: 0.9689\n",
      "iteration number: 2188\t training loss: 0.0562\tvalidation loss: 0.1308\t validation accuracy: 0.9689\n",
      "iteration number: 2189\t training loss: 0.0563\tvalidation loss: 0.1299\t validation accuracy: 0.9689\n",
      "iteration number: 2190\t training loss: 0.0556\tvalidation loss: 0.1280\t validation accuracy: 0.9711\n",
      "iteration number: 2191\t training loss: 0.0554\tvalidation loss: 0.1278\t validation accuracy: 0.9711\n",
      "iteration number: 2192\t training loss: 0.0554\tvalidation loss: 0.1279\t validation accuracy: 0.9711\n",
      "iteration number: 2193\t training loss: 0.0552\tvalidation loss: 0.1276\t validation accuracy: 0.9711\n",
      "iteration number: 2194\t training loss: 0.0559\tvalidation loss: 0.1280\t validation accuracy: 0.9711\n",
      "iteration number: 2195\t training loss: 0.0556\tvalidation loss: 0.1275\t validation accuracy: 0.9689\n",
      "iteration number: 2196\t training loss: 0.0564\tvalidation loss: 0.1296\t validation accuracy: 0.9689\n",
      "iteration number: 2197\t training loss: 0.0555\tvalidation loss: 0.1278\t validation accuracy: 0.9711\n",
      "iteration number: 2198\t training loss: 0.0557\tvalidation loss: 0.1272\t validation accuracy: 0.9689\n",
      "iteration number: 2199\t training loss: 0.0556\tvalidation loss: 0.1301\t validation accuracy: 0.9667\n",
      "iteration number: 2200\t training loss: 0.0558\tvalidation loss: 0.1309\t validation accuracy: 0.9667\n",
      "iteration number: 2201\t training loss: 0.0559\tvalidation loss: 0.1307\t validation accuracy: 0.9667\n",
      "iteration number: 2202\t training loss: 0.0559\tvalidation loss: 0.1300\t validation accuracy: 0.9667\n",
      "iteration number: 2203\t training loss: 0.0556\tvalidation loss: 0.1293\t validation accuracy: 0.9689\n",
      "iteration number: 2204\t training loss: 0.0558\tvalidation loss: 0.1295\t validation accuracy: 0.9689\n",
      "iteration number: 2205\t training loss: 0.0562\tvalidation loss: 0.1295\t validation accuracy: 0.9689\n",
      "iteration number: 2206\t training loss: 0.0554\tvalidation loss: 0.1279\t validation accuracy: 0.9689\n",
      "iteration number: 2207\t training loss: 0.0559\tvalidation loss: 0.1279\t validation accuracy: 0.9689\n",
      "iteration number: 2208\t training loss: 0.0558\tvalidation loss: 0.1273\t validation accuracy: 0.9689\n",
      "iteration number: 2209\t training loss: 0.0556\tvalidation loss: 0.1282\t validation accuracy: 0.9689\n",
      "iteration number: 2210\t training loss: 0.0556\tvalidation loss: 0.1286\t validation accuracy: 0.9689\n",
      "iteration number: 2211\t training loss: 0.0552\tvalidation loss: 0.1267\t validation accuracy: 0.9711\n",
      "iteration number: 2212\t training loss: 0.0549\tvalidation loss: 0.1261\t validation accuracy: 0.9711\n",
      "iteration number: 2213\t training loss: 0.0552\tvalidation loss: 0.1259\t validation accuracy: 0.9711\n",
      "iteration number: 2214\t training loss: 0.0548\tvalidation loss: 0.1269\t validation accuracy: 0.9711\n",
      "iteration number: 2215\t training loss: 0.0550\tvalidation loss: 0.1267\t validation accuracy: 0.9711\n",
      "iteration number: 2216\t training loss: 0.0548\tvalidation loss: 0.1266\t validation accuracy: 0.9689\n",
      "iteration number: 2217\t training loss: 0.0553\tvalidation loss: 0.1287\t validation accuracy: 0.9689\n",
      "iteration number: 2218\t training loss: 0.0551\tvalidation loss: 0.1283\t validation accuracy: 0.9689\n",
      "iteration number: 2219\t training loss: 0.0549\tvalidation loss: 0.1278\t validation accuracy: 0.9667\n",
      "iteration number: 2220\t training loss: 0.0558\tvalidation loss: 0.1288\t validation accuracy: 0.9689\n",
      "iteration number: 2221\t training loss: 0.0554\tvalidation loss: 0.1279\t validation accuracy: 0.9689\n",
      "iteration number: 2222\t training loss: 0.0555\tvalidation loss: 0.1279\t validation accuracy: 0.9689\n",
      "iteration number: 2223\t training loss: 0.0550\tvalidation loss: 0.1276\t validation accuracy: 0.9711\n",
      "iteration number: 2224\t training loss: 0.0547\tvalidation loss: 0.1276\t validation accuracy: 0.9711\n",
      "iteration number: 2225\t training loss: 0.0544\tvalidation loss: 0.1270\t validation accuracy: 0.9711\n",
      "iteration number: 2226\t training loss: 0.0545\tvalidation loss: 0.1280\t validation accuracy: 0.9711\n",
      "iteration number: 2227\t training loss: 0.0547\tvalidation loss: 0.1280\t validation accuracy: 0.9689\n",
      "iteration number: 2228\t training loss: 0.0545\tvalidation loss: 0.1259\t validation accuracy: 0.9711\n",
      "iteration number: 2229\t training loss: 0.0548\tvalidation loss: 0.1264\t validation accuracy: 0.9733\n",
      "iteration number: 2230\t training loss: 0.0552\tvalidation loss: 0.1278\t validation accuracy: 0.9711\n",
      "iteration number: 2231\t training loss: 0.0549\tvalidation loss: 0.1262\t validation accuracy: 0.9733\n",
      "**MLP with early stopping**\n",
      "Training accuracy: 0.9918\n",
      "Validation accuracy: 0.9733\n",
      "Training loss: 0.0549\n",
      "Validation loss: 0.1262\n",
      "Number of iterations: 2231\n"
     ]
    }
   ],
   "source": [
    "mlp_early_stopping = MultiLayerPerceptron(\n",
    "    X, Y, hidden_size=50, \n",
    "    activation='relu', \n",
    "    dropout=False, \n",
    "    dropout_rate=0\n",
    ")\n",
    "mlp_early_stopping.train(\n",
    "    optimizer='sgd',\n",
    "    min_iterations=500,\n",
    "    max_iterations=5000,\n",
    "    initial_step=1e-1,\n",
    "    batch_size=64,\n",
    "    early_stopping=True,\n",
    "    early_stopping_lookbehind=100,\n",
    "    early_stopping_delta=1e-4, \n",
    "    vectorized=True,\n",
    "    verbose=True\n",
    ")\n",
    "print(\"**MLP with early stopping**\")\n",
    "print(\"Training accuracy: {:.4f}\".format(mlp_early_stopping.accuracy_on_train()))\n",
    "print(\"Validation accuracy: {:.4f}\".format(mlp_early_stopping.accuracy_on_validation()))\n",
    "print(\"Training loss: {:.4f}\".format(mlp_early_stopping.training_losses_history[-1]))\n",
    "print(\"Validation loss: {:.4f}\".format(mlp_early_stopping.validation_losses_history[-1]))\n",
    "print(\"Number of iterations: {:d}\".format(len(mlp_early_stopping.training_losses_history)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp_early_stopping.plot_validation_prediction(sample_id=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â MLP with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1\t training loss: 2.3026\tvalidation loss: 2.3026\t validation accuracy: 0.0911\n",
      "iteration number: 2\t training loss: 2.3025\tvalidation loss: 2.3027\t validation accuracy: 0.0911\n",
      "iteration number: 3\t training loss: 2.3023\tvalidation loss: 2.3029\t validation accuracy: 0.0689\n",
      "iteration number: 4\t training loss: 2.3022\tvalidation loss: 2.3030\t validation accuracy: 0.0689\n",
      "iteration number: 5\t training loss: 2.3020\tvalidation loss: 2.3034\t validation accuracy: 0.1511\n",
      "iteration number: 6\t training loss: 2.3018\tvalidation loss: 2.3036\t validation accuracy: 0.1400\n",
      "iteration number: 7\t training loss: 2.3017\tvalidation loss: 2.3038\t validation accuracy: 0.0689\n",
      "iteration number: 8\t training loss: 2.3016\tvalidation loss: 2.3039\t validation accuracy: 0.0689\n",
      "iteration number: 9\t training loss: 2.3014\tvalidation loss: 2.3040\t validation accuracy: 0.0711\n",
      "iteration number: 10\t training loss: 2.3013\tvalidation loss: 2.3044\t validation accuracy: 0.0689\n",
      "iteration number: 11\t training loss: 2.3010\tvalidation loss: 2.3047\t validation accuracy: 0.1489\n",
      "iteration number: 12\t training loss: 2.3005\tvalidation loss: 2.3051\t validation accuracy: 0.1600\n",
      "iteration number: 13\t training loss: 2.3000\tvalidation loss: 2.3052\t validation accuracy: 0.0911\n",
      "iteration number: 14\t training loss: 2.2993\tvalidation loss: 2.3055\t validation accuracy: 0.0911\n",
      "iteration number: 15\t training loss: 2.2985\tvalidation loss: 2.3055\t validation accuracy: 0.0911\n",
      "iteration number: 16\t training loss: 2.2976\tvalidation loss: 2.3055\t validation accuracy: 0.0911\n",
      "iteration number: 17\t training loss: 2.2964\tvalidation loss: 2.3052\t validation accuracy: 0.0911\n",
      "iteration number: 18\t training loss: 2.2949\tvalidation loss: 2.3051\t validation accuracy: 0.1178\n",
      "iteration number: 19\t training loss: 2.2933\tvalidation loss: 2.3045\t validation accuracy: 0.1600\n",
      "iteration number: 20\t training loss: 2.2913\tvalidation loss: 2.3041\t validation accuracy: 0.1533\n",
      "iteration number: 21\t training loss: 2.2889\tvalidation loss: 2.3031\t validation accuracy: 0.1489\n",
      "iteration number: 22\t training loss: 2.2860\tvalidation loss: 2.3017\t validation accuracy: 0.2222\n",
      "iteration number: 23\t training loss: 2.2825\tvalidation loss: 2.2993\t validation accuracy: 0.2556\n",
      "iteration number: 24\t training loss: 2.2785\tvalidation loss: 2.2957\t validation accuracy: 0.3022\n",
      "iteration number: 25\t training loss: 2.2738\tvalidation loss: 2.2914\t validation accuracy: 0.2578\n",
      "iteration number: 26\t training loss: 2.2678\tvalidation loss: 2.2856\t validation accuracy: 0.2867\n",
      "iteration number: 27\t training loss: 2.2603\tvalidation loss: 2.2787\t validation accuracy: 0.3644\n",
      "iteration number: 28\t training loss: 2.2513\tvalidation loss: 2.2700\t validation accuracy: 0.3778\n",
      "iteration number: 29\t training loss: 2.2407\tvalidation loss: 2.2593\t validation accuracy: 0.3667\n",
      "iteration number: 30\t training loss: 2.2283\tvalidation loss: 2.2463\t validation accuracy: 0.3244\n",
      "iteration number: 31\t training loss: 2.2129\tvalidation loss: 2.2320\t validation accuracy: 0.3089\n",
      "iteration number: 32\t training loss: 2.1946\tvalidation loss: 2.2139\t validation accuracy: 0.2956\n",
      "iteration number: 33\t training loss: 2.1737\tvalidation loss: 2.1916\t validation accuracy: 0.2667\n",
      "iteration number: 34\t training loss: 2.1483\tvalidation loss: 2.1680\t validation accuracy: 0.3000\n",
      "iteration number: 35\t training loss: 2.1183\tvalidation loss: 2.1394\t validation accuracy: 0.3267\n",
      "iteration number: 36\t training loss: 2.0849\tvalidation loss: 2.1073\t validation accuracy: 0.3111\n",
      "iteration number: 37\t training loss: 2.0444\tvalidation loss: 2.0678\t validation accuracy: 0.3311\n",
      "iteration number: 38\t training loss: 1.9962\tvalidation loss: 2.0211\t validation accuracy: 0.3356\n",
      "iteration number: 39\t training loss: 1.9409\tvalidation loss: 1.9677\t validation accuracy: 0.3400\n",
      "iteration number: 40\t training loss: 1.8776\tvalidation loss: 1.9065\t validation accuracy: 0.3844\n",
      "iteration number: 41\t training loss: 1.8068\tvalidation loss: 1.8368\t validation accuracy: 0.4667\n",
      "iteration number: 42\t training loss: 1.7298\tvalidation loss: 1.7639\t validation accuracy: 0.5111\n",
      "iteration number: 43\t training loss: 1.6504\tvalidation loss: 1.6882\t validation accuracy: 0.5511\n",
      "iteration number: 44\t training loss: 1.5659\tvalidation loss: 1.6133\t validation accuracy: 0.5556\n",
      "iteration number: 45\t training loss: 1.4783\tvalidation loss: 1.5354\t validation accuracy: 0.5511\n",
      "iteration number: 46\t training loss: 1.4026\tvalidation loss: 1.4703\t validation accuracy: 0.5311\n",
      "iteration number: 47\t training loss: 1.3369\tvalidation loss: 1.4147\t validation accuracy: 0.5000\n",
      "iteration number: 48\t training loss: 1.2603\tvalidation loss: 1.3404\t validation accuracy: 0.5111\n",
      "iteration number: 49\t training loss: 1.1791\tvalidation loss: 1.2563\t validation accuracy: 0.5467\n",
      "iteration number: 50\t training loss: 1.0912\tvalidation loss: 1.1554\t validation accuracy: 0.6511\n",
      "iteration number: 51\t training loss: 1.0110\tvalidation loss: 1.0592\t validation accuracy: 0.7489\n",
      "iteration number: 52\t training loss: 0.9470\tvalidation loss: 0.9828\t validation accuracy: 0.7933\n",
      "iteration number: 53\t training loss: 0.9147\tvalidation loss: 0.9487\t validation accuracy: 0.7578\n",
      "iteration number: 54\t training loss: 0.8543\tvalidation loss: 0.8905\t validation accuracy: 0.7867\n",
      "iteration number: 55\t training loss: 0.7964\tvalidation loss: 0.8369\t validation accuracy: 0.8022\n",
      "iteration number: 56\t training loss: 0.7396\tvalidation loss: 0.7908\t validation accuracy: 0.7711\n",
      "iteration number: 57\t training loss: 0.7046\tvalidation loss: 0.7580\t validation accuracy: 0.7644\n",
      "iteration number: 58\t training loss: 0.6781\tvalidation loss: 0.7336\t validation accuracy: 0.7667\n",
      "iteration number: 59\t training loss: 0.6591\tvalidation loss: 0.7063\t validation accuracy: 0.7911\n",
      "iteration number: 60\t training loss: 0.6593\tvalidation loss: 0.7004\t validation accuracy: 0.7933\n",
      "iteration number: 61\t training loss: 0.6273\tvalidation loss: 0.6672\t validation accuracy: 0.7956\n",
      "iteration number: 62\t training loss: 0.5628\tvalidation loss: 0.5997\t validation accuracy: 0.8511\n",
      "iteration number: 63\t training loss: 0.5381\tvalidation loss: 0.5807\t validation accuracy: 0.8444\n",
      "iteration number: 64\t training loss: 0.5396\tvalidation loss: 0.5865\t validation accuracy: 0.8200\n",
      "iteration number: 65\t training loss: 0.5225\tvalidation loss: 0.5643\t validation accuracy: 0.8200\n",
      "iteration number: 66\t training loss: 0.4803\tvalidation loss: 0.5118\t validation accuracy: 0.8556\n",
      "iteration number: 67\t training loss: 0.4591\tvalidation loss: 0.4865\t validation accuracy: 0.8644\n",
      "iteration number: 68\t training loss: 0.4560\tvalidation loss: 0.4822\t validation accuracy: 0.8667\n",
      "iteration number: 69\t training loss: 0.4511\tvalidation loss: 0.4776\t validation accuracy: 0.8667\n",
      "iteration number: 70\t training loss: 0.4262\tvalidation loss: 0.4470\t validation accuracy: 0.8822\n",
      "iteration number: 71\t training loss: 0.4072\tvalidation loss: 0.4193\t validation accuracy: 0.8933\n",
      "iteration number: 72\t training loss: 0.3830\tvalidation loss: 0.3813\t validation accuracy: 0.9000\n",
      "iteration number: 73\t training loss: 0.3770\tvalidation loss: 0.3678\t validation accuracy: 0.8978\n",
      "iteration number: 74\t training loss: 0.3834\tvalidation loss: 0.3804\t validation accuracy: 0.8733\n",
      "iteration number: 75\t training loss: 0.3932\tvalidation loss: 0.3978\t validation accuracy: 0.8644\n",
      "iteration number: 76\t training loss: 0.3886\tvalidation loss: 0.4001\t validation accuracy: 0.8644\n",
      "iteration number: 77\t training loss: 0.3672\tvalidation loss: 0.3831\t validation accuracy: 0.8800\n",
      "iteration number: 78\t training loss: 0.3438\tvalidation loss: 0.3605\t validation accuracy: 0.8889\n",
      "iteration number: 79\t training loss: 0.3332\tvalidation loss: 0.3496\t validation accuracy: 0.8844\n",
      "iteration number: 80\t training loss: 0.3247\tvalidation loss: 0.3405\t validation accuracy: 0.8800\n",
      "iteration number: 81\t training loss: 0.3153\tvalidation loss: 0.3263\t validation accuracy: 0.8867\n",
      "iteration number: 82\t training loss: 0.3101\tvalidation loss: 0.3090\t validation accuracy: 0.9067\n",
      "iteration number: 83\t training loss: 0.2946\tvalidation loss: 0.2828\t validation accuracy: 0.9022\n",
      "iteration number: 84\t training loss: 0.2870\tvalidation loss: 0.2669\t validation accuracy: 0.9089\n",
      "iteration number: 85\t training loss: 0.2880\tvalidation loss: 0.2666\t validation accuracy: 0.9111\n",
      "iteration number: 86\t training loss: 0.2781\tvalidation loss: 0.2621\t validation accuracy: 0.9111\n",
      "iteration number: 87\t training loss: 0.2727\tvalidation loss: 0.2598\t validation accuracy: 0.9156\n",
      "iteration number: 88\t training loss: 0.2712\tvalidation loss: 0.2613\t validation accuracy: 0.9178\n",
      "iteration number: 89\t training loss: 0.2702\tvalidation loss: 0.2655\t validation accuracy: 0.9156\n",
      "iteration number: 90\t training loss: 0.2543\tvalidation loss: 0.2563\t validation accuracy: 0.9200\n",
      "iteration number: 91\t training loss: 0.2432\tvalidation loss: 0.2550\t validation accuracy: 0.9200\n",
      "iteration number: 92\t training loss: 0.2328\tvalidation loss: 0.2493\t validation accuracy: 0.9267\n",
      "iteration number: 93\t training loss: 0.2241\tvalidation loss: 0.2427\t validation accuracy: 0.9267\n",
      "iteration number: 94\t training loss: 0.2226\tvalidation loss: 0.2380\t validation accuracy: 0.9178\n",
      "iteration number: 95\t training loss: 0.2301\tvalidation loss: 0.2413\t validation accuracy: 0.9111\n",
      "iteration number: 96\t training loss: 0.2386\tvalidation loss: 0.2459\t validation accuracy: 0.9089\n",
      "iteration number: 97\t training loss: 0.2394\tvalidation loss: 0.2450\t validation accuracy: 0.9022\n",
      "iteration number: 98\t training loss: 0.2294\tvalidation loss: 0.2368\t validation accuracy: 0.9200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 99\t training loss: 0.2205\tvalidation loss: 0.2293\t validation accuracy: 0.9244\n",
      "iteration number: 100\t training loss: 0.2267\tvalidation loss: 0.2404\t validation accuracy: 0.9311\n",
      "iteration number: 101\t training loss: 0.2284\tvalidation loss: 0.2437\t validation accuracy: 0.9311\n",
      "iteration number: 102\t training loss: 0.2205\tvalidation loss: 0.2404\t validation accuracy: 0.9333\n",
      "iteration number: 103\t training loss: 0.2121\tvalidation loss: 0.2365\t validation accuracy: 0.9333\n",
      "iteration number: 104\t training loss: 0.1913\tvalidation loss: 0.2171\t validation accuracy: 0.9400\n",
      "iteration number: 105\t training loss: 0.1797\tvalidation loss: 0.2078\t validation accuracy: 0.9378\n",
      "iteration number: 106\t training loss: 0.1854\tvalidation loss: 0.2176\t validation accuracy: 0.9356\n",
      "iteration number: 107\t training loss: 0.2062\tvalidation loss: 0.2478\t validation accuracy: 0.9089\n",
      "iteration number: 108\t training loss: 0.2229\tvalidation loss: 0.2726\t validation accuracy: 0.9022\n",
      "iteration number: 109\t training loss: 0.2283\tvalidation loss: 0.2791\t validation accuracy: 0.9022\n",
      "iteration number: 110\t training loss: 0.2204\tvalidation loss: 0.2707\t validation accuracy: 0.9022\n",
      "iteration number: 111\t training loss: 0.2078\tvalidation loss: 0.2564\t validation accuracy: 0.9178\n",
      "iteration number: 112\t training loss: 0.1862\tvalidation loss: 0.2280\t validation accuracy: 0.9267\n",
      "iteration number: 113\t training loss: 0.1701\tvalidation loss: 0.2019\t validation accuracy: 0.9378\n",
      "iteration number: 114\t training loss: 0.1682\tvalidation loss: 0.1858\t validation accuracy: 0.9356\n",
      "iteration number: 115\t training loss: 0.1766\tvalidation loss: 0.1822\t validation accuracy: 0.9422\n",
      "iteration number: 116\t training loss: 0.1925\tvalidation loss: 0.1857\t validation accuracy: 0.9467\n",
      "iteration number: 117\t training loss: 0.2118\tvalidation loss: 0.1955\t validation accuracy: 0.9467\n",
      "iteration number: 118\t training loss: 0.2053\tvalidation loss: 0.1874\t validation accuracy: 0.9444\n",
      "iteration number: 119\t training loss: 0.1853\tvalidation loss: 0.1744\t validation accuracy: 0.9533\n",
      "iteration number: 120\t training loss: 0.1702\tvalidation loss: 0.1676\t validation accuracy: 0.9489\n",
      "iteration number: 121\t training loss: 0.1564\tvalidation loss: 0.1660\t validation accuracy: 0.9511\n",
      "iteration number: 122\t training loss: 0.1518\tvalidation loss: 0.1701\t validation accuracy: 0.9444\n",
      "iteration number: 123\t training loss: 0.1557\tvalidation loss: 0.1811\t validation accuracy: 0.9422\n",
      "iteration number: 124\t training loss: 0.1618\tvalidation loss: 0.1958\t validation accuracy: 0.9378\n",
      "iteration number: 125\t training loss: 0.1664\tvalidation loss: 0.2077\t validation accuracy: 0.9356\n",
      "iteration number: 126\t training loss: 0.1686\tvalidation loss: 0.2139\t validation accuracy: 0.9333\n",
      "iteration number: 127\t training loss: 0.1603\tvalidation loss: 0.2047\t validation accuracy: 0.9356\n",
      "iteration number: 128\t training loss: 0.1531\tvalidation loss: 0.1968\t validation accuracy: 0.9378\n",
      "iteration number: 129\t training loss: 0.1434\tvalidation loss: 0.1817\t validation accuracy: 0.9400\n",
      "iteration number: 130\t training loss: 0.1385\tvalidation loss: 0.1708\t validation accuracy: 0.9378\n",
      "iteration number: 131\t training loss: 0.1344\tvalidation loss: 0.1581\t validation accuracy: 0.9467\n",
      "iteration number: 132\t training loss: 0.1438\tvalidation loss: 0.1608\t validation accuracy: 0.9467\n",
      "iteration number: 133\t training loss: 0.1558\tvalidation loss: 0.1708\t validation accuracy: 0.9489\n",
      "iteration number: 134\t training loss: 0.1540\tvalidation loss: 0.1650\t validation accuracy: 0.9556\n",
      "iteration number: 135\t training loss: 0.1488\tvalidation loss: 0.1560\t validation accuracy: 0.9578\n",
      "iteration number: 136\t training loss: 0.1369\tvalidation loss: 0.1426\t validation accuracy: 0.9578\n",
      "iteration number: 137\t training loss: 0.1347\tvalidation loss: 0.1455\t validation accuracy: 0.9556\n",
      "iteration number: 138\t training loss: 0.1342\tvalidation loss: 0.1514\t validation accuracy: 0.9467\n",
      "iteration number: 139\t training loss: 0.1351\tvalidation loss: 0.1600\t validation accuracy: 0.9467\n",
      "iteration number: 140\t training loss: 0.1306\tvalidation loss: 0.1634\t validation accuracy: 0.9533\n",
      "iteration number: 141\t training loss: 0.1331\tvalidation loss: 0.1742\t validation accuracy: 0.9467\n",
      "iteration number: 142\t training loss: 0.1372\tvalidation loss: 0.1840\t validation accuracy: 0.9422\n",
      "iteration number: 143\t training loss: 0.1339\tvalidation loss: 0.1808\t validation accuracy: 0.9422\n",
      "iteration number: 144\t training loss: 0.1337\tvalidation loss: 0.1799\t validation accuracy: 0.9400\n",
      "iteration number: 145\t training loss: 0.1327\tvalidation loss: 0.1719\t validation accuracy: 0.9422\n",
      "iteration number: 146\t training loss: 0.1297\tvalidation loss: 0.1578\t validation accuracy: 0.9422\n",
      "iteration number: 147\t training loss: 0.1272\tvalidation loss: 0.1443\t validation accuracy: 0.9556\n",
      "iteration number: 148\t training loss: 0.1343\tvalidation loss: 0.1397\t validation accuracy: 0.9600\n",
      "iteration number: 149\t training loss: 0.1545\tvalidation loss: 0.1517\t validation accuracy: 0.9556\n",
      "iteration number: 150\t training loss: 0.1840\tvalidation loss: 0.1772\t validation accuracy: 0.9333\n",
      "iteration number: 151\t training loss: 0.1913\tvalidation loss: 0.1782\t validation accuracy: 0.9400\n",
      "iteration number: 152\t training loss: 0.1640\tvalidation loss: 0.1576\t validation accuracy: 0.9467\n",
      "iteration number: 153\t training loss: 0.1338\tvalidation loss: 0.1458\t validation accuracy: 0.9533\n",
      "iteration number: 154\t training loss: 0.1421\tvalidation loss: 0.1713\t validation accuracy: 0.9467\n",
      "iteration number: 155\t training loss: 0.1670\tvalidation loss: 0.2098\t validation accuracy: 0.9333\n",
      "iteration number: 156\t training loss: 0.1857\tvalidation loss: 0.2374\t validation accuracy: 0.9178\n",
      "iteration number: 157\t training loss: 0.1824\tvalidation loss: 0.2371\t validation accuracy: 0.9067\n",
      "iteration number: 158\t training loss: 0.1759\tvalidation loss: 0.2335\t validation accuracy: 0.9222\n",
      "iteration number: 159\t training loss: 0.1507\tvalidation loss: 0.2030\t validation accuracy: 0.9244\n",
      "iteration number: 160\t training loss: 0.1340\tvalidation loss: 0.1714\t validation accuracy: 0.9467\n",
      "iteration number: 161\t training loss: 0.1366\tvalidation loss: 0.1631\t validation accuracy: 0.9444\n",
      "iteration number: 162\t training loss: 0.1407\tvalidation loss: 0.1611\t validation accuracy: 0.9422\n",
      "iteration number: 163\t training loss: 0.1469\tvalidation loss: 0.1613\t validation accuracy: 0.9444\n",
      "iteration number: 164\t training loss: 0.1465\tvalidation loss: 0.1548\t validation accuracy: 0.9467\n",
      "iteration number: 165\t training loss: 0.1373\tvalidation loss: 0.1403\t validation accuracy: 0.9600\n",
      "iteration number: 166\t training loss: 0.1269\tvalidation loss: 0.1296\t validation accuracy: 0.9600\n",
      "iteration number: 167\t training loss: 0.1229\tvalidation loss: 0.1297\t validation accuracy: 0.9689\n",
      "iteration number: 168\t training loss: 0.1251\tvalidation loss: 0.1369\t validation accuracy: 0.9600\n",
      "iteration number: 169\t training loss: 0.1321\tvalidation loss: 0.1483\t validation accuracy: 0.9533\n",
      "iteration number: 170\t training loss: 0.1392\tvalidation loss: 0.1576\t validation accuracy: 0.9511\n",
      "iteration number: 171\t training loss: 0.1378\tvalidation loss: 0.1589\t validation accuracy: 0.9511\n",
      "iteration number: 172\t training loss: 0.1266\tvalidation loss: 0.1507\t validation accuracy: 0.9556\n",
      "iteration number: 173\t training loss: 0.1122\tvalidation loss: 0.1338\t validation accuracy: 0.9622\n",
      "iteration number: 174\t training loss: 0.1032\tvalidation loss: 0.1203\t validation accuracy: 0.9622\n",
      "iteration number: 175\t training loss: 0.1063\tvalidation loss: 0.1200\t validation accuracy: 0.9711\n",
      "iteration number: 176\t training loss: 0.1175\tvalidation loss: 0.1298\t validation accuracy: 0.9600\n",
      "iteration number: 177\t training loss: 0.1211\tvalidation loss: 0.1343\t validation accuracy: 0.9644\n",
      "iteration number: 178\t training loss: 0.1251\tvalidation loss: 0.1401\t validation accuracy: 0.9667\n",
      "iteration number: 179\t training loss: 0.1203\tvalidation loss: 0.1379\t validation accuracy: 0.9644\n",
      "iteration number: 180\t training loss: 0.1126\tvalidation loss: 0.1336\t validation accuracy: 0.9644\n",
      "iteration number: 181\t training loss: 0.1044\tvalidation loss: 0.1313\t validation accuracy: 0.9622\n",
      "iteration number: 182\t training loss: 0.0986\tvalidation loss: 0.1313\t validation accuracy: 0.9600\n",
      "iteration number: 183\t training loss: 0.0944\tvalidation loss: 0.1300\t validation accuracy: 0.9578\n",
      "iteration number: 184\t training loss: 0.0969\tvalidation loss: 0.1366\t validation accuracy: 0.9533\n",
      "iteration number: 185\t training loss: 0.1018\tvalidation loss: 0.1439\t validation accuracy: 0.9511\n",
      "iteration number: 186\t training loss: 0.1032\tvalidation loss: 0.1450\t validation accuracy: 0.9489\n",
      "iteration number: 187\t training loss: 0.1038\tvalidation loss: 0.1421\t validation accuracy: 0.9533\n",
      "iteration number: 188\t training loss: 0.1022\tvalidation loss: 0.1374\t validation accuracy: 0.9533\n",
      "iteration number: 189\t training loss: 0.0973\tvalidation loss: 0.1285\t validation accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 190\t training loss: 0.0948\tvalidation loss: 0.1227\t validation accuracy: 0.9644\n",
      "iteration number: 191\t training loss: 0.0910\tvalidation loss: 0.1163\t validation accuracy: 0.9689\n",
      "iteration number: 192\t training loss: 0.0880\tvalidation loss: 0.1099\t validation accuracy: 0.9733\n",
      "iteration number: 193\t training loss: 0.0862\tvalidation loss: 0.1082\t validation accuracy: 0.9711\n",
      "iteration number: 194\t training loss: 0.0879\tvalidation loss: 0.1091\t validation accuracy: 0.9711\n",
      "iteration number: 195\t training loss: 0.0884\tvalidation loss: 0.1092\t validation accuracy: 0.9711\n",
      "iteration number: 196\t training loss: 0.0873\tvalidation loss: 0.1086\t validation accuracy: 0.9711\n",
      "iteration number: 197\t training loss: 0.0860\tvalidation loss: 0.1089\t validation accuracy: 0.9711\n",
      "iteration number: 198\t training loss: 0.0864\tvalidation loss: 0.1124\t validation accuracy: 0.9711\n",
      "iteration number: 199\t training loss: 0.0871\tvalidation loss: 0.1163\t validation accuracy: 0.9689\n",
      "iteration number: 200\t training loss: 0.0850\tvalidation loss: 0.1158\t validation accuracy: 0.9711\n",
      "iteration number: 201\t training loss: 0.0818\tvalidation loss: 0.1144\t validation accuracy: 0.9711\n",
      "iteration number: 202\t training loss: 0.0796\tvalidation loss: 0.1142\t validation accuracy: 0.9644\n",
      "iteration number: 203\t training loss: 0.0783\tvalidation loss: 0.1161\t validation accuracy: 0.9600\n",
      "iteration number: 204\t training loss: 0.0804\tvalidation loss: 0.1198\t validation accuracy: 0.9578\n",
      "iteration number: 205\t training loss: 0.0827\tvalidation loss: 0.1249\t validation accuracy: 0.9578\n",
      "iteration number: 206\t training loss: 0.0838\tvalidation loss: 0.1297\t validation accuracy: 0.9600\n",
      "iteration number: 207\t training loss: 0.0890\tvalidation loss: 0.1382\t validation accuracy: 0.9578\n",
      "iteration number: 208\t training loss: 0.0902\tvalidation loss: 0.1385\t validation accuracy: 0.9556\n",
      "iteration number: 209\t training loss: 0.0840\tvalidation loss: 0.1284\t validation accuracy: 0.9644\n",
      "iteration number: 210\t training loss: 0.0802\tvalidation loss: 0.1250\t validation accuracy: 0.9644\n",
      "iteration number: 211\t training loss: 0.0765\tvalidation loss: 0.1195\t validation accuracy: 0.9644\n",
      "iteration number: 212\t training loss: 0.0773\tvalidation loss: 0.1182\t validation accuracy: 0.9644\n",
      "iteration number: 213\t training loss: 0.0818\tvalidation loss: 0.1207\t validation accuracy: 0.9644\n",
      "iteration number: 214\t training loss: 0.0901\tvalidation loss: 0.1274\t validation accuracy: 0.9644\n",
      "iteration number: 215\t training loss: 0.0898\tvalidation loss: 0.1249\t validation accuracy: 0.9644\n",
      "iteration number: 216\t training loss: 0.0843\tvalidation loss: 0.1177\t validation accuracy: 0.9667\n",
      "iteration number: 217\t training loss: 0.0753\tvalidation loss: 0.1073\t validation accuracy: 0.9711\n",
      "iteration number: 218\t training loss: 0.0726\tvalidation loss: 0.1042\t validation accuracy: 0.9689\n",
      "iteration number: 219\t training loss: 0.0783\tvalidation loss: 0.1108\t validation accuracy: 0.9667\n",
      "iteration number: 220\t training loss: 0.0892\tvalidation loss: 0.1231\t validation accuracy: 0.9600\n",
      "iteration number: 221\t training loss: 0.0963\tvalidation loss: 0.1291\t validation accuracy: 0.9556\n",
      "iteration number: 222\t training loss: 0.0916\tvalidation loss: 0.1255\t validation accuracy: 0.9644\n",
      "iteration number: 223\t training loss: 0.0821\tvalidation loss: 0.1175\t validation accuracy: 0.9667\n",
      "iteration number: 224\t training loss: 0.0769\tvalidation loss: 0.1151\t validation accuracy: 0.9644\n",
      "iteration number: 225\t training loss: 0.0766\tvalidation loss: 0.1175\t validation accuracy: 0.9644\n",
      "iteration number: 226\t training loss: 0.0773\tvalidation loss: 0.1195\t validation accuracy: 0.9644\n",
      "iteration number: 227\t training loss: 0.0771\tvalidation loss: 0.1208\t validation accuracy: 0.9622\n",
      "iteration number: 228\t training loss: 0.0762\tvalidation loss: 0.1211\t validation accuracy: 0.9644\n",
      "iteration number: 229\t training loss: 0.0740\tvalidation loss: 0.1209\t validation accuracy: 0.9600\n",
      "iteration number: 230\t training loss: 0.0690\tvalidation loss: 0.1181\t validation accuracy: 0.9600\n",
      "iteration number: 231\t training loss: 0.0651\tvalidation loss: 0.1164\t validation accuracy: 0.9622\n",
      "iteration number: 232\t training loss: 0.0658\tvalidation loss: 0.1196\t validation accuracy: 0.9578\n",
      "iteration number: 233\t training loss: 0.0688\tvalidation loss: 0.1258\t validation accuracy: 0.9556\n",
      "iteration number: 234\t training loss: 0.0700\tvalidation loss: 0.1305\t validation accuracy: 0.9533\n",
      "iteration number: 235\t training loss: 0.0689\tvalidation loss: 0.1307\t validation accuracy: 0.9533\n",
      "iteration number: 236\t training loss: 0.0655\tvalidation loss: 0.1243\t validation accuracy: 0.9533\n",
      "iteration number: 237\t training loss: 0.0616\tvalidation loss: 0.1141\t validation accuracy: 0.9667\n",
      "iteration number: 238\t training loss: 0.0606\tvalidation loss: 0.1076\t validation accuracy: 0.9644\n",
      "iteration number: 239\t training loss: 0.0621\tvalidation loss: 0.1066\t validation accuracy: 0.9689\n",
      "iteration number: 240\t training loss: 0.0641\tvalidation loss: 0.1078\t validation accuracy: 0.9689\n",
      "iteration number: 241\t training loss: 0.0640\tvalidation loss: 0.1073\t validation accuracy: 0.9711\n",
      "iteration number: 242\t training loss: 0.0647\tvalidation loss: 0.1057\t validation accuracy: 0.9711\n",
      "iteration number: 243\t training loss: 0.0658\tvalidation loss: 0.1067\t validation accuracy: 0.9711\n",
      "iteration number: 244\t training loss: 0.0666\tvalidation loss: 0.1077\t validation accuracy: 0.9689\n",
      "iteration number: 245\t training loss: 0.0708\tvalidation loss: 0.1114\t validation accuracy: 0.9689\n",
      "iteration number: 246\t training loss: 0.0799\tvalidation loss: 0.1208\t validation accuracy: 0.9667\n",
      "iteration number: 247\t training loss: 0.0825\tvalidation loss: 0.1255\t validation accuracy: 0.9578\n",
      "iteration number: 248\t training loss: 0.0727\tvalidation loss: 0.1159\t validation accuracy: 0.9644\n",
      "iteration number: 249\t training loss: 0.0649\tvalidation loss: 0.1062\t validation accuracy: 0.9644\n",
      "iteration number: 250\t training loss: 0.0630\tvalidation loss: 0.1041\t validation accuracy: 0.9600\n",
      "iteration number: 251\t training loss: 0.0638\tvalidation loss: 0.1066\t validation accuracy: 0.9644\n",
      "iteration number: 252\t training loss: 0.0658\tvalidation loss: 0.1086\t validation accuracy: 0.9622\n",
      "iteration number: 253\t training loss: 0.0644\tvalidation loss: 0.1068\t validation accuracy: 0.9644\n",
      "iteration number: 254\t training loss: 0.0632\tvalidation loss: 0.1062\t validation accuracy: 0.9667\n",
      "iteration number: 255\t training loss: 0.0614\tvalidation loss: 0.1048\t validation accuracy: 0.9689\n",
      "iteration number: 256\t training loss: 0.0608\tvalidation loss: 0.1045\t validation accuracy: 0.9689\n",
      "iteration number: 257\t training loss: 0.0598\tvalidation loss: 0.1046\t validation accuracy: 0.9667\n",
      "iteration number: 258\t training loss: 0.0604\tvalidation loss: 0.1078\t validation accuracy: 0.9667\n",
      "iteration number: 259\t training loss: 0.0610\tvalidation loss: 0.1100\t validation accuracy: 0.9667\n",
      "iteration number: 260\t training loss: 0.0589\tvalidation loss: 0.1047\t validation accuracy: 0.9689\n",
      "iteration number: 261\t training loss: 0.0575\tvalidation loss: 0.1001\t validation accuracy: 0.9756\n",
      "iteration number: 262\t training loss: 0.0569\tvalidation loss: 0.0976\t validation accuracy: 0.9711\n",
      "iteration number: 263\t training loss: 0.0570\tvalidation loss: 0.0958\t validation accuracy: 0.9711\n",
      "iteration number: 264\t training loss: 0.0573\tvalidation loss: 0.0950\t validation accuracy: 0.9778\n",
      "iteration number: 265\t training loss: 0.0578\tvalidation loss: 0.0956\t validation accuracy: 0.9733\n",
      "iteration number: 266\t training loss: 0.0602\tvalidation loss: 0.0984\t validation accuracy: 0.9778\n",
      "iteration number: 267\t training loss: 0.0632\tvalidation loss: 0.1019\t validation accuracy: 0.9733\n",
      "iteration number: 268\t training loss: 0.0632\tvalidation loss: 0.1026\t validation accuracy: 0.9733\n",
      "iteration number: 269\t training loss: 0.0611\tvalidation loss: 0.1010\t validation accuracy: 0.9756\n",
      "iteration number: 270\t training loss: 0.0567\tvalidation loss: 0.0962\t validation accuracy: 0.9778\n",
      "iteration number: 271\t training loss: 0.0552\tvalidation loss: 0.0947\t validation accuracy: 0.9733\n",
      "iteration number: 272\t training loss: 0.0549\tvalidation loss: 0.0961\t validation accuracy: 0.9689\n",
      "iteration number: 273\t training loss: 0.0550\tvalidation loss: 0.1012\t validation accuracy: 0.9711\n",
      "iteration number: 274\t training loss: 0.0566\tvalidation loss: 0.1111\t validation accuracy: 0.9667\n",
      "iteration number: 275\t training loss: 0.0613\tvalidation loss: 0.1243\t validation accuracy: 0.9578\n",
      "iteration number: 276\t training loss: 0.0657\tvalidation loss: 0.1360\t validation accuracy: 0.9533\n",
      "iteration number: 277\t training loss: 0.0642\tvalidation loss: 0.1343\t validation accuracy: 0.9533\n",
      "iteration number: 278\t training loss: 0.0621\tvalidation loss: 0.1274\t validation accuracy: 0.9622\n",
      "iteration number: 279\t training loss: 0.0649\tvalidation loss: 0.1246\t validation accuracy: 0.9600\n",
      "iteration number: 280\t training loss: 0.0676\tvalidation loss: 0.1236\t validation accuracy: 0.9644\n",
      "iteration number: 281\t training loss: 0.0672\tvalidation loss: 0.1202\t validation accuracy: 0.9644\n",
      "iteration number: 282\t training loss: 0.0668\tvalidation loss: 0.1157\t validation accuracy: 0.9667\n",
      "iteration number: 283\t training loss: 0.0644\tvalidation loss: 0.1114\t validation accuracy: 0.9711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 284\t training loss: 0.0591\tvalidation loss: 0.1056\t validation accuracy: 0.9711\n",
      "iteration number: 285\t training loss: 0.0562\tvalidation loss: 0.1012\t validation accuracy: 0.9711\n",
      "iteration number: 286\t training loss: 0.0546\tvalidation loss: 0.1004\t validation accuracy: 0.9733\n",
      "iteration number: 287\t training loss: 0.0531\tvalidation loss: 0.1002\t validation accuracy: 0.9756\n",
      "iteration number: 288\t training loss: 0.0534\tvalidation loss: 0.1013\t validation accuracy: 0.9778\n",
      "iteration number: 289\t training loss: 0.0525\tvalidation loss: 0.1011\t validation accuracy: 0.9778\n",
      "iteration number: 290\t training loss: 0.0507\tvalidation loss: 0.0989\t validation accuracy: 0.9778\n",
      "iteration number: 291\t training loss: 0.0481\tvalidation loss: 0.0968\t validation accuracy: 0.9756\n",
      "iteration number: 292\t training loss: 0.0481\tvalidation loss: 0.0989\t validation accuracy: 0.9711\n",
      "iteration number: 293\t training loss: 0.0500\tvalidation loss: 0.1044\t validation accuracy: 0.9667\n",
      "iteration number: 294\t training loss: 0.0528\tvalidation loss: 0.1111\t validation accuracy: 0.9578\n",
      "iteration number: 295\t training loss: 0.0550\tvalidation loss: 0.1176\t validation accuracy: 0.9556\n",
      "iteration number: 296\t training loss: 0.0552\tvalidation loss: 0.1170\t validation accuracy: 0.9578\n",
      "iteration number: 297\t training loss: 0.0516\tvalidation loss: 0.1109\t validation accuracy: 0.9578\n",
      "iteration number: 298\t training loss: 0.0489\tvalidation loss: 0.1053\t validation accuracy: 0.9622\n",
      "iteration number: 299\t training loss: 0.0475\tvalidation loss: 0.0993\t validation accuracy: 0.9711\n",
      "iteration number: 300\t training loss: 0.0496\tvalidation loss: 0.0990\t validation accuracy: 0.9756\n",
      "iteration number: 301\t training loss: 0.0533\tvalidation loss: 0.1032\t validation accuracy: 0.9711\n",
      "iteration number: 302\t training loss: 0.0547\tvalidation loss: 0.1041\t validation accuracy: 0.9711\n",
      "iteration number: 303\t training loss: 0.0547\tvalidation loss: 0.1029\t validation accuracy: 0.9711\n",
      "iteration number: 304\t training loss: 0.0538\tvalidation loss: 0.1029\t validation accuracy: 0.9689\n",
      "iteration number: 305\t training loss: 0.0518\tvalidation loss: 0.1022\t validation accuracy: 0.9689\n",
      "iteration number: 306\t training loss: 0.0492\tvalidation loss: 0.1015\t validation accuracy: 0.9711\n",
      "iteration number: 307\t training loss: 0.0471\tvalidation loss: 0.1021\t validation accuracy: 0.9733\n",
      "iteration number: 308\t training loss: 0.0463\tvalidation loss: 0.1027\t validation accuracy: 0.9711\n",
      "iteration number: 309\t training loss: 0.0470\tvalidation loss: 0.1039\t validation accuracy: 0.9711\n",
      "iteration number: 310\t training loss: 0.0483\tvalidation loss: 0.1044\t validation accuracy: 0.9733\n",
      "iteration number: 311\t training loss: 0.0518\tvalidation loss: 0.1080\t validation accuracy: 0.9667\n",
      "iteration number: 312\t training loss: 0.0549\tvalidation loss: 0.1113\t validation accuracy: 0.9622\n",
      "iteration number: 313\t training loss: 0.0556\tvalidation loss: 0.1111\t validation accuracy: 0.9622\n",
      "iteration number: 314\t training loss: 0.0551\tvalidation loss: 0.1086\t validation accuracy: 0.9622\n",
      "iteration number: 315\t training loss: 0.0500\tvalidation loss: 0.1011\t validation accuracy: 0.9689\n",
      "iteration number: 316\t training loss: 0.0460\tvalidation loss: 0.0946\t validation accuracy: 0.9689\n",
      "iteration number: 317\t training loss: 0.0442\tvalidation loss: 0.0920\t validation accuracy: 0.9667\n",
      "iteration number: 318\t training loss: 0.0457\tvalidation loss: 0.0933\t validation accuracy: 0.9711\n",
      "iteration number: 319\t training loss: 0.0459\tvalidation loss: 0.0939\t validation accuracy: 0.9689\n",
      "iteration number: 320\t training loss: 0.0458\tvalidation loss: 0.0949\t validation accuracy: 0.9711\n",
      "iteration number: 321\t training loss: 0.0462\tvalidation loss: 0.0958\t validation accuracy: 0.9689\n",
      "iteration number: 322\t training loss: 0.0465\tvalidation loss: 0.0971\t validation accuracy: 0.9689\n",
      "iteration number: 323\t training loss: 0.0460\tvalidation loss: 0.0989\t validation accuracy: 0.9733\n",
      "iteration number: 324\t training loss: 0.0435\tvalidation loss: 0.0980\t validation accuracy: 0.9711\n",
      "iteration number: 325\t training loss: 0.0419\tvalidation loss: 0.0998\t validation accuracy: 0.9733\n",
      "iteration number: 326\t training loss: 0.0422\tvalidation loss: 0.1029\t validation accuracy: 0.9733\n",
      "iteration number: 327\t training loss: 0.0441\tvalidation loss: 0.1069\t validation accuracy: 0.9733\n",
      "iteration number: 328\t training loss: 0.0450\tvalidation loss: 0.1091\t validation accuracy: 0.9733\n",
      "iteration number: 329\t training loss: 0.0452\tvalidation loss: 0.1111\t validation accuracy: 0.9733\n",
      "iteration number: 330\t training loss: 0.0446\tvalidation loss: 0.1097\t validation accuracy: 0.9756\n",
      "iteration number: 331\t training loss: 0.0441\tvalidation loss: 0.1086\t validation accuracy: 0.9756\n",
      "iteration number: 332\t training loss: 0.0438\tvalidation loss: 0.1073\t validation accuracy: 0.9756\n",
      "iteration number: 333\t training loss: 0.0434\tvalidation loss: 0.1060\t validation accuracy: 0.9733\n",
      "iteration number: 334\t training loss: 0.0426\tvalidation loss: 0.1042\t validation accuracy: 0.9733\n",
      "iteration number: 335\t training loss: 0.0423\tvalidation loss: 0.1033\t validation accuracy: 0.9733\n",
      "iteration number: 336\t training loss: 0.0425\tvalidation loss: 0.1049\t validation accuracy: 0.9733\n",
      "iteration number: 337\t training loss: 0.0426\tvalidation loss: 0.1063\t validation accuracy: 0.9711\n",
      "iteration number: 338\t training loss: 0.0427\tvalidation loss: 0.1078\t validation accuracy: 0.9711\n",
      "iteration number: 339\t training loss: 0.0416\tvalidation loss: 0.1062\t validation accuracy: 0.9733\n",
      "iteration number: 340\t training loss: 0.0405\tvalidation loss: 0.1043\t validation accuracy: 0.9733\n",
      "iteration number: 341\t training loss: 0.0404\tvalidation loss: 0.1037\t validation accuracy: 0.9689\n",
      "iteration number: 342\t training loss: 0.0399\tvalidation loss: 0.1005\t validation accuracy: 0.9667\n",
      "iteration number: 343\t training loss: 0.0407\tvalidation loss: 0.0992\t validation accuracy: 0.9667\n",
      "iteration number: 344\t training loss: 0.0418\tvalidation loss: 0.0969\t validation accuracy: 0.9711\n",
      "iteration number: 345\t training loss: 0.0412\tvalidation loss: 0.0933\t validation accuracy: 0.9644\n",
      "iteration number: 346\t training loss: 0.0424\tvalidation loss: 0.0926\t validation accuracy: 0.9644\n",
      "iteration number: 347\t training loss: 0.0418\tvalidation loss: 0.0934\t validation accuracy: 0.9689\n",
      "iteration number: 348\t training loss: 0.0409\tvalidation loss: 0.0947\t validation accuracy: 0.9711\n",
      "iteration number: 349\t training loss: 0.0412\tvalidation loss: 0.0982\t validation accuracy: 0.9711\n",
      "iteration number: 350\t training loss: 0.0420\tvalidation loss: 0.1010\t validation accuracy: 0.9733\n",
      "iteration number: 351\t training loss: 0.0401\tvalidation loss: 0.0997\t validation accuracy: 0.9711\n",
      "iteration number: 352\t training loss: 0.0378\tvalidation loss: 0.0988\t validation accuracy: 0.9644\n",
      "iteration number: 353\t training loss: 0.0378\tvalidation loss: 0.1009\t validation accuracy: 0.9644\n",
      "iteration number: 354\t training loss: 0.0388\tvalidation loss: 0.1041\t validation accuracy: 0.9622\n",
      "iteration number: 355\t training loss: 0.0404\tvalidation loss: 0.1061\t validation accuracy: 0.9622\n",
      "iteration number: 356\t training loss: 0.0421\tvalidation loss: 0.1083\t validation accuracy: 0.9622\n",
      "iteration number: 357\t training loss: 0.0429\tvalidation loss: 0.1087\t validation accuracy: 0.9600\n",
      "iteration number: 358\t training loss: 0.0433\tvalidation loss: 0.1085\t validation accuracy: 0.9600\n",
      "iteration number: 359\t training loss: 0.0401\tvalidation loss: 0.1029\t validation accuracy: 0.9667\n",
      "iteration number: 360\t training loss: 0.0381\tvalidation loss: 0.0992\t validation accuracy: 0.9667\n",
      "iteration number: 361\t training loss: 0.0366\tvalidation loss: 0.0971\t validation accuracy: 0.9689\n",
      "iteration number: 362\t training loss: 0.0366\tvalidation loss: 0.0975\t validation accuracy: 0.9689\n",
      "iteration number: 363\t training loss: 0.0373\tvalidation loss: 0.0985\t validation accuracy: 0.9667\n",
      "iteration number: 364\t training loss: 0.0377\tvalidation loss: 0.0981\t validation accuracy: 0.9689\n",
      "iteration number: 365\t training loss: 0.0373\tvalidation loss: 0.0963\t validation accuracy: 0.9733\n",
      "iteration number: 366\t training loss: 0.0369\tvalidation loss: 0.0938\t validation accuracy: 0.9756\n",
      "iteration number: 367\t training loss: 0.0365\tvalidation loss: 0.0920\t validation accuracy: 0.9778\n",
      "iteration number: 368\t training loss: 0.0365\tvalidation loss: 0.0906\t validation accuracy: 0.9778\n",
      "iteration number: 369\t training loss: 0.0371\tvalidation loss: 0.0899\t validation accuracy: 0.9800\n",
      "iteration number: 370\t training loss: 0.0381\tvalidation loss: 0.0910\t validation accuracy: 0.9778\n",
      "iteration number: 371\t training loss: 0.0398\tvalidation loss: 0.0926\t validation accuracy: 0.9778\n",
      "iteration number: 372\t training loss: 0.0389\tvalidation loss: 0.0918\t validation accuracy: 0.9778\n",
      "iteration number: 373\t training loss: 0.0384\tvalidation loss: 0.0911\t validation accuracy: 0.9733\n",
      "iteration number: 374\t training loss: 0.0385\tvalidation loss: 0.0899\t validation accuracy: 0.9756\n",
      "iteration number: 375\t training loss: 0.0393\tvalidation loss: 0.0898\t validation accuracy: 0.9733\n",
      "iteration number: 376\t training loss: 0.0400\tvalidation loss: 0.0912\t validation accuracy: 0.9711\n",
      "iteration number: 377\t training loss: 0.0413\tvalidation loss: 0.0928\t validation accuracy: 0.9711\n",
      "iteration number: 378\t training loss: 0.0412\tvalidation loss: 0.0919\t validation accuracy: 0.9711\n",
      "iteration number: 379\t training loss: 0.0400\tvalidation loss: 0.0899\t validation accuracy: 0.9778\n",
      "iteration number: 380\t training loss: 0.0391\tvalidation loss: 0.0885\t validation accuracy: 0.9778\n",
      "iteration number: 381\t training loss: 0.0378\tvalidation loss: 0.0858\t validation accuracy: 0.9822\n",
      "iteration number: 382\t training loss: 0.0360\tvalidation loss: 0.0840\t validation accuracy: 0.9822\n",
      "iteration number: 383\t training loss: 0.0351\tvalidation loss: 0.0837\t validation accuracy: 0.9822\n",
      "iteration number: 384\t training loss: 0.0347\tvalidation loss: 0.0841\t validation accuracy: 0.9800\n",
      "iteration number: 385\t training loss: 0.0342\tvalidation loss: 0.0834\t validation accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 386\t training loss: 0.0340\tvalidation loss: 0.0831\t validation accuracy: 0.9756\n",
      "iteration number: 387\t training loss: 0.0335\tvalidation loss: 0.0831\t validation accuracy: 0.9778\n",
      "iteration number: 388\t training loss: 0.0332\tvalidation loss: 0.0838\t validation accuracy: 0.9733\n",
      "iteration number: 389\t training loss: 0.0335\tvalidation loss: 0.0864\t validation accuracy: 0.9711\n",
      "iteration number: 390\t training loss: 0.0339\tvalidation loss: 0.0889\t validation accuracy: 0.9711\n",
      "iteration number: 391\t training loss: 0.0343\tvalidation loss: 0.0916\t validation accuracy: 0.9711\n",
      "iteration number: 392\t training loss: 0.0340\tvalidation loss: 0.0951\t validation accuracy: 0.9733\n",
      "iteration number: 393\t training loss: 0.0339\tvalidation loss: 0.0976\t validation accuracy: 0.9756\n",
      "iteration number: 394\t training loss: 0.0334\tvalidation loss: 0.0983\t validation accuracy: 0.9756\n",
      "iteration number: 395\t training loss: 0.0337\tvalidation loss: 0.0997\t validation accuracy: 0.9756\n",
      "iteration number: 396\t training loss: 0.0338\tvalidation loss: 0.0997\t validation accuracy: 0.9756\n",
      "iteration number: 397\t training loss: 0.0343\tvalidation loss: 0.1005\t validation accuracy: 0.9778\n",
      "iteration number: 398\t training loss: 0.0345\tvalidation loss: 0.0999\t validation accuracy: 0.9778\n",
      "iteration number: 399\t training loss: 0.0343\tvalidation loss: 0.0989\t validation accuracy: 0.9778\n",
      "iteration number: 400\t training loss: 0.0322\tvalidation loss: 0.0951\t validation accuracy: 0.9756\n",
      "iteration number: 401\t training loss: 0.0312\tvalidation loss: 0.0921\t validation accuracy: 0.9778\n",
      "iteration number: 402\t training loss: 0.0309\tvalidation loss: 0.0902\t validation accuracy: 0.9778\n",
      "iteration number: 403\t training loss: 0.0310\tvalidation loss: 0.0893\t validation accuracy: 0.9800\n",
      "iteration number: 404\t training loss: 0.0314\tvalidation loss: 0.0895\t validation accuracy: 0.9778\n",
      "iteration number: 405\t training loss: 0.0316\tvalidation loss: 0.0894\t validation accuracy: 0.9778\n",
      "iteration number: 406\t training loss: 0.0315\tvalidation loss: 0.0882\t validation accuracy: 0.9778\n",
      "iteration number: 407\t training loss: 0.0314\tvalidation loss: 0.0872\t validation accuracy: 0.9778\n",
      "iteration number: 408\t training loss: 0.0330\tvalidation loss: 0.0890\t validation accuracy: 0.9778\n",
      "iteration number: 409\t training loss: 0.0355\tvalidation loss: 0.0916\t validation accuracy: 0.9778\n",
      "iteration number: 410\t training loss: 0.0358\tvalidation loss: 0.0923\t validation accuracy: 0.9756\n",
      "iteration number: 411\t training loss: 0.0365\tvalidation loss: 0.0945\t validation accuracy: 0.9756\n",
      "iteration number: 412\t training loss: 0.0365\tvalidation loss: 0.0965\t validation accuracy: 0.9733\n",
      "iteration number: 413\t training loss: 0.0373\tvalidation loss: 0.0993\t validation accuracy: 0.9711\n",
      "iteration number: 414\t training loss: 0.0350\tvalidation loss: 0.0953\t validation accuracy: 0.9733\n",
      "iteration number: 415\t training loss: 0.0331\tvalidation loss: 0.0919\t validation accuracy: 0.9800\n",
      "iteration number: 416\t training loss: 0.0312\tvalidation loss: 0.0880\t validation accuracy: 0.9822\n",
      "iteration number: 417\t training loss: 0.0304\tvalidation loss: 0.0858\t validation accuracy: 0.9822\n",
      "iteration number: 418\t training loss: 0.0304\tvalidation loss: 0.0849\t validation accuracy: 0.9800\n",
      "iteration number: 419\t training loss: 0.0310\tvalidation loss: 0.0858\t validation accuracy: 0.9756\n",
      "iteration number: 420\t training loss: 0.0312\tvalidation loss: 0.0864\t validation accuracy: 0.9756\n",
      "iteration number: 421\t training loss: 0.0308\tvalidation loss: 0.0860\t validation accuracy: 0.9756\n",
      "iteration number: 422\t training loss: 0.0308\tvalidation loss: 0.0859\t validation accuracy: 0.9756\n",
      "iteration number: 423\t training loss: 0.0307\tvalidation loss: 0.0852\t validation accuracy: 0.9756\n",
      "iteration number: 424\t training loss: 0.0305\tvalidation loss: 0.0847\t validation accuracy: 0.9756\n",
      "iteration number: 425\t training loss: 0.0305\tvalidation loss: 0.0845\t validation accuracy: 0.9756\n",
      "iteration number: 426\t training loss: 0.0304\tvalidation loss: 0.0844\t validation accuracy: 0.9756\n",
      "iteration number: 427\t training loss: 0.0308\tvalidation loss: 0.0857\t validation accuracy: 0.9778\n",
      "iteration number: 428\t training loss: 0.0315\tvalidation loss: 0.0869\t validation accuracy: 0.9800\n",
      "iteration number: 429\t training loss: 0.0326\tvalidation loss: 0.0891\t validation accuracy: 0.9778\n",
      "iteration number: 430\t training loss: 0.0333\tvalidation loss: 0.0905\t validation accuracy: 0.9756\n",
      "iteration number: 431\t training loss: 0.0317\tvalidation loss: 0.0867\t validation accuracy: 0.9733\n",
      "iteration number: 432\t training loss: 0.0304\tvalidation loss: 0.0838\t validation accuracy: 0.9756\n",
      "iteration number: 433\t training loss: 0.0297\tvalidation loss: 0.0826\t validation accuracy: 0.9800\n",
      "iteration number: 434\t training loss: 0.0287\tvalidation loss: 0.0825\t validation accuracy: 0.9822\n",
      "iteration number: 435\t training loss: 0.0290\tvalidation loss: 0.0853\t validation accuracy: 0.9800\n",
      "iteration number: 436\t training loss: 0.0302\tvalidation loss: 0.0888\t validation accuracy: 0.9800\n",
      "iteration number: 437\t training loss: 0.0315\tvalidation loss: 0.0921\t validation accuracy: 0.9778\n",
      "iteration number: 438\t training loss: 0.0320\tvalidation loss: 0.0927\t validation accuracy: 0.9800\n",
      "iteration number: 439\t training loss: 0.0326\tvalidation loss: 0.0935\t validation accuracy: 0.9822\n",
      "iteration number: 440\t training loss: 0.0332\tvalidation loss: 0.0952\t validation accuracy: 0.9822\n",
      "iteration number: 441\t training loss: 0.0338\tvalidation loss: 0.0979\t validation accuracy: 0.9822\n",
      "iteration number: 442\t training loss: 0.0349\tvalidation loss: 0.1010\t validation accuracy: 0.9778\n",
      "iteration number: 443\t training loss: 0.0359\tvalidation loss: 0.1052\t validation accuracy: 0.9756\n",
      "iteration number: 444\t training loss: 0.0363\tvalidation loss: 0.1088\t validation accuracy: 0.9667\n",
      "iteration number: 445\t training loss: 0.0366\tvalidation loss: 0.1112\t validation accuracy: 0.9644\n",
      "iteration number: 446\t training loss: 0.0358\tvalidation loss: 0.1106\t validation accuracy: 0.9644\n",
      "iteration number: 447\t training loss: 0.0353\tvalidation loss: 0.1101\t validation accuracy: 0.9667\n",
      "iteration number: 448\t training loss: 0.0338\tvalidation loss: 0.1072\t validation accuracy: 0.9689\n",
      "iteration number: 449\t training loss: 0.0320\tvalidation loss: 0.1024\t validation accuracy: 0.9733\n",
      "iteration number: 450\t training loss: 0.0308\tvalidation loss: 0.0984\t validation accuracy: 0.9733\n",
      "iteration number: 451\t training loss: 0.0303\tvalidation loss: 0.0951\t validation accuracy: 0.9756\n",
      "iteration number: 452\t training loss: 0.0297\tvalidation loss: 0.0894\t validation accuracy: 0.9778\n",
      "iteration number: 453\t training loss: 0.0310\tvalidation loss: 0.0856\t validation accuracy: 0.9778\n",
      "iteration number: 454\t training loss: 0.0331\tvalidation loss: 0.0856\t validation accuracy: 0.9756\n",
      "iteration number: 455\t training loss: 0.0356\tvalidation loss: 0.0874\t validation accuracy: 0.9733\n",
      "iteration number: 456\t training loss: 0.0355\tvalidation loss: 0.0888\t validation accuracy: 0.9733\n",
      "iteration number: 457\t training loss: 0.0353\tvalidation loss: 0.0901\t validation accuracy: 0.9733\n",
      "iteration number: 458\t training loss: 0.0353\tvalidation loss: 0.0926\t validation accuracy: 0.9733\n",
      "iteration number: 459\t training loss: 0.0370\tvalidation loss: 0.0963\t validation accuracy: 0.9711\n",
      "iteration number: 460\t training loss: 0.0398\tvalidation loss: 0.1012\t validation accuracy: 0.9711\n",
      "iteration number: 461\t training loss: 0.0391\tvalidation loss: 0.1005\t validation accuracy: 0.9711\n",
      "iteration number: 462\t training loss: 0.0377\tvalidation loss: 0.0992\t validation accuracy: 0.9689\n",
      "iteration number: 463\t training loss: 0.0360\tvalidation loss: 0.0981\t validation accuracy: 0.9711\n",
      "iteration number: 464\t training loss: 0.0327\tvalidation loss: 0.0946\t validation accuracy: 0.9733\n",
      "iteration number: 465\t training loss: 0.0282\tvalidation loss: 0.0901\t validation accuracy: 0.9822\n",
      "iteration number: 466\t training loss: 0.0260\tvalidation loss: 0.0875\t validation accuracy: 0.9800\n",
      "iteration number: 467\t training loss: 0.0258\tvalidation loss: 0.0873\t validation accuracy: 0.9822\n",
      "iteration number: 468\t training loss: 0.0267\tvalidation loss: 0.0888\t validation accuracy: 0.9844\n",
      "iteration number: 469\t training loss: 0.0285\tvalidation loss: 0.0907\t validation accuracy: 0.9844\n",
      "iteration number: 470\t training loss: 0.0309\tvalidation loss: 0.0928\t validation accuracy: 0.9822\n",
      "iteration number: 471\t training loss: 0.0321\tvalidation loss: 0.0944\t validation accuracy: 0.9822\n",
      "iteration number: 472\t training loss: 0.0327\tvalidation loss: 0.0947\t validation accuracy: 0.9778\n",
      "iteration number: 473\t training loss: 0.0324\tvalidation loss: 0.0938\t validation accuracy: 0.9778\n",
      "iteration number: 474\t training loss: 0.0321\tvalidation loss: 0.0931\t validation accuracy: 0.9800\n",
      "iteration number: 475\t training loss: 0.0299\tvalidation loss: 0.0922\t validation accuracy: 0.9756\n",
      "iteration number: 476\t training loss: 0.0272\tvalidation loss: 0.0912\t validation accuracy: 0.9733\n",
      "iteration number: 477\t training loss: 0.0274\tvalidation loss: 0.0938\t validation accuracy: 0.9756\n",
      "iteration number: 478\t training loss: 0.0298\tvalidation loss: 0.0982\t validation accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 479\t training loss: 0.0337\tvalidation loss: 0.1042\t validation accuracy: 0.9711\n",
      "iteration number: 480\t training loss: 0.0368\tvalidation loss: 0.1087\t validation accuracy: 0.9689\n",
      "iteration number: 481\t training loss: 0.0378\tvalidation loss: 0.1101\t validation accuracy: 0.9689\n",
      "iteration number: 482\t training loss: 0.0372\tvalidation loss: 0.1078\t validation accuracy: 0.9667\n",
      "iteration number: 483\t training loss: 0.0343\tvalidation loss: 0.1014\t validation accuracy: 0.9644\n",
      "iteration number: 484\t training loss: 0.0329\tvalidation loss: 0.0974\t validation accuracy: 0.9689\n",
      "iteration number: 485\t training loss: 0.0318\tvalidation loss: 0.0945\t validation accuracy: 0.9711\n",
      "iteration number: 486\t training loss: 0.0317\tvalidation loss: 0.0928\t validation accuracy: 0.9756\n",
      "iteration number: 487\t training loss: 0.0284\tvalidation loss: 0.0890\t validation accuracy: 0.9800\n",
      "iteration number: 488\t training loss: 0.0258\tvalidation loss: 0.0861\t validation accuracy: 0.9822\n",
      "iteration number: 489\t training loss: 0.0255\tvalidation loss: 0.0876\t validation accuracy: 0.9800\n",
      "iteration number: 490\t training loss: 0.0274\tvalidation loss: 0.0923\t validation accuracy: 0.9844\n",
      "iteration number: 491\t training loss: 0.0292\tvalidation loss: 0.0959\t validation accuracy: 0.9822\n",
      "iteration number: 492\t training loss: 0.0313\tvalidation loss: 0.0994\t validation accuracy: 0.9756\n",
      "iteration number: 493\t training loss: 0.0315\tvalidation loss: 0.0999\t validation accuracy: 0.9778\n",
      "iteration number: 494\t training loss: 0.0310\tvalidation loss: 0.0993\t validation accuracy: 0.9778\n",
      "iteration number: 495\t training loss: 0.0290\tvalidation loss: 0.0959\t validation accuracy: 0.9778\n",
      "iteration number: 496\t training loss: 0.0276\tvalidation loss: 0.0929\t validation accuracy: 0.9778\n",
      "iteration number: 497\t training loss: 0.0261\tvalidation loss: 0.0891\t validation accuracy: 0.9800\n",
      "iteration number: 498\t training loss: 0.0248\tvalidation loss: 0.0862\t validation accuracy: 0.9844\n",
      "iteration number: 499\t training loss: 0.0249\tvalidation loss: 0.0848\t validation accuracy: 0.9844\n",
      "iteration number: 500\t training loss: 0.0258\tvalidation loss: 0.0854\t validation accuracy: 0.9822\n",
      "iteration number: 501\t training loss: 0.0268\tvalidation loss: 0.0862\t validation accuracy: 0.9778\n",
      "iteration number: 502\t training loss: 0.0281\tvalidation loss: 0.0873\t validation accuracy: 0.9778\n",
      "iteration number: 503\t training loss: 0.0288\tvalidation loss: 0.0875\t validation accuracy: 0.9756\n",
      "iteration number: 504\t training loss: 0.0301\tvalidation loss: 0.0873\t validation accuracy: 0.9756\n",
      "iteration number: 505\t training loss: 0.0315\tvalidation loss: 0.0874\t validation accuracy: 0.9733\n",
      "iteration number: 506\t training loss: 0.0315\tvalidation loss: 0.0854\t validation accuracy: 0.9778\n",
      "iteration number: 507\t training loss: 0.0285\tvalidation loss: 0.0809\t validation accuracy: 0.9756\n",
      "iteration number: 508\t training loss: 0.0261\tvalidation loss: 0.0789\t validation accuracy: 0.9800\n",
      "iteration number: 509\t training loss: 0.0250\tvalidation loss: 0.0786\t validation accuracy: 0.9800\n",
      "iteration number: 510\t training loss: 0.0252\tvalidation loss: 0.0802\t validation accuracy: 0.9800\n",
      "iteration number: 511\t training loss: 0.0260\tvalidation loss: 0.0822\t validation accuracy: 0.9822\n",
      "iteration number: 512\t training loss: 0.0249\tvalidation loss: 0.0787\t validation accuracy: 0.9844\n",
      "iteration number: 513\t training loss: 0.0241\tvalidation loss: 0.0755\t validation accuracy: 0.9822\n",
      "iteration number: 514\t training loss: 0.0252\tvalidation loss: 0.0741\t validation accuracy: 0.9822\n",
      "iteration number: 515\t training loss: 0.0280\tvalidation loss: 0.0757\t validation accuracy: 0.9778\n",
      "iteration number: 516\t training loss: 0.0286\tvalidation loss: 0.0780\t validation accuracy: 0.9778\n",
      "iteration number: 517\t training loss: 0.0268\tvalidation loss: 0.0785\t validation accuracy: 0.9778\n",
      "iteration number: 518\t training loss: 0.0237\tvalidation loss: 0.0793\t validation accuracy: 0.9800\n",
      "iteration number: 519\t training loss: 0.0223\tvalidation loss: 0.0816\t validation accuracy: 0.9822\n",
      "iteration number: 520\t training loss: 0.0226\tvalidation loss: 0.0856\t validation accuracy: 0.9822\n",
      "iteration number: 521\t training loss: 0.0246\tvalidation loss: 0.0911\t validation accuracy: 0.9756\n",
      "iteration number: 522\t training loss: 0.0269\tvalidation loss: 0.0952\t validation accuracy: 0.9800\n",
      "iteration number: 523\t training loss: 0.0261\tvalidation loss: 0.0930\t validation accuracy: 0.9822\n",
      "iteration number: 524\t training loss: 0.0244\tvalidation loss: 0.0892\t validation accuracy: 0.9844\n",
      "iteration number: 525\t training loss: 0.0228\tvalidation loss: 0.0848\t validation accuracy: 0.9844\n",
      "iteration number: 526\t training loss: 0.0221\tvalidation loss: 0.0810\t validation accuracy: 0.9800\n",
      "iteration number: 527\t training loss: 0.0242\tvalidation loss: 0.0806\t validation accuracy: 0.9778\n",
      "iteration number: 528\t training loss: 0.0283\tvalidation loss: 0.0835\t validation accuracy: 0.9756\n",
      "iteration number: 529\t training loss: 0.0326\tvalidation loss: 0.0873\t validation accuracy: 0.9689\n",
      "iteration number: 530\t training loss: 0.0308\tvalidation loss: 0.0852\t validation accuracy: 0.9733\n",
      "iteration number: 531\t training loss: 0.0281\tvalidation loss: 0.0829\t validation accuracy: 0.9756\n",
      "iteration number: 532\t training loss: 0.0258\tvalidation loss: 0.0822\t validation accuracy: 0.9756\n",
      "iteration number: 533\t training loss: 0.0240\tvalidation loss: 0.0817\t validation accuracy: 0.9756\n",
      "iteration number: 534\t training loss: 0.0233\tvalidation loss: 0.0826\t validation accuracy: 0.9778\n",
      "iteration number: 535\t training loss: 0.0229\tvalidation loss: 0.0836\t validation accuracy: 0.9800\n",
      "iteration number: 536\t training loss: 0.0232\tvalidation loss: 0.0854\t validation accuracy: 0.9778\n",
      "iteration number: 537\t training loss: 0.0235\tvalidation loss: 0.0864\t validation accuracy: 0.9800\n",
      "iteration number: 538\t training loss: 0.0232\tvalidation loss: 0.0866\t validation accuracy: 0.9800\n",
      "iteration number: 539\t training loss: 0.0232\tvalidation loss: 0.0873\t validation accuracy: 0.9800\n",
      "iteration number: 540\t training loss: 0.0231\tvalidation loss: 0.0877\t validation accuracy: 0.9800\n",
      "iteration number: 541\t training loss: 0.0226\tvalidation loss: 0.0856\t validation accuracy: 0.9800\n",
      "iteration number: 542\t training loss: 0.0229\tvalidation loss: 0.0843\t validation accuracy: 0.9800\n",
      "iteration number: 543\t training loss: 0.0237\tvalidation loss: 0.0837\t validation accuracy: 0.9800\n",
      "iteration number: 544\t training loss: 0.0257\tvalidation loss: 0.0846\t validation accuracy: 0.9778\n",
      "iteration number: 545\t training loss: 0.0281\tvalidation loss: 0.0865\t validation accuracy: 0.9778\n",
      "iteration number: 546\t training loss: 0.0308\tvalidation loss: 0.0898\t validation accuracy: 0.9733\n",
      "iteration number: 547\t training loss: 0.0290\tvalidation loss: 0.0873\t validation accuracy: 0.9756\n",
      "iteration number: 548\t training loss: 0.0268\tvalidation loss: 0.0845\t validation accuracy: 0.9756\n",
      "iteration number: 549\t training loss: 0.0236\tvalidation loss: 0.0817\t validation accuracy: 0.9733\n",
      "iteration number: 550\t training loss: 0.0224\tvalidation loss: 0.0815\t validation accuracy: 0.9711\n",
      "iteration number: 551\t training loss: 0.0229\tvalidation loss: 0.0843\t validation accuracy: 0.9733\n",
      "iteration number: 552\t training loss: 0.0240\tvalidation loss: 0.0872\t validation accuracy: 0.9733\n",
      "iteration number: 553\t training loss: 0.0244\tvalidation loss: 0.0885\t validation accuracy: 0.9733\n",
      "iteration number: 554\t training loss: 0.0235\tvalidation loss: 0.0869\t validation accuracy: 0.9733\n",
      "iteration number: 555\t training loss: 0.0218\tvalidation loss: 0.0826\t validation accuracy: 0.9778\n",
      "iteration number: 556\t training loss: 0.0212\tvalidation loss: 0.0790\t validation accuracy: 0.9778\n",
      "iteration number: 557\t training loss: 0.0216\tvalidation loss: 0.0774\t validation accuracy: 0.9800\n",
      "iteration number: 558\t training loss: 0.0222\tvalidation loss: 0.0773\t validation accuracy: 0.9800\n",
      "iteration number: 559\t training loss: 0.0241\tvalidation loss: 0.0781\t validation accuracy: 0.9800\n",
      "iteration number: 560\t training loss: 0.0246\tvalidation loss: 0.0787\t validation accuracy: 0.9800\n",
      "iteration number: 561\t training loss: 0.0238\tvalidation loss: 0.0795\t validation accuracy: 0.9800\n",
      "iteration number: 562\t training loss: 0.0230\tvalidation loss: 0.0802\t validation accuracy: 0.9800\n",
      "iteration number: 563\t training loss: 0.0219\tvalidation loss: 0.0816\t validation accuracy: 0.9800\n",
      "iteration number: 564\t training loss: 0.0211\tvalidation loss: 0.0829\t validation accuracy: 0.9800\n",
      "iteration number: 565\t training loss: 0.0208\tvalidation loss: 0.0846\t validation accuracy: 0.9800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 566\t training loss: 0.0207\tvalidation loss: 0.0863\t validation accuracy: 0.9800\n",
      "iteration number: 567\t training loss: 0.0205\tvalidation loss: 0.0864\t validation accuracy: 0.9800\n",
      "iteration number: 568\t training loss: 0.0205\tvalidation loss: 0.0868\t validation accuracy: 0.9800\n",
      "iteration number: 569\t training loss: 0.0206\tvalidation loss: 0.0871\t validation accuracy: 0.9800\n",
      "iteration number: 570\t training loss: 0.0209\tvalidation loss: 0.0876\t validation accuracy: 0.9778\n",
      "iteration number: 571\t training loss: 0.0209\tvalidation loss: 0.0885\t validation accuracy: 0.9756\n",
      "iteration number: 572\t training loss: 0.0211\tvalidation loss: 0.0916\t validation accuracy: 0.9778\n",
      "iteration number: 573\t training loss: 0.0214\tvalidation loss: 0.0927\t validation accuracy: 0.9778\n",
      "iteration number: 574\t training loss: 0.0221\tvalidation loss: 0.0938\t validation accuracy: 0.9711\n",
      "iteration number: 575\t training loss: 0.0228\tvalidation loss: 0.0946\t validation accuracy: 0.9756\n",
      "iteration number: 576\t training loss: 0.0227\tvalidation loss: 0.0932\t validation accuracy: 0.9800\n",
      "iteration number: 577\t training loss: 0.0216\tvalidation loss: 0.0891\t validation accuracy: 0.9822\n",
      "iteration number: 578\t training loss: 0.0204\tvalidation loss: 0.0847\t validation accuracy: 0.9844\n",
      "iteration number: 579\t training loss: 0.0199\tvalidation loss: 0.0806\t validation accuracy: 0.9822\n",
      "iteration number: 580\t training loss: 0.0203\tvalidation loss: 0.0781\t validation accuracy: 0.9844\n",
      "iteration number: 581\t training loss: 0.0207\tvalidation loss: 0.0768\t validation accuracy: 0.9822\n",
      "iteration number: 582\t training loss: 0.0205\tvalidation loss: 0.0773\t validation accuracy: 0.9778\n",
      "iteration number: 583\t training loss: 0.0199\tvalidation loss: 0.0767\t validation accuracy: 0.9822\n",
      "iteration number: 584\t training loss: 0.0195\tvalidation loss: 0.0771\t validation accuracy: 0.9889\n",
      "iteration number: 585\t training loss: 0.0193\tvalidation loss: 0.0771\t validation accuracy: 0.9889\n",
      "iteration number: 586\t training loss: 0.0192\tvalidation loss: 0.0771\t validation accuracy: 0.9889\n",
      "iteration number: 587\t training loss: 0.0195\tvalidation loss: 0.0777\t validation accuracy: 0.9867\n",
      "iteration number: 588\t training loss: 0.0200\tvalidation loss: 0.0794\t validation accuracy: 0.9867\n",
      "iteration number: 589\t training loss: 0.0207\tvalidation loss: 0.0808\t validation accuracy: 0.9867\n",
      "iteration number: 590\t training loss: 0.0211\tvalidation loss: 0.0816\t validation accuracy: 0.9844\n",
      "iteration number: 591\t training loss: 0.0214\tvalidation loss: 0.0826\t validation accuracy: 0.9844\n",
      "iteration number: 592\t training loss: 0.0216\tvalidation loss: 0.0833\t validation accuracy: 0.9844\n",
      "iteration number: 593\t training loss: 0.0213\tvalidation loss: 0.0831\t validation accuracy: 0.9844\n",
      "iteration number: 594\t training loss: 0.0212\tvalidation loss: 0.0835\t validation accuracy: 0.9844\n",
      "iteration number: 595\t training loss: 0.0210\tvalidation loss: 0.0831\t validation accuracy: 0.9756\n",
      "iteration number: 596\t training loss: 0.0212\tvalidation loss: 0.0829\t validation accuracy: 0.9756\n",
      "iteration number: 597\t training loss: 0.0211\tvalidation loss: 0.0819\t validation accuracy: 0.9800\n",
      "iteration number: 598\t training loss: 0.0208\tvalidation loss: 0.0808\t validation accuracy: 0.9778\n",
      "iteration number: 599\t training loss: 0.0205\tvalidation loss: 0.0807\t validation accuracy: 0.9800\n",
      "iteration number: 600\t training loss: 0.0204\tvalidation loss: 0.0811\t validation accuracy: 0.9800\n",
      "iteration number: 601\t training loss: 0.0207\tvalidation loss: 0.0823\t validation accuracy: 0.9822\n",
      "iteration number: 602\t training loss: 0.0209\tvalidation loss: 0.0839\t validation accuracy: 0.9800\n",
      "iteration number: 603\t training loss: 0.0201\tvalidation loss: 0.0834\t validation accuracy: 0.9822\n",
      "iteration number: 604\t training loss: 0.0184\tvalidation loss: 0.0809\t validation accuracy: 0.9844\n",
      "iteration number: 605\t training loss: 0.0181\tvalidation loss: 0.0804\t validation accuracy: 0.9778\n",
      "iteration number: 606\t training loss: 0.0189\tvalidation loss: 0.0809\t validation accuracy: 0.9800\n",
      "iteration number: 607\t training loss: 0.0201\tvalidation loss: 0.0817\t validation accuracy: 0.9778\n",
      "iteration number: 608\t training loss: 0.0233\tvalidation loss: 0.0864\t validation accuracy: 0.9756\n",
      "iteration number: 609\t training loss: 0.0273\tvalidation loss: 0.0924\t validation accuracy: 0.9711\n",
      "iteration number: 610\t training loss: 0.0265\tvalidation loss: 0.0919\t validation accuracy: 0.9689\n",
      "iteration number: 611\t training loss: 0.0247\tvalidation loss: 0.0912\t validation accuracy: 0.9689\n",
      "iteration number: 612\t training loss: 0.0218\tvalidation loss: 0.0880\t validation accuracy: 0.9778\n",
      "iteration number: 613\t training loss: 0.0194\tvalidation loss: 0.0846\t validation accuracy: 0.9800\n",
      "iteration number: 614\t training loss: 0.0176\tvalidation loss: 0.0820\t validation accuracy: 0.9822\n",
      "**MLP with momentum**\n",
      "Training accuracy: 0.9978\n",
      "Validation accuracy: 0.9822\n",
      "Training loss: 0.0176\n",
      "Validation loss: 0.0820\n",
      "Number of iterations: 614\n"
     ]
    }
   ],
   "source": [
    "mlp_with_momentum = MultiLayerPerceptron(\n",
    "    X, Y, hidden_size=50, \n",
    "    activation='relu', \n",
    "    dropout=False, \n",
    "    dropout_rate=0\n",
    ")\n",
    "mlp_with_momentum.train(\n",
    "    optimizer='sgd',\n",
    "    momentum=True,\n",
    "    min_iterations=500,\n",
    "    max_iterations=5000,\n",
    "    initial_step=1e-1,\n",
    "    batch_size=64,\n",
    "    early_stopping=True,\n",
    "    early_stopping_lookbehind=100,\n",
    "    early_stopping_delta=1e-4, \n",
    "    vectorized=True,\n",
    "    verbose=True\n",
    ")\n",
    "print(\"**MLP with momentum**\")\n",
    "print(\"Training accuracy: {:.4f}\".format(mlp_with_momentum.accuracy_on_train()))\n",
    "print(\"Validation accuracy: {:.4f}\".format(mlp_with_momentum.accuracy_on_validation()))\n",
    "print(\"Training loss: {:.4f}\".format(mlp_with_momentum.training_losses_history[-1]))\n",
    "print(\"Validation loss: {:.4f}\".format(mlp_with_momentum.validation_losses_history[-1]))\n",
    "print(\"Number of iterations: {:d}\".format(len(mlp_with_momentum.training_losses_history)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp_early_stopping.plot_loss_history(add_to_title=\"early stopping\")\n",
    "mlp_with_momentum.plot_loss_history(add_to_title=\"early stopping and momentum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Comparing without and with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**MLP without early stopping nor dropout**\n",
      "Training accuracy: 0.9807\n",
      "Validation accuracy: 0.9600\n",
      "Training loss: 0.0839\n",
      "Validation loss: 0.1496\n"
     ]
    }
   ],
   "source": [
    "mlp_without_dropout = MultiLayerPerceptron(\n",
    "    X, Y, hidden_size=50, \n",
    "    activation='relu', \n",
    "    dropout=False\n",
    ")\n",
    "mlp_without_dropout.train(\n",
    "    optimizer='sgd',\n",
    "    min_iterations=500,\n",
    "    max_iterations=5000,\n",
    "    initial_step=1e-1,\n",
    "    batch_size=64,\n",
    "    early_stopping=True,\n",
    "    early_stopping_lookbehind=100,\n",
    "    early_stopping_delta=1e-4, \n",
    "    vectorized=True,\n",
    "    verbose=False\n",
    ")\n",
    "print(\"**MLP without early stopping nor dropout**\")\n",
    "print(\"Training accuracy: {:.4f}\".format(mlp_without_dropout.accuracy_on_train()))\n",
    "print(\"Validation accuracy: {:.4f}\".format(mlp_without_dropout.accuracy_on_validation()))\n",
    "print(\"Training loss: {:.4f}\".format(mlp_without_dropout.training_losses_history[-1]))\n",
    "print(\"Validation loss: {:.4f}\".format(mlp_without_dropout.validation_losses_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**MLP with dropout**\n",
      "Training accuracy: 0.9688\n",
      "Validation accuracy: 0.9667\n",
      "Training loss: 0.1013\n",
      "Validation loss: 0.1353\n"
     ]
    }
   ],
   "source": [
    "mlp_with_dropout = MultiLayerPerceptron(\n",
    "    X, Y, hidden_size=50,\n",
    "    activation='relu',\n",
    "    dropout=True,\n",
    "    dropout_rate=0.25\n",
    ")\n",
    "mlp_with_dropout.train(\n",
    "    optimizer='sgd',\n",
    "    min_iterations=500,\n",
    "    max_iterations=5000,\n",
    "    initial_step=1e-1,\n",
    "    batch_size=64,\n",
    "    early_stopping=True,\n",
    "    early_stopping_lookbehind=100,\n",
    "    early_stopping_delta=1e-4, \n",
    "    vectorized=True,\n",
    "    verbose=False\n",
    ")\n",
    "print(\"**MLP with dropout**\")\n",
    "print(\"Training accuracy: {:.4f}\".format(mlp_with_dropout.accuracy_on_train()))\n",
    "print(\"Validation accuracy: {:.4f}\".format(mlp_with_dropout.accuracy_on_validation()))\n",
    "print(\"Training loss: {:.4f}\".format(mlp_with_dropout.training_losses_history[-1]))\n",
    "print(\"Validation loss: {:.4f}\".format(mlp_with_dropout.validation_losses_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp_without_dropout.plot_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp_with_dropout.plot_loss_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Multiclass classification MLP with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Implement the same network architecture with Keras;\n",
    "    - First using the Sequential API\n",
    "    - Secondly using the functional API\n",
    "\n",
    "#### - Check that the Keras model can approximately reproduce the behavior of the Numpy model.\n",
    "\n",
    "#### - Compute the negative log likelihood of a sample 42 in the validation set (can use `model.predict_proba`).\n",
    "\n",
    "#### - Compute the average negative log-likelihood on the full validation set.\n",
    "\n",
    "#### - Compute the average negative log-likelihood  on the full training set and check that you can get the value of the loss reported by Keras.\n",
    "\n",
    "#### - Is the model overfitting or underfitting? (ensure that the model has fully converged by increasing the number of epochs to 500 or more if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X[0].shape[0]\n",
    "n_classes = len(np.unique(Y_tr))\n",
    "n_hidden = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with relu activation\n",
      "Train on 1347 samples, validate on 450 samples\n",
      "Epoch 1/100\n",
      "1347/1347 [==============================] - 0s 149us/step - loss: 2.3828 - acc: 0.0839 - val_loss: 2.2774 - val_acc: 0.1733\n",
      "Epoch 2/100\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 2.2363 - acc: 0.1886 - val_loss: 2.1684 - val_acc: 0.2533\n",
      "Epoch 3/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 2.1322 - acc: 0.2576 - val_loss: 2.0574 - val_acc: 0.3267\n",
      "Epoch 4/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 2.0183 - acc: 0.3267 - val_loss: 1.9365 - val_acc: 0.3800\n",
      "Epoch 5/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 1.8908 - acc: 0.3950 - val_loss: 1.8003 - val_acc: 0.5067\n",
      "Epoch 6/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 1.7473 - acc: 0.5145 - val_loss: 1.6506 - val_acc: 0.5756\n",
      "Epoch 7/100\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 1.5920 - acc: 0.5716 - val_loss: 1.4920 - val_acc: 0.6200\n",
      "Epoch 8/100\n",
      "1347/1347 [==============================] - 0s 67us/step - loss: 1.4195 - acc: 0.6570 - val_loss: 1.3056 - val_acc: 0.7044\n",
      "Epoch 9/100\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 1.2241 - acc: 0.7491 - val_loss: 1.1334 - val_acc: 0.7644\n",
      "Epoch 10/100\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 1.0473 - acc: 0.8040 - val_loss: 0.9830 - val_acc: 0.8067\n",
      "Epoch 11/100\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.8977 - acc: 0.8352 - val_loss: 0.8613 - val_acc: 0.8222\n",
      "Epoch 12/100\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.7782 - acc: 0.8745 - val_loss: 0.7672 - val_acc: 0.8444\n",
      "Epoch 13/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.6859 - acc: 0.8820 - val_loss: 0.6917 - val_acc: 0.8511\n",
      "Epoch 14/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.6135 - acc: 0.8946 - val_loss: 0.6337 - val_acc: 0.8556\n",
      "Epoch 15/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.5548 - acc: 0.9065 - val_loss: 0.5857 - val_acc: 0.8622\n",
      "Epoch 16/100\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.5073 - acc: 0.9139 - val_loss: 0.5460 - val_acc: 0.8711\n",
      "Epoch 17/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.4670 - acc: 0.9213 - val_loss: 0.5132 - val_acc: 0.8733\n",
      "Epoch 18/100\n",
      "1347/1347 [==============================] - 0s 42us/step - loss: 0.4340 - acc: 0.9169 - val_loss: 0.4818 - val_acc: 0.8778\n",
      "Epoch 19/100\n",
      "1347/1347 [==============================] - 0s 47us/step - loss: 0.4039 - acc: 0.9228 - val_loss: 0.4566 - val_acc: 0.8889\n",
      "Epoch 20/100\n",
      "1347/1347 [==============================] - 0s 47us/step - loss: 0.3771 - acc: 0.9324 - val_loss: 0.4370 - val_acc: 0.8889\n",
      "Epoch 21/100\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.3564 - acc: 0.9302 - val_loss: 0.4166 - val_acc: 0.8911\n",
      "Epoch 22/100\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.3351 - acc: 0.9347 - val_loss: 0.3996 - val_acc: 0.8933\n",
      "Epoch 23/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.3191 - acc: 0.9324 - val_loss: 0.3825 - val_acc: 0.9000\n",
      "Epoch 24/100\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.3024 - acc: 0.9362 - val_loss: 0.3690 - val_acc: 0.9022\n",
      "Epoch 25/100\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 0.2877 - acc: 0.9399 - val_loss: 0.3573 - val_acc: 0.9022\n",
      "Epoch 26/100\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.2765 - acc: 0.9414 - val_loss: 0.3446 - val_acc: 0.9067\n",
      "Epoch 27/100\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 0.2636 - acc: 0.9436 - val_loss: 0.3342 - val_acc: 0.9022\n",
      "Epoch 28/100\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2536 - acc: 0.9473 - val_loss: 0.3244 - val_acc: 0.9067\n",
      "Epoch 29/100\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.2432 - acc: 0.9458 - val_loss: 0.3151 - val_acc: 0.9067\n",
      "Epoch 30/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2339 - acc: 0.9525 - val_loss: 0.3072 - val_acc: 0.9156\n",
      "Epoch 31/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2245 - acc: 0.9540 - val_loss: 0.2994 - val_acc: 0.9111\n",
      "Epoch 32/100\n",
      "1347/1347 [==============================] - 0s 27us/step - loss: 0.2164 - acc: 0.9584 - val_loss: 0.2924 - val_acc: 0.9156\n",
      "Epoch 33/100\n",
      "1347/1347 [==============================] - 0s 27us/step - loss: 0.2092 - acc: 0.9562 - val_loss: 0.2865 - val_acc: 0.9200\n",
      "Epoch 34/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.2017 - acc: 0.9599 - val_loss: 0.2777 - val_acc: 0.9178\n",
      "Epoch 35/100\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.1954 - acc: 0.9621 - val_loss: 0.2730 - val_acc: 0.9156\n",
      "Epoch 36/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1897 - acc: 0.9569 - val_loss: 0.2662 - val_acc: 0.9267\n",
      "Epoch 37/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1836 - acc: 0.9614 - val_loss: 0.2593 - val_acc: 0.9244\n",
      "Epoch 38/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1784 - acc: 0.9614 - val_loss: 0.2541 - val_acc: 0.9244\n",
      "Epoch 39/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1737 - acc: 0.9629 - val_loss: 0.2520 - val_acc: 0.9267\n",
      "Epoch 40/100\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.1686 - acc: 0.9651 - val_loss: 0.2482 - val_acc: 0.9311\n",
      "Epoch 41/100\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.1635 - acc: 0.9651 - val_loss: 0.2423 - val_acc: 0.9289\n",
      "Epoch 42/100\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1593 - acc: 0.9696 - val_loss: 0.2374 - val_acc: 0.9289\n",
      "Epoch 43/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1550 - acc: 0.9696 - val_loss: 0.2332 - val_acc: 0.9333\n",
      "Epoch 44/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1518 - acc: 0.9718 - val_loss: 0.2296 - val_acc: 0.9333\n",
      "Epoch 45/100\n",
      "1347/1347 [==============================] - 0s 26us/step - loss: 0.1479 - acc: 0.9725 - val_loss: 0.2260 - val_acc: 0.9356\n",
      "Epoch 46/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1452 - acc: 0.9762 - val_loss: 0.2207 - val_acc: 0.9378\n",
      "Epoch 47/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1406 - acc: 0.9770 - val_loss: 0.2219 - val_acc: 0.9378\n",
      "Epoch 48/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1373 - acc: 0.9755 - val_loss: 0.2181 - val_acc: 0.9356\n",
      "Epoch 49/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1348 - acc: 0.9770 - val_loss: 0.2160 - val_acc: 0.9356\n",
      "Epoch 50/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1321 - acc: 0.9770 - val_loss: 0.2104 - val_acc: 0.9400\n",
      "Epoch 51/100\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1360 - acc: 0.9710 - val_loss: 0.2084 - val_acc: 0.9378\n",
      "Epoch 52/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1269 - acc: 0.9777 - val_loss: 0.2044 - val_acc: 0.9378\n",
      "Epoch 53/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1234 - acc: 0.9800 - val_loss: 0.2024 - val_acc: 0.9422\n",
      "Epoch 54/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1214 - acc: 0.9785 - val_loss: 0.1990 - val_acc: 0.9422\n",
      "Epoch 55/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1205 - acc: 0.9785 - val_loss: 0.2000 - val_acc: 0.9378\n",
      "Epoch 56/100\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.1175 - acc: 0.9800 - val_loss: 0.1967 - val_acc: 0.9400\n",
      "Epoch 57/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1183 - acc: 0.9770 - val_loss: 0.1957 - val_acc: 0.9356\n",
      "Epoch 58/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1134 - acc: 0.9792 - val_loss: 0.1921 - val_acc: 0.9400\n",
      "Epoch 59/100\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1111 - acc: 0.9792 - val_loss: 0.1897 - val_acc: 0.9400\n",
      "Epoch 60/100\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1093 - acc: 0.9807 - val_loss: 0.1886 - val_acc: 0.9422\n",
      "Epoch 61/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1074 - acc: 0.9807 - val_loss: 0.1867 - val_acc: 0.9467\n",
      "Epoch 62/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1058 - acc: 0.9807 - val_loss: 0.1840 - val_acc: 0.9422\n",
      "Epoch 63/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1039 - acc: 0.9807 - val_loss: 0.1819 - val_acc: 0.9422\n",
      "Epoch 64/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1026 - acc: 0.9800 - val_loss: 0.1791 - val_acc: 0.9422\n",
      "Epoch 65/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1005 - acc: 0.9822 - val_loss: 0.1816 - val_acc: 0.9467\n",
      "Epoch 66/100\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.0993 - acc: 0.9807 - val_loss: 0.1768 - val_acc: 0.9422\n",
      "Epoch 67/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0984 - acc: 0.9800 - val_loss: 0.1782 - val_acc: 0.9444\n",
      "Epoch 68/100\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0967 - acc: 0.9807 - val_loss: 0.1774 - val_acc: 0.9400\n",
      "Epoch 69/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0941 - acc: 0.9837 - val_loss: 0.1725 - val_acc: 0.9444\n",
      "Epoch 70/100\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0932 - acc: 0.9814 - val_loss: 0.1752 - val_acc: 0.9444\n",
      "Epoch 71/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0920 - acc: 0.9837 - val_loss: 0.1726 - val_acc: 0.9489\n",
      "Epoch 72/100\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0905 - acc: 0.9837 - val_loss: 0.1690 - val_acc: 0.9444\n",
      "Epoch 73/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0893 - acc: 0.9837 - val_loss: 0.1703 - val_acc: 0.9467\n",
      "Epoch 74/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0882 - acc: 0.9829 - val_loss: 0.1652 - val_acc: 0.9467\n",
      "Epoch 75/100\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0899 - acc: 0.9844 - val_loss: 0.1667 - val_acc: 0.9467\n",
      "Epoch 76/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0856 - acc: 0.9859 - val_loss: 0.1647 - val_acc: 0.9511\n",
      "Epoch 77/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0843 - acc: 0.9866 - val_loss: 0.1645 - val_acc: 0.9511\n",
      "Epoch 78/100\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0830 - acc: 0.9859 - val_loss: 0.1624 - val_acc: 0.9556\n",
      "Epoch 79/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0817 - acc: 0.9859 - val_loss: 0.1614 - val_acc: 0.9444\n",
      "Epoch 80/100\n",
      "1347/1347 [==============================] - 0s 27us/step - loss: 0.0811 - acc: 0.9866 - val_loss: 0.1620 - val_acc: 0.9467\n",
      "Epoch 81/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0798 - acc: 0.9866 - val_loss: 0.1592 - val_acc: 0.9467\n",
      "Epoch 82/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0798 - acc: 0.9866 - val_loss: 0.1569 - val_acc: 0.9533\n",
      "Epoch 83/100\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0781 - acc: 0.9859 - val_loss: 0.1590 - val_acc: 0.9533\n",
      "Epoch 84/100\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0777 - acc: 0.9859 - val_loss: 0.1569 - val_acc: 0.9533\n",
      "Epoch 85/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0757 - acc: 0.9881 - val_loss: 0.1580 - val_acc: 0.9533\n",
      "Epoch 86/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0779 - acc: 0.9852 - val_loss: 0.1554 - val_acc: 0.9511\n",
      "Epoch 87/100\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0756 - acc: 0.9881 - val_loss: 0.1548 - val_acc: 0.9556\n",
      "Epoch 88/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0729 - acc: 0.9881 - val_loss: 0.1544 - val_acc: 0.9533\n",
      "Epoch 89/100\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0720 - acc: 0.9889 - val_loss: 0.1525 - val_acc: 0.9556\n",
      "Epoch 90/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0715 - acc: 0.9874 - val_loss: 0.1516 - val_acc: 0.9556\n",
      "Epoch 91/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0704 - acc: 0.9866 - val_loss: 0.1531 - val_acc: 0.9511\n",
      "Epoch 92/100\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0694 - acc: 0.9896 - val_loss: 0.1489 - val_acc: 0.9556\n",
      "Epoch 93/100\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0691 - acc: 0.9881 - val_loss: 0.1478 - val_acc: 0.9533\n",
      "Epoch 94/100\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.0683 - acc: 0.9896 - val_loss: 0.1480 - val_acc: 0.9556\n",
      "Epoch 95/100\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.0670 - acc: 0.9896 - val_loss: 0.1480 - val_acc: 0.9533\n",
      "Epoch 96/100\n",
      "1347/1347 [==============================] - 0s 27us/step - loss: 0.0663 - acc: 0.9889 - val_loss: 0.1460 - val_acc: 0.9533\n",
      "Epoch 97/100\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0653 - acc: 0.9889 - val_loss: 0.1457 - val_acc: 0.9578\n",
      "Epoch 98/100\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0647 - acc: 0.9903 - val_loss: 0.1455 - val_acc: 0.9578\n",
      "Epoch 99/100\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0641 - acc: 0.9896 - val_loss: 0.1453 - val_acc: 0.9578\n",
      "Epoch 100/100\n",
      "1347/1347 [==============================] - 0s 27us/step - loss: 0.0636 - acc: 0.9903 - val_loss: 0.1456 - val_acc: 0.9533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d6c4cc438>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(n_hidden, input_dim=n_features, activation=activation))\n",
    "keras_model.add(Dense(n_classes, activation='softmax'))\n",
    "keras_model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "keras_model.fit(\n",
    "    X_tr, to_categorical(Y_tr, num_classes=n_classes),\n",
    "    epochs=100, batch_size=32, verbose=1,\n",
    "    validation_data=(X_val, to_categorical(Y_val, num_classes=n_classes))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sample 42**\n",
      "Negative loglikelihood: 0.0014035\n"
     ]
    }
   ],
   "source": [
    "index_sample = 42\n",
    "\n",
    "Y_val_one_hot_sample = to_categorical(Y_val[index_sample], num_classes=n_classes)\n",
    "Y_val_pred_sample = np.squeeze(keras_model.predict_proba(np.expand_dims(X_val[index_sample], axis=0)))\n",
    "nll_val_sample = -np.log(Y_val_one_hot_sample.dot(Y_val_pred_sample))\n",
    "print(\"**Sample {:d}**\".format(index_sample))\n",
    "print(\"Negative loglikelihood: {:.7f}\".format(nll_val_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Whole validation set**\n",
      "Negative loglikelihood: 0.1456044\n"
     ]
    }
   ],
   "source": [
    "Y_val_one_hot = to_categorical(Y_val, num_classes=n_classes)\n",
    "Y_val_pred = keras_model.predict_proba(X_val)\n",
    "nll_val = np.matmul(Y_val_one_hot.reshape(-1, 1, n_classes), Y_val_pred.reshape(-1, n_classes, 1))\n",
    "nll_val = -np.mean(np.log(np.squeeze(nll_val)))\n",
    "print(\"**Whole validation set**\")\n",
    "print(\"Negative loglikelihood: {:.7f}\".format(nll_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Whole training set**\n",
      "Our negative loglikelihood: 0.0615000\n",
      "Keras negative loglikelihood: 0.0615000\n"
     ]
    }
   ],
   "source": [
    "Y_tr_one_hot = to_categorical(Y_tr, num_classes=n_classes)\n",
    "Y_tr_pred = keras_model.predict_proba(X_tr)\n",
    "nll_tr = np.matmul(Y_tr_one_hot.reshape(-1, 1, n_classes), Y_tr_pred.reshape(-1, n_classes, 1))\n",
    "nll_tr = -np.mean(np.log(np.squeeze(nll_tr)))\n",
    "print(\"**Whole training set**\")\n",
    "print(\"Our negative loglikelihood: {:.7f}\".format(nll_tr))\n",
    "keras_nll_tr = keras.losses.categorical_crossentropy(K.constant(Y_tr_one_hot), K.constant(Y_tr_pred))\n",
    "keras_nll_tr = np.mean(K.eval(keras_nll_tr))\n",
    "print(\"Keras negative loglikelihood: {:.7f}\".format(keras_nll_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with relu activation\n",
      "Train on 1347 samples, validate on 450 samples\n",
      "Epoch 1/300\n",
      "1347/1347 [==============================] - 0s 170us/step - loss: 2.3759 - acc: 0.1039 - val_loss: 2.2631 - val_acc: 0.1711\n",
      "Epoch 2/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 2.1950 - acc: 0.2168 - val_loss: 2.1302 - val_acc: 0.2489\n",
      "Epoch 3/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 2.0650 - acc: 0.2947 - val_loss: 2.0116 - val_acc: 0.2956\n",
      "Epoch 4/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 1.9462 - acc: 0.3281 - val_loss: 1.8976 - val_acc: 0.3311\n",
      "Epoch 5/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 1.8272 - acc: 0.3808 - val_loss: 1.7785 - val_acc: 0.3956\n",
      "Epoch 6/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 1.6988 - acc: 0.4573 - val_loss: 1.6469 - val_acc: 0.4756\n",
      "Epoch 7/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 1.5531 - acc: 0.5241 - val_loss: 1.5011 - val_acc: 0.5356\n",
      "Epoch 8/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 1.4104 - acc: 0.5798 - val_loss: 1.3638 - val_acc: 0.6267\n",
      "Epoch 9/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 1.2740 - acc: 0.6875 - val_loss: 1.2353 - val_acc: 0.7178\n",
      "Epoch 10/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 1.1487 - acc: 0.7498 - val_loss: 1.1176 - val_acc: 0.7533\n",
      "Epoch 11/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 1.0356 - acc: 0.7966 - val_loss: 1.0122 - val_acc: 0.7978\n",
      "Epoch 12/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.9318 - acc: 0.8181 - val_loss: 0.9225 - val_acc: 0.8089\n",
      "Epoch 13/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.8456 - acc: 0.8434 - val_loss: 0.8428 - val_acc: 0.8111\n",
      "Epoch 14/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.7653 - acc: 0.8523 - val_loss: 0.7714 - val_acc: 0.8356\n",
      "Epoch 15/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.6989 - acc: 0.8634 - val_loss: 0.7163 - val_acc: 0.8356\n",
      "Epoch 16/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.6446 - acc: 0.8679 - val_loss: 0.6696 - val_acc: 0.8556\n",
      "Epoch 17/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.5940 - acc: 0.8797 - val_loss: 0.6237 - val_acc: 0.8667\n",
      "Epoch 18/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.5515 - acc: 0.8886 - val_loss: 0.5857 - val_acc: 0.8689\n",
      "Epoch 19/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.5125 - acc: 0.9013 - val_loss: 0.5552 - val_acc: 0.8578\n",
      "Epoch 20/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.4801 - acc: 0.9065 - val_loss: 0.5285 - val_acc: 0.8733\n",
      "Epoch 21/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.4518 - acc: 0.9072 - val_loss: 0.5024 - val_acc: 0.8689\n",
      "Epoch 22/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.4241 - acc: 0.9169 - val_loss: 0.4821 - val_acc: 0.8756\n",
      "Epoch 23/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.4019 - acc: 0.9169 - val_loss: 0.4613 - val_acc: 0.8800\n",
      "Epoch 24/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.3806 - acc: 0.9220 - val_loss: 0.4457 - val_acc: 0.8844\n",
      "Epoch 25/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.3640 - acc: 0.9250 - val_loss: 0.4300 - val_acc: 0.8867\n",
      "Epoch 26/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.3468 - acc: 0.9265 - val_loss: 0.4141 - val_acc: 0.8889\n",
      "Epoch 27/300\n",
      "1347/1347 [==============================] - 0s 27us/step - loss: 0.3306 - acc: 0.9317 - val_loss: 0.4017 - val_acc: 0.8911\n",
      "Epoch 28/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.3171 - acc: 0.9324 - val_loss: 0.3913 - val_acc: 0.8889\n",
      "Epoch 29/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.3036 - acc: 0.9369 - val_loss: 0.3806 - val_acc: 0.8956\n",
      "Epoch 30/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.2916 - acc: 0.9384 - val_loss: 0.3684 - val_acc: 0.9000\n",
      "Epoch 31/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.2831 - acc: 0.9391 - val_loss: 0.3612 - val_acc: 0.9000\n",
      "Epoch 32/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.2701 - acc: 0.9451 - val_loss: 0.3503 - val_acc: 0.8978\n",
      "Epoch 33/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.2611 - acc: 0.9458 - val_loss: 0.3438 - val_acc: 0.8956\n",
      "Epoch 34/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2541 - acc: 0.9458 - val_loss: 0.3345 - val_acc: 0.9000\n",
      "Epoch 35/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2445 - acc: 0.9480 - val_loss: 0.3285 - val_acc: 0.8978\n",
      "Epoch 36/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.2369 - acc: 0.9510 - val_loss: 0.3215 - val_acc: 0.9022\n",
      "Epoch 37/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2301 - acc: 0.9525 - val_loss: 0.3149 - val_acc: 0.9044\n",
      "Epoch 38/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2230 - acc: 0.9562 - val_loss: 0.3099 - val_acc: 0.9044\n",
      "Epoch 39/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2163 - acc: 0.9547 - val_loss: 0.3018 - val_acc: 0.9044\n",
      "Epoch 40/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2099 - acc: 0.9599 - val_loss: 0.2968 - val_acc: 0.9044\n",
      "Epoch 41/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2048 - acc: 0.9577 - val_loss: 0.2906 - val_acc: 0.9067\n",
      "Epoch 42/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1994 - acc: 0.9592 - val_loss: 0.2887 - val_acc: 0.9156\n",
      "Epoch 43/300\n",
      "1347/1347 [==============================] - 0s 27us/step - loss: 0.1954 - acc: 0.9607 - val_loss: 0.2798 - val_acc: 0.9067\n",
      "Epoch 44/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1892 - acc: 0.9584 - val_loss: 0.2782 - val_acc: 0.9111\n",
      "Epoch 45/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1846 - acc: 0.9614 - val_loss: 0.2728 - val_acc: 0.9089\n",
      "Epoch 46/300\n",
      "1347/1347 [==============================] - 0s 43us/step - loss: 0.1808 - acc: 0.9592 - val_loss: 0.2684 - val_acc: 0.9244\n",
      "Epoch 47/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.1760 - acc: 0.9621 - val_loss: 0.2663 - val_acc: 0.9178\n",
      "Epoch 48/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1720 - acc: 0.9629 - val_loss: 0.2605 - val_acc: 0.9222\n",
      "Epoch 49/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1686 - acc: 0.9629 - val_loss: 0.2566 - val_acc: 0.9267\n",
      "Epoch 50/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1659 - acc: 0.9644 - val_loss: 0.2564 - val_acc: 0.9244\n",
      "Epoch 51/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1620 - acc: 0.9629 - val_loss: 0.2519 - val_acc: 0.9289\n",
      "Epoch 52/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1588 - acc: 0.9666 - val_loss: 0.2507 - val_acc: 0.9244\n",
      "Epoch 53/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1546 - acc: 0.9659 - val_loss: 0.2459 - val_acc: 0.9311\n",
      "Epoch 54/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1512 - acc: 0.9673 - val_loss: 0.2414 - val_acc: 0.9333\n",
      "Epoch 55/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.1486 - acc: 0.9681 - val_loss: 0.2375 - val_acc: 0.9356\n",
      "Epoch 56/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.1466 - acc: 0.9681 - val_loss: 0.2359 - val_acc: 0.9333\n",
      "Epoch 57/300\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 0.1433 - acc: 0.9673 - val_loss: 0.2338 - val_acc: 0.9289\n",
      "Epoch 58/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1406 - acc: 0.9696 - val_loss: 0.2330 - val_acc: 0.9333\n",
      "Epoch 59/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1389 - acc: 0.9696 - val_loss: 0.2314 - val_acc: 0.9267\n",
      "Epoch 60/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1363 - acc: 0.9710 - val_loss: 0.2257 - val_acc: 0.9356\n",
      "Epoch 61/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1332 - acc: 0.9703 - val_loss: 0.2236 - val_acc: 0.9356\n",
      "Epoch 62/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1315 - acc: 0.9733 - val_loss: 0.2221 - val_acc: 0.9378\n",
      "Epoch 63/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1284 - acc: 0.9733 - val_loss: 0.2194 - val_acc: 0.9356\n",
      "Epoch 64/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1265 - acc: 0.9733 - val_loss: 0.2184 - val_acc: 0.9378\n",
      "Epoch 65/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1254 - acc: 0.9718 - val_loss: 0.2151 - val_acc: 0.9378\n",
      "Epoch 66/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1229 - acc: 0.9755 - val_loss: 0.2143 - val_acc: 0.9378\n",
      "Epoch 67/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1208 - acc: 0.9755 - val_loss: 0.2100 - val_acc: 0.9444\n",
      "Epoch 68/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1188 - acc: 0.9748 - val_loss: 0.2094 - val_acc: 0.9444\n",
      "Epoch 69/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1169 - acc: 0.9777 - val_loss: 0.2090 - val_acc: 0.9400\n",
      "Epoch 70/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1149 - acc: 0.9777 - val_loss: 0.2064 - val_acc: 0.9467\n",
      "Epoch 71/300\n",
      "1347/1347 [==============================] - 0s 49us/step - loss: 0.1134 - acc: 0.9792 - val_loss: 0.2055 - val_acc: 0.9422\n",
      "Epoch 72/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1111 - acc: 0.9755 - val_loss: 0.2031 - val_acc: 0.9444\n",
      "Epoch 73/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.1099 - acc: 0.9800 - val_loss: 0.2010 - val_acc: 0.9444\n",
      "Epoch 74/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1077 - acc: 0.9785 - val_loss: 0.1988 - val_acc: 0.9444\n",
      "Epoch 75/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1065 - acc: 0.9792 - val_loss: 0.1975 - val_acc: 0.9444\n",
      "Epoch 76/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1052 - acc: 0.9792 - val_loss: 0.1958 - val_acc: 0.9422\n",
      "Epoch 77/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1038 - acc: 0.9770 - val_loss: 0.1953 - val_acc: 0.9444\n",
      "Epoch 78/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1019 - acc: 0.9800 - val_loss: 0.1924 - val_acc: 0.9444\n",
      "Epoch 79/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1040 - acc: 0.9800 - val_loss: 0.1938 - val_acc: 0.9444\n",
      "Epoch 80/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0995 - acc: 0.9807 - val_loss: 0.1903 - val_acc: 0.9444\n",
      "Epoch 81/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.0988 - acc: 0.9814 - val_loss: 0.1895 - val_acc: 0.9422\n",
      "Epoch 82/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0969 - acc: 0.9822 - val_loss: 0.1891 - val_acc: 0.9444\n",
      "Epoch 83/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0960 - acc: 0.9785 - val_loss: 0.1869 - val_acc: 0.9444\n",
      "Epoch 84/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0940 - acc: 0.9829 - val_loss: 0.1876 - val_acc: 0.9444\n",
      "Epoch 85/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0930 - acc: 0.9829 - val_loss: 0.1876 - val_acc: 0.9467\n",
      "Epoch 86/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0921 - acc: 0.9807 - val_loss: 0.1889 - val_acc: 0.9467\n",
      "Epoch 87/300\n",
      "1347/1347 [==============================] - 0s 46us/step - loss: 0.0909 - acc: 0.9822 - val_loss: 0.1836 - val_acc: 0.9444\n",
      "Epoch 88/300\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 0.0895 - acc: 0.9829 - val_loss: 0.1818 - val_acc: 0.9444\n",
      "Epoch 89/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.0888 - acc: 0.9822 - val_loss: 0.1793 - val_acc: 0.9444\n",
      "Epoch 90/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.0880 - acc: 0.9837 - val_loss: 0.1824 - val_acc: 0.9422\n",
      "Epoch 91/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.0857 - acc: 0.9837 - val_loss: 0.1838 - val_acc: 0.9467\n",
      "Epoch 92/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0854 - acc: 0.9822 - val_loss: 0.1809 - val_acc: 0.9400\n",
      "Epoch 93/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0833 - acc: 0.9829 - val_loss: 0.1798 - val_acc: 0.9444\n",
      "Epoch 94/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.0828 - acc: 0.9837 - val_loss: 0.1774 - val_acc: 0.9444\n",
      "Epoch 95/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0818 - acc: 0.9837 - val_loss: 0.1756 - val_acc: 0.9400\n",
      "Epoch 96/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.0808 - acc: 0.9844 - val_loss: 0.1748 - val_acc: 0.9444\n",
      "Epoch 97/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.0800 - acc: 0.9837 - val_loss: 0.1759 - val_acc: 0.9422\n",
      "Epoch 98/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0792 - acc: 0.9859 - val_loss: 0.1729 - val_acc: 0.9444\n",
      "Epoch 99/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0784 - acc: 0.9844 - val_loss: 0.1742 - val_acc: 0.9444\n",
      "Epoch 100/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.0772 - acc: 0.9859 - val_loss: 0.1737 - val_acc: 0.9467\n",
      "Epoch 101/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.0765 - acc: 0.9859 - val_loss: 0.1721 - val_acc: 0.9467\n",
      "Epoch 102/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0755 - acc: 0.9859 - val_loss: 0.1748 - val_acc: 0.9444\n",
      "Epoch 103/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.0751 - acc: 0.9852 - val_loss: 0.1723 - val_acc: 0.9467\n",
      "Epoch 104/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0738 - acc: 0.9859 - val_loss: 0.1710 - val_acc: 0.9444\n",
      "Epoch 105/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.0736 - acc: 0.9844 - val_loss: 0.1716 - val_acc: 0.9444\n",
      "Epoch 106/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.0726 - acc: 0.9866 - val_loss: 0.1686 - val_acc: 0.9422\n",
      "Epoch 107/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0714 - acc: 0.9874 - val_loss: 0.1685 - val_acc: 0.9444\n",
      "Epoch 108/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0706 - acc: 0.9866 - val_loss: 0.1680 - val_acc: 0.9444\n",
      "Epoch 109/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0695 - acc: 0.9881 - val_loss: 0.1660 - val_acc: 0.9444\n",
      "Epoch 110/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0689 - acc: 0.9852 - val_loss: 0.1672 - val_acc: 0.9422\n",
      "Epoch 111/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0682 - acc: 0.9866 - val_loss: 0.1667 - val_acc: 0.9400\n",
      "Epoch 112/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0676 - acc: 0.9866 - val_loss: 0.1650 - val_acc: 0.9444\n",
      "Epoch 113/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0670 - acc: 0.9881 - val_loss: 0.1624 - val_acc: 0.9467\n",
      "Epoch 114/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.0663 - acc: 0.9874 - val_loss: 0.1641 - val_acc: 0.9422\n",
      "Epoch 115/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0651 - acc: 0.9874 - val_loss: 0.1652 - val_acc: 0.9400\n",
      "Epoch 116/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0643 - acc: 0.9859 - val_loss: 0.1639 - val_acc: 0.9444\n",
      "Epoch 117/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0634 - acc: 0.9874 - val_loss: 0.1646 - val_acc: 0.9422\n",
      "Epoch 118/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0633 - acc: 0.9866 - val_loss: 0.1634 - val_acc: 0.9422\n",
      "Epoch 119/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.0625 - acc: 0.9881 - val_loss: 0.1616 - val_acc: 0.9444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.0621 - acc: 0.9874 - val_loss: 0.1643 - val_acc: 0.9467\n",
      "Epoch 121/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0609 - acc: 0.9874 - val_loss: 0.1633 - val_acc: 0.9467\n",
      "Epoch 122/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0606 - acc: 0.9874 - val_loss: 0.1606 - val_acc: 0.9467\n",
      "Epoch 123/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0600 - acc: 0.9881 - val_loss: 0.1595 - val_acc: 0.9444\n",
      "Epoch 124/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0601 - acc: 0.9889 - val_loss: 0.1610 - val_acc: 0.9444\n",
      "Epoch 125/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0590 - acc: 0.9889 - val_loss: 0.1615 - val_acc: 0.9444\n",
      "Epoch 126/300\n",
      "1347/1347 [==============================] - 0s 44us/step - loss: 0.0618 - acc: 0.9889 - val_loss: 0.1607 - val_acc: 0.9378\n",
      "Epoch 127/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.0577 - acc: 0.9896 - val_loss: 0.1594 - val_acc: 0.9467\n",
      "Epoch 128/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0577 - acc: 0.9881 - val_loss: 0.1565 - val_acc: 0.9489\n",
      "Epoch 129/300\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 0.0565 - acc: 0.9889 - val_loss: 0.1557 - val_acc: 0.9467\n",
      "Epoch 130/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.0554 - acc: 0.9889 - val_loss: 0.1562 - val_acc: 0.9489\n",
      "Epoch 131/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0555 - acc: 0.9881 - val_loss: 0.1581 - val_acc: 0.9444\n",
      "Epoch 132/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.0555 - acc: 0.9903 - val_loss: 0.1554 - val_acc: 0.9467\n",
      "Epoch 133/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0534 - acc: 0.9903 - val_loss: 0.1570 - val_acc: 0.9467\n",
      "Epoch 134/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0535 - acc: 0.9903 - val_loss: 0.1590 - val_acc: 0.9444\n",
      "Epoch 135/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0528 - acc: 0.9896 - val_loss: 0.1573 - val_acc: 0.9467\n",
      "Epoch 136/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0518 - acc: 0.9903 - val_loss: 0.1559 - val_acc: 0.9467\n",
      "Epoch 137/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0515 - acc: 0.9903 - val_loss: 0.1561 - val_acc: 0.9467\n",
      "Epoch 138/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0512 - acc: 0.9903 - val_loss: 0.1578 - val_acc: 0.9489\n",
      "Epoch 139/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0507 - acc: 0.9911 - val_loss: 0.1553 - val_acc: 0.9489\n",
      "Epoch 140/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0501 - acc: 0.9903 - val_loss: 0.1553 - val_acc: 0.9489\n",
      "Epoch 141/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.0501 - acc: 0.9918 - val_loss: 0.1534 - val_acc: 0.9489\n",
      "Epoch 142/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.0491 - acc: 0.9903 - val_loss: 0.1540 - val_acc: 0.9489\n",
      "Epoch 143/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0488 - acc: 0.9911 - val_loss: 0.1542 - val_acc: 0.9489\n",
      "Epoch 144/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0485 - acc: 0.9911 - val_loss: 0.1520 - val_acc: 0.9467\n",
      "Epoch 145/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0479 - acc: 0.9903 - val_loss: 0.1558 - val_acc: 0.9489\n",
      "Epoch 146/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0475 - acc: 0.9903 - val_loss: 0.1565 - val_acc: 0.9489\n",
      "Epoch 147/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0470 - acc: 0.9911 - val_loss: 0.1554 - val_acc: 0.9489\n",
      "Epoch 148/300\n",
      "1347/1347 [==============================] - 0s 43us/step - loss: 0.0462 - acc: 0.9918 - val_loss: 0.1530 - val_acc: 0.9489\n",
      "Epoch 149/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0461 - acc: 0.9911 - val_loss: 0.1558 - val_acc: 0.9467\n",
      "Epoch 150/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0453 - acc: 0.9918 - val_loss: 0.1536 - val_acc: 0.9489\n",
      "Epoch 151/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0453 - acc: 0.9911 - val_loss: 0.1550 - val_acc: 0.9489\n",
      "Epoch 152/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0443 - acc: 0.9918 - val_loss: 0.1533 - val_acc: 0.9467\n",
      "Epoch 153/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0447 - acc: 0.9911 - val_loss: 0.1557 - val_acc: 0.9489\n",
      "Epoch 154/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.0437 - acc: 0.9918 - val_loss: 0.1507 - val_acc: 0.9444\n",
      "Epoch 155/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0432 - acc: 0.9926 - val_loss: 0.1515 - val_acc: 0.9444\n",
      "Epoch 156/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0435 - acc: 0.9918 - val_loss: 0.1549 - val_acc: 0.9489\n",
      "Epoch 157/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0420 - acc: 0.9933 - val_loss: 0.1527 - val_acc: 0.9489\n",
      "Epoch 158/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0415 - acc: 0.9926 - val_loss: 0.1545 - val_acc: 0.9489\n",
      "Epoch 159/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0415 - acc: 0.9926 - val_loss: 0.1510 - val_acc: 0.9467\n",
      "Epoch 160/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0407 - acc: 0.9933 - val_loss: 0.1519 - val_acc: 0.9467\n",
      "Epoch 161/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0403 - acc: 0.9933 - val_loss: 0.1536 - val_acc: 0.9467\n",
      "Epoch 162/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0404 - acc: 0.9926 - val_loss: 0.1505 - val_acc: 0.9444\n",
      "Epoch 163/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0395 - acc: 0.9948 - val_loss: 0.1539 - val_acc: 0.9467\n",
      "Epoch 164/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0396 - acc: 0.9926 - val_loss: 0.1535 - val_acc: 0.9489\n",
      "Epoch 165/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0388 - acc: 0.9933 - val_loss: 0.1517 - val_acc: 0.9467\n",
      "Epoch 166/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0384 - acc: 0.9941 - val_loss: 0.1503 - val_acc: 0.9444\n",
      "Epoch 167/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0383 - acc: 0.9941 - val_loss: 0.1512 - val_acc: 0.9467\n",
      "Epoch 168/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0378 - acc: 0.9933 - val_loss: 0.1507 - val_acc: 0.9489\n",
      "Epoch 169/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0383 - acc: 0.9926 - val_loss: 0.1492 - val_acc: 0.9444\n",
      "Epoch 170/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0376 - acc: 0.9948 - val_loss: 0.1501 - val_acc: 0.9467\n",
      "Epoch 171/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0365 - acc: 0.9941 - val_loss: 0.1499 - val_acc: 0.9467\n",
      "Epoch 172/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0359 - acc: 0.9941 - val_loss: 0.1529 - val_acc: 0.9467\n",
      "Epoch 173/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0360 - acc: 0.9933 - val_loss: 0.1489 - val_acc: 0.9444\n",
      "Epoch 174/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0351 - acc: 0.9941 - val_loss: 0.1501 - val_acc: 0.9467\n",
      "Epoch 175/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0354 - acc: 0.9948 - val_loss: 0.1523 - val_acc: 0.9467\n",
      "Epoch 176/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.0345 - acc: 0.9941 - val_loss: 0.1498 - val_acc: 0.9467\n",
      "Epoch 177/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.0344 - acc: 0.9948 - val_loss: 0.1507 - val_acc: 0.9467\n",
      "Epoch 178/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0337 - acc: 0.9941 - val_loss: 0.1501 - val_acc: 0.9467\n",
      "Epoch 179/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0335 - acc: 0.9941 - val_loss: 0.1491 - val_acc: 0.9444\n",
      "Epoch 180/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0338 - acc: 0.9948 - val_loss: 0.1481 - val_acc: 0.9467\n",
      "Epoch 181/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0334 - acc: 0.9941 - val_loss: 0.1498 - val_acc: 0.9467\n",
      "Epoch 182/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0329 - acc: 0.9941 - val_loss: 0.1493 - val_acc: 0.9467\n",
      "Epoch 183/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0328 - acc: 0.9948 - val_loss: 0.1477 - val_acc: 0.9422\n",
      "Epoch 184/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0318 - acc: 0.9948 - val_loss: 0.1524 - val_acc: 0.9467\n",
      "Epoch 185/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0318 - acc: 0.9941 - val_loss: 0.1519 - val_acc: 0.9444\n",
      "Epoch 186/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0311 - acc: 0.9955 - val_loss: 0.1481 - val_acc: 0.9467\n",
      "Epoch 187/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0312 - acc: 0.9941 - val_loss: 0.1508 - val_acc: 0.9467\n",
      "Epoch 188/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0302 - acc: 0.9948 - val_loss: 0.1510 - val_acc: 0.9444\n",
      "Epoch 189/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.0307 - acc: 0.9941 - val_loss: 0.1532 - val_acc: 0.9467\n",
      "Epoch 190/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0299 - acc: 0.9948 - val_loss: 0.1487 - val_acc: 0.9467\n",
      "Epoch 191/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0295 - acc: 0.9955 - val_loss: 0.1486 - val_acc: 0.9467\n",
      "Epoch 192/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0297 - acc: 0.9948 - val_loss: 0.1496 - val_acc: 0.9467\n",
      "Epoch 193/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0288 - acc: 0.9955 - val_loss: 0.1487 - val_acc: 0.9467\n",
      "Epoch 194/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0286 - acc: 0.9955 - val_loss: 0.1499 - val_acc: 0.9467\n",
      "Epoch 195/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0282 - acc: 0.9955 - val_loss: 0.1501 - val_acc: 0.9444\n",
      "Epoch 196/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0285 - acc: 0.9933 - val_loss: 0.1491 - val_acc: 0.9489\n",
      "Epoch 197/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0279 - acc: 0.9963 - val_loss: 0.1490 - val_acc: 0.9467\n",
      "Epoch 198/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0275 - acc: 0.9970 - val_loss: 0.1496 - val_acc: 0.9467\n",
      "Epoch 199/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0273 - acc: 0.9963 - val_loss: 0.1506 - val_acc: 0.9467\n",
      "Epoch 200/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0267 - acc: 0.9963 - val_loss: 0.1532 - val_acc: 0.9489\n",
      "Epoch 201/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0266 - acc: 0.9970 - val_loss: 0.1509 - val_acc: 0.9489\n",
      "Epoch 202/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.0306 - acc: 0.9941 - val_loss: 0.1484 - val_acc: 0.9444\n",
      "Epoch 203/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0268 - acc: 0.9963 - val_loss: 0.1476 - val_acc: 0.9444\n",
      "Epoch 204/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.0263 - acc: 0.9970 - val_loss: 0.1523 - val_acc: 0.9511\n",
      "Epoch 205/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0258 - acc: 0.9978 - val_loss: 0.1516 - val_acc: 0.9467\n",
      "Epoch 206/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.0254 - acc: 0.9978 - val_loss: 0.1496 - val_acc: 0.9467\n",
      "Epoch 207/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0249 - acc: 0.9970 - val_loss: 0.1493 - val_acc: 0.9467\n",
      "Epoch 208/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.0246 - acc: 0.9978 - val_loss: 0.1504 - val_acc: 0.9467\n",
      "Epoch 209/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0245 - acc: 0.9970 - val_loss: 0.1477 - val_acc: 0.9444\n",
      "Epoch 210/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0256 - acc: 0.9978 - val_loss: 0.1523 - val_acc: 0.9467\n",
      "Epoch 211/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.0261 - acc: 0.9985 - val_loss: 0.1539 - val_acc: 0.9511\n",
      "Epoch 212/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.0247 - acc: 0.9970 - val_loss: 0.1536 - val_acc: 0.9489\n",
      "Epoch 213/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0236 - acc: 0.9970 - val_loss: 0.1516 - val_acc: 0.9489\n",
      "Epoch 214/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0236 - acc: 0.9978 - val_loss: 0.1494 - val_acc: 0.9467\n",
      "Epoch 215/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0239 - acc: 0.9978 - val_loss: 0.1517 - val_acc: 0.9489\n",
      "Epoch 216/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.0230 - acc: 0.9978 - val_loss: 0.1491 - val_acc: 0.9489\n",
      "Epoch 217/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0226 - acc: 0.9978 - val_loss: 0.1523 - val_acc: 0.9489\n",
      "Epoch 218/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0225 - acc: 0.9985 - val_loss: 0.1489 - val_acc: 0.9467\n",
      "Epoch 219/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0220 - acc: 0.9978 - val_loss: 0.1523 - val_acc: 0.9489\n",
      "Epoch 220/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0220 - acc: 0.9978 - val_loss: 0.1490 - val_acc: 0.9489\n",
      "Epoch 221/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0219 - acc: 0.9978 - val_loss: 0.1532 - val_acc: 0.9467\n",
      "Epoch 222/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0215 - acc: 0.9978 - val_loss: 0.1515 - val_acc: 0.9489\n",
      "Epoch 223/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0214 - acc: 0.9978 - val_loss: 0.1502 - val_acc: 0.9467\n",
      "Epoch 224/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.0210 - acc: 0.9985 - val_loss: 0.1513 - val_acc: 0.9467\n",
      "Epoch 225/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0210 - acc: 0.9985 - val_loss: 0.1513 - val_acc: 0.9489\n",
      "Epoch 226/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0212 - acc: 0.9978 - val_loss: 0.1504 - val_acc: 0.9489\n",
      "Epoch 227/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0206 - acc: 0.9985 - val_loss: 0.1528 - val_acc: 0.9489\n",
      "Epoch 228/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0204 - acc: 0.9978 - val_loss: 0.1515 - val_acc: 0.9489\n",
      "Epoch 229/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0201 - acc: 0.9985 - val_loss: 0.1532 - val_acc: 0.9489\n",
      "Epoch 230/300\n",
      "1347/1347 [==============================] - 0s 27us/step - loss: 0.0198 - acc: 0.9985 - val_loss: 0.1514 - val_acc: 0.9489\n",
      "Epoch 231/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0202 - acc: 0.9978 - val_loss: 0.1504 - val_acc: 0.9467\n",
      "Epoch 232/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0198 - acc: 0.9985 - val_loss: 0.1524 - val_acc: 0.9422\n",
      "Epoch 233/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0194 - acc: 0.9985 - val_loss: 0.1514 - val_acc: 0.9467\n",
      "Epoch 234/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0204 - acc: 0.9978 - val_loss: 0.1540 - val_acc: 0.9467\n",
      "Epoch 235/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0191 - acc: 0.9985 - val_loss: 0.1541 - val_acc: 0.9489\n",
      "Epoch 236/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0190 - acc: 0.9985 - val_loss: 0.1494 - val_acc: 0.9444\n",
      "Epoch 237/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0191 - acc: 0.9985 - val_loss: 0.1514 - val_acc: 0.9489\n",
      "Epoch 238/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.0184 - acc: 0.9985 - val_loss: 0.1531 - val_acc: 0.9467\n",
      "Epoch 239/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0185 - acc: 0.9985 - val_loss: 0.1518 - val_acc: 0.9489\n",
      "Epoch 240/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0184 - acc: 0.9985 - val_loss: 0.1500 - val_acc: 0.9511\n",
      "Epoch 241/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0181 - acc: 0.9985 - val_loss: 0.1510 - val_acc: 0.9444\n",
      "Epoch 242/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0175 - acc: 0.9985 - val_loss: 0.1524 - val_acc: 0.9467\n",
      "Epoch 243/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0177 - acc: 0.9985 - val_loss: 0.1520 - val_acc: 0.9444\n",
      "Epoch 244/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0176 - acc: 0.9985 - val_loss: 0.1527 - val_acc: 0.9511\n",
      "Epoch 245/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.0173 - acc: 0.9985 - val_loss: 0.1532 - val_acc: 0.9467\n",
      "Epoch 246/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.0170 - acc: 0.9985 - val_loss: 0.1517 - val_acc: 0.9467\n",
      "Epoch 247/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0168 - acc: 0.9985 - val_loss: 0.1528 - val_acc: 0.9489\n",
      "Epoch 248/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0168 - acc: 0.9985 - val_loss: 0.1516 - val_acc: 0.9467\n",
      "Epoch 249/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0167 - acc: 0.9985 - val_loss: 0.1540 - val_acc: 0.9467\n",
      "Epoch 250/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0179 - acc: 0.9978 - val_loss: 0.1547 - val_acc: 0.9467\n",
      "Epoch 251/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0164 - acc: 0.9985 - val_loss: 0.1508 - val_acc: 0.9489\n",
      "Epoch 252/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0161 - acc: 0.9985 - val_loss: 0.1515 - val_acc: 0.9467\n",
      "Epoch 253/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0163 - acc: 0.9985 - val_loss: 0.1526 - val_acc: 0.9489\n",
      "Epoch 254/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0158 - acc: 0.9985 - val_loss: 0.1513 - val_acc: 0.9467\n",
      "Epoch 255/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0155 - acc: 0.9985 - val_loss: 0.1527 - val_acc: 0.9489\n",
      "Epoch 256/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0153 - acc: 0.9985 - val_loss: 0.1516 - val_acc: 0.9489\n",
      "Epoch 257/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0153 - acc: 0.9985 - val_loss: 0.1532 - val_acc: 0.9489\n",
      "Epoch 258/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0152 - acc: 0.9985 - val_loss: 0.1538 - val_acc: 0.9467\n",
      "Epoch 259/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0148 - acc: 0.9985 - val_loss: 0.1518 - val_acc: 0.9489\n",
      "Epoch 260/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0148 - acc: 0.9985 - val_loss: 0.1539 - val_acc: 0.9467\n",
      "Epoch 261/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0146 - acc: 0.9985 - val_loss: 0.1527 - val_acc: 0.9489\n",
      "Epoch 262/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0144 - acc: 0.9985 - val_loss: 0.1522 - val_acc: 0.9511\n",
      "Epoch 263/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0146 - acc: 0.9985 - val_loss: 0.1510 - val_acc: 0.9489\n",
      "Epoch 264/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.0143 - acc: 0.9993 - val_loss: 0.1552 - val_acc: 0.9511\n",
      "Epoch 265/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.0142 - acc: 0.9985 - val_loss: 0.1537 - val_acc: 0.9511\n",
      "Epoch 266/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0138 - acc: 0.9985 - val_loss: 0.1539 - val_acc: 0.9467\n",
      "Epoch 267/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.0137 - acc: 0.9985 - val_loss: 0.1550 - val_acc: 0.9489\n",
      "Epoch 268/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.0137 - acc: 0.9985 - val_loss: 0.1535 - val_acc: 0.9489\n",
      "Epoch 269/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0136 - acc: 0.9985 - val_loss: 0.1544 - val_acc: 0.9489\n",
      "Epoch 270/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0134 - acc: 0.9985 - val_loss: 0.1559 - val_acc: 0.9467\n",
      "Epoch 271/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.0134 - acc: 0.9985 - val_loss: 0.1547 - val_acc: 0.9511\n",
      "Epoch 272/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0131 - acc: 0.9985 - val_loss: 0.1534 - val_acc: 0.9489\n",
      "Epoch 273/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.0128 - acc: 0.9993 - val_loss: 0.1549 - val_acc: 0.9467\n",
      "Epoch 274/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.0128 - acc: 0.9985 - val_loss: 0.1564 - val_acc: 0.9444\n",
      "Epoch 275/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0126 - acc: 0.9993 - val_loss: 0.1528 - val_acc: 0.9489\n",
      "Epoch 276/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0126 - acc: 0.9993 - val_loss: 0.1560 - val_acc: 0.9489\n",
      "Epoch 277/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0124 - acc: 0.9993 - val_loss: 0.1584 - val_acc: 0.9422\n",
      "Epoch 278/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0123 - acc: 0.9985 - val_loss: 0.1527 - val_acc: 0.9467\n",
      "Epoch 279/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0123 - acc: 0.9993 - val_loss: 0.1563 - val_acc: 0.9489\n",
      "Epoch 280/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0121 - acc: 0.9985 - val_loss: 0.1551 - val_acc: 0.9467\n",
      "Epoch 281/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0121 - acc: 0.9985 - val_loss: 0.1552 - val_acc: 0.9489\n",
      "Epoch 282/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0120 - acc: 0.9993 - val_loss: 0.1568 - val_acc: 0.9489\n",
      "Epoch 283/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0118 - acc: 0.9993 - val_loss: 0.1540 - val_acc: 0.9467\n",
      "Epoch 284/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.1550 - val_acc: 0.9467\n",
      "Epoch 285/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.1540 - val_acc: 0.9467\n",
      "Epoch 286/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.0115 - acc: 0.9993 - val_loss: 0.1559 - val_acc: 0.9489\n",
      "Epoch 287/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.1555 - val_acc: 0.9489\n",
      "Epoch 288/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.1541 - val_acc: 0.9511\n",
      "Epoch 289/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0111 - acc: 0.9993 - val_loss: 0.1560 - val_acc: 0.9467\n",
      "Epoch 290/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.0108 - acc: 1.0000 - val_loss: 0.1550 - val_acc: 0.9467\n",
      "Epoch 291/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.0108 - acc: 0.9993 - val_loss: 0.1557 - val_acc: 0.9467\n",
      "Epoch 292/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.0108 - acc: 1.0000 - val_loss: 0.1580 - val_acc: 0.9444\n",
      "Epoch 293/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.0106 - acc: 0.9993 - val_loss: 0.1570 - val_acc: 0.9467\n",
      "Epoch 294/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.1572 - val_acc: 0.9511\n",
      "Epoch 295/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.1578 - val_acc: 0.9489\n",
      "Epoch 296/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.0101 - acc: 0.9993 - val_loss: 0.1560 - val_acc: 0.9467\n",
      "Epoch 297/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.1539 - val_acc: 0.9444\n",
      "Epoch 298/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.1550 - val_acc: 0.9489\n",
      "Epoch 299/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.1563 - val_acc: 0.9489\n",
      "Epoch 300/300\n",
      "1347/1347 [==============================] - 0s 27us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.1549 - val_acc: 0.9511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d6c0ef9b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "\n",
    "inputs = Input(shape=(n_features,))\n",
    "h_layer = Dense(n_hidden, activation=activation)(inputs)\n",
    "y_layer = Dense(n_classes, activation='softmax')(h_layer)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=y_layer)\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(\n",
    "    X_tr, to_categorical(Y_tr, num_classes=n_classes), \n",
    "    epochs=300, verbose=1, batch_size=32,\n",
    "    validation_data=(X_val, to_categorical(Y_val, num_classes=n_classes))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that you know if the model is underfitting or overfitting:\n",
    "<span style=\"color:green\">\n",
    "Underfitting case: model not complex enough (not enough parameters and depth) to learn dependencies between $X$ and $Y$<br/>\n",
    "Overfitting case: model too complex, learns statistically pointless dependencies between $X_{tr}$ and $Y_{tr}$\n",
    "</span>\n",
    "#### - In case of underfitting, can you explain why ? Also change the structure of the 2 previous networks to cancell underfitting\n",
    "#### - In case of overfitting, explain why and change the structure of the 2 previous networks to cancell the overfitting\n",
    "<span style=\"color:green\">\n",
    "We can observe a slight case of overfitting here, with a gap of about 5% between the training and the validation accuracies.<br/>Adding a Dropout with a small $p$ rate can be thus considered.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential without overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with relu activation\n",
      "Train on 1347 samples, validate on 450 samples\n",
      "Epoch 1/300\n",
      "1347/1347 [==============================] - 0s 205us/step - loss: 2.3129 - acc: 0.1069 - val_loss: 2.2536 - val_acc: 0.1489\n",
      "Epoch 2/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 2.2161 - acc: 0.1782 - val_loss: 2.1620 - val_acc: 0.2533\n",
      "Epoch 3/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 2.0967 - acc: 0.2821 - val_loss: 2.0234 - val_acc: 0.3711\n",
      "Epoch 4/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 1.9307 - acc: 0.3823 - val_loss: 1.8425 - val_acc: 0.4689\n",
      "Epoch 5/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 1.7535 - acc: 0.4625 - val_loss: 1.6596 - val_acc: 0.5533\n",
      "Epoch 6/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 1.5866 - acc: 0.5434 - val_loss: 1.5003 - val_acc: 0.6333\n",
      "Epoch 7/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 1.4583 - acc: 0.6110 - val_loss: 1.3542 - val_acc: 0.6978\n",
      "Epoch 8/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 1.3037 - acc: 0.6615 - val_loss: 1.2231 - val_acc: 0.7578\n",
      "Epoch 9/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 1.1822 - acc: 0.7008 - val_loss: 1.1079 - val_acc: 0.7867\n",
      "Epoch 10/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 1.0789 - acc: 0.7394 - val_loss: 1.0099 - val_acc: 0.7956\n",
      "Epoch 11/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.9946 - acc: 0.7587 - val_loss: 0.9283 - val_acc: 0.8022\n",
      "Epoch 12/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.9402 - acc: 0.7624 - val_loss: 0.8584 - val_acc: 0.8333\n",
      "Epoch 13/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.8870 - acc: 0.7736 - val_loss: 0.8020 - val_acc: 0.8422\n",
      "Epoch 14/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.8301 - acc: 0.7892 - val_loss: 0.7478 - val_acc: 0.8533\n",
      "Epoch 15/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.7951 - acc: 0.7951 - val_loss: 0.7066 - val_acc: 0.8622\n",
      "Epoch 16/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.7361 - acc: 0.8196 - val_loss: 0.6645 - val_acc: 0.8800\n",
      "Epoch 17/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.7200 - acc: 0.8151 - val_loss: 0.6288 - val_acc: 0.8844\n",
      "Epoch 18/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.7180 - acc: 0.7988 - val_loss: 0.5979 - val_acc: 0.8844\n",
      "Epoch 19/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.6664 - acc: 0.8196 - val_loss: 0.5717 - val_acc: 0.8889\n",
      "Epoch 20/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.6504 - acc: 0.8233 - val_loss: 0.5469 - val_acc: 0.8867\n",
      "Epoch 21/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.6174 - acc: 0.8352 - val_loss: 0.5245 - val_acc: 0.8911\n",
      "Epoch 22/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.5985 - acc: 0.8404 - val_loss: 0.5029 - val_acc: 0.8933\n",
      "Epoch 23/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.6083 - acc: 0.8241 - val_loss: 0.4872 - val_acc: 0.8978\n",
      "Epoch 24/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.5597 - acc: 0.8500 - val_loss: 0.4674 - val_acc: 0.9089\n",
      "Epoch 25/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.5522 - acc: 0.8471 - val_loss: 0.4522 - val_acc: 0.9089\n",
      "Epoch 26/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.5419 - acc: 0.8515 - val_loss: 0.4361 - val_acc: 0.9089\n",
      "Epoch 27/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.5016 - acc: 0.8597 - val_loss: 0.4230 - val_acc: 0.9067\n",
      "Epoch 28/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.5191 - acc: 0.8515 - val_loss: 0.4127 - val_acc: 0.9067\n",
      "Epoch 29/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.5071 - acc: 0.8456 - val_loss: 0.4003 - val_acc: 0.9089\n",
      "Epoch 30/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.4819 - acc: 0.8500 - val_loss: 0.3885 - val_acc: 0.9067\n",
      "Epoch 31/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.4994 - acc: 0.8441 - val_loss: 0.3813 - val_acc: 0.9111\n",
      "Epoch 32/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.4706 - acc: 0.8634 - val_loss: 0.3697 - val_acc: 0.9156\n",
      "Epoch 33/300\n",
      "1347/1347 [==============================] - ETA: 0s - loss: 0.2491 - acc: 0.937 - 0s 36us/step - loss: 0.4390 - acc: 0.8723 - val_loss: 0.3617 - val_acc: 0.9156\n",
      "Epoch 34/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.4407 - acc: 0.8612 - val_loss: 0.3518 - val_acc: 0.9089\n",
      "Epoch 35/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.4566 - acc: 0.8545 - val_loss: 0.3458 - val_acc: 0.9111\n",
      "Epoch 36/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.4452 - acc: 0.8708 - val_loss: 0.3397 - val_acc: 0.9067\n",
      "Epoch 37/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.4579 - acc: 0.8575 - val_loss: 0.3325 - val_acc: 0.9178\n",
      "Epoch 38/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.4420 - acc: 0.8731 - val_loss: 0.3252 - val_acc: 0.9178\n",
      "Epoch 39/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.4266 - acc: 0.8619 - val_loss: 0.3208 - val_acc: 0.9067\n",
      "Epoch 40/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.4122 - acc: 0.8731 - val_loss: 0.3123 - val_acc: 0.9200\n",
      "Epoch 41/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.3969 - acc: 0.8812 - val_loss: 0.3072 - val_acc: 0.9178\n",
      "Epoch 42/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.3997 - acc: 0.8760 - val_loss: 0.3013 - val_acc: 0.9222\n",
      "Epoch 43/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.4027 - acc: 0.8760 - val_loss: 0.3006 - val_acc: 0.9156\n",
      "Epoch 44/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.4310 - acc: 0.8589 - val_loss: 0.2972 - val_acc: 0.9244\n",
      "Epoch 45/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.4072 - acc: 0.8775 - val_loss: 0.2926 - val_acc: 0.9200\n",
      "Epoch 46/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.3546 - acc: 0.8938 - val_loss: 0.2844 - val_acc: 0.9244\n",
      "Epoch 47/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.3816 - acc: 0.8797 - val_loss: 0.2817 - val_acc: 0.9222\n",
      "Epoch 48/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.3706 - acc: 0.8916 - val_loss: 0.2800 - val_acc: 0.9222\n",
      "Epoch 49/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.3690 - acc: 0.8849 - val_loss: 0.2744 - val_acc: 0.9244\n",
      "Epoch 50/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.3635 - acc: 0.8805 - val_loss: 0.2712 - val_acc: 0.9244\n",
      "Epoch 51/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.3516 - acc: 0.8946 - val_loss: 0.2650 - val_acc: 0.9333\n",
      "Epoch 52/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.3778 - acc: 0.8886 - val_loss: 0.2645 - val_acc: 0.9289\n",
      "Epoch 53/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.3581 - acc: 0.8857 - val_loss: 0.2637 - val_acc: 0.9311\n",
      "Epoch 54/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.3792 - acc: 0.8775 - val_loss: 0.2617 - val_acc: 0.9333\n",
      "Epoch 55/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.3543 - acc: 0.8812 - val_loss: 0.2553 - val_acc: 0.9333\n",
      "Epoch 56/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.3295 - acc: 0.9020 - val_loss: 0.2524 - val_acc: 0.9333\n",
      "Epoch 57/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.3504 - acc: 0.8894 - val_loss: 0.2482 - val_acc: 0.9356\n",
      "Epoch 58/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.3109 - acc: 0.9109 - val_loss: 0.2457 - val_acc: 0.9400\n",
      "Epoch 59/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.3428 - acc: 0.8953 - val_loss: 0.2458 - val_acc: 0.9356\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 52us/step - loss: 0.3501 - acc: 0.8924 - val_loss: 0.2431 - val_acc: 0.9356\n",
      "Epoch 61/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.3348 - acc: 0.8879 - val_loss: 0.2420 - val_acc: 0.9356\n",
      "Epoch 62/300\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 0.3372 - acc: 0.8901 - val_loss: 0.2401 - val_acc: 0.9333\n",
      "Epoch 63/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.3443 - acc: 0.8879 - val_loss: 0.2367 - val_acc: 0.9378\n",
      "Epoch 64/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.3477 - acc: 0.8827 - val_loss: 0.2369 - val_acc: 0.9333\n",
      "Epoch 65/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.3572 - acc: 0.8790 - val_loss: 0.2330 - val_acc: 0.9400\n",
      "Epoch 66/300\n",
      "1347/1347 [==============================] - 0s 43us/step - loss: 0.3104 - acc: 0.9057 - val_loss: 0.2295 - val_acc: 0.9444\n",
      "Epoch 67/300\n",
      "1347/1347 [==============================] - 0s 50us/step - loss: 0.2974 - acc: 0.9042 - val_loss: 0.2283 - val_acc: 0.9400\n",
      "Epoch 68/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.3062 - acc: 0.8998 - val_loss: 0.2248 - val_acc: 0.9378\n",
      "Epoch 69/300\n",
      "1347/1347 [==============================] - 0s 51us/step - loss: 0.3150 - acc: 0.9042 - val_loss: 0.2251 - val_acc: 0.9356\n",
      "Epoch 70/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.3071 - acc: 0.9102 - val_loss: 0.2231 - val_acc: 0.9378\n",
      "Epoch 71/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.3185 - acc: 0.8990 - val_loss: 0.2209 - val_acc: 0.9378\n",
      "Epoch 72/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2969 - acc: 0.9146 - val_loss: 0.2188 - val_acc: 0.9400\n",
      "Epoch 73/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2905 - acc: 0.9109 - val_loss: 0.2150 - val_acc: 0.9378\n",
      "Epoch 74/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.3018 - acc: 0.9020 - val_loss: 0.2183 - val_acc: 0.9378\n",
      "Epoch 75/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2958 - acc: 0.9027 - val_loss: 0.2162 - val_acc: 0.9356\n",
      "Epoch 76/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2866 - acc: 0.9057 - val_loss: 0.2123 - val_acc: 0.9422\n",
      "Epoch 77/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.2863 - acc: 0.9079 - val_loss: 0.2097 - val_acc: 0.9400\n",
      "Epoch 78/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2736 - acc: 0.9087 - val_loss: 0.2098 - val_acc: 0.9378\n",
      "Epoch 79/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.3137 - acc: 0.8931 - val_loss: 0.2091 - val_acc: 0.9333\n",
      "Epoch 80/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2922 - acc: 0.9027 - val_loss: 0.2099 - val_acc: 0.9333\n",
      "Epoch 81/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.3050 - acc: 0.9013 - val_loss: 0.2072 - val_acc: 0.9378\n",
      "Epoch 82/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2619 - acc: 0.9213 - val_loss: 0.2040 - val_acc: 0.9356\n",
      "Epoch 83/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2841 - acc: 0.9094 - val_loss: 0.2028 - val_acc: 0.9422\n",
      "Epoch 84/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.2905 - acc: 0.9042 - val_loss: 0.2027 - val_acc: 0.9400\n",
      "Epoch 85/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2716 - acc: 0.9139 - val_loss: 0.1977 - val_acc: 0.9400\n",
      "Epoch 86/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2693 - acc: 0.9079 - val_loss: 0.1977 - val_acc: 0.9400\n",
      "Epoch 87/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2677 - acc: 0.9087 - val_loss: 0.1981 - val_acc: 0.9400\n",
      "Epoch 88/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.2669 - acc: 0.9102 - val_loss: 0.1936 - val_acc: 0.9489\n",
      "Epoch 89/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2761 - acc: 0.9094 - val_loss: 0.1972 - val_acc: 0.9467\n",
      "Epoch 90/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2662 - acc: 0.9183 - val_loss: 0.1934 - val_acc: 0.9444\n",
      "Epoch 91/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2580 - acc: 0.9176 - val_loss: 0.1917 - val_acc: 0.9444\n",
      "Epoch 92/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2838 - acc: 0.9020 - val_loss: 0.1908 - val_acc: 0.9467\n",
      "Epoch 93/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2502 - acc: 0.9198 - val_loss: 0.1886 - val_acc: 0.9467\n",
      "Epoch 94/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2643 - acc: 0.9146 - val_loss: 0.1882 - val_acc: 0.9444\n",
      "Epoch 95/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2730 - acc: 0.9109 - val_loss: 0.1870 - val_acc: 0.9422\n",
      "Epoch 96/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2637 - acc: 0.9124 - val_loss: 0.1868 - val_acc: 0.9467\n",
      "Epoch 97/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2370 - acc: 0.9213 - val_loss: 0.1840 - val_acc: 0.9467\n",
      "Epoch 98/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2692 - acc: 0.9161 - val_loss: 0.1850 - val_acc: 0.9467\n",
      "Epoch 99/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2669 - acc: 0.9087 - val_loss: 0.1825 - val_acc: 0.9467\n",
      "Epoch 100/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2482 - acc: 0.9213 - val_loss: 0.1805 - val_acc: 0.9489\n",
      "Epoch 101/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2596 - acc: 0.9131 - val_loss: 0.1803 - val_acc: 0.9467\n",
      "Epoch 102/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.2425 - acc: 0.9213 - val_loss: 0.1805 - val_acc: 0.9489\n",
      "Epoch 103/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2578 - acc: 0.9235 - val_loss: 0.1789 - val_acc: 0.9489\n",
      "Epoch 104/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2417 - acc: 0.9228 - val_loss: 0.1791 - val_acc: 0.9511\n",
      "Epoch 105/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.2457 - acc: 0.9161 - val_loss: 0.1795 - val_acc: 0.9467\n",
      "Epoch 106/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.2311 - acc: 0.9228 - val_loss: 0.1783 - val_acc: 0.9489\n",
      "Epoch 107/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2627 - acc: 0.9109 - val_loss: 0.1803 - val_acc: 0.9467\n",
      "Epoch 108/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.2575 - acc: 0.9191 - val_loss: 0.1785 - val_acc: 0.9444\n",
      "Epoch 109/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2537 - acc: 0.9154 - val_loss: 0.1747 - val_acc: 0.9467\n",
      "Epoch 110/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.2223 - acc: 0.9176 - val_loss: 0.1744 - val_acc: 0.9444\n",
      "Epoch 111/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.2534 - acc: 0.9183 - val_loss: 0.1726 - val_acc: 0.9511\n",
      "Epoch 112/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.2349 - acc: 0.9213 - val_loss: 0.1711 - val_acc: 0.9467\n",
      "Epoch 113/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2345 - acc: 0.9183 - val_loss: 0.1725 - val_acc: 0.9489\n",
      "Epoch 114/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2287 - acc: 0.9295 - val_loss: 0.1708 - val_acc: 0.9489\n",
      "Epoch 115/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.2316 - acc: 0.9161 - val_loss: 0.1701 - val_acc: 0.9467\n",
      "Epoch 116/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.2238 - acc: 0.9302 - val_loss: 0.1718 - val_acc: 0.9422\n",
      "Epoch 117/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2218 - acc: 0.9250 - val_loss: 0.1722 - val_acc: 0.9444\n",
      "Epoch 118/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2616 - acc: 0.9176 - val_loss: 0.1716 - val_acc: 0.9422\n",
      "Epoch 119/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.2120 - acc: 0.9324 - val_loss: 0.1674 - val_acc: 0.9556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2192 - acc: 0.9347 - val_loss: 0.1664 - val_acc: 0.9511\n",
      "Epoch 121/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2164 - acc: 0.9280 - val_loss: 0.1655 - val_acc: 0.9511\n",
      "Epoch 122/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2352 - acc: 0.9258 - val_loss: 0.1691 - val_acc: 0.9489\n",
      "Epoch 123/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2127 - acc: 0.9354 - val_loss: 0.1646 - val_acc: 0.9533\n",
      "Epoch 124/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2310 - acc: 0.9280 - val_loss: 0.1673 - val_acc: 0.9467\n",
      "Epoch 125/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2262 - acc: 0.9250 - val_loss: 0.1675 - val_acc: 0.9467\n",
      "Epoch 126/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2382 - acc: 0.9258 - val_loss: 0.1636 - val_acc: 0.9489\n",
      "Epoch 127/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2221 - acc: 0.9220 - val_loss: 0.1623 - val_acc: 0.9467\n",
      "Epoch 128/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2383 - acc: 0.9183 - val_loss: 0.1621 - val_acc: 0.9444\n",
      "Epoch 129/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2357 - acc: 0.9295 - val_loss: 0.1617 - val_acc: 0.9467\n",
      "Epoch 130/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2285 - acc: 0.9265 - val_loss: 0.1635 - val_acc: 0.9467\n",
      "Epoch 131/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2308 - acc: 0.9272 - val_loss: 0.1645 - val_acc: 0.9489\n",
      "Epoch 132/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2230 - acc: 0.9272 - val_loss: 0.1617 - val_acc: 0.9533\n",
      "Epoch 133/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.2110 - acc: 0.9302 - val_loss: 0.1622 - val_acc: 0.9489\n",
      "Epoch 134/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2251 - acc: 0.9332 - val_loss: 0.1621 - val_acc: 0.9489\n",
      "Epoch 135/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2156 - acc: 0.9280 - val_loss: 0.1582 - val_acc: 0.9533\n",
      "Epoch 136/300\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 0.2047 - acc: 0.9310 - val_loss: 0.1602 - val_acc: 0.9489\n",
      "Epoch 137/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1889 - acc: 0.9421 - val_loss: 0.1555 - val_acc: 0.9467\n",
      "Epoch 138/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2181 - acc: 0.9339 - val_loss: 0.1554 - val_acc: 0.9511\n",
      "Epoch 139/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2236 - acc: 0.9272 - val_loss: 0.1572 - val_acc: 0.9489\n",
      "Epoch 140/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1928 - acc: 0.9339 - val_loss: 0.1563 - val_acc: 0.9533\n",
      "Epoch 141/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2220 - acc: 0.9258 - val_loss: 0.1548 - val_acc: 0.9511\n",
      "Epoch 142/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2202 - acc: 0.9295 - val_loss: 0.1553 - val_acc: 0.9511\n",
      "Epoch 143/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2230 - acc: 0.9272 - val_loss: 0.1571 - val_acc: 0.9511\n",
      "Epoch 144/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2041 - acc: 0.9384 - val_loss: 0.1578 - val_acc: 0.9489\n",
      "Epoch 145/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1999 - acc: 0.9347 - val_loss: 0.1530 - val_acc: 0.9578\n",
      "Epoch 146/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2044 - acc: 0.9317 - val_loss: 0.1508 - val_acc: 0.9533\n",
      "Epoch 147/300\n",
      "1347/1347 [==============================] - 0s 42us/step - loss: 0.2038 - acc: 0.9302 - val_loss: 0.1504 - val_acc: 0.9533\n",
      "Epoch 148/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2009 - acc: 0.9362 - val_loss: 0.1493 - val_acc: 0.9533\n",
      "Epoch 149/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2075 - acc: 0.9354 - val_loss: 0.1524 - val_acc: 0.9467\n",
      "Epoch 150/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2070 - acc: 0.9332 - val_loss: 0.1530 - val_acc: 0.9489\n",
      "Epoch 151/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2160 - acc: 0.9332 - val_loss: 0.1510 - val_acc: 0.9556\n",
      "Epoch 152/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2183 - acc: 0.9228 - val_loss: 0.1507 - val_acc: 0.9511\n",
      "Epoch 153/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1892 - acc: 0.9436 - val_loss: 0.1473 - val_acc: 0.9511\n",
      "Epoch 154/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1985 - acc: 0.9421 - val_loss: 0.1452 - val_acc: 0.9533\n",
      "Epoch 155/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1733 - acc: 0.9443 - val_loss: 0.1438 - val_acc: 0.9511\n",
      "Epoch 156/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.2025 - acc: 0.9362 - val_loss: 0.1468 - val_acc: 0.9511\n",
      "Epoch 157/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1925 - acc: 0.9376 - val_loss: 0.1463 - val_acc: 0.9511\n",
      "Epoch 158/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1932 - acc: 0.9376 - val_loss: 0.1477 - val_acc: 0.9533\n",
      "Epoch 159/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2043 - acc: 0.9287 - val_loss: 0.1468 - val_acc: 0.9511\n",
      "Epoch 160/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1977 - acc: 0.9332 - val_loss: 0.1440 - val_acc: 0.9556\n",
      "Epoch 161/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2198 - acc: 0.9287 - val_loss: 0.1465 - val_acc: 0.9489\n",
      "Epoch 162/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1819 - acc: 0.9428 - val_loss: 0.1490 - val_acc: 0.9444\n",
      "Epoch 163/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1985 - acc: 0.9324 - val_loss: 0.1492 - val_acc: 0.9467\n",
      "Epoch 164/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1890 - acc: 0.9458 - val_loss: 0.1470 - val_acc: 0.9444\n",
      "Epoch 165/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1920 - acc: 0.9339 - val_loss: 0.1488 - val_acc: 0.9444\n",
      "Epoch 166/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2141 - acc: 0.9310 - val_loss: 0.1475 - val_acc: 0.9489\n",
      "Epoch 167/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1829 - acc: 0.9391 - val_loss: 0.1440 - val_acc: 0.9511\n",
      "Epoch 168/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1849 - acc: 0.9391 - val_loss: 0.1433 - val_acc: 0.9511\n",
      "Epoch 169/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1814 - acc: 0.9451 - val_loss: 0.1451 - val_acc: 0.9467\n",
      "Epoch 170/300\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: 0.1756 - acc: 0.9406 - val_loss: 0.1457 - val_acc: 0.9467\n",
      "Epoch 171/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1793 - acc: 0.9406 - val_loss: 0.1414 - val_acc: 0.9556\n",
      "Epoch 172/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2161 - acc: 0.9258 - val_loss: 0.1442 - val_acc: 0.9444\n",
      "Epoch 173/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1987 - acc: 0.9295 - val_loss: 0.1441 - val_acc: 0.9489\n",
      "Epoch 174/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1879 - acc: 0.9399 - val_loss: 0.1420 - val_acc: 0.9444\n",
      "Epoch 175/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1998 - acc: 0.9339 - val_loss: 0.1397 - val_acc: 0.9511\n",
      "Epoch 176/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1949 - acc: 0.9384 - val_loss: 0.1422 - val_acc: 0.9489\n",
      "Epoch 177/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1963 - acc: 0.9339 - val_loss: 0.1423 - val_acc: 0.9511\n",
      "Epoch 178/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1956 - acc: 0.9347 - val_loss: 0.1413 - val_acc: 0.9511\n",
      "Epoch 179/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1942 - acc: 0.9317 - val_loss: 0.1408 - val_acc: 0.9533\n",
      "Epoch 180/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1976 - acc: 0.9265 - val_loss: 0.1418 - val_acc: 0.9489\n",
      "Epoch 181/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1962 - acc: 0.9354 - val_loss: 0.1425 - val_acc: 0.9467\n",
      "Epoch 182/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1896 - acc: 0.9369 - val_loss: 0.1415 - val_acc: 0.9489\n",
      "Epoch 183/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1978 - acc: 0.9324 - val_loss: 0.1418 - val_acc: 0.9489\n",
      "Epoch 184/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1732 - acc: 0.9436 - val_loss: 0.1378 - val_acc: 0.9511\n",
      "Epoch 185/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1861 - acc: 0.9414 - val_loss: 0.1400 - val_acc: 0.9489\n",
      "Epoch 186/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1892 - acc: 0.9310 - val_loss: 0.1391 - val_acc: 0.9489\n",
      "Epoch 187/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1974 - acc: 0.9332 - val_loss: 0.1413 - val_acc: 0.9467\n",
      "Epoch 188/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1786 - acc: 0.9399 - val_loss: 0.1390 - val_acc: 0.9422\n",
      "Epoch 189/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1855 - acc: 0.9399 - val_loss: 0.1383 - val_acc: 0.9511\n",
      "Epoch 190/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1871 - acc: 0.9369 - val_loss: 0.1369 - val_acc: 0.9533\n",
      "Epoch 191/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1970 - acc: 0.9376 - val_loss: 0.1378 - val_acc: 0.9467\n",
      "Epoch 192/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1880 - acc: 0.9399 - val_loss: 0.1368 - val_acc: 0.9533\n",
      "Epoch 193/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1785 - acc: 0.9414 - val_loss: 0.1375 - val_acc: 0.9511\n",
      "Epoch 194/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1803 - acc: 0.9495 - val_loss: 0.1343 - val_acc: 0.9467\n",
      "Epoch 195/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1700 - acc: 0.9458 - val_loss: 0.1349 - val_acc: 0.9489\n",
      "Epoch 196/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1892 - acc: 0.9347 - val_loss: 0.1323 - val_acc: 0.9511\n",
      "Epoch 197/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1844 - acc: 0.9384 - val_loss: 0.1382 - val_acc: 0.9444\n",
      "Epoch 198/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1802 - acc: 0.9354 - val_loss: 0.1385 - val_acc: 0.9467\n",
      "Epoch 199/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1957 - acc: 0.9302 - val_loss: 0.1359 - val_acc: 0.9489\n",
      "Epoch 200/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1728 - acc: 0.9406 - val_loss: 0.1357 - val_acc: 0.9467\n",
      "Epoch 201/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1771 - acc: 0.9414 - val_loss: 0.1328 - val_acc: 0.9533\n",
      "Epoch 202/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2093 - acc: 0.9339 - val_loss: 0.1361 - val_acc: 0.9467\n",
      "Epoch 203/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1910 - acc: 0.9369 - val_loss: 0.1381 - val_acc: 0.9467\n",
      "Epoch 204/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1806 - acc: 0.9428 - val_loss: 0.1377 - val_acc: 0.9489\n",
      "Epoch 205/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1713 - acc: 0.9473 - val_loss: 0.1376 - val_acc: 0.9444\n",
      "Epoch 206/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1842 - acc: 0.9369 - val_loss: 0.1361 - val_acc: 0.9444\n",
      "Epoch 207/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1866 - acc: 0.9362 - val_loss: 0.1340 - val_acc: 0.9533\n",
      "Epoch 208/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1766 - acc: 0.9391 - val_loss: 0.1315 - val_acc: 0.9489\n",
      "Epoch 209/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1704 - acc: 0.9443 - val_loss: 0.1338 - val_acc: 0.9511\n",
      "Epoch 210/300\n",
      "1347/1347 [==============================] - 0s 42us/step - loss: 0.1804 - acc: 0.9369 - val_loss: 0.1328 - val_acc: 0.9511\n",
      "Epoch 211/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1597 - acc: 0.9451 - val_loss: 0.1301 - val_acc: 0.9556\n",
      "Epoch 212/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1608 - acc: 0.9495 - val_loss: 0.1305 - val_acc: 0.9511\n",
      "Epoch 213/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1664 - acc: 0.9428 - val_loss: 0.1334 - val_acc: 0.9533\n",
      "Epoch 214/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1589 - acc: 0.9436 - val_loss: 0.1323 - val_acc: 0.9467\n",
      "Epoch 215/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1806 - acc: 0.9451 - val_loss: 0.1304 - val_acc: 0.9533\n",
      "Epoch 216/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1552 - acc: 0.9525 - val_loss: 0.1341 - val_acc: 0.9489\n",
      "Epoch 217/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1595 - acc: 0.9465 - val_loss: 0.1325 - val_acc: 0.9511\n",
      "Epoch 218/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1778 - acc: 0.9384 - val_loss: 0.1324 - val_acc: 0.9511\n",
      "Epoch 219/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1807 - acc: 0.9451 - val_loss: 0.1295 - val_acc: 0.9511\n",
      "Epoch 220/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.1854 - acc: 0.9399 - val_loss: 0.1325 - val_acc: 0.9489\n",
      "Epoch 221/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1700 - acc: 0.9465 - val_loss: 0.1317 - val_acc: 0.9467\n",
      "Epoch 222/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1651 - acc: 0.9399 - val_loss: 0.1281 - val_acc: 0.9489\n",
      "Epoch 223/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1592 - acc: 0.9465 - val_loss: 0.1271 - val_acc: 0.9533\n",
      "Epoch 224/300\n",
      "1347/1347 [==============================] - 0s 48us/step - loss: 0.1825 - acc: 0.9347 - val_loss: 0.1328 - val_acc: 0.9489\n",
      "Epoch 225/300\n",
      "1347/1347 [==============================] - 0s 42us/step - loss: 0.1770 - acc: 0.9391 - val_loss: 0.1306 - val_acc: 0.9511\n",
      "Epoch 226/300\n",
      "1347/1347 [==============================] - 0s 49us/step - loss: 0.1645 - acc: 0.9443 - val_loss: 0.1290 - val_acc: 0.9556\n",
      "Epoch 227/300\n",
      "1347/1347 [==============================] - 0s 44us/step - loss: 0.1725 - acc: 0.9384 - val_loss: 0.1278 - val_acc: 0.9489\n",
      "Epoch 228/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1722 - acc: 0.9421 - val_loss: 0.1299 - val_acc: 0.9511\n",
      "Epoch 229/300\n",
      "1347/1347 [==============================] - 0s 52us/step - loss: 0.1791 - acc: 0.9391 - val_loss: 0.1279 - val_acc: 0.9489\n",
      "Epoch 230/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1585 - acc: 0.9473 - val_loss: 0.1309 - val_acc: 0.9533\n",
      "Epoch 231/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.1740 - acc: 0.9458 - val_loss: 0.1289 - val_acc: 0.9511\n",
      "Epoch 232/300\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 0.1690 - acc: 0.9391 - val_loss: 0.1278 - val_acc: 0.9511\n",
      "Epoch 233/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1696 - acc: 0.9414 - val_loss: 0.1294 - val_acc: 0.9489\n",
      "Epoch 234/300\n",
      "1347/1347 [==============================] - 0s 65us/step - loss: 0.1790 - acc: 0.9347 - val_loss: 0.1296 - val_acc: 0.9511\n",
      "Epoch 235/300\n",
      "1347/1347 [==============================] - 0s 68us/step - loss: 0.1751 - acc: 0.9414 - val_loss: 0.1300 - val_acc: 0.9533\n",
      "Epoch 236/300\n",
      "1347/1347 [==============================] - 0s 61us/step - loss: 0.1560 - acc: 0.9503 - val_loss: 0.1262 - val_acc: 0.9489\n",
      "Epoch 237/300\n",
      "1347/1347 [==============================] - 0s 43us/step - loss: 0.1489 - acc: 0.9532 - val_loss: 0.1268 - val_acc: 0.9511\n",
      "Epoch 238/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1584 - acc: 0.9510 - val_loss: 0.1247 - val_acc: 0.9533\n",
      "Epoch 239/300\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 0.1784 - acc: 0.9369 - val_loss: 0.1272 - val_acc: 0.9533\n",
      "Epoch 240/300\n",
      "1347/1347 [==============================] - 0s 42us/step - loss: 0.1557 - acc: 0.9421 - val_loss: 0.1244 - val_acc: 0.9533\n",
      "Epoch 241/300\n",
      "1347/1347 [==============================] - 0s 59us/step - loss: 0.1662 - acc: 0.9421 - val_loss: 0.1276 - val_acc: 0.9511\n",
      "Epoch 242/300\n",
      "1347/1347 [==============================] - 0s 42us/step - loss: 0.1669 - acc: 0.9436 - val_loss: 0.1310 - val_acc: 0.9511\n",
      "Epoch 243/300\n",
      "1347/1347 [==============================] - 0s 59us/step - loss: 0.1627 - acc: 0.9473 - val_loss: 0.1247 - val_acc: 0.9489\n",
      "Epoch 244/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1673 - acc: 0.9436 - val_loss: 0.1234 - val_acc: 0.9511\n",
      "Epoch 245/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1441 - acc: 0.9525 - val_loss: 0.1236 - val_acc: 0.9533\n",
      "Epoch 246/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1381 - acc: 0.9577 - val_loss: 0.1217 - val_acc: 0.9533\n",
      "Epoch 247/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1732 - acc: 0.9339 - val_loss: 0.1245 - val_acc: 0.9556\n",
      "Epoch 248/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.1576 - acc: 0.9480 - val_loss: 0.1230 - val_acc: 0.9511\n",
      "Epoch 249/300\n",
      "1347/1347 [==============================] - 0s 43us/step - loss: 0.1630 - acc: 0.9421 - val_loss: 0.1226 - val_acc: 0.9533\n",
      "Epoch 250/300\n",
      "1347/1347 [==============================] - 0s 42us/step - loss: 0.1704 - acc: 0.9428 - val_loss: 0.1233 - val_acc: 0.9533\n",
      "Epoch 251/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1691 - acc: 0.9406 - val_loss: 0.1229 - val_acc: 0.9556\n",
      "Epoch 252/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1507 - acc: 0.9555 - val_loss: 0.1239 - val_acc: 0.9511\n",
      "Epoch 253/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1539 - acc: 0.9458 - val_loss: 0.1264 - val_acc: 0.9533\n",
      "Epoch 254/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1796 - acc: 0.9399 - val_loss: 0.1259 - val_acc: 0.9511\n",
      "Epoch 255/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1583 - acc: 0.9451 - val_loss: 0.1236 - val_acc: 0.9533\n",
      "Epoch 256/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1830 - acc: 0.9384 - val_loss: 0.1233 - val_acc: 0.9511\n",
      "Epoch 257/300\n",
      "1347/1347 [==============================] - 0s 53us/step - loss: 0.1542 - acc: 0.9510 - val_loss: 0.1266 - val_acc: 0.9556\n",
      "Epoch 258/300\n",
      "1347/1347 [==============================] - 0s 61us/step - loss: 0.1543 - acc: 0.9480 - val_loss: 0.1243 - val_acc: 0.9533\n",
      "Epoch 259/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1707 - acc: 0.9428 - val_loss: 0.1202 - val_acc: 0.9511\n",
      "Epoch 260/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1762 - acc: 0.9332 - val_loss: 0.1239 - val_acc: 0.9533\n",
      "Epoch 261/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1584 - acc: 0.9458 - val_loss: 0.1258 - val_acc: 0.9556\n",
      "Epoch 262/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1405 - acc: 0.9577 - val_loss: 0.1222 - val_acc: 0.9511\n",
      "Epoch 263/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1516 - acc: 0.9495 - val_loss: 0.1211 - val_acc: 0.9533\n",
      "Epoch 264/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1566 - acc: 0.9406 - val_loss: 0.1223 - val_acc: 0.9533\n",
      "Epoch 265/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1374 - acc: 0.9569 - val_loss: 0.1178 - val_acc: 0.9556\n",
      "Epoch 266/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1706 - acc: 0.9369 - val_loss: 0.1192 - val_acc: 0.9556\n",
      "Epoch 267/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1490 - acc: 0.9510 - val_loss: 0.1163 - val_acc: 0.9556\n",
      "Epoch 268/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1789 - acc: 0.9354 - val_loss: 0.1166 - val_acc: 0.9578\n",
      "Epoch 269/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1640 - acc: 0.9414 - val_loss: 0.1179 - val_acc: 0.9600\n",
      "Epoch 270/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1369 - acc: 0.9503 - val_loss: 0.1171 - val_acc: 0.9578\n",
      "Epoch 271/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1565 - acc: 0.9436 - val_loss: 0.1187 - val_acc: 0.9556\n",
      "Epoch 272/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1580 - acc: 0.9480 - val_loss: 0.1199 - val_acc: 0.9578\n",
      "Epoch 273/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1665 - acc: 0.9428 - val_loss: 0.1200 - val_acc: 0.9578\n",
      "Epoch 274/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1742 - acc: 0.9384 - val_loss: 0.1186 - val_acc: 0.9533\n",
      "Epoch 275/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1658 - acc: 0.9436 - val_loss: 0.1233 - val_acc: 0.9533\n",
      "Epoch 276/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1527 - acc: 0.9510 - val_loss: 0.1190 - val_acc: 0.9556\n",
      "Epoch 277/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1667 - acc: 0.9376 - val_loss: 0.1199 - val_acc: 0.9578\n",
      "Epoch 278/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1476 - acc: 0.9443 - val_loss: 0.1206 - val_acc: 0.9556\n",
      "Epoch 279/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1629 - acc: 0.9443 - val_loss: 0.1163 - val_acc: 0.9556\n",
      "Epoch 280/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1393 - acc: 0.9562 - val_loss: 0.1146 - val_acc: 0.9556\n",
      "Epoch 281/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1437 - acc: 0.9525 - val_loss: 0.1155 - val_acc: 0.9556\n",
      "Epoch 282/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.1564 - acc: 0.9421 - val_loss: 0.1176 - val_acc: 0.9578\n",
      "Epoch 283/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1462 - acc: 0.9495 - val_loss: 0.1157 - val_acc: 0.9556\n",
      "Epoch 284/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1647 - acc: 0.9473 - val_loss: 0.1213 - val_acc: 0.9511\n",
      "Epoch 285/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1462 - acc: 0.9562 - val_loss: 0.1157 - val_acc: 0.9578\n",
      "Epoch 286/300\n",
      "1347/1347 [==============================] - 0s 29us/step - loss: 0.1766 - acc: 0.9399 - val_loss: 0.1199 - val_acc: 0.9556\n",
      "Epoch 287/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1537 - acc: 0.9480 - val_loss: 0.1184 - val_acc: 0.9556\n",
      "Epoch 288/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1626 - acc: 0.9488 - val_loss: 0.1192 - val_acc: 0.9533\n",
      "Epoch 289/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1530 - acc: 0.9510 - val_loss: 0.1268 - val_acc: 0.9578\n",
      "Epoch 290/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1571 - acc: 0.9480 - val_loss: 0.1225 - val_acc: 0.9533\n",
      "Epoch 291/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1383 - acc: 0.9517 - val_loss: 0.1217 - val_acc: 0.9533\n",
      "Epoch 292/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1580 - acc: 0.9436 - val_loss: 0.1238 - val_acc: 0.9533\n",
      "Epoch 293/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1403 - acc: 0.9510 - val_loss: 0.1183 - val_acc: 0.9556\n",
      "Epoch 294/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1593 - acc: 0.9517 - val_loss: 0.1194 - val_acc: 0.9533\n",
      "Epoch 295/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1736 - acc: 0.9443 - val_loss: 0.1181 - val_acc: 0.9578\n",
      "Epoch 296/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1405 - acc: 0.9555 - val_loss: 0.1171 - val_acc: 0.9578\n",
      "Epoch 297/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1429 - acc: 0.9488 - val_loss: 0.1160 - val_acc: 0.9578\n",
      "Epoch 298/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1333 - acc: 0.9577 - val_loss: 0.1172 - val_acc: 0.9556\n",
      "Epoch 299/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1437 - acc: 0.9532 - val_loss: 0.1183 - val_acc: 0.9556\n",
      "Epoch 300/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1409 - acc: 0.9510 - val_loss: 0.1167 - val_acc: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d51d6bd68>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Dense(n_hidden, input_dim=n_features, activation=activation))\n",
    "keras_model.add(Dropout(0.1))\n",
    "keras_model.add(Dense(n_classes, activation='softmax'))\n",
    "keras_model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "keras_model.fit(\n",
    "    X_tr, to_categorical(Y_tr, num_classes=n_classes),\n",
    "    epochs=300, batch_size=32, verbose=1,\n",
    "    validation_data=(X_val, to_categorical(Y_val, num_classes=n_classes))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional without overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with relu activation\n",
      "Train on 1347 samples, validate on 450 samples\n",
      "Epoch 1/300\n",
      "1347/1347 [==============================] - 0s 227us/step - loss: 2.2965 - acc: 0.1314 - val_loss: 2.2107 - val_acc: 0.1889\n",
      "Epoch 2/300\n",
      "1347/1347 [==============================] - 0s 43us/step - loss: 2.1783 - acc: 0.1982 - val_loss: 2.1019 - val_acc: 0.2244\n",
      "Epoch 3/300\n",
      "1347/1347 [==============================] - 0s 43us/step - loss: 2.0653 - acc: 0.2509 - val_loss: 1.9783 - val_acc: 0.3067\n",
      "Epoch 4/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 1.9338 - acc: 0.3252 - val_loss: 1.8308 - val_acc: 0.3622\n",
      "Epoch 5/300\n",
      "1347/1347 [==============================] - 0s 42us/step - loss: 1.7841 - acc: 0.4143 - val_loss: 1.6727 - val_acc: 0.4956\n",
      "Epoch 6/300\n",
      "1347/1347 [==============================] - 0s 43us/step - loss: 1.6374 - acc: 0.4751 - val_loss: 1.5140 - val_acc: 0.5844\n",
      "Epoch 7/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 1.4820 - acc: 0.5486 - val_loss: 1.3575 - val_acc: 0.6556\n",
      "Epoch 8/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 1.3432 - acc: 0.6169 - val_loss: 1.2031 - val_acc: 0.7489\n",
      "Epoch 9/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 1.2181 - acc: 0.6741 - val_loss: 1.0597 - val_acc: 0.8044\n",
      "Epoch 10/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 1.0871 - acc: 0.7275 - val_loss: 0.9299 - val_acc: 0.8244\n",
      "Epoch 11/300\n",
      "1347/1347 [==============================] - 0s 49us/step - loss: 0.9550 - acc: 0.7699 - val_loss: 0.8175 - val_acc: 0.8533\n",
      "Epoch 12/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.8892 - acc: 0.7765 - val_loss: 0.7264 - val_acc: 0.8778\n",
      "Epoch 13/300\n",
      "1347/1347 [==============================] - 0s 42us/step - loss: 0.8007 - acc: 0.7996 - val_loss: 0.6551 - val_acc: 0.8844\n",
      "Epoch 14/300\n",
      "1347/1347 [==============================] - 0s 46us/step - loss: 0.7326 - acc: 0.8092 - val_loss: 0.5966 - val_acc: 0.8978\n",
      "Epoch 15/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.6868 - acc: 0.8241 - val_loss: 0.5501 - val_acc: 0.8978\n",
      "Epoch 16/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.6339 - acc: 0.8300 - val_loss: 0.5113 - val_acc: 0.9022\n",
      "Epoch 17/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.6336 - acc: 0.8144 - val_loss: 0.4788 - val_acc: 0.9133\n",
      "Epoch 18/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.5889 - acc: 0.8344 - val_loss: 0.4494 - val_acc: 0.9178\n",
      "Epoch 19/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.5847 - acc: 0.8300 - val_loss: 0.4272 - val_acc: 0.9178\n",
      "Epoch 20/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.5472 - acc: 0.8471 - val_loss: 0.4087 - val_acc: 0.9178\n",
      "Epoch 21/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.5061 - acc: 0.8627 - val_loss: 0.3854 - val_acc: 0.9244\n",
      "Epoch 22/300\n",
      "1347/1347 [==============================] - 0s 44us/step - loss: 0.5124 - acc: 0.8463 - val_loss: 0.3710 - val_acc: 0.9244\n",
      "Epoch 23/300\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 0.5143 - acc: 0.8478 - val_loss: 0.3626 - val_acc: 0.9222\n",
      "Epoch 24/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.4684 - acc: 0.8671 - val_loss: 0.3444 - val_acc: 0.9311\n",
      "Epoch 25/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.4676 - acc: 0.8641 - val_loss: 0.3333 - val_acc: 0.9244\n",
      "Epoch 26/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.4503 - acc: 0.8716 - val_loss: 0.3195 - val_acc: 0.9356\n",
      "Epoch 27/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.4264 - acc: 0.8827 - val_loss: 0.3093 - val_acc: 0.9311\n",
      "Epoch 28/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.4299 - acc: 0.8723 - val_loss: 0.2998 - val_acc: 0.9378\n",
      "Epoch 29/300\n",
      "1347/1347 [==============================] - 0s 43us/step - loss: 0.4181 - acc: 0.8834 - val_loss: 0.2905 - val_acc: 0.9378\n",
      "Epoch 30/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.4125 - acc: 0.8812 - val_loss: 0.2813 - val_acc: 0.9422\n",
      "Epoch 31/300\n",
      "1347/1347 [==============================] - 0s 43us/step - loss: 0.4081 - acc: 0.8760 - val_loss: 0.2755 - val_acc: 0.9444\n",
      "Epoch 32/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.3968 - acc: 0.8775 - val_loss: 0.2716 - val_acc: 0.9356\n",
      "Epoch 33/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.3883 - acc: 0.8686 - val_loss: 0.2620 - val_acc: 0.9444\n",
      "Epoch 34/300\n",
      "1347/1347 [==============================] - 0s 55us/step - loss: 0.3744 - acc: 0.9035 - val_loss: 0.2542 - val_acc: 0.9444\n",
      "Epoch 35/300\n",
      "1347/1347 [==============================] - 0s 45us/step - loss: 0.3875 - acc: 0.8812 - val_loss: 0.2522 - val_acc: 0.9400\n",
      "Epoch 36/300\n",
      "1347/1347 [==============================] - 0s 44us/step - loss: 0.3654 - acc: 0.8834 - val_loss: 0.2492 - val_acc: 0.9400\n",
      "Epoch 37/300\n",
      "1347/1347 [==============================] - 0s 50us/step - loss: 0.3572 - acc: 0.8961 - val_loss: 0.2425 - val_acc: 0.9444\n",
      "Epoch 38/300\n",
      "1347/1347 [==============================] - 0s 48us/step - loss: 0.3712 - acc: 0.8849 - val_loss: 0.2351 - val_acc: 0.9467\n",
      "Epoch 39/300\n",
      "1347/1347 [==============================] - 0s 46us/step - loss: 0.3573 - acc: 0.8961 - val_loss: 0.2348 - val_acc: 0.9422\n",
      "Epoch 40/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.3503 - acc: 0.8931 - val_loss: 0.2321 - val_acc: 0.9400\n",
      "Epoch 41/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.3456 - acc: 0.8916 - val_loss: 0.2260 - val_acc: 0.9422\n",
      "Epoch 42/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.3464 - acc: 0.8834 - val_loss: 0.2209 - val_acc: 0.9467\n",
      "Epoch 43/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.3312 - acc: 0.8931 - val_loss: 0.2199 - val_acc: 0.9444\n",
      "Epoch 44/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.3215 - acc: 0.9050 - val_loss: 0.2142 - val_acc: 0.9444\n",
      "Epoch 45/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.3492 - acc: 0.8909 - val_loss: 0.2145 - val_acc: 0.9400\n",
      "Epoch 46/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.3203 - acc: 0.9020 - val_loss: 0.2103 - val_acc: 0.9400\n",
      "Epoch 47/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.3084 - acc: 0.9057 - val_loss: 0.2043 - val_acc: 0.9444\n",
      "Epoch 48/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.3081 - acc: 0.9013 - val_loss: 0.2025 - val_acc: 0.9467\n",
      "Epoch 49/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.3101 - acc: 0.9035 - val_loss: 0.2017 - val_acc: 0.9444\n",
      "Epoch 50/300\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 0.3073 - acc: 0.9042 - val_loss: 0.1988 - val_acc: 0.9467\n",
      "Epoch 51/300\n",
      "1347/1347 [==============================] - 0s 42us/step - loss: 0.3077 - acc: 0.9102 - val_loss: 0.1982 - val_acc: 0.9444\n",
      "Epoch 52/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.2908 - acc: 0.9117 - val_loss: 0.1954 - val_acc: 0.9467\n",
      "Epoch 53/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.3061 - acc: 0.9035 - val_loss: 0.1930 - val_acc: 0.9467\n",
      "Epoch 54/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2904 - acc: 0.9087 - val_loss: 0.1926 - val_acc: 0.9444\n",
      "Epoch 55/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.3072 - acc: 0.8998 - val_loss: 0.1912 - val_acc: 0.9467\n",
      "Epoch 56/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.3019 - acc: 0.9050 - val_loss: 0.1882 - val_acc: 0.9444\n",
      "Epoch 57/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.2708 - acc: 0.9154 - val_loss: 0.1839 - val_acc: 0.9489\n",
      "Epoch 58/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2809 - acc: 0.9161 - val_loss: 0.1853 - val_acc: 0.9422\n",
      "Epoch 59/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.2688 - acc: 0.9161 - val_loss: 0.1820 - val_acc: 0.9511\n",
      "Epoch 60/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2630 - acc: 0.9235 - val_loss: 0.1815 - val_acc: 0.9489\n",
      "Epoch 61/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.2669 - acc: 0.9117 - val_loss: 0.1795 - val_acc: 0.9467\n",
      "Epoch 62/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.2689 - acc: 0.9235 - val_loss: 0.1768 - val_acc: 0.9467\n",
      "Epoch 63/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.2870 - acc: 0.9102 - val_loss: 0.1781 - val_acc: 0.9467\n",
      "Epoch 64/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2699 - acc: 0.9272 - val_loss: 0.1725 - val_acc: 0.9511\n",
      "Epoch 65/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2611 - acc: 0.9154 - val_loss: 0.1749 - val_acc: 0.9444\n",
      "Epoch 66/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.2581 - acc: 0.9198 - val_loss: 0.1713 - val_acc: 0.9511\n",
      "Epoch 67/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2569 - acc: 0.9250 - val_loss: 0.1716 - val_acc: 0.9489\n",
      "Epoch 68/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2575 - acc: 0.9176 - val_loss: 0.1743 - val_acc: 0.9422\n",
      "Epoch 69/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2648 - acc: 0.9228 - val_loss: 0.1731 - val_acc: 0.9444\n",
      "Epoch 70/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2420 - acc: 0.9220 - val_loss: 0.1689 - val_acc: 0.9489\n",
      "Epoch 71/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2499 - acc: 0.9139 - val_loss: 0.1662 - val_acc: 0.9467\n",
      "Epoch 72/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2287 - acc: 0.9369 - val_loss: 0.1688 - val_acc: 0.9467\n",
      "Epoch 73/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.2285 - acc: 0.9376 - val_loss: 0.1674 - val_acc: 0.9467\n",
      "Epoch 74/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2326 - acc: 0.9280 - val_loss: 0.1667 - val_acc: 0.9489\n",
      "Epoch 75/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2337 - acc: 0.9243 - val_loss: 0.1628 - val_acc: 0.9533\n",
      "Epoch 76/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2388 - acc: 0.9213 - val_loss: 0.1626 - val_acc: 0.9489\n",
      "Epoch 77/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2467 - acc: 0.9169 - val_loss: 0.1638 - val_acc: 0.9444\n",
      "Epoch 78/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2399 - acc: 0.9228 - val_loss: 0.1571 - val_acc: 0.9556\n",
      "Epoch 79/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2341 - acc: 0.9258 - val_loss: 0.1622 - val_acc: 0.9533\n",
      "Epoch 80/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2350 - acc: 0.9243 - val_loss: 0.1589 - val_acc: 0.9533\n",
      "Epoch 81/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.2302 - acc: 0.9295 - val_loss: 0.1621 - val_acc: 0.9444\n",
      "Epoch 82/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2173 - acc: 0.9362 - val_loss: 0.1609 - val_acc: 0.9489\n",
      "Epoch 83/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2180 - acc: 0.9339 - val_loss: 0.1604 - val_acc: 0.9467\n",
      "Epoch 84/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2108 - acc: 0.9302 - val_loss: 0.1548 - val_acc: 0.9511\n",
      "Epoch 85/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2130 - acc: 0.9280 - val_loss: 0.1562 - val_acc: 0.9489\n",
      "Epoch 86/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.2409 - acc: 0.9206 - val_loss: 0.1512 - val_acc: 0.9533\n",
      "Epoch 87/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.2348 - acc: 0.9295 - val_loss: 0.1539 - val_acc: 0.9556\n",
      "Epoch 88/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2183 - acc: 0.9317 - val_loss: 0.1522 - val_acc: 0.9556\n",
      "Epoch 89/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2280 - acc: 0.9228 - val_loss: 0.1547 - val_acc: 0.9489\n",
      "Epoch 90/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2134 - acc: 0.9339 - val_loss: 0.1539 - val_acc: 0.9511\n",
      "Epoch 91/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2349 - acc: 0.9198 - val_loss: 0.1545 - val_acc: 0.9533\n",
      "Epoch 92/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2118 - acc: 0.9339 - val_loss: 0.1515 - val_acc: 0.9511\n",
      "Epoch 93/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2198 - acc: 0.9220 - val_loss: 0.1486 - val_acc: 0.9556\n",
      "Epoch 94/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2185 - acc: 0.9295 - val_loss: 0.1469 - val_acc: 0.9600\n",
      "Epoch 95/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2130 - acc: 0.9369 - val_loss: 0.1525 - val_acc: 0.9533\n",
      "Epoch 96/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2276 - acc: 0.9272 - val_loss: 0.1519 - val_acc: 0.9489\n",
      "Epoch 97/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.2146 - acc: 0.9362 - val_loss: 0.1495 - val_acc: 0.9556\n",
      "Epoch 98/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.2106 - acc: 0.9317 - val_loss: 0.1494 - val_acc: 0.9489\n",
      "Epoch 99/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.2042 - acc: 0.9317 - val_loss: 0.1506 - val_acc: 0.9467\n",
      "Epoch 100/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1941 - acc: 0.9384 - val_loss: 0.1514 - val_acc: 0.9489\n",
      "Epoch 101/300\n",
      "1347/1347 [==============================] - 0s 45us/step - loss: 0.2249 - acc: 0.9295 - val_loss: 0.1516 - val_acc: 0.9489\n",
      "Epoch 102/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2030 - acc: 0.9362 - val_loss: 0.1488 - val_acc: 0.9533\n",
      "Epoch 103/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.2014 - acc: 0.9406 - val_loss: 0.1482 - val_acc: 0.9511\n",
      "Epoch 104/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2052 - acc: 0.9369 - val_loss: 0.1520 - val_acc: 0.9511\n",
      "Epoch 105/300\n",
      "1347/1347 [==============================] - 0s 43us/step - loss: 0.1904 - acc: 0.9391 - val_loss: 0.1465 - val_acc: 0.9556\n",
      "Epoch 106/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.2000 - acc: 0.9287 - val_loss: 0.1458 - val_acc: 0.9511\n",
      "Epoch 107/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2077 - acc: 0.9354 - val_loss: 0.1425 - val_acc: 0.9533\n",
      "Epoch 108/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.2056 - acc: 0.9384 - val_loss: 0.1439 - val_acc: 0.9556\n",
      "Epoch 109/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1958 - acc: 0.9347 - val_loss: 0.1468 - val_acc: 0.9533\n",
      "Epoch 110/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2008 - acc: 0.9406 - val_loss: 0.1440 - val_acc: 0.9511\n",
      "Epoch 111/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.2139 - acc: 0.9213 - val_loss: 0.1453 - val_acc: 0.9511\n",
      "Epoch 112/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1992 - acc: 0.9414 - val_loss: 0.1481 - val_acc: 0.9511\n",
      "Epoch 113/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1943 - acc: 0.9302 - val_loss: 0.1462 - val_acc: 0.9533\n",
      "Epoch 114/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.2100 - acc: 0.9235 - val_loss: 0.1489 - val_acc: 0.9489\n",
      "Epoch 115/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1978 - acc: 0.9391 - val_loss: 0.1447 - val_acc: 0.9533\n",
      "Epoch 116/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1981 - acc: 0.9369 - val_loss: 0.1493 - val_acc: 0.9511\n",
      "Epoch 117/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1942 - acc: 0.9369 - val_loss: 0.1447 - val_acc: 0.9511\n",
      "Epoch 118/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1903 - acc: 0.9391 - val_loss: 0.1451 - val_acc: 0.9511\n",
      "Epoch 119/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1823 - acc: 0.9421 - val_loss: 0.1450 - val_acc: 0.9533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1784 - acc: 0.9406 - val_loss: 0.1442 - val_acc: 0.9511\n",
      "Epoch 121/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1803 - acc: 0.9384 - val_loss: 0.1413 - val_acc: 0.9533\n",
      "Epoch 122/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1828 - acc: 0.9428 - val_loss: 0.1404 - val_acc: 0.9533\n",
      "Epoch 123/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2018 - acc: 0.9362 - val_loss: 0.1438 - val_acc: 0.9511\n",
      "Epoch 124/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1750 - acc: 0.9421 - val_loss: 0.1428 - val_acc: 0.9511\n",
      "Epoch 125/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2006 - acc: 0.9354 - val_loss: 0.1459 - val_acc: 0.9511\n",
      "Epoch 126/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1865 - acc: 0.9391 - val_loss: 0.1535 - val_acc: 0.9444\n",
      "Epoch 127/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1853 - acc: 0.9354 - val_loss: 0.1408 - val_acc: 0.9533\n",
      "Epoch 128/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.2080 - acc: 0.9339 - val_loss: 0.1405 - val_acc: 0.9511\n",
      "Epoch 129/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1681 - acc: 0.9428 - val_loss: 0.1388 - val_acc: 0.9533\n",
      "Epoch 130/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1804 - acc: 0.9362 - val_loss: 0.1403 - val_acc: 0.9489\n",
      "Epoch 131/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.1922 - acc: 0.9317 - val_loss: 0.1366 - val_acc: 0.9533\n",
      "Epoch 132/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1619 - acc: 0.9473 - val_loss: 0.1390 - val_acc: 0.9511\n",
      "Epoch 133/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1695 - acc: 0.9451 - val_loss: 0.1355 - val_acc: 0.9511\n",
      "Epoch 134/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1892 - acc: 0.9310 - val_loss: 0.1383 - val_acc: 0.9511\n",
      "Epoch 135/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1666 - acc: 0.9428 - val_loss: 0.1380 - val_acc: 0.9533\n",
      "Epoch 136/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1813 - acc: 0.9451 - val_loss: 0.1357 - val_acc: 0.9533\n",
      "Epoch 137/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1937 - acc: 0.9354 - val_loss: 0.1420 - val_acc: 0.9511\n",
      "Epoch 138/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.1866 - acc: 0.9354 - val_loss: 0.1385 - val_acc: 0.9511\n",
      "Epoch 139/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1639 - acc: 0.9510 - val_loss: 0.1316 - val_acc: 0.9533\n",
      "Epoch 140/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1540 - acc: 0.9488 - val_loss: 0.1366 - val_acc: 0.9533\n",
      "Epoch 141/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1605 - acc: 0.9517 - val_loss: 0.1342 - val_acc: 0.9533\n",
      "Epoch 142/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1765 - acc: 0.9399 - val_loss: 0.1355 - val_acc: 0.9533\n",
      "Epoch 143/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1611 - acc: 0.9488 - val_loss: 0.1353 - val_acc: 0.9533\n",
      "Epoch 144/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1697 - acc: 0.9451 - val_loss: 0.1336 - val_acc: 0.9556\n",
      "Epoch 145/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1635 - acc: 0.9458 - val_loss: 0.1367 - val_acc: 0.9533\n",
      "Epoch 146/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1963 - acc: 0.9369 - val_loss: 0.1418 - val_acc: 0.9511\n",
      "Epoch 147/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1794 - acc: 0.9399 - val_loss: 0.1331 - val_acc: 0.9511\n",
      "Epoch 148/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1725 - acc: 0.9369 - val_loss: 0.1367 - val_acc: 0.9511\n",
      "Epoch 149/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1481 - acc: 0.9517 - val_loss: 0.1311 - val_acc: 0.9556\n",
      "Epoch 150/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1726 - acc: 0.9458 - val_loss: 0.1350 - val_acc: 0.9533\n",
      "Epoch 151/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1835 - acc: 0.9280 - val_loss: 0.1373 - val_acc: 0.9533\n",
      "Epoch 152/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1731 - acc: 0.9428 - val_loss: 0.1378 - val_acc: 0.9511\n",
      "Epoch 153/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1713 - acc: 0.9480 - val_loss: 0.1338 - val_acc: 0.9511\n",
      "Epoch 154/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.1789 - acc: 0.9369 - val_loss: 0.1330 - val_acc: 0.9556\n",
      "Epoch 155/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1636 - acc: 0.9473 - val_loss: 0.1311 - val_acc: 0.9533\n",
      "Epoch 156/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1621 - acc: 0.9458 - val_loss: 0.1287 - val_acc: 0.9533\n",
      "Epoch 157/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1698 - acc: 0.9428 - val_loss: 0.1356 - val_acc: 0.9511\n",
      "Epoch 158/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1486 - acc: 0.9517 - val_loss: 0.1318 - val_acc: 0.9533\n",
      "Epoch 159/300\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 0.1654 - acc: 0.9428 - val_loss: 0.1363 - val_acc: 0.9533\n",
      "Epoch 160/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1472 - acc: 0.9525 - val_loss: 0.1328 - val_acc: 0.9533\n",
      "Epoch 161/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1715 - acc: 0.9428 - val_loss: 0.1345 - val_acc: 0.9533\n",
      "Epoch 162/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1689 - acc: 0.9384 - val_loss: 0.1363 - val_acc: 0.9533\n",
      "Epoch 163/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1668 - acc: 0.9369 - val_loss: 0.1283 - val_acc: 0.9556\n",
      "Epoch 164/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1751 - acc: 0.9480 - val_loss: 0.1325 - val_acc: 0.9533\n",
      "Epoch 165/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1556 - acc: 0.9451 - val_loss: 0.1283 - val_acc: 0.9556\n",
      "Epoch 166/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1761 - acc: 0.9421 - val_loss: 0.1304 - val_acc: 0.9533\n",
      "Epoch 167/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1666 - acc: 0.9428 - val_loss: 0.1343 - val_acc: 0.9511\n",
      "Epoch 168/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1615 - acc: 0.9458 - val_loss: 0.1324 - val_acc: 0.9533\n",
      "Epoch 169/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1629 - acc: 0.9503 - val_loss: 0.1306 - val_acc: 0.9533\n",
      "Epoch 170/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1519 - acc: 0.9458 - val_loss: 0.1270 - val_acc: 0.9578\n",
      "Epoch 171/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.1666 - acc: 0.9436 - val_loss: 0.1325 - val_acc: 0.9533\n",
      "Epoch 172/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1339 - acc: 0.9614 - val_loss: 0.1282 - val_acc: 0.9556\n",
      "Epoch 173/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1563 - acc: 0.9480 - val_loss: 0.1325 - val_acc: 0.9533\n",
      "Epoch 174/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1704 - acc: 0.9414 - val_loss: 0.1308 - val_acc: 0.9533\n",
      "Epoch 175/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1450 - acc: 0.9488 - val_loss: 0.1329 - val_acc: 0.9556\n",
      "Epoch 176/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1633 - acc: 0.9443 - val_loss: 0.1325 - val_acc: 0.9556\n",
      "Epoch 177/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1269 - acc: 0.9636 - val_loss: 0.1282 - val_acc: 0.9511\n",
      "Epoch 178/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1956 - acc: 0.9302 - val_loss: 0.1330 - val_acc: 0.9511\n",
      "Epoch 179/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1467 - acc: 0.9473 - val_loss: 0.1312 - val_acc: 0.9511\n",
      "Epoch 180/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1489 - acc: 0.9495 - val_loss: 0.1287 - val_acc: 0.9556\n",
      "Epoch 181/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1551 - acc: 0.9495 - val_loss: 0.1310 - val_acc: 0.9556\n",
      "Epoch 182/300\n",
      "1347/1347 [==============================] - 0s 54us/step - loss: 0.1602 - acc: 0.9540 - val_loss: 0.1322 - val_acc: 0.9489\n",
      "Epoch 183/300\n",
      "1347/1347 [==============================] - 0s 45us/step - loss: 0.1554 - acc: 0.9473 - val_loss: 0.1335 - val_acc: 0.9556\n",
      "Epoch 184/300\n",
      "1347/1347 [==============================] - 0s 42us/step - loss: 0.1448 - acc: 0.9510 - val_loss: 0.1307 - val_acc: 0.9533\n",
      "Epoch 185/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1625 - acc: 0.9391 - val_loss: 0.1316 - val_acc: 0.9489\n",
      "Epoch 186/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1526 - acc: 0.9510 - val_loss: 0.1318 - val_acc: 0.9511\n",
      "Epoch 187/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1413 - acc: 0.9555 - val_loss: 0.1304 - val_acc: 0.9511\n",
      "Epoch 188/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.1540 - acc: 0.9428 - val_loss: 0.1312 - val_acc: 0.9511\n",
      "Epoch 189/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1473 - acc: 0.9510 - val_loss: 0.1304 - val_acc: 0.9511\n",
      "Epoch 190/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1401 - acc: 0.9547 - val_loss: 0.1340 - val_acc: 0.9511\n",
      "Epoch 191/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1447 - acc: 0.9555 - val_loss: 0.1327 - val_acc: 0.9511\n",
      "Epoch 192/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1421 - acc: 0.9503 - val_loss: 0.1292 - val_acc: 0.9511\n",
      "Epoch 193/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1469 - acc: 0.9555 - val_loss: 0.1292 - val_acc: 0.9511\n",
      "Epoch 194/300\n",
      "1347/1347 [==============================] - 0s 46us/step - loss: 0.1528 - acc: 0.9532 - val_loss: 0.1280 - val_acc: 0.9556\n",
      "Epoch 195/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1470 - acc: 0.9540 - val_loss: 0.1345 - val_acc: 0.9556\n",
      "Epoch 196/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1559 - acc: 0.9503 - val_loss: 0.1317 - val_acc: 0.9533\n",
      "Epoch 197/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1353 - acc: 0.9569 - val_loss: 0.1292 - val_acc: 0.9533\n",
      "Epoch 198/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1479 - acc: 0.9465 - val_loss: 0.1281 - val_acc: 0.9511\n",
      "Epoch 199/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1562 - acc: 0.9525 - val_loss: 0.1324 - val_acc: 0.9489\n",
      "Epoch 200/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1387 - acc: 0.9555 - val_loss: 0.1310 - val_acc: 0.9511\n",
      "Epoch 201/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1332 - acc: 0.9532 - val_loss: 0.1294 - val_acc: 0.9533\n",
      "Epoch 202/300\n",
      "1347/1347 [==============================] - 0s 50us/step - loss: 0.1543 - acc: 0.9399 - val_loss: 0.1301 - val_acc: 0.9511\n",
      "Epoch 203/300\n",
      "1347/1347 [==============================] - 0s 52us/step - loss: 0.1414 - acc: 0.9495 - val_loss: 0.1287 - val_acc: 0.9556\n",
      "Epoch 204/300\n",
      "1347/1347 [==============================] - 0s 53us/step - loss: 0.1364 - acc: 0.9577 - val_loss: 0.1357 - val_acc: 0.9511\n",
      "Epoch 205/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.1420 - acc: 0.9517 - val_loss: 0.1238 - val_acc: 0.9600\n",
      "Epoch 206/300\n",
      "1347/1347 [==============================] - 0s 43us/step - loss: 0.1437 - acc: 0.9532 - val_loss: 0.1287 - val_acc: 0.9533\n",
      "Epoch 207/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.1419 - acc: 0.9473 - val_loss: 0.1308 - val_acc: 0.9556\n",
      "Epoch 208/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1554 - acc: 0.9495 - val_loss: 0.1254 - val_acc: 0.9578\n",
      "Epoch 209/300\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 0.1331 - acc: 0.9569 - val_loss: 0.1270 - val_acc: 0.9533\n",
      "Epoch 210/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1312 - acc: 0.9577 - val_loss: 0.1305 - val_acc: 0.9533\n",
      "Epoch 211/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1437 - acc: 0.9584 - val_loss: 0.1237 - val_acc: 0.9556\n",
      "Epoch 212/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1635 - acc: 0.9465 - val_loss: 0.1303 - val_acc: 0.9533\n",
      "Epoch 213/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1299 - acc: 0.9592 - val_loss: 0.1277 - val_acc: 0.9556\n",
      "Epoch 214/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1320 - acc: 0.9577 - val_loss: 0.1260 - val_acc: 0.9578\n",
      "Epoch 215/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1381 - acc: 0.9540 - val_loss: 0.1269 - val_acc: 0.9556\n",
      "Epoch 216/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1308 - acc: 0.9525 - val_loss: 0.1259 - val_acc: 0.9556\n",
      "Epoch 217/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.1153 - acc: 0.9651 - val_loss: 0.1283 - val_acc: 0.9533\n",
      "Epoch 218/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1215 - acc: 0.9614 - val_loss: 0.1299 - val_acc: 0.9533\n",
      "Epoch 219/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1230 - acc: 0.9614 - val_loss: 0.1263 - val_acc: 0.9600\n",
      "Epoch 220/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1340 - acc: 0.9540 - val_loss: 0.1274 - val_acc: 0.9600\n",
      "Epoch 221/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1588 - acc: 0.9443 - val_loss: 0.1304 - val_acc: 0.9533\n",
      "Epoch 222/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1502 - acc: 0.9488 - val_loss: 0.1351 - val_acc: 0.9533\n",
      "Epoch 223/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1477 - acc: 0.9458 - val_loss: 0.1296 - val_acc: 0.9533\n",
      "Epoch 224/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1237 - acc: 0.9644 - val_loss: 0.1337 - val_acc: 0.9533\n",
      "Epoch 225/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1417 - acc: 0.9503 - val_loss: 0.1360 - val_acc: 0.9489\n",
      "Epoch 226/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1415 - acc: 0.9503 - val_loss: 0.1308 - val_acc: 0.9511\n",
      "Epoch 227/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1541 - acc: 0.9451 - val_loss: 0.1337 - val_acc: 0.9533\n",
      "Epoch 228/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.1274 - acc: 0.9562 - val_loss: 0.1325 - val_acc: 0.9511\n",
      "Epoch 229/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1291 - acc: 0.9555 - val_loss: 0.1308 - val_acc: 0.9533\n",
      "Epoch 230/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1159 - acc: 0.9577 - val_loss: 0.1301 - val_acc: 0.9556\n",
      "Epoch 231/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1505 - acc: 0.9510 - val_loss: 0.1293 - val_acc: 0.9556\n",
      "Epoch 232/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1465 - acc: 0.9480 - val_loss: 0.1349 - val_acc: 0.9556\n",
      "Epoch 233/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1541 - acc: 0.9473 - val_loss: 0.1291 - val_acc: 0.9533\n",
      "Epoch 234/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1476 - acc: 0.9503 - val_loss: 0.1290 - val_acc: 0.9556\n",
      "Epoch 235/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1356 - acc: 0.9584 - val_loss: 0.1305 - val_acc: 0.9511\n",
      "Epoch 236/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1229 - acc: 0.9599 - val_loss: 0.1289 - val_acc: 0.9556\n",
      "Epoch 237/300\n",
      "1347/1347 [==============================] - 0s 41us/step - loss: 0.1429 - acc: 0.9503 - val_loss: 0.1337 - val_acc: 0.9556\n",
      "Epoch 238/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1361 - acc: 0.9532 - val_loss: 0.1265 - val_acc: 0.9578\n",
      "Epoch 239/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1390 - acc: 0.9495 - val_loss: 0.1325 - val_acc: 0.9489\n",
      "Epoch 240/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1369 - acc: 0.9577 - val_loss: 0.1297 - val_acc: 0.9533\n",
      "Epoch 241/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.1318 - acc: 0.9577 - val_loss: 0.1294 - val_acc: 0.9533\n",
      "Epoch 242/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1178 - acc: 0.9659 - val_loss: 0.1230 - val_acc: 0.9556\n",
      "Epoch 243/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1345 - acc: 0.9488 - val_loss: 0.1269 - val_acc: 0.9533\n",
      "Epoch 244/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1527 - acc: 0.9465 - val_loss: 0.1296 - val_acc: 0.9533\n",
      "Epoch 245/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1243 - acc: 0.9607 - val_loss: 0.1254 - val_acc: 0.9556\n",
      "Epoch 246/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1231 - acc: 0.9592 - val_loss: 0.1243 - val_acc: 0.9556\n",
      "Epoch 247/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1203 - acc: 0.9577 - val_loss: 0.1286 - val_acc: 0.9533\n",
      "Epoch 248/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1312 - acc: 0.9599 - val_loss: 0.1328 - val_acc: 0.9489\n",
      "Epoch 249/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1532 - acc: 0.9488 - val_loss: 0.1308 - val_acc: 0.9511\n",
      "Epoch 250/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1254 - acc: 0.9577 - val_loss: 0.1302 - val_acc: 0.9533\n",
      "Epoch 251/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1144 - acc: 0.9636 - val_loss: 0.1245 - val_acc: 0.9578\n",
      "Epoch 252/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1364 - acc: 0.9532 - val_loss: 0.1273 - val_acc: 0.9556\n",
      "Epoch 253/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1483 - acc: 0.9532 - val_loss: 0.1297 - val_acc: 0.9533\n",
      "Epoch 254/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1282 - acc: 0.9547 - val_loss: 0.1276 - val_acc: 0.9533\n",
      "Epoch 255/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1236 - acc: 0.9562 - val_loss: 0.1264 - val_acc: 0.9556\n",
      "Epoch 256/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1347 - acc: 0.9584 - val_loss: 0.1321 - val_acc: 0.9511\n",
      "Epoch 257/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1420 - acc: 0.9592 - val_loss: 0.1330 - val_acc: 0.9489\n",
      "Epoch 258/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1226 - acc: 0.9621 - val_loss: 0.1296 - val_acc: 0.9511\n",
      "Epoch 259/300\n",
      "1347/1347 [==============================] - 0s 39us/step - loss: 0.1291 - acc: 0.9503 - val_loss: 0.1318 - val_acc: 0.9533\n",
      "Epoch 260/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1342 - acc: 0.9517 - val_loss: 0.1291 - val_acc: 0.9533\n",
      "Epoch 261/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1341 - acc: 0.9577 - val_loss: 0.1258 - val_acc: 0.9556\n",
      "Epoch 262/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1220 - acc: 0.9644 - val_loss: 0.1311 - val_acc: 0.9511\n",
      "Epoch 263/300\n",
      "1347/1347 [==============================] - 0s 44us/step - loss: 0.0996 - acc: 0.9725 - val_loss: 0.1269 - val_acc: 0.9533\n",
      "Epoch 264/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1242 - acc: 0.9584 - val_loss: 0.1289 - val_acc: 0.9578\n",
      "Epoch 265/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1141 - acc: 0.9651 - val_loss: 0.1262 - val_acc: 0.9556\n",
      "Epoch 266/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1179 - acc: 0.9644 - val_loss: 0.1284 - val_acc: 0.9533\n",
      "Epoch 267/300\n",
      "1347/1347 [==============================] - 0s 40us/step - loss: 0.1293 - acc: 0.9614 - val_loss: 0.1298 - val_acc: 0.9556\n",
      "Epoch 268/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1296 - acc: 0.9532 - val_loss: 0.1307 - val_acc: 0.9511\n",
      "Epoch 269/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1454 - acc: 0.9503 - val_loss: 0.1313 - val_acc: 0.9489\n",
      "Epoch 270/300\n",
      "1347/1347 [==============================] - 0s 36us/step - loss: 0.1277 - acc: 0.9651 - val_loss: 0.1335 - val_acc: 0.9511\n",
      "Epoch 271/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1240 - acc: 0.9577 - val_loss: 0.1287 - val_acc: 0.9578\n",
      "Epoch 272/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1408 - acc: 0.9488 - val_loss: 0.1257 - val_acc: 0.9556\n",
      "Epoch 273/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1123 - acc: 0.9666 - val_loss: 0.1318 - val_acc: 0.9489\n",
      "Epoch 274/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1249 - acc: 0.9592 - val_loss: 0.1303 - val_acc: 0.9511\n",
      "Epoch 275/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1190 - acc: 0.9584 - val_loss: 0.1275 - val_acc: 0.9533\n",
      "Epoch 276/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1145 - acc: 0.9659 - val_loss: 0.1294 - val_acc: 0.9533\n",
      "Epoch 277/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1312 - acc: 0.9555 - val_loss: 0.1288 - val_acc: 0.9556\n",
      "Epoch 278/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1219 - acc: 0.9584 - val_loss: 0.1312 - val_acc: 0.9511\n",
      "Epoch 279/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1330 - acc: 0.9525 - val_loss: 0.1277 - val_acc: 0.9533\n",
      "Epoch 280/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1187 - acc: 0.9651 - val_loss: 0.1314 - val_acc: 0.9511\n",
      "Epoch 281/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1246 - acc: 0.9592 - val_loss: 0.1297 - val_acc: 0.9511\n",
      "Epoch 282/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1175 - acc: 0.9659 - val_loss: 0.1297 - val_acc: 0.9511\n",
      "Epoch 283/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1220 - acc: 0.9562 - val_loss: 0.1282 - val_acc: 0.9533\n",
      "Epoch 284/300\n",
      "1347/1347 [==============================] - 0s 37us/step - loss: 0.1188 - acc: 0.9592 - val_loss: 0.1288 - val_acc: 0.9511\n",
      "Epoch 285/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1264 - acc: 0.9621 - val_loss: 0.1296 - val_acc: 0.9533\n",
      "Epoch 286/300\n",
      "1347/1347 [==============================] - 0s 32us/step - loss: 0.1039 - acc: 0.9666 - val_loss: 0.1278 - val_acc: 0.9511\n",
      "Epoch 287/300\n",
      "1347/1347 [==============================] - 0s 31us/step - loss: 0.1147 - acc: 0.9614 - val_loss: 0.1264 - val_acc: 0.9578\n",
      "Epoch 288/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1177 - acc: 0.9577 - val_loss: 0.1288 - val_acc: 0.9556\n",
      "Epoch 289/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1216 - acc: 0.9621 - val_loss: 0.1302 - val_acc: 0.9489\n",
      "Epoch 290/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1165 - acc: 0.9562 - val_loss: 0.1281 - val_acc: 0.9578\n",
      "Epoch 291/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1393 - acc: 0.9473 - val_loss: 0.1298 - val_acc: 0.9600\n",
      "Epoch 292/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1029 - acc: 0.9659 - val_loss: 0.1316 - val_acc: 0.9511\n",
      "Epoch 293/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1105 - acc: 0.9644 - val_loss: 0.1298 - val_acc: 0.9511\n",
      "Epoch 294/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1186 - acc: 0.9614 - val_loss: 0.1314 - val_acc: 0.9600\n",
      "Epoch 295/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1398 - acc: 0.9540 - val_loss: 0.1285 - val_acc: 0.9578\n",
      "Epoch 296/300\n",
      "1347/1347 [==============================] - 0s 34us/step - loss: 0.1318 - acc: 0.9562 - val_loss: 0.1298 - val_acc: 0.9511\n",
      "Epoch 297/300\n",
      "1347/1347 [==============================] - 0s 33us/step - loss: 0.1010 - acc: 0.9696 - val_loss: 0.1316 - val_acc: 0.9578\n",
      "Epoch 298/300\n",
      "1347/1347 [==============================] - 0s 30us/step - loss: 0.1155 - acc: 0.9599 - val_loss: 0.1332 - val_acc: 0.9489\n",
      "Epoch 299/300\n",
      "1347/1347 [==============================] - 0s 35us/step - loss: 0.1264 - acc: 0.9562 - val_loss: 0.1300 - val_acc: 0.9511\n",
      "Epoch 300/300\n",
      "1347/1347 [==============================] - 0s 38us/step - loss: 0.1085 - acc: 0.9614 - val_loss: 0.1271 - val_acc: 0.9533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5d55faecc0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "\n",
    "inputs = Input(shape=(n_features,))\n",
    "h_layer = Dense(n_hidden, activation=activation)(inputs)\n",
    "h_layer_dropout = Dropout(0.1)(h_layer)\n",
    "y_layer = Dense(n_classes, activation='softmax')(h_layer_dropout)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=y_layer)\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(\n",
    "    X_tr, to_categorical(Y_tr, num_classes=n_classes), \n",
    "    epochs=300, verbose=1, batch_size=32,\n",
    "    validation_data=(X_val, to_categorical(Y_val, num_classes=n_classes))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
