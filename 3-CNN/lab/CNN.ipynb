{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.5.2"}}, "nbformat": 4, "nbformat_minor": 0, "cells": [{"cell_type": "code", "execution_count": 1, "metadata": {"collapsed": false}, "outputs": [], "source": ["import os\n", "from os import listdir, path\n", "from zipfile import ZipFile\n", "import random\n", "import numpy as np\n", "import keras\n", "from keras.preprocessing.image import img_to_array, load_img\n", "import tensorflow as tf\n", "import matplotlib.pyplot as plt\n", "from skimage.io import imread\n", "from sklearn.preprocessing import MinMaxScaler\n", "from sklearn.model_selection import train_test_split"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Unzip the data on disk"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"collapsed": false}, "outputs": [], "source": ["if not path.exists('data/cat/'):\n", "    print('Extracting cat image files...')\n", "    zf = ZipFile('data/cat.zip')\n", "    zf.extractall('data/')\n", "if not path.exists('data/dog/'):\n", "    print('Extracting dog image files...')\n", "    zf = ZipFile('data/dog.zip')\n", "    zf.extractall('data/')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["###\u00a0Display utility functions"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"collapsed": false}, "outputs": [], "source": ["def show(image):\n", "    plt.imshow(np.squeeze(image.astype(\"uint8\")), cmap=\"gray\")\n", "    plt.title(\"image shape: \"+ str(image.shape), fontsize=14)\n", "    plt.axis('off');\n", "    \n", "def show_multiple(images, figsize):\n", "    fig, ax = plt.subplots(ncols=len(images), figsize=figsize)\n", "    for col, image in zip(ax, images):\n", "        col.imshow(np.squeeze(image.astype(\"uint8\")), cmap=\"gray\")\n", "        col.set_title(\"image shape: \"+ str(image.shape), fontsize=14)\n", "    plt.tight_layout()\n", "    plt.axis('off');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# I - Introduction to Tensorflow and convolution filters"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"../images/standard_vs_depthwise_conv.png\" style=\"width: 850px;\"/>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**- Left: standard convolution, the whole kernel is parsing the input tensor for each output channel dimension**\n", "\n", "**- Right: depthwise convolution, each slide of the kernel is parsing each input dimension. The result is constructed afterward using a concatenation of the feature maps. That is particularly useful to retrieve a valid RGB image**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Sample image example"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"collapsed": false}, "outputs": [], "source": ["sample_image = imread(\"data/panda.jpg\")"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"collapsed": false}, "outputs": [], "source": ["show(sample_image)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### I - A) Simple box blur kernel"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Input placeholders\n", "\n", "- The placeholder is a variable that doesn't have a value yet in the symbolic graph. The value will be fed when running the session by passing the `feed_dict` argument\n", "- If the placeholder is a k-dimensional tensor, we need to specify its shape. \n", "- It is possible to leave the shape variable by putting `None` values in the shape\n", "\n", "#### 2d convolution with tensorflow:\n", "- https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d\n", "- https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"collapsed": false}, "outputs": [], "source": ["def conv_2d(x, k, strides, padding, conv_type):\n", "    if conv_type == 'depthwise':\n", "        return tf.nn.depthwise_conv2d(\n", "            x, k, strides=strides, padding=padding\n", "        )\n", "    elif conv_type == 'standard':\n", "        return tf.nn.conv2d(\n", "            x, k, strides=strides, padding=padding\n", "        )   "]}, {"cell_type": "code", "execution_count": 7, "metadata": {"collapsed": false}, "outputs": [], "source": ["def visualize_kernel(kernel):\n", "    # move the channel dimension to the first one\n", "    # this way, it is easier to see the spacial organization of the kernel with print\n", "    print(np.transpose(kernel, (2, 0, 1)))"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"collapsed": false}, "outputs": [], "source": ["kernel_data = np.ones(shape=(5, 5, 3)).astype(np.float32)\n", "kernel_data /= kernel_data.sum(0).sum(0)\n", "visualize_kernel(kernel_data)"]}, {"cell_type": "code", "execution_count": 9, "metadata": {"collapsed": false}, "outputs": [], "source": ["image = tf.placeholder(tf.float32, shape=(None, None, None, 3)) # [batch, height, width, channels]\n", "kernel = tf.placeholder(tf.float32, shape=(5, 5, 3, 1)) # [filter_height, filter_width, in_channels, out_channels]\n", "\n", "output_image = conv_2d(image, kernel, strides=(1, 1, 1, 1), padding='SAME', conv_type='depthwise')\n", "\n", "with tf.Session() as sess:\n", "    image_batch_expanded = np.expand_dims(sample_image, axis=0)\n", "    kernel_data_expanded = np.expand_dims(kernel_data, axis=-1)\n", "    print('Kernel shape: %s' % str(kernel_data_expanded.shape))\n", "    feed_dict = {image: image_batch_expanded, kernel: kernel_data_expanded}\n", "    feature_map = sess.run(output_image, feed_dict=feed_dict)\n", "    show(feature_map)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Questions**\n", "- Explain what happened here: what transformation has been applied to the image?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### I - B) Identity kernel"]}, {"cell_type": "code", "execution_count": 10, "metadata": {"collapsed": false}, "outputs": [], "source": ["kernel_data = np.zeros(shape=(3, 3, 3)).astype(np.float32)\n", "kernel_data[1, 1, :] = 1\n", "visualize_kernel(kernel_data)"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"collapsed": false}, "outputs": [], "source": ["image = tf.placeholder(tf.float32, shape=(None, None, None, 3))\n", "kernel = tf.placeholder(tf.float32, shape=(3, 3, 3, 1))\n", "\n", "output_same_padding = conv_2d(image, kernel, strides=(1, 1, 1, 1), \n", "                              padding='SAME', conv_type='depthwise')\n", "output_valid_padding = conv_2d(image, kernel, strides=(1, 1, 1, 1), \n", "                               padding='VALID', conv_type='depthwise')\n", "output_larger_strides = conv_2d(image, kernel, strides=(1, 10, 10, 1), \n", "                                padding='SAME', conv_type='depthwise')\n", "\n", "with tf.Session() as sess:\n", "    image_batch_expanded = np.expand_dims(sample_image, axis=0)\n", "    kernel_data_expanded = np.expand_dims(kernel_data, axis=-1)\n", "    feed_dict = {image: image_batch_expanded, kernel: kernel_data_expanded}\n", "    feature_map_same_padding, feature_map_valid_padding, feature_map_larger_strides = sess.run(\n", "            [output_same_padding, output_valid_padding, output_larger_strides], \n", "            feed_dict=feed_dict\n", "    )\n", "    show_multiple([\n", "        feature_map_same_padding, \n", "        feature_map_valid_padding, \n", "        feature_map_larger_strides\n", "    ], figsize=(16, 12))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Questions**\n", "- Try to modify the strides and the type of padding. What are the effects on the final output?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### I - C) Line detection kernel on greyscale transformed image"]}, {"cell_type": "code", "execution_count": 12, "metadata": {"collapsed": false}, "outputs": [], "source": ["grey_sample_image = np.expand_dims(sample_image.sum(axis=2) / 3., axis=-1)\n", "show(grey_sample_image)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercice**\n", "\n", "Try to implement a kernel that does line or edge detection:\n", "- https://en.wikipedia.org/wiki/Kernel_(image_processing)\n", "- https://en.wikipedia.org/wiki/Sobel_operator"]}, {"cell_type": "code", "execution_count": 13, "metadata": {"collapsed": false}, "outputs": [], "source": ["#\u00a0Implement a 3x3 edge detection kernel\n", "line_detection_kernel = np.asarray(\n", "    [\n", "        # TODO:\n", "        [0., 0., 0.],\n", "        [0., 0., 0.],\n", "        [0., 0., 0.]\n", "    ]\n", ")\n", "\n", "kernel_data = np.expand_dims(line_detection_kernel, axis=-1)\n", "visualize_kernel(kernel_data)"]}, {"cell_type": "code", "execution_count": 14, "metadata": {"collapsed": false}, "outputs": [], "source": ["image = tf.placeholder(tf.float32, shape=(None, None, None, 1))\n", "kernel = tf.placeholder(tf.float32, shape=(3, 3, 1, 1))\n", "\n", "output_line_detection = conv_2d(image, kernel, strides=(1, 1, 1, 1), \n", "                                padding='SAME', conv_type='standard')\n", "\n", "with tf.Session() as sess:\n", "    image_batch = np.expand_dims(grey_sample_image, axis=0)\n", "    kernel_data = np.expand_dims(kernel_data, axis=-1)\n", "    feed_dict = {image: image_batch, kernel: kernel_data}\n", "    feature_map = sess.run(output_line_detection, feed_dict=feed_dict)\n", "    show(feature_map)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### I - D) Max and average pooling"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercice**\n", "\n", "Now define a Max Pooling and an Average Pooling operations on our image.<br/>\n", "Then apply it using a tf.Session\n", "- https://www.tensorflow.org/api_docs/python/tf/nn/max_pool\n", "- https://www.tensorflow.org/api_docs/python/tf/nn/avg_pool\n", "\n", "Again, try to make the `ksize` and `strides` parameters fluctuate"]}, {"cell_type": "code", "execution_count": 15, "metadata": {"collapsed": false}, "outputs": [], "source": ["image = tf.placeholder(tf.float32, [None, None, None, 3])\n", "# TODO:\n", "output_max_pool = None\n", "output_avg_pool = None\n", "\n", "with tf.Session() as sess:\n", "    feed_dict={image:[sample_image], kernel: kernel_data}\n", "    # TODO:\n", "    feature_map_max_pool, feature_map_avg_pool = None, None\n", "    # TODO:\n", "    show_multiple([sample_image, sample_image], figsize=(8, 6))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# II - Training a ConvNet with Keras"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Utility functions"]}, {"cell_type": "code", "execution_count": 16, "metadata": {"collapsed": false}, "outputs": [], "source": ["def get_splitted_data_with_size(image_size, sample_size, test_ratio, classes, seed):\n", "    X, Y = [], []\n", "    for label, animal in enumerate(classes):\n", "        files = listdir(path.join('data', animal))\n", "        random.shuffle(files)\n", "        files = files[:(sample_size // len(classes))]\n", "        for file in files:\n", "            img = load_img(path.join('data', animal, file), \n", "                           target_size=image_size)\n", "            X.append(img_to_array(img))\n", "            Y.append(label)\n", "    return train_test_split(np.asarray(X), np.asarray(Y), test_size=test_ratio, random_state=seed)"]}, {"cell_type": "code", "execution_count": 17, "metadata": {"collapsed": false}, "outputs": [], "source": ["def plot_model_history(model_history):\n", "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n", "    for ax, metric, name in zip(axs, ['acc', 'loss'], ['Accuracy', 'Loss']):\n", "        ax.plot(\n", "            range(1, len(model_history.history[metric]) + 1), \n", "            model_history.history[metric]\n", "        )\n", "        ax.plot(\n", "            range(1, len(model_history.history['val_' + metric]) + 1), \n", "            model_history.history['val_' + metric]\n", "        )\n", "        ax.set_title('Model ' + name)\n", "        ax.set_ylabel(name)\n", "        ax.set_xlabel('Epoch')\n", "        ax.legend(['train', 'val'], loc='best')\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": 18, "metadata": {"collapsed": false}, "outputs": [], "source": ["def scale_data(X_tr, X_val, return_scaler=False):\n", "    shape_tr, shape_val = X_tr.shape, X_val.shape\n", "    X_tr_flat = np.ravel(X_tr).reshape(-1, 1)\n", "    X_val_flat = np.ravel(X_val).reshape(-1, 1)\n", "    min_max_scaler = MinMaxScaler()\n", "    X_tr_scaled = min_max_scaler.fit_transform(X_tr_flat).reshape(shape_tr)\n", "    X_val_scaled = min_max_scaler.transform(X_val_flat).reshape(shape_val)\n", "    if not return_scaler:\n", "        return X_tr_scaled, X_val_scaled\n", "    else:\n", "        return X_tr_scaled, X_val_scaled, min_max_scaler\n", "    \n", "def apply_scaling(X, scaler):\n", "    shape_X = X.shape\n", "    X_flat = np.ravel(X).reshape(-1, 1)\n", "    X_scaled = scaler.transform(X_flat).reshape(shape_X)\n", "    return X_scaled"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### II - A)\u00a0Load, resize and scale the data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It is advised to fix a relatively small image_size, for instance (32, 32, 3), to avoid suffering from slow calculation"]}, {"cell_type": "code", "execution_count": 19, "metadata": {"collapsed": false}, "outputs": [], "source": ["image_size = (32, 32, 3)\n", "sample_size = 10000\n", "\n", "classes = ['cat', 'dog']\n", "X_tr, X_val, Y_tr, Y_val = get_splitted_data_with_size(\n", "    image_size=image_size, sample_size=sample_size, test_ratio=0.25, classes=classes, seed=42\n", ")"]}, {"cell_type": "code", "execution_count": 20, "metadata": {"collapsed": false}, "outputs": [], "source": ["X_tr.shape, X_val.shape, Y_tr.shape, Y_val.shape"]}, {"cell_type": "code", "execution_count": 21, "metadata": {"collapsed": false}, "outputs": [], "source": ["i = np.random.choice(len(X_tr))\n", "show(X_tr[i])\n", "print('True label: {0}'.format(classes[Y_tr[i]]))"]}, {"cell_type": "code", "execution_count": 22, "metadata": {"collapsed": false}, "outputs": [], "source": ["X_tr_scaled, X_val_scaled, scaler = scale_data(X_tr, X_val, return_scaler=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### II - B)\u00a0Design and train a ConvNet from scratch"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercice**\n", "\n", "- Implement a Convolutional Network using the Keras Sequential API\n", "- Typically, you would use series of convolutional blocs: \n", "\n", "`\n", "model.add(Conv2D(output_filter, (kernel_height, kernel_width), padding, input_shape=(input_height, input_width, input_filter)))\n", "model.add(Activation(activation))\n", "model.add(BatchNormalization())\n", "`\n", "- Usually, the output_filter size grows accross the network\n", "- End the network with a `Flatten` layer followed by a final `Dense` layer\n", "- Be careful with the shapes accross the network, the activation functions used, the optimizer, and the loss function\n", "- Don't forget to use Dropout layers to avoid overfitting issues"]}, {"cell_type": "code", "execution_count": 23, "metadata": {"collapsed": false}, "outputs": [], "source": ["from keras.models import Sequential\n", "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n", "from keras.layers import Conv2D, MaxPooling2D\n", "from keras.optimizers import Adam\n", "\n", "\"\"\"\n", "Return a compiled Keras model\n", "\"\"\"\n", "def design_and_compile_model():\n", "    model = Sequential()\n", "    # TODO:\n", "\n", "    # TODO:\n", "    return None\n", "\n", "    # Compiling the model adds a loss function, optimiser and metrics to track during training\n", "    model.compile(\n", "        optimizer=None,\n", "        loss=None,\n", "        metrics=None\n", "    )\n", "    \n", "    # TODO:"]}, {"cell_type": "code", "execution_count": 24, "metadata": {"collapsed": false}, "outputs": [], "source": ["design_and_compile_model().summary() if design_and_compile_model() else None"]}, {"cell_type": "code", "execution_count": 25, "metadata": {"collapsed": false}, "outputs": [], "source": ["batch_size = 128\n", "num_epochs = 20  # The number of epochs (full passes through the data) to train for\n", "\n", "model = design_and_compile_model()\n", "\n", "# The fit function allows you to fit the compiled model to some training data\n", "if model:\n", "    model_history = model.fit(\n", "        x=X_tr_scaled, \n", "        y=Y_tr, \n", "        batch_size=batch_size, \n", "        epochs=num_epochs,\n", "        verbose=1,\n", "        validation_data=(X_val_scaled, Y_val)\n", "    )\n", "    print('Training complete')\n", "else:\n", "    model_history = None"]}, {"cell_type": "code", "execution_count": 26, "metadata": {"collapsed": false}, "outputs": [], "source": ["plot_model_history(model_history) if model_history else None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### II - C)\u00a0Improve it using data augmentation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercice**\n", "\n", "- Try to improve the effectiveness of your network using some Data Augmentation\n", "- Basically, it consists in building a `ImageDataGenerator` fitted on your training dataset\n", "- Then you will be able to generate new consistent samples, and refit your model using the `fit_generator` Keras method"]}, {"cell_type": "code", "execution_count": 27, "metadata": {"collapsed": false}, "outputs": [], "source": ["from keras.preprocessing.image import ImageDataGenerator\n", "\n", "# Instantiate a ImageDataGenerator object with the right parameters and then fit it on your training dataset\n", "# TODO:"]}, {"cell_type": "code", "execution_count": 28, "metadata": {"collapsed": false}, "outputs": [], "source": ["model = design_and_compile_model()\n", "# Fit your model with model.fit_generator() and feed it with data_generator.flow()\n", "# TODO:"]}, {"cell_type": "code", "execution_count": 29, "metadata": {"collapsed": false}, "outputs": [], "source": ["plot_model_history(model_history) if model_history else None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# III - Transfer learning\n", "\n", "Objectives:\n", "- Classify an image by loading a pre-trained ResNet50 model using Keras Zoo\n", "    - No training required\n", "    - Decode an ImageNet prediction\n", "- Build a headless model and compute representations of images \n", "    - Retrain a model from representations of images for your own classification task: here cat vs dog dataset"]}, {"cell_type": "code", "execution_count": 30, "metadata": {"collapsed": false}, "outputs": [], "source": ["cat_sample_path = \"data/cat/cat_1.jpg\"\n", "dog_sample_path = \"data/dog/dog_1.jpg\"\n", "resnet_input_size = (224, 224)"]}, {"cell_type": "code", "execution_count": 31, "metadata": {"collapsed": false}, "outputs": [], "source": ["from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n", "from keras.models import Model\n", "from skimage.transform import resize\n", "\n", "model_ResNet50 = ResNet50(include_top=True, weights='imagenet')\n", "model_ResNet50.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that there are way more trainable parameters than before"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### III - A) Classify of an image using pre-trained weights\n", "\n", "**Exercise**\n", "- Open an image, preprocess it and build a batch of 1 image\n", "- Use the model to classify this image\n", "- Decode the predictions using `decode_predictions` from Keras\n", "\n", "Notes:\n", "- You may use `preprocess_input` for preprocessing the image. \n", "- Test your code with `\"data/cat/cat_1.jpg\"` \n", "- ResNet has been trained on (width, height) images of (224,224) and range of pixel intensities in `[0, 255]`.\n", "    - [skimage.transform.resize](http://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.resize) has a `preserve_range` keyword useful in that matter "]}, {"cell_type": "code", "execution_count": 32, "metadata": {"collapsed": false}, "outputs": [], "source": ["img = imread(cat_sample_path)\n", "img_resized = resize(img, resnet_input_size, mode='reflect', preserve_range=True)\n", "show(img_resized)\n", "\n", "# Use preprocess_input() to apply the same preprocessing as ResNet, \n", "# get the prediction from the loaded model, and then decode the predictions\n", "\n", "# TODO:\n", "decoded_predictions = None\n", "\n", "if decoded_predictions:\n", "    for _, name, score in decoded_predictions:\n", "        print(name, score)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### III - B) Build a headless model and compute representations of images"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Model has $177$ layers\n", "- See where we should stop to have the extracted feature and start building a new classficlation model from here"]}, {"cell_type": "code", "execution_count": 33, "metadata": {"collapsed": false}, "outputs": [], "source": ["print(len(model_ResNet50.layers))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Let's remove the last dense classification layer that is specific \n", "to the image net classes and use the previous layers (after flattening) as a feature extractors\n", "- Use ResNet input layer and last layer of extracted features to build a feature extractor model\n", "    - Use Keras functional API"]}, {"cell_type": "code", "execution_count": 34, "metadata": {"collapsed": false}, "outputs": [], "source": ["# Create a truncated Model using ResNet50.input and the before last layer\n", "\n", "# TODO:\n", "feat_extractor_model = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When using this model we need to be careful to apply the same image processing as was used during the training, otherwise the marginal distribution of the input pixels might not be on the right scale:"]}, {"cell_type": "code", "execution_count": 35, "metadata": {"collapsed": false}, "outputs": [], "source": ["def preprocess_resnet(x, size):\n", "    x = resize(x, size, mode='reflect', preserve_range=True)\n", "    x = np.expand_dims(x, axis=0)\n", "    if x.ndim == 3:\n", "        x = np.expand_dims(x, axis=0)\n", "    return preprocess_input(x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This model extracts high level concepts from any image that has been preprocessed like the images ResNet trained on.\n", "The model transforms a preprocessed (224, 224) RGB image into a long vector of activations.\n", "Each activation refers to some concept statistically connected to a bunch of different classes."]}, {"cell_type": "code", "execution_count": 36, "metadata": {"collapsed": false}, "outputs": [], "source": ["cat_img = imread(cat_sample_path)\n", "cat_img_processed = preprocess_resnet(cat_img, resnet_input_size)\n", "if feat_extractor_model:\n", "    cat_representation = feat_extractor_model.predict(cat_img_processed)\n", "    print(\"Cat deep representation shape: (%d, %d)\" % cat_representation.shape)\n", "    for activation in np.ravel(cat_representation):\n", "        print(activation)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Questions**\n", "- What is the number of $0$s in the cat representation vector ?\n", "- Can you find any negative values?\n", "- Why are there $0$ values ? What does it mean ?\n", "- Extract ResNet representations of other dogs and cats. Are the zeros at the same places in vector ?\n", "    - Explain why or give an intuition of it"]}, {"cell_type": "code", "execution_count": 37, "metadata": {"collapsed": false}, "outputs": [], "source": ["if feat_extractor_model:\n", "    plt.hist(np.where(cat_representation == 0)[1])\n", "    plt.title(\"cat zeros positions\")\n", "    plt.show()\n", "\n", "    dog_img = imread(dog_sample_path)\n", "    dog_img_processed = preprocess_resnet(dog_img, resnet_input_size)\n", "    dog_representation = feat_extractor_model.predict(dog_img_processed)\n", "\n", "    plt.hist(np.where(dog_representation == 0)[1])\n", "    plt.title(\"dog zeros positions\")\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### III - C) Retrain a model from computed representations of images\n", "\n", "For this session we are going to use the dataset of the dogs-vs-cats we already used in part $2$."]}, {"cell_type": "code", "execution_count": 38, "metadata": {"collapsed": false}, "outputs": [], "source": ["classes = ['cat', 'dog']\n", "X_tr, X_val, Y_tr, Y_val = get_splitted_data_with_size(\n", "    image_size=(224, 224, 3), sample_size=2000, test_ratio=0.25, classes=classes, seed=42\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Questions**\n", "- Inference time takes a long time only for $2000$ images\n", "    - Explain why it would be much faster using a GPU"]}, {"cell_type": "code", "execution_count": 39, "metadata": {"collapsed": false}, "outputs": [], "source": ["if feat_extractor_model:\n", "    X_extracted_tr = feat_extractor_model.predict(preprocess_input(X_tr), verbose=1)\n", "    X_extracted_val = feat_extractor_model.predict(preprocess_input(X_val), verbose=1)\n", "    print('Done extracting resnet50 features..')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Define a classification model fed with the newly created X and Y\n", "    - Remember that X is now a set of ResNet representations of the images\n", "- Use either functional of sequential Keras apis\n", "- Display training and validation accuracies"]}, {"cell_type": "code", "execution_count": 40, "metadata": {"collapsed": false}, "outputs": [], "source": ["from keras.models import Sequential\n", "from keras.layers import Dense, Dropout\n", "from keras.optimizers import Adam\n", "\n", "# TODO:\n", "transfer_model = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Questions**\n", "- How high in validation accuracy did you get ? \n", "- Compare to your previous classification model in part 2. Does it perform worse ? Better ? Why ?\n", "- Did you observe overfitting during training ? Why ?\n", "    - If yes, what did you do to avoid it ?"]}]}