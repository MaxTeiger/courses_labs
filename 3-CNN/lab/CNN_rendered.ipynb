{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.5.2"}}, "nbformat": 4, "nbformat_minor": 0, "cells": [{"cell_type": "code", "execution_count": 1, "metadata": {"collapsed": false}, "outputs": [], "source": ["import os\n", "from os import listdir, path\n", "from zipfile import ZipFile\n", "import random\n", "import numpy as np\n", "import keras\n", "from keras.preprocessing.image import img_to_array, load_img\n", "import tensorflow as tf\n", "import matplotlib.pyplot as plt\n", "from skimage.io import imread\n", "from sklearn.preprocessing import MinMaxScaler\n", "from sklearn.model_selection import train_test_split"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Unzip the data on disk"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"collapsed": false}, "outputs": [], "source": ["if not path.exists('data/cat/'):\n", "    print('Extracting cat image files...')\n", "    zf = ZipFile('data/cat.zip')\n", "    zf.extractall('data/')\n", "if not path.exists('data/dog/'):\n", "    print('Extracting dog image files...')\n", "    zf = ZipFile('data/dog.zip')\n", "    zf.extractall('data/')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["###\u00a0Display utility functions"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"collapsed": false}, "outputs": [], "source": ["def show(image):\n", "    plt.imshow(np.squeeze(image.astype(\"uint8\")), cmap=\"gray\")\n", "    plt.title(\"image shape: \"+ str(image.shape), fontsize=14)\n", "    plt.axis('off');\n", "    \n", "def show_multiple(images, figsize):\n", "    fig, ax = plt.subplots(ncols=len(images), figsize=figsize)\n", "    for col, image in zip(ax, images):\n", "        col.imshow(np.squeeze(image.astype(\"uint8\")), cmap=\"gray\")\n", "        col.set_title(\"image shape: \"+ str(image.shape), fontsize=14)\n", "    plt.tight_layout()\n", "    plt.axis('off');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# I - Introduction to Tensorflow and convolution filters"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"../images/standard_vs_depthwise_conv.png\" style=\"width: 850px;\"/>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**- Left: standard convolution, the whole kernel is parsing the input tensor for each output channel dimension**\n", "\n", "**- Right: depthwise convolution, each slide of the kernel is parsing each input dimension. The result is constructed afterward using a concatenation of the feature maps. That is particularly useful to retrieve a valid RGB image**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Sample image example"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"collapsed": false}, "outputs": [], "source": ["sample_image = imread(\"data/panda.jpg\")"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"collapsed": false}, "outputs": [], "source": ["show(sample_image)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### I - A) Simple box blur kernel"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Input placeholders\n", "\n", "- The placeholder is a variable that doesn't have a value yet in the symbolic graph. The value will be fed when running the session by passing the `feed_dict` argument\n", "- If the placeholder is a k-dimensional tensor, we need to specify its shape. \n", "- It is possible to leave the shape variable by putting `None` values in the shape\n", "\n", "#### 2d convolution with tensorflow:\n", "- https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d\n", "- https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"collapsed": false}, "outputs": [], "source": ["def conv_2d(x, k, strides, padding, conv_type):\n", "    if conv_type == 'depthwise':\n", "        return tf.nn.depthwise_conv2d(\n", "            x, k, strides=strides, padding=padding\n", "        )\n", "    elif conv_type == 'standard':\n", "        return tf.nn.conv2d(\n", "            x, k, strides=strides, padding=padding\n", "        )   "]}, {"cell_type": "code", "execution_count": 7, "metadata": {"collapsed": false}, "outputs": [], "source": ["def visualize_kernel(kernel):\n", "    # move the channel dimension to the first one\n", "    # this way, it is easier to see the spacial organization of the kernel with print\n", "    print(np.transpose(kernel, (2, 0, 1)))"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"collapsed": false}, "outputs": [], "source": ["kernel_data = np.ones(shape=(5, 5, 3)).astype(np.float32)\n", "kernel_data /= kernel_data.sum(0).sum(0)\n", "visualize_kernel(kernel_data)"]}, {"cell_type": "code", "execution_count": 9, "metadata": {"collapsed": false}, "outputs": [], "source": ["image = tf.placeholder(tf.float32, shape=(None, None, None, 3)) # [batch, height, width, channels]\n", "kernel = tf.placeholder(tf.float32, shape=(5, 5, 3, 1)) # [filter_height, filter_width, in_channels, out_channels]\n", "\n", "output_image = conv_2d(image, kernel, strides=(1, 1, 1, 1), padding='SAME', conv_type='depthwise')\n", "\n", "with tf.Session() as sess:\n", "    image_batch_expanded = np.expand_dims(sample_image, axis=0)\n", "    kernel_data_expanded = np.expand_dims(kernel_data, axis=-1)\n", "    print('Kernel shape: %s' % str(kernel_data_expanded.shape))\n", "    feed_dict = {image: image_batch_expanded, kernel: kernel_data_expanded}\n", "    feature_map = sess.run(output_image, feed_dict=feed_dict)\n", "    show(feature_map)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Questions**\n", "- Explain what happened here: what transformation has been applied to the image?\n", "\n", "<span style=\"color:green\">\n", "Each kernel weight has the value $\\frac{1}{height \\cdot width}$.<br/>\n", "Applying a 2D convolution with it will have an averaging effect: each value in the outputted feature map will be the average of the neighboring pixels in the input image.<br/>\n", "This transformation in particular is called a box blur filter.\n", "</span>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### I - B) Identity kernel"]}, {"cell_type": "code", "execution_count": 10, "metadata": {"collapsed": false}, "outputs": [], "source": ["kernel_data = np.zeros(shape=(3, 3, 3)).astype(np.float32)\n", "kernel_data[1, 1, :] = 1\n", "visualize_kernel(kernel_data)"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"collapsed": false}, "outputs": [], "source": ["image = tf.placeholder(tf.float32, shape=(None, None, None, 3))\n", "kernel = tf.placeholder(tf.float32, shape=(3, 3, 3, 1))\n", "\n", "output_same_padding = conv_2d(image, kernel, strides=(1, 1, 1, 1), \n", "                              padding='SAME', conv_type='depthwise')\n", "output_valid_padding = conv_2d(image, kernel, strides=(1, 1, 1, 1), \n", "                               padding='VALID', conv_type='depthwise')\n", "output_larger_strides = conv_2d(image, kernel, strides=(1, 10, 10, 1), \n", "                                padding='SAME', conv_type='depthwise')\n", "\n", "with tf.Session() as sess:\n", "    image_batch_expanded = np.expand_dims(sample_image, axis=0)\n", "    kernel_data_expanded = np.expand_dims(kernel_data, axis=-1)\n", "    feed_dict = {image: image_batch_expanded, kernel: kernel_data_expanded}\n", "    feature_map_same_padding, feature_map_valid_padding, feature_map_larger_strides = sess.run(\n", "            [output_same_padding, output_valid_padding, output_larger_strides], \n", "            feed_dict=feed_dict\n", "    )\n", "    show_multiple([\n", "        feature_map_same_padding, \n", "        feature_map_valid_padding, \n", "        feature_map_larger_strides\n", "    ], figsize=(16, 12))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Questions**\n", "- Try to modify the strides and the padding type. What are the effects on the final output?\n", "\n", "<ul>\n", "<li style=\"color:green\">\n", "Stride is the number of pixels with which we slide our filter, horizontally or vertically, while applying our convolution products. Increasing it has for main effect to decrease the final output size: in our case, having a $(10, 10)$ slide divides the final image shape by a factor of $10$ horizontally and $10$ vertically.<br/>\n", "<li style=\"color:green\">In addition, we can choose different padding types.<br/> \n", "The parameter 'SAME' results in an identical output shape for a minimal stride size, with $0$ values virtually added to the edges of the image proportionally to the kernel size.<br/> \n", "On the other hand, the parameter 'VALID' will lead to convolution operations being performed only if the kernel lies entirely over the image, contrary to the case with padding.\n", "For instance, a convolution layer with (1,1) stride and this parameter will lead the final output size to have $K - 1$ fewer elements along both dimensions.\n", "</ul>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### I - C) Line detection kernel on greyscale transformed image"]}, {"cell_type": "code", "execution_count": 12, "metadata": {"collapsed": false}, "outputs": [], "source": ["grey_sample_image = np.expand_dims(sample_image.sum(axis=2) / 3., axis=-1)\n", "show(grey_sample_image)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercice**\n", "\n", "Try to implement a kernel that does line or edge detection:\n", "- https://en.wikipedia.org/wiki/Kernel_(image_processing)\n", "- https://en.wikipedia.org/wiki/Sobel_operator"]}, {"cell_type": "code", "execution_count": 13, "metadata": {"collapsed": false}, "outputs": [], "source": ["line_detection_kernel = np.asarray(\n", "    [\n", "        [1, 0., -1],\n", "        [2, 0., -2],\n", "        [1, 0., -1]\n", "    ]\n", ")\n", "\n", "kernel_data = np.expand_dims(line_detection_kernel, axis=-1)\n", "visualize_kernel(kernel_data)"]}, {"cell_type": "code", "execution_count": 14, "metadata": {"collapsed": false}, "outputs": [], "source": ["image = tf.placeholder(tf.float32, shape=(None, None, None, 1))\n", "kernel = tf.placeholder(tf.float32, shape=(3, 3, 1, 1))\n", "\n", "output_line_detection = conv_2d(image, kernel, strides=(1, 1, 1, 1), \n", "                                padding='SAME', conv_type='standard')\n", "\n", "with tf.Session() as sess:\n", "    image_batch = np.expand_dims(grey_sample_image, axis=0)\n", "    kernel_data = np.expand_dims(kernel_data, axis=-1)\n", "    feed_dict = {image: image_batch, kernel: kernel_data}\n", "    feature_map = sess.run(output_line_detection, feed_dict=feed_dict)\n", "    show(feature_map)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### I - D) Max and average pooling"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercice**\n", "\n", "Now define a Max Pooling and an Average Pooling operations on our image.<br/>\n", "Then apply it using a tf.Session\n", "- https://www.tensorflow.org/api_docs/python/tf/nn/max_pool\n", "- https://www.tensorflow.org/api_docs/python/tf/nn/avg_pool\n", "\n", "Again, try to make fluctuate the `ksize` and `strides` parameters"]}, {"cell_type": "code", "execution_count": 15, "metadata": {"collapsed": false}, "outputs": [], "source": ["image = tf.placeholder(tf.float32, [None, None, None, 3])\n", "output_max_pool = tf.nn.max_pool(\n", "    image, ksize=[1, 20, 20, 1], strides=[1, 2, 2, 1], padding='SAME'\n", ")\n", "output_avg_pool = tf.nn.avg_pool(\n", "    image, ksize=[1, 20, 20, 1], strides=[1, 2, 2, 1], padding='SAME'\n", ")\n", "\n", "with tf.Session() as sess:\n", "    feed_dict={image:[sample_image], kernel: kernel_data}\n", "    feature_map_max_pool, feature_map_avg_pool = sess.run(\n", "        [output_max_pool, output_avg_pool], feed_dict=feed_dict\n", "    )\n", "    show_multiple([feature_map_max_pool, feature_map_avg_pool], figsize=(8, 6))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# II - Training a ConvNet with Keras"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Utility functions"]}, {"cell_type": "code", "execution_count": 16, "metadata": {"collapsed": false}, "outputs": [], "source": ["def get_splitted_data_with_size(image_size, sample_size, test_ratio, classes, seed):\n", "    X, Y = [], []\n", "    for label, animal in enumerate(classes):\n", "        files = listdir(path.join('data', animal))\n", "        random.shuffle(files)\n", "        files = files[:(sample_size // len(classes))]\n", "        for file in files:\n", "            img = load_img(path.join('data', animal, file), \n", "                           target_size=image_size)\n", "            X.append(img_to_array(img))\n", "            Y.append(label)\n", "    return train_test_split(np.asarray(X), np.asarray(Y), test_size=test_ratio, random_state=seed)"]}, {"cell_type": "code", "execution_count": 17, "metadata": {"collapsed": false}, "outputs": [], "source": ["def plot_model_history(model_history):\n", "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n", "    for ax, metric, name in zip(axs, ['acc', 'loss'], ['Accuracy', 'Loss']):\n", "        ax.plot(\n", "            range(1, len(model_history.history[metric]) + 1), \n", "            model_history.history[metric]\n", "        )\n", "        ax.plot(\n", "            range(1, len(model_history.history['val_' + metric]) + 1), \n", "            model_history.history['val_' + metric]\n", "        )\n", "        ax.set_title('Model ' + name)\n", "        ax.set_ylabel(name)\n", "        ax.set_xlabel('Epoch')\n", "        ax.legend(['train', 'val'], loc='best')\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": 18, "metadata": {"collapsed": false}, "outputs": [], "source": ["def scale_data(X_tr, X_val, return_scaler=False):\n", "    shape_tr, shape_val = X_tr.shape, X_val.shape\n", "    X_tr_flat = np.ravel(X_tr).reshape(-1, 1)\n", "    X_val_flat = np.ravel(X_val).reshape(-1, 1)\n", "    min_max_scaler = MinMaxScaler()\n", "    X_tr_scaled = min_max_scaler.fit_transform(X_tr_flat).reshape(shape_tr)\n", "    X_val_scaled = min_max_scaler.transform(X_val_flat).reshape(shape_val)\n", "    if not return_scaler:\n", "        return X_tr_scaled, X_val_scaled\n", "    else:\n", "        return X_tr_scaled, X_val_scaled, min_max_scaler\n", "    \n", "def apply_scaling(X, scaler):\n", "    shape_X = X.shape\n", "    X_flat = np.ravel(X).reshape(-1, 1)\n", "    X_scaled = scaler.transform(X_flat).reshape(shape_X)\n", "    return X_scaled"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### II - A)\u00a0Load, resize and scale the data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It is advised to fix a relatively small image_size, for instance (32, 32, 3), to avoid suffering from slow calculation"]}, {"cell_type": "code", "execution_count": 19, "metadata": {"collapsed": false}, "outputs": [], "source": ["image_size = (32, 32, 3)\n", "\n", "classes = ['cat', 'dog']\n", "X_tr, X_val, Y_tr, Y_val = get_splitted_data_with_size(\n", "    image_size=image_size, sample_size=10000, test_ratio=0.25, classes=classes, seed=42\n", ")"]}, {"cell_type": "code", "execution_count": 20, "metadata": {"collapsed": false}, "outputs": [], "source": ["X_tr.shape, X_val.shape, Y_tr.shape, Y_val.shape"]}, {"cell_type": "code", "execution_count": 21, "metadata": {"collapsed": false}, "outputs": [], "source": ["i = np.random.choice(len(X_tr))\n", "show(X_tr[i])\n", "print('True label: {0}'.format(classes[Y_tr[i]]))"]}, {"cell_type": "code", "execution_count": 22, "metadata": {"collapsed": false}, "outputs": [], "source": ["X_tr_scaled, X_val_scaled, scaler = scale_data(X_tr, X_val, return_scaler=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### II - B)\u00a0Design and train a ConvNet from scratch"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercice**\n", "\n", "- Implement a Convolutional Network using the Keras Sequential API\n", "- Typically, you would use series of convolutional blocs: \n", "\n", "`\n", "model.add(Conv2D(output_filter, (kernel_height, kernel_width), padding, input_shape=(input_height, input_width, input_filter)))\n", "model.add(Activation(activation))\n", "model.add(BatchNormalization())\n", "`\n", "- Usually, the output_filter size grows accross the network\n", "- End the network with a `Flatten` layer followed by a final `Dense` layer\n", "- Be careful with the shapes accross the network, the activation functions used, the optimizer, and the loss function\n", "- Don't forget to use Dropout layers to avoid overfitting issues"]}, {"cell_type": "code", "execution_count": 23, "metadata": {"collapsed": false}, "outputs": [], "source": ["from keras.models import Sequential\n", "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n", "from keras.layers import Conv2D, MaxPooling2D\n", "from keras.optimizers import Adam\n", "\n", "def design_and_compile_model():\n", "    model = Sequential()\n", "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n", "    model.add(Activation('elu'))\n", "    model.add(BatchNormalization())\n", "    model.add(Conv2D(32, (3, 3), padding='same'))\n", "    model.add(Activation('elu'))\n", "    model.add(BatchNormalization())\n", "    model.add(MaxPooling2D(pool_size=(2,2)))\n", "    model.add(Dropout(0.2))\n", "\n", "    model.add(Conv2D(64, (3, 3), padding='same'))\n", "    model.add(Activation('elu'))\n", "    model.add(BatchNormalization())\n", "    model.add(Conv2D(64, (3, 3), padding='same'))\n", "    model.add(Activation('elu'))\n", "    model.add(BatchNormalization())\n", "    model.add(MaxPooling2D(pool_size=(2,2)))\n", "    model.add(Dropout(0.3))\n", "\n", "    model.add(Conv2D(128, (3, 3), padding='same'))\n", "    model.add(Activation('elu'))\n", "    model.add(BatchNormalization())\n", "    model.add(Conv2D(128, (3, 3), padding='same'))\n", "    model.add(Activation('elu'))\n", "    model.add(BatchNormalization())\n", "    model.add(MaxPooling2D(pool_size=(2,2)))\n", "    model.add(Dropout(0.4))\n", "\n", "    model.add(Flatten())\n", "    model.add(Dense(1, activation='sigmoid'))\n", "\n", "    # Compiling the model adds a loss function, optimiser and metrics to track during training\n", "    model.compile(\n", "        optimizer=Adam(),\n", "        loss='binary_crossentropy',\n", "        metrics=['accuracy']\n", "    )\n", "\n", "    return model"]}, {"cell_type": "code", "execution_count": 24, "metadata": {"collapsed": false}, "outputs": [], "source": ["design_and_compile_model().summary()"]}, {"cell_type": "code", "execution_count": 25, "metadata": {"collapsed": false}, "outputs": [], "source": ["batch_size = 128\n", "num_epochs = 20  # The number of epochs (full passes through the data) to train for\n", "\n", "model = design_and_compile_model()\n", "\n", "# The fit function allows you to fit the compiled model to some training data\n", "model_history = model.fit(\n", "    x=X_tr_scaled, \n", "    y=Y_tr, \n", "    batch_size=batch_size, \n", "    epochs=num_epochs,\n", "    verbose=1,\n", "    validation_data=(X_val_scaled, Y_val)\n", ")\n", "print('Training complete')"]}, {"cell_type": "code", "execution_count": 26, "metadata": {"collapsed": false}, "outputs": [], "source": ["plot_model_history(model_history)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### II - C)\u00a0Improve it using data augmentation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercice**\n", "\n", "- Try to improve the effectiveness of your network using some Data Augmentation\n", "- Basically, it consists in building a `ImageDataGenerator` fitted on your training dataset\n", "- Then you will be able to generate new consistent samples, and refit your model using the `fit_generator` Keras method"]}, {"cell_type": "code", "execution_count": 27, "metadata": {"collapsed": false}, "outputs": [], "source": ["from keras.preprocessing.image import ImageDataGenerator\n", "\n", "data_generator = ImageDataGenerator(\n", "    rotation_range=15,\n", "    width_shift_range=0.1,\n", "    height_shift_range=0.1,\n", "    horizontal_flip=True,\n", "    preprocessing_function=lambda x: apply_scaling(x, scaler)\n", "    )\n", "data_generator.fit(X_tr)"]}, {"cell_type": "code", "execution_count": 28, "metadata": {"collapsed": false}, "outputs": [], "source": ["model = design_and_compile_model()\n", "\n", "model_history = model.fit_generator(data_generator.flow(X_tr, Y_tr, batch_size=batch_size),\\\n", "                    steps_per_epoch=len(X_tr) // batch_size, epochs=20,\\\n", "                    verbose=1, validation_data=(apply_scaling(X_val, scaler), Y_val))"]}, {"cell_type": "code", "execution_count": 29, "metadata": {"collapsed": false}, "outputs": [], "source": ["plot_model_history(model_history)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# III - Transfer learning\n", "\n", "Objectives:\n", "- Classify an image by loading a pre-trained ResNet50 model using Keras Zoo\n", "    - No training required\n", "    - Decode an ImageNet prediction\n", "- Build a headless model and compute representations of images \n", "    - Retrain a model from representations of images for your own classification task: here cat vs dog dataset"]}, {"cell_type": "code", "execution_count": 30, "metadata": {"collapsed": false}, "outputs": [], "source": ["cat_sample_path = \"data/cat/cat_1.jpg\"\n", "dog_sample_path = \"data/dog/dog_1.jpg\"\n", "resnet_input_size = (224, 224)"]}, {"cell_type": "code", "execution_count": 31, "metadata": {"collapsed": false}, "outputs": [], "source": ["from keras.applications.resnet50 import ResNet50\n", "from keras.models import Model\n", "from skimage.transform import resize\n", "from keras.applications.resnet50 import preprocess_input\n", "from keras.applications.imagenet_utils import decode_predictions\n", "\n", "model_ResNet50 = ResNet50(include_top=True, weights='imagenet')\n", "model_ResNet50.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### III - A) Classify of an image using pre-trained weights\n", "\n", "**Exercise**\n", "- Open an image, preprocess it and build a batch of 1 image\n", "- Use the model to classify this image\n", "- Decode the predictions using `decode_predictions` from Keras\n", "\n", "Notes:\n", "- You may use `preprocess_input` for preprocessing the image. \n", "- Test your code with `\"data/cat/cat_1.jpg\"` \n", "- ResNet has been trained on (width, height) images of (224,224) and range of pixel intensities in `[0, 255]`.\n", "    - [skimage.transform.resize](http://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.resize) has a `preserve_range` keyword useful in that matter "]}, {"cell_type": "code", "execution_count": 32, "metadata": {"collapsed": false}, "outputs": [], "source": ["img = imread(cat_sample_path)\n", "img_resized = resize(img, resnet_input_size, mode='reflect', preserve_range=True)\n", "show(img_resized)\n", "\n", "img_batch = preprocess_input(np.expand_dims(img_resized, axis=0)) \n", "predictions = model_ResNet50.predict(img_batch)\n", "decoded_predictions = decode_predictions(predictions)\n", "\n", "for _, name, score in decoded_predictions[0]:\n", "    print(name, score)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### III - B) Build a headless model and compute representations of images"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Model has $177$ layers\n", "- See where we should stop to have the extracted feature and start building a new classficlation model from here"]}, {"cell_type": "code", "execution_count": 33, "metadata": {"collapsed": false}, "outputs": [], "source": ["print(len(model_ResNet50.layers))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Let's remove the last dense classification layer that is specific \n", "to the image net classes and use the previous layers (after flattening) as a feature extractors\n", "- Use ResNet input layer and last layer of extracted features to build a feature extractor model\n", "    - Use Keras functional API"]}, {"cell_type": "code", "execution_count": 34, "metadata": {"collapsed": false}, "outputs": [], "source": ["# Create a truncated Model using ResNet50.input and the before last layer\n", "output = model_ResNet50.layers[-2].output\n", "feat_extractor_model = Model(model_ResNet50.input, output)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When using this model we need to be careful to apply the same image processing as was used during the training, otherwise the marginal distribution of the input pixels might not be on the right scale:"]}, {"cell_type": "code", "execution_count": 35, "metadata": {"collapsed": false}, "outputs": [], "source": ["def preprocess_resnet(x, size):\n", "    x = resize(x, size, mode='reflect', preserve_range=True)\n", "    x = np.expand_dims(x, axis=0)\n", "    if x.ndim == 3:\n", "        x = np.expand_dims(x, axis=0)\n", "    return preprocess_input(x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This model extracts high level concepts from any image that has been preprocessed like the images ResNet trained on.\n", "The model transforms a preprocessed (224, 224) RGB image into a long vector of activations.\n", "Each activation refers to some concept statistically connected to a bunch of different classes."]}, {"cell_type": "code", "execution_count": 36, "metadata": {"collapsed": false}, "outputs": [], "source": ["cat_img = imread(cat_sample_path)\n", "cat_img_processed = preprocess_resnet(cat_img, resnet_input_size)\n", "cat_representation = feat_extractor_model.predict(cat_img_processed)\n", "print(\"Cat deep representation shape: (%d, %d)\" % cat_representation.shape)\n", "for activation in np.ravel(cat_representation):\n", "    print(activation)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Questions**\n", "- What is the number of $0$s in the cat representation vector ?\n", "\n", "<span style=\"color:green\">\n", "$182$ zeros found in the cat representation vector. That is $8.89\\%$ of all activations\n", "</span>\n", "\n", "- Can you find any negative values?\n", "\n", "<span style=\"color:green\">\n", "No negative values found at this stage\n", "</span>\n", "\n", "- Why are there $0$ values ? What does it mean ?\n", "\n", "<span style=\"color:green\">\n", "As mentioned above, each activation refers to some concept statistically connected to a bunch of different classes. A $0$ activation, or close to $0$, means that the high-level concept being looked for by the network in not found in the image\n", "</span>\n", "\n", "- Extract ResNet representations of other dogs and cats. Are the zeros at the same places in vector ?\n", "    - Explain why or give an intuition of it\n", "\n", "<span style=\"color:green\">\n", "Distributions of $0s$ activations are different for dogs and cats.  \n", "That is because to make the difference between these $2$ classes there are concepts that are  specific to dogs or cats. That is concepts for which $p(dog|concept)\\approx1$ and $p(cat|concept)\\approx0$ or $p(cat|concept)\\approx1$ and $p(dog|concept)\\approx0$\n", "Thus concepts likely to be there for a class and not the other, leading to $0$ activations at some indexes for a class and not the other\n", "</span>"]}, {"cell_type": "code", "execution_count": 37, "metadata": {"collapsed": false}, "outputs": [], "source": ["print(\"**Number of 0s in the cat representation vector**\")\n", "nb_zeros = sum(np.ravel(cat_representation) == 0)\n", "print(nb_zeros, \"zeros\")\n", "print(\"{:.2f}% of zeros\".format(nb_zeros/float(len(np.ravel(cat_representation)))*100))\n", "\n", "print(\"**Number of negative values in the cat representation vector**\")\n", "nb_negative_values = sum(np.ravel(cat_representation) < 0)\n", "print(nb_negative_values, \"negative values\")"]}, {"cell_type": "code", "execution_count": 38, "metadata": {"collapsed": false}, "outputs": [], "source": ["plt.hist(np.where(cat_representation == 0)[1])\n", "plt.title(\"cat zeros positions\")\n", "plt.show()\n", "\n", "dog_img = imread(dog_sample_path)\n", "dog_img_processed = preprocess_resnet(dog_img, resnet_input_size)\n", "dog_representation = feat_extractor_model.predict(dog_img_processed)\n", "\n", "plt.hist(np.where(dog_representation == 0)[1])\n", "plt.title(\"dog zeros positions\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### III - C) Retrain a model from computed representations of images\n", "\n", "For this session we are going to use the dataset of the dogs-vs-cats we already used in part $2$."]}, {"cell_type": "code", "execution_count": 39, "metadata": {"collapsed": false}, "outputs": [], "source": ["classes = ['cat', 'dog']\n", "X_tr, X_val, Y_tr, Y_val = get_splitted_data_with_size(\n", "    image_size=(224, 224, 3), sample_size=2000, test_ratio=0.25, classes=classes, seed=42\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Questions**\n", "- Inference time takes a long time only for $2000$ images\n", "    - Explain why it would be much faster using a GPU\n", "    \n", "<span style=\"color:green\">\n", "GPU allows to perform parallel computing.\n", "Most of the work during inference is about computing convolutions. Each convolution operation in a convolution layer is independant from the others. That means its outcome doesn't depend on the outcome of the others. All convolution operations could theoretically be performed at the same time to produce the feature map. Thus convolution layers are highly parallelizable and suit GPU usage\n", "</span>"]}, {"cell_type": "code", "execution_count": 40, "metadata": {"collapsed": false}, "outputs": [], "source": ["X_extracted_tr = feat_extractor_model.predict(preprocess_input(X_tr), verbose=1)\n", "X_extracted_val = feat_extractor_model.predict(preprocess_input(X_val), verbose=1)\n", "print('Done extracting resnet50 features..')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Define a classification model fed with the newly created X and Y\n", "    - Remember that X is now a set of ResNet representations of the images\n", "- Use either functional of sequential Keras apis\n", "- Display training and validation accuracies"]}, {"cell_type": "code", "execution_count": 41, "metadata": {"collapsed": false}, "outputs": [], "source": ["from keras.models import Sequential\n", "from keras.layers import Dense, Dropout\n", "\n", "from keras.optimizers import Adam\n", "\n", "n_features = X_extracted_tr.shape[1]\n", "transfer_model = Sequential()\n", "transfer_model.add(Dropout(0.6))\n", "transfer_model.add(Dense(1, input_dim=n_features, activation='sigmoid'))\n", "transfer_model.compile(optimizer=Adam(),\n", "                  loss='binary_crossentropy', metrics=['accuracy'])\n", "transfer_model.fit(X_extracted_tr, Y_tr,\n", "              validation_data=(X_extracted_val, Y_val),\n", "              verbose=1, epochs=30)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Questions**\n", "- How high in validation accuracy did you get ? \n", "- Compare to your previous classification model in part 2. Does it perform worse ? Better ? Why ?\n", "\n", "<span style=\"color:green\">\n", "Compared to the previous model, this one performs much better. That is due to the concepts extracted by resnet. They are all relevant with respect to some classes, there are no concept looked for in the images that is pure noise. \n", "The extraction itself from resnet of the main concepts for our predictions task is of much better quality and generalizable since it has been obtained from training on a huge dataset of millions of images that match better the actual distribution\n", "</span>\n", "\n", "- Did you observe overfitting during training ? Why ? If yes, what did you do to avoid it ?\n", "    \n", "<span style=\"color:green\">\n", "Regularization techniques like dropout must be used to prevent the model from relying on data patterns only found in training set\n", "</span>"]}, {"cell_type": "code", "execution_count": 42, "metadata": {"collapsed": false}, "outputs": [], "source": [""]}]}